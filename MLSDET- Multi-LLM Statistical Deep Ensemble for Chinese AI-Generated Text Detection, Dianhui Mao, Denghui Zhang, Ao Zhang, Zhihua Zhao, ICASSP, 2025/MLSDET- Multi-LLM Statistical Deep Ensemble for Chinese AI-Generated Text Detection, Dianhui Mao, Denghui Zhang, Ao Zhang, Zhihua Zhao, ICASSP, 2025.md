# MLSDET: Multi-LLM Statistical Deep Ensemble for Chinese AI-Generated Text Detection

Dianhui Mao<sup>∗</sup> , Denghui Zhang<sup>∗</sup> , Ao Zhang<sup>∗</sup> , Zhihua Zhao† ∗ School of Computer Science and Engineering Beijing Technology and Business University, China † School of Law China University of Political Science and Law Beijing, China

*Abstract*—With the rapid advancements in pre-trained large language models like ChatGPT, the surge of AI-generated text, particularly in Chinese, has presented significant challenges to existing detection systems due to its increasing realism and complexity. To address this, we introduce MLSDET: a groundbreaking Multi-LLM Statistical Deep Ensemble framework designed for high-precision detection of AIgenerated Chinese text. MLSDET uniquely integrates a Mixture of Experts (MoE) architecture with a novel cross-entropy metric, setting a new benchmark for robustness and generalization. By employing a diverse ensemble of large language models (LLMs), including Qwen, Wenzhong-GPT2, and LLaMA, our approach extracts intricate features such as log-rank, entropy, log-likelihood, and the newly introduced LLMs-crossEntropy, accurately capturing both model consensus and the statistical distribution differences between AI-generated and humanauthored text. Experimental results on the HC3-Chinese dataset show that MLSDET surpasses traditional zero-shot methods like CLTR by 15.94% in F1 score and competes effectively with existing methods, offering a scalable solution for real-world applications.

*Index Terms*—AI-generated text detection, Mixture of Experts, Statistical Features.

#### I. INTRODUCTION

With the rapid advancements in pre-trained large language models(LLMs) such as GPT-4 and Claude [1], significant progress has been made in the field of natural language processing.These developments have facilitated the application of AI-generated text in various domains, including question-answering systems [2], medical diagnostics [3], and educational services [4].Particularly, the emergence of ChatGPT has further enhanced the realism and complexity of AIgenerated text, posing unprecedented challenges to existing detection systems. These systems struggle with robustness and generalization when distinguishing between AI-generated and human-authored text.

Currently, AI-generated text detection methods [5] primarily fall into two categories: feature-based statistical methods [6] and deep learning methods using pre-trained language models [7]. Statistical feature-based approaches focus on differentiating text sources by differences in feature distributions. Gehrmann et al. [8] introduced statistical features for detecting LLM-generated text, which include the probability of each word, the absolute rank of these probabilities, and the entropy of the predicted word distribution at each position. They developed a tool that visualizes the probability distribution, rank distribution, and entropy distribution of text words, aiding humans in identifying AI-generated content. Mitchell et al. [9] improves discrimination using only the log probabilities computed by the correlation model and a random perturbation of the passage by another generalized pre-trained language model (e.g., T5).At the same time, Bao et al. [10] proposed a conditional probability curvature for detection based on detectgpt, which improves the detection speed. There are also some complex scores computed based on statistical features such as Binoculars score proposed by Hans et al. [11] and

Denghui Zhang is corresponding author. Email: zdh13141@gmail.com

GEC Score by Wu et al. [12] which use perplexity and grammatical error correction for text detection respectively.

The method of using a pre-trained model is to fine-tune the model with a large amount of data to make it a classifier. For example, Wang et al. [13] proposed AI-generated text detection and categorization based on BERT deep learning algorithm, which provides an effective solution for related fields.Zellers et al. [14] published a text generation model called GROVER and fine-tuned GROVER for detection. They found that increasing the size of the detection model improved accuracy. Solaiman et al. [15] developed a GPT-2 Detector by fine-tuning RoBERTa to detect text generated by GPT-2. Guo et al. [16] compiled the HC3 dataset, which includes both human-written texts and texts generated by ChatGPT. They utilized this dataset to fine-tune RoBERTa, resulting in the development of a tool known as the ChatGPT Detector.The first class of approaches assumes that existing language models can capture the statistical properties of text generation through feature correction, while the latter relies on training sophisticated deep neural networks for text classification.However, most existing methods focus on English text and struggle with Chinese text detection, while zero-shot approaches suffer from poor generalization, and deep learning methods require large labeled datasets, with token-level mechanisms being vulnerable to attacks [17], reducing robustness [18].

To address these challenges, we propose MLSDET (Multi-LLM Statistical Deep Ensemble), a novel framework specifically designed for detecting AI-generated Chinese text. Our solution includes integrating multiple LLMs, such as Qwen [19], Wenzhong-GPT2, and LLaMA, to extract complex features like log-rank, entropy, log-likelihood, and the newly introduced LLMs-crossEntropy. This enhances detection accuracy and robustness. We adopt a Mixture of Experts (MoE) architecture [20], where multiple expert models handle different data distributions, reducing overfitting and improving generalization. The introduction of the LLMs-crossEntropy metric captures subtle differences between AI-generated and humanauthored text, significantly enhancing detection performance. Our main contributions are listed as follows:

- We present a groundbreaking Multi-LLM Statistical Deep Ensemble framework that leverages the strengths of multiple LLMs to enhance detection accuracy.
- We introduce LLMs-crossEntropy, a new statistical characterization that significantly improves the detection performance by capturing subtle differences between AI-generated and humanwritten text.
- Our approach uses a Mixture of Experts (MoE) architecture [20] to integrate multiple base models, enhancing the diversity and representativeness of the combined features.
- Evaluating the generalization ability of MLSDET on multiple datasets shows that the generalization ability of MLSDET is

![](_page_1_Figure_0.jpeg)

Fig. 1: Overall technology roadmap.

also competitive with existing zero-sample methods.

## II. METHODOLOGY

In this section, we describe the methodology employed in our proposed MLSDET: Multi-LLM Statistical Deep Ensemble for Chinese AI-Generated Text Detection. Our approach involves two primary stages: Feature Extraction and Feature Classification. The overall technical route is illustrated in Fig.1.

#### *A. Feature Extraction Models and Multi-Feature*

The MoE model enhances the accuracy of downstream tasks by integrating multiple base models. In practical scenarios, datasets may contain data from multiple sources, such as GPT-2, GPT-3.5, LLAMA, and other models for text generation tasks. Due to differences in the data distribution from these sources, a single model is often proficient at handling some parts of the data but not others. The MoE model addresses this issue by employing multiple underlying large models (Experts) to handle data from different sources. Each Expert network specializes in a specific data region, generating better results for that region compared to other Expert networks. Inspired by this, we initially chose the weighted fusion of logits distributions from different LLM outputs as proposed by the PackLLM framework [21]. However, experiments showed that this approach significantly affected the original logits distribution for the same text. Therefore, we ultimately adopted a cooperative MoE model architecture without mandatory restrictions, utilizing only the concat operation.

In this step, we chose three pairs of large language models as "experts", each sharing the same lexer but differing in model size or command fine-tuning, as follows: 1)Qwen/Qwen2- 7B and Qwen/Qwen2-7B-Instruct. 2)IDEA-CCNL/Wenzhong-GPT2- 110M and IDEACCNL/ Wenzhong-GPT2-3.5B. 3)unsloth/llama-3- 8b and unsloth/llama-3-8b-Instruct. These models have been trained on Chinese corpus and can effectively process Chinese text. The MoE model for the ith input sample x<sup>i</sup> in the dataset proceeds as follows.Suppose there are k models, each of which generates a feature vector f<sup>i</sup> (where i=1,2,... ,k), and that each feature vector has n features.

$$F\_i = \begin{bmatrix} f\_{i1} \begin{pmatrix} x\_i \end{pmatrix}, f\_{i2} \begin{pmatrix} x\_i \end{pmatrix}, \dots, f\_{in} \begin{pmatrix} x\_i \end{pmatrix} \end{bmatrix}^T \tag{I}$$

$$F = [\;\_1F\_1, F\_2, \dots, F\_n\;]\tag{2}$$

where fij denotes the jth feature generated by model i.

Multi-feature detection is more robust than single-feature detection because other features can compensate when one experiences a distribution shift. The connections between features allow multifeature detection to outperform single-feature detection. To optimize the MoE approach, we align individual features and improve classifier efficiency by computing sentence- and paragraph-level features, creating a simple feature matrix for neural network processing. In contrast, word-level features often require pre-trained models, adding complexity. Since LLM-generated texts typically have higher log-likelihood, lower log-rank, and lower entropy, MLSDET uses log-rank, log-likelihood, entropy, and the newly proposed LLMscrossEntropy as input features. The specific computed features are as follows:

$$\log\max k = \frac{1}{n} \sum\_{i=1}^{n} \log o\_{\theta}(x\_i | x\_{$$

$$\log \text{ likelihood} = \frac{1}{n} \sum\_{i=1}^{n} \log p\_{\theta} \left( x\_i \mid x\_{$$

$$entropy = -\frac{1}{n} \sum\_{i=1}^{n} \sum\_{m} p\_{\theta}(x\_i = m | x\_{$$

where pθ(xi|x<i) denotes the probability of token xi, oθ(xi|x<i) denotes the absolute rank of probability of token x<sup>i</sup> for original text X = {x1, x2, . . . , xi}.

#### *B. Feature New feature distribution—LLMs-crossEntropy*

Cross-entropy is a statistical measure of the difference between two probability distributions. In our MLSDET method, we use two pretrained LLMs: one "original" model to calculate text perplexity and a "fine-tuned" model to predict the next word's perplexity. We first compute the text's perplexity under the original model and then use the fine-tuned model to calculate the predicted perplexity of the next word at each position, allowing for comparison. LLMs-crossEntropy is defined as follows:

$$LLMs -CrossEntropy(x) = -\frac{1}{N} \sum\_{i=1}^{N} M\_1(x\_i) \cdot \log(M\_2(x\_i)) \tag{6}$$

where M1(x)<sup>i</sup> denotes the prediction probability of the first model M<sup>1</sup> for word x<sup>i</sup> at position i, M2(x)<sup>i</sup> denotes the prediction probability of the first model M<sup>2</sup> for word x<sup>i</sup> at position i, and N denotes the input sequence x is the length of the input sequence.

Human-generated texts typically have higher perplexity due to their complexity, while machine-generated texts show lower perplexity as models are more familiar with their patterns. We tested our model across various domains, including law, finance, and medicine. Fig.2. shows that human texts have higher cross-entropy values, while machine texts tend to have lower values. This distinction in crossentropy distribution allows us to effectively differentiate between human and machine-generated texts, enabling efficient detection of AI-generated content.

# *C. Simple Feature Classifier*

In our research, we chose a simple 6-layer neural network classifier over more complex models like RoBERTa due to several key factors. The simplicity allows for faster computation and training, essential for quick-response applications. Additionally, our statistical features, structured as a 4x3 matrix, do not require the complexity of models designed for large-scale data like RoBERTa. A simpler model also reduces the risk of overfitting, especially with limited training data. Our classifier consists of 1 input layer, 4 hidden layers with ReLU activations, and 1 output layer using a sigmoid function for binary classification. This approach efficiently captures key patterns and yields strong results, outperforming many zero-shot methods.

# III. EXPERIMENTAL

#### *A. Dataset and Evaluation Metric*

In this study, we use a publicly available dataset from the STADEE paper [22].This dataset includes both ChatGPT-generated and humanauthored chinese texts and covers multiple domains. Specifically, our experimental dataset is divided into three parts: HC3-Chinese [16], ChatGPT-CNews, and CPM-CNews.

In this study, the measure of detector performance is the F1 score.A larger F1 score indicates better detector performance. To ensure the reliability and accuracy of the results, we used several performance metrics in the training and validation phases, including Precision and Recall. Precision reflects the accuracy of the detector in recognizing positive class samples, while Recall evaluates how well the detector covers actual positive class samples.

## *B. Experimental Settings*

For our proposed MLSDET framework, we did not consider the resource consumption in the feature extraction phase, but used three A800 graphics cards for inference computation. In the feature classification stage, we constructed a simple neural network using the torch framework with detailed parameter settings: Max Sequence Length: 1024; Batch Size: 32; Epoch: 120; Initial Learning Rate for Adam: 0.001

## *C. Result*

We split the dataset into training, validation, and test sets (7:1:2) and evaluated different neural models across various domains (Table I). MLSDET consistently outperformed models like RoBERTa, GLTR, and STADEE, showing its robustness. In-domain, MLSDET excelled by leveraging specific dataset features (e.g., HC3-finance, HC3-medicine), while RoBERTa's top F1 score of 98.37% came from fine-tuning. Out-of-domain, MLSDET outperformed RoBERTa in finance, medicine, and psychology by 14.61%, 11.07%, 21.76%, and 13.18%, respectively. The robustness of the MLSDET model is further verified in the In-the-wild experiments. Although the distribution of the test data is obviously different from that of the training data. the MLSDET has an efficient feature extraction and classification mechanism and is unlikely to be disturbed by data irregularities. Performance results for all datasets are in Table I.

TABLE I: Indicator F1(%) values for different data domains

| Data Dis         | Train                                    | RoBERTa | GLTR  | STADE | MLSDT  |
|------------------|------------------------------------------|---------|-------|-------|--------|
| tribution        | Data (Test                               |         |       |       | (ours) |
|                  | Data)                                    |         |       |       |        |
| In-domain        | HC3-all<br>(HC3-all)                     | 98.37   | 77.77 | 87.05 | 93.71  |
|                  | HC3-<br>finance<br>(ChatGPT<br>CNews)    | 73.56   | 73.38 | 82.24 | 88.17  |
| Out-of<br>domain | HC3-<br>medicine<br>(ChatGPT<br>CNews)   | 81.73   | 68.22 | 85.11 | 92.80  |
|                  | HC3-<br>psychology<br>(ChatGPT<br>CNews) | 70.87   | 52.77 | 80.41 | 92.63  |
|                  | HC3-law<br>(ChatGPT<br>CNews)            | 66.95   | 66.67 | 68.64 | 69.60  |
|                  | HC3-mix<br>(ChatGPT<br>CNews)            | 80.77   | 68.48 | 86.35 | 93.95  |
| In-the<br>wild   | CPM<br>CNews<br>(HC3-all)                | 76.46   | 78.05 | 82.62 | 86.57  |

To further test the performance of the MLSDET network, we conducted tests on all datasets , and recorded the evaluation metrics in detail, as shown in Table II:

TABLE II: MLSDET model metrics under all datasets

| Indi         | HC3-Chinese |             |     |      |                 |    |                                                                    |     | Chat<br>GPT | CPM<br>CN |
|--------------|-------------|-------------|-----|------|-----------------|----|--------------------------------------------------------------------|-----|-------------|-----------|
| cators       | baike       | fin<br>ance | law | medi | cine nlpcc open | qa | psych<br>ology                                                     | all | CN<br>ews   | ews       |
| Prec         |             |             |     |      |                 |    | ision 95.13 97.92 94.85 97.63 94.24 95.56 98.10 96.66 97.69 95.20  |     |             |           |
|              |             |             |     |      |                 |    | Recall 93.07 92.38 92.93 95.37 92.43 97.97 98.57 90.94 99.48 99.59 |     |             |           |
| F1-<br>score |             |             |     |      |                 |    | 95.24 95.07 93.88 96.49 93.33 96.75 98.34 93.71 98.57 97.34        |     |             |           |

## *D. Analysis*

Table III illustrates the effects of different configurations of hidden layers [23] and neurons on the performance of our model across various domains in the HC3-Chinese dataset. The configurations tested include combinations of 2, 3, and 4 hidden layers with either 8 or 16 neurons per layer.

The results show that increasing the number of hidden layers and neurons generally improves the model's performance. For instance, the configuration with 4 hidden layers and 16 neurons achieved the highest F1 scores in multiple domains: 95.07% in finance, 95.24% in law, 97.94% in medicine, 95.38% in nlpcc, 96.75% in openqa, and 98.34% in psychology. This indicates that deeper networks with more neurons can better capture the complexity and nuances of the

![](_page_3_Figure_0.jpeg)

![](_page_3_Figure_1.jpeg)

![](_page_3_Figure_2.jpeg)

Fig. 2: Differences in statistical distribution.

TABLE III: Effects of different hidden layers and neurons

| Layers,<br>Nums | HC3-Chinese |         |       |              |       |        |                |  |  |
|-----------------|-------------|---------|-------|--------------|-------|--------|----------------|--|--|
|                 | baike       | finance | law   | medi<br>cine | nlpcc | openqa | psych<br>ology |  |  |
| [2,8]           | 95.07       | 94.65   | 93.88 | 96.50        | 91.85 | 96.41  | 96.96          |  |  |
| [2,16]          | 96.24       | 94.24   | 92.71 | 97.71        | 92.70 | 96.31  | 98.12          |  |  |
| [3,8]           | 95.52       | 93.95   | 93.26 | 97.22        | 91.72 | 95.92  | 96.98          |  |  |
| [3,16]          | 95.61       | 95.67   | 93.33 | 97.94        | 93.21 | 96.32  | 98.12          |  |  |
| [4,8]           | 95.06       | 94.80   | 93.26 | 97.22        | 92.85 | 95.29  | 96.09          |  |  |
| [4,16]          | 95.24       | 95.07   | 93.88 | 96.49        | 93.33 | 96.75  | 98.34          |  |  |

TABLE IV: Validity of different combinations of features

|          | F1-score |                |                   |       |
|----------|----------|----------------|-------------------|-------|
| log rank | entropy  | log likelihood | LLMs-crossEntropy |       |
| YES      | NO       | NO             | NO                | 87.81 |
| NO       | YES      | NO             | NO                | 85.91 |
| NO       | NO       | YES            | NO                | 88.56 |
| YES      | YES      | NO             | NO                | 92.47 |
| YES      | NO       | YES            | NO                | 88.95 |
| NO       | YES      | YES            | NO                | 91.87 |
| YES      | YES      | YES            | NO                | 92.45 |
| YES      | YES      | YES            | YES               | 93.71 |

data, leading to improved performance. However, the configuration with 2 hidden layers and 16 neurons also performed exceptionally well in some domains, such as finance (96.24%) and law (94.24%), suggesting that for certain types of data, a simpler architecture might suffice.

In this paper we present a new statistical feature, so a discussion of feature combinations is necessary. Table IV examines the impact of different combinations of statistical features on the average F1 score for the HC3-Chinese dataset. The features considered include log rank, entropy, log likelihood, and LLMs-crossEntropy. Each row represents a different combination of these features, with a corresponding average F1 score.

The results indicate that adding more features improves performance, with all four features (log rank, entropy, log likelihood, and LLMs-crossEntropy) yielding the highest F1 score of 93.71%. Combinations with LLMs-crossEntropy consistently perform better, highlighting its importance in improving detection accuracy.

# IV. CONCLUSION

In this paper, we introduced MLSDET, a novel Multi-LLM Statistical Deep Ensemble framework for detecting AI-generated Chinese text. The MLSDET framework is able to utilize multiple Large Language Models (LLMs) in order to capture complex features such as log rank entropy, log-likelihood, and LLMs-crossEntropy, which are crucial for distinguishing human-authored text from AI-generated text. The experimental results show that the method significantly improves the F1 scores in different domains with f1 values around 95%, which highlights the robustness and generalization ability of the method. Furthermore, our experiments underscored the importance of feature diversity and the effectiveness of the MoE model in handling domain-specific language intricacies. The detailed analysis of feature combinations confirmed that incorporating multiple statistical features, especially LLMs-crossEntropy, substantially enhances detection accuracy.The simplicity and efficiency of the chosen 6-layer neural network classifier also played a pivotal role in ensuring quick computation and minimizing overfitting, making MLSDET a practical solution for real-world applications.

Future work will focus on extending the MoE architecture [24] to support multilingual text detection and exploring its scalability across different languages and text generation models. This extension is expected to further improve the detection framework's adaptability and efficacy in a broader range of applications.

#### ACKNOWLEDGMENT

The research reported in this paper is supported in part by the Beijing Municipal Natural Science Foundation under Grant 9232005

#### REFERENCES

- [1] M. Rahman, S. Khatoonabadi, A. Abdellatif, and E. Shihab, "Automatic detection of llm-generated code: A case study of claude 3 haiku," *arXiv preprint arXiv:2409.01382*, 2024.
- [2] J. Kasai, K. Sakaguchi, R. Le Bras, A. Asai, X. Yu, D. Radev, N. A. Smith, Y. Choi, K. Inui *et al.*, "Realtime qa: what's the answer right now?" *Advances in Neural Information Processing Systems*, vol. 36, 2024.
- [3] L. Caruccio, S. Cirillo, G. Polese, G. Solimando, S. Sundaramurthy, and G. Tortora, "Can chatgpt provide intelligent diagnoses? a comparative study between predictive models and chatgpt to define a new medical diagnostic bot," *Expert Systems with Applications*, vol. 235, p. 121186, 2024.
- [4] T. T. A. Ngo, T. T. Tran, G. K. An, and P. T. Nguyen, "Chatgpt for educational purposes: Investigating the impact of knowledge management factors on student satisfaction and continuous usage," *IEEE Transactions on Learning Technologies*, 2024.
- [5] A. Akram, "An empirical study of ai generated text detection tools," *arXiv preprint arXiv:2310.01423*, 2023.
- [6] R. Diandaru, L. Susanto, Z. Tang, A. Purwarianti, and D. Wijaya, "What linguistic features and languages are important in llm translation?" *arXiv preprint arXiv:2402.13917*, 2024.
- [7] J. Li, T. Tang, W. X. Zhao, J.-Y. Nie, and J.-R. Wen, "Pre-trained language models for text generation: A survey," *ACM Computing Surveys*, vol. 56, no. 9, pp. 1–39, 2024.
- [8] S. Gehrmann, H. Strobelt, and A. M. Rush, "Gltr: Statistical detection and visualization of generated text," *arXiv preprint arXiv:1906.04043*, 2019.
- [9] E. Mitchell, Y. Lee, A. Khazatsky, C. D. Manning, and C. Finn, "Detectgpt: Zero-shot machine-generated text detection using probability curvature," in *International Conference on Machine Learning*. PMLR, 2023, pp. 24 950–24 962.
- [10] G. Bao, Y. Zhao, Z. Teng, L. Yang, and Y. Zhang, "Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature," *arXiv preprint arXiv:2310.05130*, 2023.
- [11] A. Hans, A. Schwarzschild, V. Cherepanova, H. Kazemi, A. Saha, M. Goldblum, J. Geiping, and T. Goldstein, "Spotting llms with binoculars: Zero-shot detection of machine-generated text," *arXiv preprint arXiv:2401.12070*, 2024.
- [12] J. Wu, R. Zhan, D. F. Wong, S. Yang, X. Liu, L. S. Chao, and M. Zhang, "Who wrote this? the key to zero-shot llm-generated text detection is gecscore," *arXiv preprint arXiv:2405.04286*, 2024.
- [13] H. Wang, J. Li, and Z. Li, "Ai-generated text detection and classification based on bert deep learning algorithm," *arXiv preprint arXiv:2405.16422*, 2024.
- [14] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, and Y. Choi, "Defending against neural fake news," *Advances in neural information processing systems*, vol. 32, 2019.
- [15] I. Solaiman, M. Brundage, J. Clark, A. Askell, A. Herbert-Voss, J. Wu, A. Radford, G. Krueger, J. W. Kim, S. Kreps *et al.*, "Release strategies and the social impacts of language models," *arXiv preprint arXiv:1908.09203*, 2019.
- [16] B. Guo, X. Zhang, Z. Wang, M. Jiang, J. Nie, Y. Ding, J. Yue, and Y. Wu, "How close is chatgpt to human experts? comparison corpus, evaluation, and detection," *arXiv preprint arXiv:2301.07597*, 2023.
- [17] H. Stiff and F. Johansson, "Detecting computer-generated disinformation," *International Journal of Data Science and Analytics*, vol. 13, no. 4, pp. 363–383, 2022.
- [18] F. Huang, H. Kwak, and J. An, "Token-ensemble text generation: On attacking the automatic ai-generated text detection," *arXiv preprint arXiv:2402.11167*, 2024.
- [19] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang *et al.*, "Qwen technical report," *arXiv preprint arXiv:2309.16609*, 2023.
- [20] I. C. Gormley and S. Fruhwirth-Schnatter, "Mixture of experts models," ¨ in *Handbook of mixture analysis*. Chapman and Hall/CRC, 2019, pp. 271–307.
- [21] C. Mavromatis, P. Karypis, and G. Karypis, "Pack of llms: Model fusion at test-time via perplexity optimization," *arXiv preprint arXiv:2404.11531*, 2024.
- [22] Z. Chen and H. Liu, "Stadee: Statistics-based deep detection of machine generated text," in *International Conference on Intelligent Computing*. Springer, 2023, pp. 732–743.
- [23] M. Uzair and N. Jamil, "Effects of hidden layers on the efficiency of neural networks," in *2020 IEEE 23rd international multitopic conference (INMIC)*. IEEE, 2020, pp. 1–6.
- [24] J. Zheng, L. Zhang, Y. Wu, and C. Zhao, "Text region multiple information perception network for scene text detection," in *ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*. IEEE, 2024, pp. 7820–7824.