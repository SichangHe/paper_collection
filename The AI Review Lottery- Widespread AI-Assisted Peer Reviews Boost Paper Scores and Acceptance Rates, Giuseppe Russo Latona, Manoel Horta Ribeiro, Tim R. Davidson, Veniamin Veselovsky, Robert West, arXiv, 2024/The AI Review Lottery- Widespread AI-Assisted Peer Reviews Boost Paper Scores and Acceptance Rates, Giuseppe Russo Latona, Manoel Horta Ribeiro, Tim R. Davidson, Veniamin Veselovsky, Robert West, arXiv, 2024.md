# The Ai Review Lottery: Widespread Ai-Assisted Peer Reviews Boost Paper Scores And Acceptance Rates

Giuseppe Russo Latona,∗ Manoel Horta Ribeiro,† Tim R. Davidson,†
Veniamin Veselovsky,† Robert West∗
EPFL
Journals and conferences worry that peer reviews assisted by artificial intelligence (AI), in particular, large language models (LLMs), may negatively influence the validity and fairness of the peer-review system, a cornerstone of modern science. In this work, we address this concern with a quasiexperimental study of the prevalence and impact of AI-assisted peer reviews in the context of the 2024 International Conference on Learning Representations (ICLR), a large and prestigious machine-learning conference. Our contributions are threefold. Firstly, we obtain a lower bound for the prevalence of AI-assisted reviews at ICLR 2024 using the GPTZero LLM detector, estimating that at least 15.8% of reviews were written with AI assistance. Secondly, we estimate the impact of AI-assisted reviews on submission scores. Considering pairs of reviews with different scores assigned to the same paper, we find that in 53.4% of pairs the AI-assisted review scores higher than the human review (p = 0.002; relative difference in probability of scoring higher: +14.4% in favor of AI-assisted reviews). Thirdly, we assess the impact of receiving an AI-assisted peer review on submission acceptance. In a matched study, submissions near the acceptance threshold that received an AI-assisted peer review were 4.9 percentage points (p = 0.024) more likely to be accepted than submissions that did not. Overall, we show that AI-assisted reviews are consequential to the peer-review process and offer a discussion on future implications of current trends.

Peer review is central to the modern scientific process and the current epistemic and social status of science [12, 43, 50]. The system is used by journals and conferences to ensure the validity and significance of research findings [47, 48] and by funding institutions to allocate grants [32, 33, 49]. Society treats peer-reviewed research differently from nonpeer-reviewed research: it is prioritized by policy advisory groups like the Intergovernmental Panel on Climate Change [1, 7], holds a special status in the courtroom [2], and is often a hard requirement for researchers to progress in their academic career [41]. At the same time, the peer-review system is under mounting pressure [1, 45]: the number of researchers [13]
and the number of papers published per researcher [8]
have been growing rapidly, causing the volume of papers that require reviews to outpace the number of qualified reviewers [4, 27]. This can lead to so-called
"reviewer fatigue" [10] and make recruiting qualified reviewers challenging for some journals [16].

Adding to these problems, the recent emergence and popularization of large language models (LLMs)
have raised further concerns within the academic community. One key concern is that overburdened scientists [6, 17] may resort to using increasingly capable LLMs for peer review [19]. While reviews written with LLMs generally resemble "real" reviews, reduced reviewer input could lead to the scientific merit of

∗ Correspondence: robert.west@epfl.ch, giuseppe.russo@epfl.ch
† Equal contributions, random order.
submissions being incorrectly judged [14]. Scientists' reliance on LLMs to write peer reviews (hereinafter called AI-assisted reviews) could thus decrease the peer-review system's reliability and harm its social and epistemic functions [19]. In response, multiple journals and conferences have already felt obliged to regulate or prohibit the use of LLMs in the peerreviewing process [9, 20, 30, 42].

Despite the increased concerns around AI-assisted reviews, the central question remains unanswered:
How do AI-assisted reviews influence peer-review outcomes? Unfortunately, disentangling the causal effect of AI-assisted reviews is challenging for at least the following reasons: Firstly, distinguishing between texts generated by LLMs and texts generated by humans is difficult for machine-learning models and humans alike [21, 40]. Secondly, even if one can accurately detect their use, it is unclear what role LLMs play in writing AI-assisted reviews: Do they serve as enhanced spell-checkers? Or rather to formulate the core arguments of a review? In the former case, using LLMs may improve the writing quality of reviewers with English as a second language [3, 26], while in the latter, it may threaten the essence of the peerreview process itself [14, 19]. Finally, even in the pessimistic case where LLMs are used to formulate core arguments, it is unclear whether their impact on paper acceptance decisions is substantial. Past work suggests that random chance plays a substantial role in the acceptance of papers into conferences and journals [15, 31], which could render the noise added by AI-assisted reviews inconsequential.

![1_image_0.png](1_image_0.png)

FIG. 1. Overview of our quasi-experimental approach to estimate the prevalence and causal effects of AI-assisted reviews. Study 1: Estimating the prevalence of AI-assisted reviews by classifying each review as human or AI-assisted using an out-of-the-box LLM-detection model. Study 2: Estimating the effect of AI-assisted reviews on paper scores by comparing the scores of human and AI-assisted reviews assigned to the same paper (thus controlling for properties of the reviewed paper). Study 3: Estimating the effect of AI-assisted reviews on acceptance rate: we match papers into pairs
⟨*i, j*⟩ such that (1) i and j are similar in content, (2) i and j received the same number m of reviews, (3) i received exactly one AI-assisted review, and j none, (4) i's m − 1 human scores are identical to m − 1 of j's m human scores. We then estimate the causal effect of AI-assisted reviews on paper acceptance as the difference in acceptance rates between i and j in matched pairs.
We address these challenges in the context of a highprofile machine-learning conference, the International Conference on Learning Representations (ICLR). This conference is unique in adopting an open peer-review model, in which all reviews are visible and easily retrievable, regardless of whether papers have been accepted or not. The ICLR reviewing process happens in roughly five steps: (1) reviewers "bid" on papers they would like to review based on their expertise and interests, after which an assignment algorithm is run, taking into account reviewers' bids, expertise, potential conflicts of interests, and "reviewer diversity" per paper; (2) papers typically receive three or more reviews that rate the contribution on a scale from 1 to 10 (where 5 and 6 represent borderline scores around the acceptance threshold) and provide a confidence level from 1 to 5; (3) authors engage with the reviewers in an asynchronous discussion period; (4) reviews are collated and weighted by so-called "area chairs" into a meta-review recommending acceptance or rejection; (5) "senior area chairs" and "program chairs" help calibrate the area chairs' recommendations and collectively determine the final decision.

To estimate the causal impact of AI-assisted peer reviews on submission scores and acceptance rates, we consider all ICLR submissions (n = 7,404) and reviews (n = 28,028) of 2024, extracted through the application programming interface (API) of OpenReview, the platform where ICLR's reviewing process is hosted. We conduct three studies (overview in Figure 1). In Study 1, we use the commercially available GPTZero LLM detector [46] to identify reviews that were likely written with the assistance of an LLM (see Materials and Methods; our analysis indicates that the model has a low false-positive rate for the data at hand) and quantify the prevalence of AI-assisted reviews. We then conduct two quasiexperimental studies to identify the causal impact of receiving an AI-assisted review. In Study 2, we estimate the effect of AI-assisted reviews on scores, contrasting AI-assisted reviews with human reviews assigned to the same paper. In Study 3, we estimate the effect of AI-assisted reviews on acceptance rates, by comparing outcomes within pairs of papers similar in topic, reviews, and scores, but where exactly one of the papers received an AI-assisted review.

We make three main findings. In Study 1, we find strong evidence that AI-assisted reviews were highly prevalent at ICLR 2024, with at least 15.8% of reviews written with LLM assistance according to the GPTZero LLM detector. In Study 2, we find that AI-assisted reviews typically increased average submission scores: considering pairs of reviews with different scores assigned to the same paper, AI-assisted scores were higher than human scores in 53.4% of pairs
(p = 0.002; relative difference in probability of scoring higher: +14.4% in favor of AI-assisted reviews). In Study 3, we find that AI-assisted reviews boost papers' acceptance rate, especially for submissions with borderline scores: receiving an AI-assisted review increased the acceptance rate by 3.1 percentage points
(p = 0.024) on average, and by as much as 4.9 percentage points (p = 0.024) for borderline submissions, corresponding to a 31.1% relative increase in odds of acceptance (p = 0.031). In summary, our findings suggest that AI-assisted reviews were widespread at ICLR 2024 and impacted scores and acceptance rates, corroborating concerns that AI use can reduce the utility of and trust in peer-reviewing. We open-source our code and annotated data allowing other scholars to conveniently replicate and extend our findings: https:
//github.com/epfl-dlab/AIReviewLottery.

## Results

Study 1: Prevalence of AI-assisted reviews. We classify each review as AI-assisted or human using GPTZero, a commercial LLM detector [46]. The final classification was done on 26 April 2024 to ensure that a single API checkpoint (dated 4 April 2024) can be used to reproduce our results. We label reviews as AIassisted if the overall probability of the review being human-generated is below 0.5 (we found our analysis robust to this threshold, see Appendix B). We repeat this analysis for each year between 2018 and 2024.

Since LLMs only became widely available after ChatGPT debuted in November 2022 [35] (i.e., after the reviewing cycle for ICLR 2023), we use reviews from 2018 to 2023 to estimate GPTZero's false-positive rate
(FPR) and correct the 2024 estimate by removing the average FPR of previous years from the 2024 prevalence estimate (see Materials and Methods). We do not estimate or correct for GPTZero's false-negative rate, meaning that the results provided here are a lower bound of actual LLM prevalence.

With this method, we estimate that 15.8% of ICLR
reviews in 2024 were crafted with the assistance of an LLM, or 4,428 of the 28,028 reviews submitted that year; 49.4% of all submissions received at least one review classified as AI-assisted by GPTZero. Figure 2

![2_image_0.png](2_image_0.png)

FIG. 2. Estimated prevalence of AI-assisted ICLR
reviews 2018–2024 (Study 1). Using the LLM detector's predictions in pre-ChatGPT years (2018–2023) to calculate its false-positive rate, we estimate that 15.8% of reviews in 2024 were AI-assisted (prevalence minus projection in the plot). We estimated 95% confidence intervals using bootstrap resampling for the prevalence (gray line), but they are too small to be visible. For the projection (orange line; the average prevalence between 2018 and 2022), we plot an error bar corresponding to the prevalence ranges observed in previous years.
illustrates the fraction of reviews classified as AIassisted across the years. These results are consistent with those of concurrent analyses [23, 24] that used a different methodology. Study 2: Effect of AI-assisted reviews on paper scores. Having determined that AI-assisted reviews were common in ICLR 2024, we next estimate their causal effect on paper scores. As illustrated in Figure 1 (Study 2), we focus on submissions with at least one AI-assisted review (according to GPTZero, per Study 1) and at least two human reviews (n = 3,357 submissions). For each such submission with at least three reviews, we let the score of one of the human reviews be the reference score (rref) and estimate the difference between the score of an AI-assisted review
(rai) and another human review (rh) Note that each paper has multiple possible combinations of human, AI-assisted, and reference reviews. We consider all combinations (n = 9,666) and ensure the validity of our results by weighting analyses such that each paper contributes equally to the results and using robust standard errors clustered at the paper level.

Overall, our setup compares pairs of reviews assigned to the same paper, which controls for paper-level confounders, e.g., that specific topics might attract higher- or lower-quality reviews.

We find that, on average, AI-assisted reviews were 0.14 points (95% CI [0.08, 0.19]) higher than human reviews. In Figure 3, we plot the average difference between human and AI-assisted scores of the same

![3_image_0.png](3_image_0.png)

FIG. 3. Mean submission-level differences between AI-assisted and human reviews as a function of human reference scores (Study 2). We consider submissions with at least three reviews, where at least one is AI-assisted and at least two are human. Then, we select a human review as the reference review (with score rref) and estimate the average difference between AI-assisted and human reviews (rAI − rh).

In the plot, we show the average difference (y-axis) for each possible score of the reference review (x-axis). AI-assisted reviews consistently give higher scores than human reviews.
submission (y-axis) as a function of human reference scores (x-axis). We note that AI-assisted reviews consistently assign higher scores than human reviews.

For instance, when the score of the human reference review equals 1 (lowest possible score), AI-assisted reviews tend to score submissions 0.45 (95% CI [0.13, 0.78]) points higher than human reviews do. Given that scores are ordinal rather than scalar, we complement the previous result with an ordinal regression analysis [25]. Using a proportional odds model [28],
we regress the score of reviews as a function of an indicator variable coding whether the review was AIassisted. We estimate that if we select two reviews such that they have different scores and such that exactly one is AI-assisted, the probability that the review with the higher score is the AI-assisted one equals 53.4% (relative probability difference: +14.4%; p = 0.002).

Study 3: Effect of AI-assisted reviews on acceptance rate. Although we have shown that AIassisted reviews boost submissions' average scores, this does not automatically translate into better chances of acceptance. For instance, it could be that predominantly submissions with very low or high average scores receive AI-assisted reviews, which would likely render the boosted average scores inconsequential for acceptance. Crucially, simply comparing the average acceptance rate of submissions that received AI-assisted reviews with those that did not is insufficient to estimate the causal effect of AI assistance.

For example, submission- and review-related features

$$\left({2}\right)$$

might confound treatment (receiving an AI-assisted
review) and outcome (being accepted). To control
for possible confounders, we thus isolate the effect of
AI-assisted reviews on acceptance in a matched study,
where we compare submissions that are similar or
identical in a wide range of aspects, but one received
an AI-assisted review and the other did not.
Matching is done in two steps. Firstly, we select
submissions that received exactly one AI-assisted review. For each such submission i we then curate a
set of possible matches consisting of submissions that
received the same number of reviews as i, all of which are classified as human, and all but one of which have
scores identical to the scores of i's human reviews.
For example, if a submission received two human
reviews with scores of 6 and 5 and an AI-assisted
review with a score of 8, a possible candidate might
have received three human reviews with scores of 6, 5, and 7 (see Figure 1, Study 3). Secondly, we rank these possible matches by measuring their semantic
similarity with i (embedding abstracts and review
content with Sentence-BERT [37]; see Materials and
Methods) and choose the best-matching candidate as
i's match j.
Considering all submissions in this matched sample
(n = 5,132), we estimate the effect of receiving an
AI-assisted review on the acceptance of a submission
k by fitting a logistic regression
$$\operatorname{logit}(y_{k})=\alpha+\beta\cdot L_{k}+\gamma\cdot\mathbf{X}_{k},\qquad\quad(1)$$
where Lk ∈ {0, 1} indicates whether k had an AIassisted review and Xk is a vector with the same
control variables used for matching. We also estimate the equivalent linear regression
In both regressions, β captures the difference in acceptance (in log odds for the logistic regression and
percentage points for the linear regression) between
submissions receiving vs. not receiving an AI-assisted review, ceteris paribus.
The fitted coefficients reveal that submissions that
received AI-assisted reviews had 13.8% higher odds
of being accepted compared to those that did not
(p = 0.024), or alternatively, had 3.1 percentage points
higher chances of being accepted (p = 0.024). This average effect, however, downplays the potential impact
of AI-assisted reviews, as submissions whose other reviews have very low or very high scores are less
likely to have their acceptance decision flipped by the
AI-assisted review.
We thus study the heterogeneity of the effect by
stratifying the matched sample (see Figure 4(A)).
Each matched pair of submissions with m reviews
shares m − 1 human review scores. We take the average value among these m − 1 review scores and place
$$y_{k}=\alpha+\beta\cdot L_{k}+\gamma\cdot\mathbf{X}_{k}.$$
each pair of matched submissions in one of seven bins,
[1, 2), [2, 3)*, . . . ,* [7, 8) (see y-axis of Figure 4(A)). For example, the matched submissions in Figure 1 (Study 3) would be placed in the [5, 6) bin since the average human score among the review scores they share is 5.5
(see Materials and Methods for details on the matching). We then repeat the regression for the matched submissions in each bin, finding that the effect is especially pronounced for borderline submissions, i.e.,
those in the [5, 6) bin. Note that per the reviewer instructions, scores 5 and 6 correspond to slightly below and slightly above acceptance. More precisely, among these borderline submissions, the acceptance rate of submissions with at least one AI-assisted review is 4.9 percentage points (p = 0.024; linear regression)
higher than that of submissions with only human reviews, corresponding to a 31.1% (p = 0.031; logistic regression) relative increase in the odds of being accepted. This is substantial, as the [5, 6) bin contains 20.7% of all submissions in the matched sample. Further, with a 73.6% acceptance rate in the [5, 6) bin
(for submissions without AI-assisted reviews in the matched sample), the 4.9 percentage points of absolute increase corresponds to a 6.5% relative increase in the chance of being accepted. (For a sensitivity analysis, see Appendix D.)

## Discussion

We studied whether AI-assisted reviews affected the peer-review process of the ICLR 2024 machine learning conference by (1) estimating their prevalence;
(2) comparing the scores of AI-assisted vs. human reviews for the same submission; and (3) comparing the acceptance rate of submissions that received AIassisted reviews to that of similar submissions that did not. Our results suggest that (1) LLMs were widely used in the peer-reviewing process of ICLR 2024; (2)
receiving an AI-assisted review inflated submission scores; and (3) AI-assisted reviews boosted acceptance rates, especially for borderline submissions.

These findings have important ramifications. They raise the concern that, in an already overloaded peerreview system, AI-assisted reviews can reduce trust in the process—and science as a whole—by introducing a new point of failure. This may weaken the epistemic status of a system already deemed "unscientific" by some [38]. Scientific works that express views relating to societal norms and values may be at even greater risk due to the known biases present in LLMs [18], which may reward research that aligns with the implicit values of the LLMs used by reviewers.

As the landscape of LLMs and LLM usage changes, our findings highlight the urgent need to establish baselines and ongoing measurements, accompanying

![4_image_0.png](4_image_0.png)

FIG. 4. Effect of receiving an AI-assisted review on submission acceptance (Study 3). (A) We stratify the effect of AI-assisted reviews on submission acceptance by matched submissions' average score across the human reviews they received (y-axis). We find a particularly pronounced effect for "borderline" submissions (average score between 5 and 6), with an increased acceptance rate of 4.9 percentage points percentage points (p = 0.024). Overall, we find that submissions that received an AI-assisted review are 3.1 percentage points percentage points more likely to be accepted
(p = 0.024). (B) Acceptance rate and (C) prevalence of submissions for submissions receiving only human reviews across human-score bins. E.g., 20.7% of submissions were in the [5, 6)
bin, and submissions receiving only human reviews in this bin were accepted 73.6% of the times.

## The Co-Evolution Of Llms And Peer-Reviewing.

Earlier, we identified three key trends likely pushing reviewers to resort to AI assistance: the increasing submission volume, the dwindling reviewers-tosubmissions ratio, and the improving quality of LLM
tools. The pressure induced by these trends will be amplified for more senior peer-review roles, such as (senior) area chairs, who might thus be similarly tempted to lean on AI assistance for writing metareviews and making editorial decisions. Yet, unlike the first peer-review layer, which consists of multiple reviews, meta-reviews present a single point of failure. Furthermore, they are often shorter and contain less forced structure, simplifying the usage and complicating the detection of AI assistance. Given the revealed prevalence of AI-assisted reviews, a more subtle consequence of AI-assisted editorial decisions could emerge: there has been increasing evidence that LLMs exhibit a preference toward their own outputs [5, 36, 52], which could influence decisions to favor AI-assisted reviews disproportionally. This preference towards LLM-generated work could even lead authors to "game" the system by writing text that aligns with popular model preferences, e.g., tailoring the submission's content to receive better (automated)
reviews or injecting special (hidden) instructions into manuscripts.

Despite this study's contribution to quantifying the causal effects of AI-assisted reviews, it is limited in several ways. Firstly, the methodology proposed here requires both accepted and rejected submissions, alongside the scores of the individual reviews. ICLR is one of the few scientific venues that provide conditions for external researchers to carry out our methodology.

Nevertheless, we note that many of the top AI conferences' reviewer pools are very similar to ICLR's. For example, the reviewer pools of ICLR 2024 and the 2023 Conference on Neural Information Processing Systems (NeurIPS) had 53.4% overlap (NeurIPS is another prestigious AI conference; see Appendix F
for overlap comparisons to other conferences). We thus conjecture that these strongly related conferences might be subject to similar dynamics as ICLR
2024.

Secondly, we estimate the causal effect of AIassisted reviews on submission-related outcomes, which differs from estimating the effect of AI-assisted reviews on the quality of the reviews. It could be that the quality of reviews remains essentially unchanged, and what changes is their delivery: shorter reviews that previously did not meaningfully engage with a submission may have been substituted by more verbose, eloquently written LLM reviews. Assessing the quality of AI-assisted reviews constitutes an important avenue for future research. Thirdly, although we have established that AIassisted reviews were distinct from human reviews assigned to the same or similar papers, our results provide limited insights into how LLMs and other AI tools are used in peer-reviewing. It could be that two modes of usage are prevalent, e.g., improving the text of a previously self-written review vs. feeding the paper that is to be reviewed to an LLM and copying the LLM-written review verbatim, and that only the latter impacts peer-reviewing outcomes. Understanding nuances around these usage modes is of key importance for future decision-making. Yet, it may require a different register of research methods from those deployed here, based on directly engaging with reviewers via interviews, focus groups, and surveys.

It is important to emphasize that using LLMs in reviewing may not be categorically wrong. There are various areas where LLMs may improve the current peer-review process. For example, LLMs could offer reviewers feedback to improve writing clarity, detect flawed critiques to reduce misunderstanding, or help contextualize the importance of a submission's findings. They might even provide new ways to tackle problems poorly addressed by the current peer-review process, e.g., by conducting automated tests to alleviate the "reproducibility crisis" [22]. Nonetheless, moving forward, it seems imperative to rethink reporting requirements at every level to ensure the integrity, validity, and transparency of peer review. Defining requirements and formulating unified guidelines for the integration of LLMs in peer review will require community participation. Similar to a workshop on peer review held at the 2012 International Conference on Machine Learning (ICML) [44] or the quadrennial International Congress on Peer Review and Scientific Publication [34], it may again be time to organize a workshop discussing the future of peer review.

## Materials And Methods

Dataset. We analyze submission and review data from ICLR, a leading venue in machine learning that publicly releases all peer reviews and decisions after concluding the peer-review process. Data extraction was done with the official OpenReview API and consists of all conference submissions and reviews from 2018 to 2024. The final dataset comprises 23,959 main conference submissions and 86,690 reviews, each with overall ratings, textual explanations, and confidence scores. In addition to a written review, each review at ICLR contains an overall ordinal rating r ∈ {1, 3, 5, 6, 8, 10}. Scores 1 and 3 indicate lowquality submissions that should typically be rejected; scores 5 and 6 indicate borderline submissions; and scores 8 and 10 indicate high-quality submissions that should typically be accepted. Each score is accompanied by a short textual description, e.g., the description for score 5 reads "Marginally below the acceptance threshold." We depict descriptions for all scores in the Appendix A.

Prevalence of AI-assisted reviews. We use GPTZero, a commercially available LLM detector.

For each review, GPTZero calculates the probability of it being entirely human-generated, entirely AIgenerated, or "mixed." We label reviews as AI-assisted if the probability of the review being human-generated is below 0.5 (for analyses with varying threshold values, see Appendix B to ensure robustness). Accordingly, reviews with a human-generated score below 0.5 indicate a cumulative probability of being entirely AI-written or "mixed" greater than 0.5. Using reviews written before ChatGPT's popularization, we estimate that the model's false positive ratio (FPR)
is 1.7% for the data studied. Under the assumption that the FPR remains the same for 2024 reviews, we estimate a lower bound of the overall prevalence of AI assistance in 2024 by subtracting the FPR from the fraction of reviews classified by GPTZero to be AIassisted. This is similar to well-established methods to prevent misclassification bias [29]. It is important to note that GPTZero was not trained on pre-ChatGPT
submission reviews as human text (Alex Cui, CTO of GPTZero, personal communication, 22 April 2024),
which could bias the FPR estimate and thus our results.

Proportional odds model. Since ratings are ordinal, using a linear regression model to estimate score differences can lead to systematic errors as the response categories of an ordinal variable may not be equidistant [11, 25]. A solution to this issue is using "cumulative" ordinal models that assume that the observed ordinal variable comes from categorizing a latent, non-observable, continuous variable [11]. Here, we use one such model, a "proportional odds model" of the form

$$\log{\frac{\Pr(r_{k}\leq a)}{\Pr(r_{k}>a)}}=\alpha_{a}-\gamma\cdot L_{k},\qquad\qquad(3)$$

where a ∈ {1, 3, 5, 6, 8} represents the possible values the review k might take; βa and βAa are level-specific coefficients; R is the set containing all possible review scores; Lk is an indicator variable that equals 1 when the review is AI-assisted; Under this specification, e γ corresponds to the odds ratio

$$e^{\gamma}={\frac{\frac{\Pr(r{>}a|L)}{\Pr(r{\leq}a|L)}}{\frac{\Pr(r{>}a|\neg L)}{\Pr(r{\leq}a|\neg L)}}},\quad{\mathrm{for~all~}}a\in\{1,3,5,6,8\}.\quad(4)$$

While the regression estimates the odds ratios, we also present results in an equivalent but easier-to-interpret fashion. Suppose we pick two reviews with different scores assigned to the same submission, exactly one of which is AI-assisted. The odds ratio e γestimated by the model corresponds to the odds κ that the AIassisted review scores higher in the paired scenario delineated above. Note that κ = x/(1 − x), where x is the probability of the AI-assisted review having a higher score. This x is the number we report in the paper, e.g., 53.4% in Figure 1. See Appendix C for the full regression table.

Matching. We estimate the effect of AI-assisted reviews on submissions' acceptance employing a twostep matching procedure. For each submission that received exactly one AI-assisted review, we first construct a set of submissions that have the same number of reviews, no AI-assisted reviews, and matching human-review scores for all but the AI-assisted review.

Then, we use content-based matching to determine the best match among the candidate set. For each submission and set of submission candidates, this involves: (1) computing the embeddings for the abstract and the content of reviews associated with the submission using Sentence-BERT [37], (2) concatenating these embeddings into a single vector, and (3)
measuring the cosine similarity between the vector of the submission that received the AI-assisted review and each candidate submission.

We select the candidate with the highest cosine similarity. We only keep matches with a cosine similarity above a threshold of 0.1 (the effects of changing this threshold are discussed in the Appendix D). This process matches 98.5% of potential pairs, or 2,580 out of 2,619 submissions. Additionally, we conducted several checks to ensure the quality of our matches.

We first compared the content similarity between the matched sample and a sample of "randomly" matched submissions. This unmatched sample consists of the matched sample's submissions that received an AIassisted review and a randomly sampled submission that did not receive an AI-assisted review. We compare the overlap of the keywords (as submitted by the authors) used in the matched sample with the overlap of the unmatched sample. We observe a significantly higher overlap for the matched sample. We further compared the similarity between the Sentence-BERT
embeddings of the abstracts within the matched sample. These similarity scores were statistically compared to those of the unmatched sample to confirm a higher and statistically significant difference. We provide more details on these analyses in Appendix D
and a sample of matched abstracts in Appendix G.

GPTZero robustness checks. Given that our analysis is based on GPTZero predictions of LLMgenerated text, we further assess the reliability of this classifier. We construct a vocabulary consisting of all words used in ICLR 2024 reviews. Then, for each word in this vocabulary, we compute the ratio between the number of LLM-generated reviews containing the word and the number of human reviews containing the word. Among the words with the highest ratio, we find words identified by other work as indicative of LLM
use [23, 24], e.g., "delve", "bolster", and "illustrates";
see Appendix E for more details. Beyond this wordlevel ratio check, we vary the adopted threshold of 0.5 used to label a review as AI-assisted. Our analysis detailed in Appendix B shows that our results remain robust across different values of this threshold.

Acknowledgments. The authors would like to thank Alex Cui and the GPTZero team for their generous support of our work. Robert West's lab is partly supported by grants from the Swiss National Science Foundation (200021_185043, TMSGI2_211379), Swiss Data Science Center (P22_08), H2020 (952215),
Google, and Microsoft.

[1] Bruce Alberts, Brooks Hanson, and Katrina L Kelner. Reviewing peer review, 2008. [2] Thomas D Albright. A scientist's take on scientific evidence in the courtroom. *Proceedings of the National Academy* of Sciences, 120(41):e2301839120, 2023.

[3] Tatsuya Amano, Valeria Ramírez-Castañeda, Violeta Berdejo-Espinola, Israel Borokini, Shawan Chowdhury, Marina Golivets, Juan David González-Trujillo, Flavia Montaño-Centellas, Kumar Paudel, Rachel Louise White, et al. The manifold costs of being a non-native english speaker in science. *PLoS Biology*, 21(7):e3002184, 2023.

[4] Martijn Arns. Open access is tiring out peer reviewers. *Nature*, 515(7528):467–467, 2014.

[5] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. Benchmarking foundation models with language-model-as-an-examiner. *NeurIPS*, 36, 2024.

[6] Adrian Barnett, Inger Mewburn, and Sara Schroter. Working 9 to 5, not the way to make an academic living:
observational analysis of manuscript and peer review submissions over time. bmj, 367, 2019.

[7] Silke Beck and Martin Mahony. The ipcc and the new map of science and politics. Wiley Interdisciplinary Reviews:
Climate Change, 9(6):e547, 2018.

[8] Lutz Bornmann and Rüdiger Mutz. Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references. *Journal of the association for information science and technology*,
66(11):2215–2222, 2015.

[9] Jordan Boyd-Graber, Naoaki Okazaki, and Anna Rogers. Acl 2023 policy on ai writing assistance. https:
//2023.aclweb.org/blog/ACL-2023-policy, 2023.

[10] Marijke Breuning, Jeremy Backstrom, Jeremy Brannon, Benjamin Isaak Gross, and Michael Widmeier. Reviewer fatigue? why scholars decline to review their peers' work. *PS: Political Science & Politics*, 48(4):595–600, 2015.

[11] Paul-Christian Bürkner and Matti Vuorre. Ordinal regression models in psychology: A tutorial. *Advances in* Methods and Practices in Psychological Science, 2(1):77–101, 2019.

[12] Daryl E Chubin and Edward J Hackett. *Peerless science: Peer review and US science policy*. State University of New York Press, 1990.

[13] Clarivate. Global State of peer review report. https://clarivate.com/lp/global-state-of-peer-review-report/, 2018.

[14] Tjibbe Donker. The dangers of using large language models for peer review. *The Lancet Infectious Diseases*,
23(7):781, 2023.

[15] Justin Esarey. Does peer review identify the best papers? a simulation study of editors, reviewers, and the scientific publication process. *PS: Political Science & Politics*, 50(4):963–969, 2017.

[16] Charles W Fox, Arianne YK Albert, and Timothy H Vines. Recruitment of reviewers is becoming harder at some journals: a test of the influence of reviewer fatigue at six journals in ecology and evolution. Research Integrity and Peer Review, 2:1–6, 2017.

[17] Yu Geng, Renmeng Cao, Xiaopu Han, Wencan Tian, Guangyao Zhang, and Xianwen Wang. Scientists are working overtime: when do scientists download scientific papers? *Scientometrics*, 127(11):6413–6429, 2022.

[18] Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte. The political ideology of conversational ai: Converging evidence on chatgpt's pro-environmental, left-libertarian orientation. *arXiv preprint arXiv:2301.01768*, 2023.

[19] Mohammad Hosseini and Serge PJM Horbach. Fighting reviewer fatigue or amplifying bias? considerations and recommendations for use of chatgpt and other large language models in scholarly peer review. *Research integrity* and peer review, 8(1):4, 2023.

[20] ICML. ICML 2023. https://icml.cc/Conferences/2023/llm-policy, 2024.

[21] Maurice Jakesch, Jeffrey T Hancock, and Mor Naaman. Human heuristics for ai-generated language are flawed.

Proceedings of the National Academy of Sciences, 120(11):e2208839120, 2023.

[22] Sayash Kapoor and Arvind Narayanan. Leakage and the reproducibility crisis in machine-learning-based science.

Patterns, 4(9), 2023.

[23] Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, et al. Monitoring ai-modified content at scale: A case study on the impact of chatgpt on ai conference peer reviews. *arXiv preprint arXiv:2403.07183*, 2024.

[24] Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, et al. Mapping the increasing use of llms in scientific papers. *arXiv preprint arXiv:2404.01268*,
2024.

[25] Torrin M Liddell and John K Kruschke. Analyzing ordinal data with metric models: What could possibly go wrong? *Journal of Experimental Social Psychology*, 79:328–348, 2018.

[26] Zhicheng Lin. Techniques for supercharging academic writing with generative ai. Nature Biomedical Engineering, pages 1–6, 2024.

[27] Taryn MacKinney. Scholarly Peer Review is an Age-Old Practice, But Publishing is Changing. http://www.aps.

org/publications/apsnews/202310/peer-review.cfm, October 2023.

[28] Peter McCullagh. Regression models for ordinal data. Journal of the Royal Statistical Society: Series B
(Methodological), 42(2):109–127, 1980.

[29] Bruce D Meyer and Nikolas Mittag. Misclassification in binary choice models. *Journal of Econometrics*, 200(2):295–
311, 2017.

[30] Nature. Artificial Intelligence (AI), 2024.

[31] Neurips. The NeurIPS 2021 Consistency Experiment. https://blog.neurips.cc/2021/12/08/the-neurips-2021consistency-experiment/, 2021.

[32] NIH. Peer Review. https://grants.nih.gov/grants/peer-review.htm, 2024. [33] NSF. Merit Review | NSF - National Science Foundation. https://www.nsf.gov/bfa/dias/policy/merit_review/,
2024.

[34] International Congress on Peer Review and Scientific Publication. Peer review congress. https://
peerreviewcongress.org/. Accessed on 2024-04-18.

[35] OpenAI. Introducing ChatGPT, November 2024. [36] Arjun Panickssery, Samuel R. Bowman, and Shi Feng. Llm evaluators recognize and favor their own generations, 2024.

[37] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. *EMNLP*,
2019.

[38] Drummond Rennie. Let's make peer review scientific. *Nature*, 535(7610):31–33, 2016. [39] Paul R Rosenbaum. Sensitivity analysis in observational studies. *Encyclopedia of statistics in behavioral science*,
2005.

[40] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. Can aigenerated text be reliably detected? *arXiv preprint arXiv:2303.11156*, 2023.

[41] Sarvenaz Sarabipour, Humberto J Debat, Edward Emmott, Steven J Burgess, Benjamin Schwessinger, and Zach Hensel. On the value of preprints: An early career researcher perspective. *PLoS biology*, 17(2):e3000151, 2019.

[42] Science. Peer Review at Science Journals. https://www.science.org/content/page/peer-review-sciencepublications, 2024.

[43] Richard Smith. Peer review: a flawed process at the heart of science and journals. Journal of the royal society of medicine, 99(4):178–182, 2006.

[44] David Soergel, Adam Saunders, and Andrew McCallum. Open scholarship and peer review: a time for experimentation. *ICML*, 2013.

[45] Jonathan P Tennant. The state of the art in peer review. *FEMS Microbiology letters*, 365(19):fny204, 2018.

[46] Edward Tian, Alexander Cui, and Alex Adam. Gptzero: Towards detection of ai-generated text using zero-shot and supervised methods, 2023.

[47] Inga Vesper. Peer reviewers unmasked: largest global survey reveals trends. *Nature*, 10, 2018.

[48] Mark Ware and Michael Mabe. The stm report: An overview of scientific and scholarly journal publishing, 2015. [49] Simon Wessely. Peer review of grant applications: what do we know? *The lancet*, 352(9124):301–305, 1998. [50] Torsten Wilholt. Epistemic trust in science. *The British Journal for the Philosophy of Science*, 2013.

[51] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. *ICLR*, 2020.

[52] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. *NeurIPS*, 36, 2024.

## A: Dataset

Submissions. When a submission is made to ICLR and it is not withdrawn before the submission deadline, it is hosted on OpenReview. As mentioned in the section Materials and Methods, we use the OpenReview API to collect all submissions made to ICLR between 2018-2024, including the titles, abstracts, introductions, keywords, and author institutions. This collection resulted in 23,959 main conference submissions from 46,257 authors and 1,263 institutions (see Table I).

| Year   | Reviews   | Submissions   | Acceptance   | AI-assisted reviews   |
|--------|-----------|---------------|--------------|-----------------------|
| 2018   | 2921      | 1007          | 36.0%        | 57                    |
| 2019   | 4734      | 1569          | 31.5%        | 95                    |
| 2020   | 7783      | 2593          | 26.5%        | 123                   |
| 2021   | 11488     | 3009          | 29.1%        | 216                   |
| 2022   | 13161     | 3422          | 32.0%        | 164                   |
| 2023   | 18575     | 4955          | 24.3%        | 176                   |
| 2024   | 28028     | 7404          | 30.5%        | 4887                  |
| Total  | 86690     | 23959         | -            | -                     |

TABLE I. Number of ICLR reviews and submissions per year. We report the number of AI-assisted reviews detected by GPTZero (without correcting for the model's false positive rate).

Reviews. Similarly, we collect all 86,690 reviews spanning from 2018 to 2024 (see table I). These reviews include textual evaluations, confidence scores, and overall score ratings. The confidence score was measured using a consistent scale of one to five during the observation period. The overall score rating scale consists of a one to ten scale but with only specific scores possible (1, 3, 5, 6, 8, 10). Each of these scores comes with a detailed definition to provide guidance on their meaning:
- 1: strong reject. - 3: reject, not good enough.

- 5: marginally below the acceptance threshold. - 6: marginally above the acceptance threshold.

- 8: accept, good submission - 10: strong accept, should be highlighted at the conference.

## B: On The Robustness Of The Ai-Assisted Labeling Threshold

We label reviews as AI-assisted if GPTZero predicts their probability of being human written as less than 0.5. Here, we explore the robustness of our findings in the three studies conducted (prevalence, AI-assisted vs. human scores, and acceptance analysis) by varying this threshold. Specifically, we select the following threshold values: [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45]. We choose thresholds *lower* than 0.5 because higher thresholds would yield higher false-positive rates and, thus, higher bias in the estimates (as human reviews would be classified as AI-assisted). We depict results in Figure 5, confirming that our findings remain valid when changing the labeling threshold. Prevalence Analysis. Figure 5(A) shows the prevalence of AI-assisted reviews corrected for the false positive rate, as done in the primary analysis. Interestingly, even under a stringent threshold—classifying a review as AI-assisted if the probability of being human is less than 0.05—we still find that over 7% of reviews submitted to ICLR are AI-assisted.

Reviews Scores Difference Analysis. Figure 5(B) shows the increase in the odds of receiving a higher score from an AI-assisted review over a human review across thresholds, calculated using the proportional odds model (see Equation 3). We further reproduce Figure 3 using different thresholds in Figure 6, showing that the findings are robust to the picked threshold.

Acceptance Analysis. Figure 5(C) shows the average increase in odds across thresholds, calculated using the logistic regression depicted in Equation 1. The effect sizes remain consistent, confirming the stability of our threshold settings. We further reproduce Figure 4(A) using different thresholds in Figure 7, showing that the findings are robust to the picked threshold.

![10_image_0.png](10_image_0.png)

FIG. 5. Robustness of the AI-assisted reviews labeling threshold. The plots show the robustness of the 0.5 threshold used to label reviews as AI-assisted or human. The plots show the prevalence analysis (A), the reviews score difference analysis
(B), and the acceptance analysis (C) when varying the threshold.

![11_image_0.png](11_image_0.png)

FIG. 6. Mean submission-level differences between AI-assisted and human reviews, as a function of human reference scores for all labeling thresholds considered. This figure reproduces Figure 3 using different thresholds to label reviews as AI-assisted or human. 

![11_image_1.png](11_image_1.png)

FIG. 7. Effects of receiving an AI-assisted review on acceptance. This figure reproduces Figure 4(A) using different thresholds to label reviews as AI-assisted or human.

C: On the Effect of LLMs on Review Scores

| Variable                              | Coefficient Std.Error P-value   |        |           |
|---------------------------------------|---------------------------------|--------|-----------|
| γ                                     | 0.1463                          | 0.0335 | 1.336e-05 |
| α1                                    | -2.1794                         | 0.2230 | < 2.2e16  |
| α3                                    | 0.7121                          | 0.2036 | 0.0004    |
| α5                                    | 2.1793                          | 0.2049 | < 2.2e-16 |
| α6                                    | 3.9637                          | 0.2084 | < 2.2e-16 |
| α8                                    | 7.6799                          | 0.2723 | < 2.2e-16 |
| β3                                    | 0.9800                          | 0.2064 | 2.110e-06 |
| β5                                    | 1.5785                          | 0.2070 | 2.813e-14 |
| β6                                    | 2.4094                          | 0.2081 | < 2.2e-16 |
| β8                                    | 2.9347                          | 0.2143 | < 2.2e-16 |
| Model Summary: Number of observations | 19332                           |        |           |
| Log-likelihood                        | -6346.89                        |        |           |
| AIC                                   | 12705.78                        |        |           |

TABLE II. Ordinal regression results. Coefficients and model summary (see Eq 3)

## D: On The Effect Of Llms On Submissions Acceptance

Assessing Matching. The matching procedure used to estimate the effect of AI-assisted reviews on acceptance rates consists of two steps: (1) an exact match based on the number of reviews and the scores assigned to the submissions, and (2) a content-based match computed using the cosine similarity of the embeddings of abstracts and content of their associated reviews. We only include matches in our analysis where the cosine similarity exceeds 0.1 (we refer to this value as a matching threshold).

Figure 8 shows how the impact of receiving an AI-assisted review on the acceptance rates of "borderline" papers remains consistent across various matching thresholds in our stratified analysis. The effect is similar to what we observed in our main analysis and shows no statistical difference, confirming the reliability of our matching approach. We assess the quality of our matched sample by comparing the content similarity between matched submissions and a sample of "unmatched" submissions (for each submission in the matched sample, we randomly sample a submission from ICLR 2024). We used these samples in two robustness checks:
- *Analysis of Keywords Similarity* For each sample, we checked for keyword overlap in each pair, hereinafter
"hits". The frequency of hits in the matched sample was 19.6%, substantially higher than the 1.0%
observed in the unmatched sample, where each AI-assisted submission was randomly paired with another submission from ICLR 2024. Additionally, we analyzed the most common keywords in the AI-assisted submissions, calculating the frequency of these words in both the matched and unmatched submissions
(see Figure 9).

- *Embeddings Similarity* For each sample, we measure the similarity between the abstracts of matched pairs of submissions using the BERTScores [51]. BERTScore is a metric used to measure textual similarity sensitive to semantic content. This is done by calculating the cosine similarity of BERT embeddings between corresponding tokens in two texts. The average BERTScore for pairs in the matched sample was 0.836, compared to 0.822 in the unmatched sample, confirming the robustness of our content-based matching process (p<0.001 in a K-S test).

Regression Analysis. We report the regression coefficients and model summary for the regressions shown in the main paper in Table III and Table IV.

Sensitivity Analysis. Our results rely on the assumption that there are no confounders that affect both the probability of receiving an AI-assisted review and the acceptance/rejection outcome. Sensitivity analysis is a way of quantifying how the results of our study would change if this assumption is violated [39]. This notion is quantified by the sensitivity Γ, which specifies the ratio by which the probability of receiving an AI-assisted review of two matched submissions would need to differ to result in a p-value above the significance threshold. Large values of Γ correspond to more robust conclusions. For the chosen p = 0.05, we measured the effect of AI-assisted reviews on acceptance decisions. For borderline papers (in the [5, 6) bin) we obtain a Γ of 1.07, which implies that, within matched pairs, a submission's probability of receiving an AI-assisted review could take on any value between 1/(1 + Γ) = 0.48 and Γ/(1 + Γ) = 0.52 without changing our decision of rejecting the null hypothesis of no effect.

FIG. 8. Sensitivity of the matching threshold. We show how the stratified effects for the acceptance analysis change when

![14_image_0.png](14_image_0.png)

varying the matching threshold of 0.1. Effect sizes remain qualitatively similar to the one estimated in our main analysis (black dash line) showing the robustness of our matching.

| Variable                               | Coefficient Std.Error P-value   |       |       |
|----------------------------------------|---------------------------------|-------|-------|
| Intercept                              | -0.5327                         | 0.041 | 0.000 |
| β                                      | 0.1289                          | 0.057 | 0.024 |
| Model Summary: Number of Observations: | 5180                            |       |       |
| Df Residuals:                          | 5178                            |       |       |
| Psuedo R-squared.:                     | 0.00073                         |       |       |
| Log-Likelihood:                        | -3450.0                         |       |       |

| Variable                               | Coefficient Std.Error P-value   |       |       |
|----------------------------------------|---------------------------------|-------|-------|
| Intercept                              | 0.3699                          | 0.010 | 0.000 |
| β                                      | 0.0305                          | 0.014 | 0.024 |
| Model Summary: Number of Observations: | 5180                            |       |       |
| Df Residuals:                          | 5178                            |       |       |
| Adj. R-squared.:                       | 0.001                           |       |       |
| Log-Likelihood:                        | -3616.6                         |       |       |

TABLE III. Logistic regression results. Coefficients and model summary (see Eq. 3).

TABLE IV. Ordinary least squares. Coefficients and model summary (see Eq. 4).

![15_image_0.png](15_image_0.png) 

FIG. 9. Difference in keywords frequency. In purple we show the difference in keywords frequency for the matched (purple)
and unmatched sample (orange). We consistently observe that the matched submissions consistently have lower topic discrepancies compared to the unmatched sample.

| Variable                        | Coefficient Std.Err P-value   |       |       |
|---------------------------------|-------------------------------|-------|-------|
| Intercept                       | 0.2143                        | 0.049 | 0.000 |
| Bin=2-3                         | -0.0589                       | 0.053 | 0.267 |
| Bin=3-4                         | 0.0481                        | 0.052 | 0.270 |
| Bin=4-5                         | -0.0569                       | 0.051 | 0.347 |
| Bin=5-6                         | 0.5218                        | 0.051 | 0.000 |
| Bin=6-7                         | 0.7554                        | 0.070 | 0.000 |
| Bin=7-8                         | 0.7857                        | 0.412 | 0.057 |
| β· Bin=1-2                      | -0.0143                       | 0.069 | 0.836 |
| β· Bin=2-3                      | 0.0301                        | 0.029 | 0.299 |
| β· Bin=3-4                      | 0.0426                        | 0.023 | 0.069 |
| β· Bin=4-5                      | 0.0110                        | 0.021 | 0.607 |
| β· Bin=5-6                      | 0.0486                        | 0.022 | 0.024 |
| β· Bin=6-7                      | -0.0152                       | 0.071 | 0.832 |
| β· Bin=7-8                      | 6.37e−15                      | 0.578 | 1.000 |
| Model Summary No. Observations: | 5180                          |       |       |
| Df Residuals:                   | 5166                          |       |       |
| Pseudo R-squ.:                  | 0.2290                        |       |       |
| Log-Likelihood:                 | -2659.0                       |       |       |

TABLE V. Stratified Regression Analysis (Linear Regression). Estimated coefficients to compute the average increase in the acceptance rate of submissions that received AI-assisted reviews conditioned on the average human score that the submission received.

| Variable                        | Coefficient Std.Err P-value   |        |         |
|---------------------------------|-------------------------------|--------|---------|
| Intercept                       | -1.2993                       | 0.291  | 0.000   |
| Bin=2-3                         | -0.3937                       | 0.322  | 0.222   |
| Bin=3-4                         | -0.3786                       | 0.312  | 0.225   |
| Bin=4-5                         | 0.2659                        | 0.303  | 0.381   |
| Bin=5-6                         | 2.3251                        | 0.303  | 0.000   |
| Bin=6-7                         | 4.7650                        | 0.775  | 0.000   |
| Bin=7-8                         | 29.9196                       | 1.64e6 | 1.000   |
| β·Bin=1-2                       | -0.0870                       | 0.417  | 0.835   |
| β·Bin=2-3                       | 0.2132                        | 0.189  | 0.259   |
| β·Bin=3-4                       | 0.2916                        | 0.150  | 0.052   |
| β·Bin=4-5                       | 0.0563                        | 0.119  | 0.635   |
| β·Bin=5-6                       | 0.2675                        | 0.124  | 0.031   |
| β·Bin=6-7                       | -0.4212                       | 0.930  | 0.0.651 |
| β·Bin=7-8                       | -11.9045                      | 1.64e6 | 1.000   |
| Model Summary No. Observations: | 5180                          |        |         |
| Df Residuals:                   | 5166                          |        |         |
| Pseudo R-squ.:                  | 0.2290                        |        |         |
| Log-Likelihood:                 | -2659.0                       |        |         |

TABLE VI. Stratified Regression Analysis (Logistic Regression). Estimated coefficients to compute the increase in the log-odds acceptance rate of submissions that received AI-assisted reviews conditioned on the average human score that the submission received.

## E: Prevalence Checks

Interpreting GPTZero Predictions Using Word Frequencies. To enhance the interpretability of GPTZero's prediction, we investigate if reviews containing words that are typically associated with AIgenerated text are more likely to be predicted as AI-assisted. To do so, we define a dictionary of all words in our dataset and filter it down to either nouns, verbs, or adjectives. Then, for each word w, we compute the probability of a review, r, being classified as AI-assisted, LLMr, if that word is present, denoted as P r(LLMr|w). We illustrate the 30 most predictive words in Table VII alongside their probability.

| Words and Probabilities                                                                                                                                            |                                        |                     |                       |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------|---------------------|-----------------------|
| underscores (0.780),                                                                                                                                               | necessitating (0.747),                 | delves (0.741),     | adaptability (0.731), |
| delved (0.727),                                                                                                                                                    | delve (0.722)                          | elucidated (0.709), | underscore (0.695),   |
| credibility (0.688), advancements (0.687), elucidation (0.686), underpinnings (0.681). equitable (0.679), perplexing (0.676), excels (0.674), intricacies (0.672), |                                        |                     |                       |
| persuasiveness (0.670),                                                                                                                                            | delineation (0.667),                   | elucidate (0.667),  | provision (0.658),    |
| bolster (0.654),                                                                                                                                                   | discourse (0.652), meticulous (0.652), | endeavors (0.650),  |                       |
| tangible (0.650), commendable (0.645), showcasing (0.643),                                                                                                         | imperative (0.642),                    |                     |                       |
| encompassing (0.638),                                                                                                                                              | offering (0.633)                       |                     |                       |

TABLE VII. Predominant LLM-Associated Words. This table displays the most predictive words in GPTZero classifications. The probability indicates the likelihood that a review containing a specific word is classified as AI-assisted.

## F: Overlap Of Conference Reviewers And Area Chairs

We collected data on Area-Chair and reviewer volunteers of several top AI conferences directly from their official web pages. Specifically, we collected data from the conference on "Artificial Intelligence and Statistics"
(AISTATS), the "International Conference on Learning Representations" (ICLR), the "International Conference on Machine Learning" (ICML), and the "Conference on Neural Information Processing Systems" (NeurIPS).

We measure the overlap in exact names across the different conferences by taking the Jaccard similarity. In Table VIII we illustrate the overlap in reviewers, whereas in Table IX we show the overlap in area chairs and meta reviewers.

|              | AISTATS   | AISTATS   | ICML   | ICML   | NeurIPS   | NeurIPS   | ICLR 2023   |
|--------------|-----------|-----------|--------|--------|-----------|-----------|-------------|
|              | 2023      | 2024      | 2022   | 2023   | 2022      | 2023      |             |
| AISTATS 2023 | -         | -         | -      | -      | -         | -         | -           |
| AISTATS 2024 | 0.264     | -         | -      | -      | -         | -         | -           |
| ICML 2022    | 0.092     | 0.075     | -      | -      | -         | -         | -           |
| ICML 2023    | 0.095     | 0.082     | 0.327  | -      | -         | -         | -           |
| NeurIPS 2022 | 0.078     | 0.066     | 0.402  | 0.460  | -         | -         | -           |
| NeurIPS 2023 | 0.076     | 0.069     | 0.284  | 0.327  | 0.400     | -         | -           |
| ICLR 2023    | 0.070     | 0.059     | 0.269  | 0.299  | 0.342     | 0.237     | -           |
| ICLR 2024    | 0.073     | 0.076     | 0.261  | 0.312  | 0.330     | 0.534     | 0.265       |

| AISTATS   | ICML   | ICML   | NeurIPS   | NeurIPS   | ICLR 2023   |       |       |
|-----------|--------|--------|-----------|-----------|-------------|-------|-------|
| 2024      | 2022   | 2023   | 2022      | 2023      |             |       |       |
| ICLR 2024 | 0.046  | 0.054  | 0.153     | 0.248     | 0.242       | 0.254 | 0.332 |

TABLE VIII. Jaccard similarity between reviewers across conferences. A similarity of 1 indicates full overlap, and a similarity of 0 indicates no reviewer overlap.

TABLE IX. Jaccard similarity between area chairs and meta reviewers across conferences. A similarity of 1 indicates full overlap, and a similarity of 0 indicates no reviewer overlap.

## G: Examples Of Matched Submissions

In Table X, we provide examples of matched papers abstracts that received an AI-assisted review with those papers that received only human reviews. These examples are randomly sampled from the set of matches found using the procedure described in Materials and Methods.

|                                   | TABLE X: Examples of papers' abstracts that received an AI-assisted review and of abstract of papers that received only human reviews.   |
|-----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------|
| Abstract with AI-assisted reviews | Abstract with human reviews                                                                                                              |
| In this paper, we consider offline reinforcement learning (RL) problems. Within this setting, posterior sampling has been rarely used, perhaps partly due to its explorative nature. The only work using posterior sampling for offline RL that we are aware of is the model-based posterior sampling of Ueara et al.. However, this framework does not permit any tractable algorithm (not even in the linear models) where simulations of posterior samples become challenging, especially in high dimensions. In addition, the algorithm only admits a weak form of guarantees - Bayesian sub-optimality bounds which depend on the prior distribution. To address these problems, we propose and analyze the use of Markov Chain Monte Carlo methods for offline RL. We show that for low-rank Markov decision processes (MDPs), using the Langevin Monte Carlo (LMC) algorithm, our algorithm obtains the (frequentist) suboptimality bound that competes against any comparator policy π and interpolates between O˜(H2 d p Cπ/K) and O˜(H2p dCπ/K), where Cπ is the concentrability coefficient of π, d is the dimension of the linear feature, H is the episode length, and K is the number of episodes in the offline data. For general MDPs with overparameterized neural network function approximation, we show that our LMC-based algorithm obtains the sub-optimality bounds of O˜(H2.5 d˜ p Cπ/K), where d˜is the effective dimension of the neural network. Finally, we collaborate our findings with numerical evaluations to demonstrate that LMC-based algorithms could be both efficient and competitive for offline RL in high dimensions. Branch-and-bound (B&B) has long been favored for tackling complex Mixed Integer Programming (MIP) problems, where the choice of branching strategy plays a pivotal role. Recently, Imitation Learning (IL)-based policies have emerged as potent alternatives to traditional rule-based approaches. However, it is nontrivial of acquiring high-quality training samples, and IL often converges to suboptimal variable choices for branching, restricting the overall performance. In response to these challenges, we propose a novel hybrid online and offline reinforcement learning (RL) approach to enhance the branching policy by cost-effective training sample augmentation. In online phase, we train an online RL agent to dynamically decide the sample generation processes, drawing from either the learning-based policy or the expert policy. The objective here is to strike an optimal balance between the exploration and exploitation of the sample generation process. In offline phase, a value function is trained to fit the cumulative reward for each decision and to filter the samples with high cumulative returns. This dual-purpose function not only reduces training complexity but also enhances the quality of the samples. To assess the efficacy of our proposed data augmentation mechanism, we conduct comprehensive evaluations across a range of MIP problems. The results consistently show that our method excels in making superior branching decisions compared to state-of-the-art learning-based models and the open-source solver SCIP. Notably, it even often outperforms the commercial solver Gurobi. Scientific processes are often modelled by sets of differential equations. As datasets grow, individually fitting these models and quantifying their uncertainties becomes a computationally challenging task. In this paper, we focus on improving the scalability of a particular class of stochastic dynamical model, called latent force models. These offer a balance between data-driven and mechanistic inference in dynamical systems, achieved by deriving a kernel function over a low-dimensional latent force. However, exact computation of posterior kernel terms is rarely tractable, requiring approximations for complex scenarios such as nonlinear dynamics. We overcome this issue by posing the problem as meta-learning the class of latent force models corresponding to a set of differential equations. By employing a deep kernel along with a sensible function embedding, we demonstrate the ability to extrapolate from simulations to real experimental datasets. Finally, we show how our model scales compared with other approximations. In this paper, we investigate the generalization error of deep physical models with latent variables. Deep physical models, such as Hamiltonian Neural Networks, are neural network models for learning equations of motion from observational data of physical phenomena and have attracted much attention in recent years. In particular, in such cases, the data are not completely random, but rather given as random trajectories. We provide an error bound for the case where the training data are given in such a way. Our results show that it is important to collect data from many trajectories, rather than simply collecting a large number of data, to improve generalization performance. In addition, an important application of the combination of deep physics models with latent variables is the interpolation of images from videos while preserving the laws of physics, such as the energy conservation law. However, when the frame interval of the video is large, it can be difficult to preserve the laws of physics. In this paper, we show that it is possible to interpolate the images from videos so that the laws of physics are preserved, provided that the generalization error is sufficiently small Continued on next page                                   |                                                                                                                                          |

|                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 21   |
|----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------|
|                      | Given an image of a natural scene, we are able to quickly decompose it into a set of components such as objects, lighting, shadows, and foreground. We can then picture how the image would look if we were to recombine certain components with those from other images, for instance producing a scene with a set of objects from our bedroom and animals from a zoo under the lighting conditions of a forest even if we have never seen such a scene in real life before. We present a method to decompose an image into such compositional components. Our approach, Decomp Diffusion, is an unsupervised method which, when given a single image, infers a set of different components in the image, each represented by a diffusion model. We demonstrate how components can capture different factors of the scene, ranging from global scene descriptors (shadows, foreground, facial expression) to local scene descriptors (objects). We further illustrate how inferred factors can be flexibly composed, even with factors inferred from other models, to generate a variety of scenes sharply different than those seen in training time. |      |
|                      | TABLE X - continued from previous page                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |      |
| AI-assisted Abstract | Human Abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |      |
| Image restoration problems are typically ill-posed in the sense that each degraded image can be restored in infinitely many valid ways. To accommodate this, many works generate a diverse set of outputs by attempting to randomly sample from the posterior distribution of natural images given the degraded input. Here we argue that this strategy is commonly of limited practical value because of the heavy tail of the posterior distribution. Consider for example inpainting a missing region of the sky in an image. Since there is a high probability that the missing region contains no object but clouds, any set of samples from the posterior would be entirely dominated by (practically identical) completions of sky. However, arguably, presenting users with only one clear sky completion, along with several alternative solutions such as airships, birds, and balloons, would better outline the set of possibilities. In this paper, we initiate the study of **meaningfully diverse** image restoration. We explore several post-processing approaches that can be combined with any diverse image restoration method to yield semantically meaningful diversity. Moreover, we propose a practical approach for allowing diffusion based image restoration methods to generate meaningfully diverse outputs, while incurring only negligent computational overhead. We conduct extensive user studies to analyze the proposed techniques, and find the strategy of reducing similarity between outputs to be significantly favorable over posterior sampling. The excessive computational requirements of modern artificial neural networks (ANNs) are posing limitations on Deep neural networks have demonstrated remarkable performance in various tasks. With a growing need for sparse the machines that can run them. Sparsification of ANNs is deep learning, model compression techniques, especially often motivated by time, memory and energy savings only pruning, have gained significant attention. However, conventional pruning techniques can inadvertently exacerbate during model inference, yielding no benefits during training. A growing body of work is now focusing on providing the algorithmic bias, resulting in unequal predictions. To address this, we define a fair pruning task where a sparse benefits of model sparsification also during training. While these methods greatly improve the training efficiency, the model is derived subject to fairness requirements. In particular, we propose a framework to jointly optimize the training algorithms yielding the most accurate models still materialize the dense weights, or compute dense gradients pruning mask and weight update processes with fairness during training. We propose an efficient, always-sparse constraints. This framework is engineered to compress models that maintain performance while ensuring fairness training algorithm which improves the accuracy over previous methods. Additionally, our method has excellent in a single execution. To this end, we formulate the fair scaling to larger and sparser models, supported by its linear time complexity with respect to the model width during pruning problem as a novel constrained bi-level optimization task and derive efficient and effective solving strategies. training and inference. We evaluate our method on CIFAR10/100 and ImageNet using ResNet, VGG, and ViT models, We design experiments spanning various datasets and settings to validate our proposed method. Our empirical and compare it against a range of sparsification methods. analysis contrasts our framework with several mainstream pruning strategies, emphasizing our method's superiority in maintaining model fairness, performance, and efficiency. Continued on next page                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |      |

|                      |                                        | 22   |
|----------------------|----------------------------------------|------|
|                      | Few-shot object detection (FSOD) benchmarks have advanced techniques for detecting new categories using limited annotations. Existing FSOD benchmarks re-purpose wellestablished datasets like COCO by partitioning categories into base and novel classes for pre-training and fine-tuning respectively. However, these benchmarks do not reflect how FSOD is deployed in practice. Rather than pre-training on only a small number of categories, we argue that it is more practical to download a foundational model (e.g., a vision-language model (VLM) pretrained on web-scale data) and finetune it for specific applications. Surprisingly, we find that zero-shot inference from foundational VLMs like GroundingDINO significantly outperform state-of-theart methods (48.3 vs. 33.1 AP) on COCO, suggesting that few-shot detection should be reframed in the context of foundation models. In this work, we propose a new FSOD benchmark protocol that evaluates detectors pre-trained on any external dataset (not including the target dataset), and finetuned on K-shot annotations per C target classes. Further, we note that FSOD benchmarks are actually federated datasets, which are exhaustively annotated for a single category only on a subset of data. We leverage this insight and propose simple strategies for fine-tuning VLMs to improve FSOD. We demonstrate the effectiveness of our approach on LVIS and nuImages                                        |      |
|                      | TABLE X - continued from previous page |      |
| AI-assisted Abstract | Human Abstract                         |      |
| Single Domain Generalization Object Detection (S-DGOD) is a challenging yet practical task, where we only have access to data from one specific source domain to train an object detection network, but have to generalize to numerous unseen target domains. Recent works point out that the learning dynamics of Deep Neural Networks (DNNs) are biased by gradient descent to learn simple semantics, which are usually non-causal and spuriously correlated to the ground truth labels, as a result, DNN-based object detection networks fail to consistently generalize well in the Out-of-Domain (OoD) scenario. In this paper, we focus on S-DGOD based on theoretical analysis, exploring a classic and widely-used approach, Generalizable Reweighting (GRW), which iteratively reweightes the training samples to improve generalization performance. In our theoretical analysis, we first identify that the vanilla GRW hardly outperforms Empirical Risk Minimization (ERM) in the S-DGOD scenario. To provide a generalization guarantee, we further derive Certifiable Feature Perturbation (CFP) based on our theory, which aims to train a robust object detection network against additional perturbations added to the extracted features. We demonstrate that GRW works well with CFP in achieving OoD generalization, thus, surpassing ERM by a large margin under worse conditions. This brand new reweighting strategy is named Certifiable Reweighting (CARD). Our extensive experiments show that the proposed CARD achieves SOTA performance compared to baseline methods on the five urban-scene S-DGOD benchmarks. The performance of machine learning models on new data is critical for their success in real-world applications. However, the model's performance may deteriorate if the new data is sampled from a different distribution than the training data. Current methods to detect shifts in the input or output data distributions have limitations in identifying model behavior changes. In this paper, we define explanation shift as the statistical comparison between how predictions from training data are explained and how predictions on new data are explained. We propose explanation shift as a key indicator to investigate the interaction between distribution shifts and learned models. We introduce an Explanation Shift Detector that operates on the explanation distributions, providing more sensitive and explainable changes in interactions between distribution shifts and learned models. We compare explanation shifts with other methods that are based on distribution shifts, showing that monitoring for explanation shifts results in more sensitive indicators for varying model behavior. We provide theoretical and experimental evidence and demonstrate the effectiveness of our approach on synthetic and real data. Additionally, we release an open-source Python package, skshift, which implements our method and provides usage tutorials for further reproducibility. Synthetic data has the distinct advantage of building a large-scale labeled dataset for almost free. Still, it should be carefully integrated into learning; otherwise, the expected performance gains are difficult to achieve. The biggest hurdle for synthetic data to achieve increased training performance is the domain gap with the (real) test data. As a common solution to deal with the domain gap, the sim2real transformation is used, and its quality is affected by three factors: i) the real data serving as a reference when calculating the domain gap, ii) the synthetic data chosen to avoid the transformation quality degradation, and iii) the synthetic data pool from which the synthetic data is selected. In this paper, we investigate the impact of these factors on maximizing the effectiveness of synthetic data in training in terms of improving learning performance and acquiring domain generalization ability–two main benefits expected of using synthetic data. As an evaluation metric for the second benefit, we introduce a method for measuring the distribution gap between two datasets, which is derived as the normalized sum of the Mahalanobis distances of all test data. As a result, we have discovered several important findings that have never been investigated or have been used previously without accurate understanding. We expect that these findings can break the current trend of either naively using or being hesitant to use synthetic data in machine learning due to the lack of understanding, leading to more appropriate use in future research. Continued on next page                      |                                        |      |

|                      |                                        | 23   |
|----------------------|----------------------------------------|------|
|                      | TABLE X - continued from previous page |      |
| AI-assisted Abstract | Human Abstract                         |      |
| Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs' performance. In this study, we propose three stable classes of Neural SDEs: Langevin-type SDE, Linear Noise SDE, and Geometric SDE. Then, we rigorously demonstrate their robustness in maintaining excellent performance under distribution shift, while effectively preventing overfitting. To assess the effectiveness of our approach, we conduct extensive experiments on four benchmark datasets for interpolation, forecasting, and classification tasks, and analyze the robustness of our methods with 30 public datasets under different missing rates. Our results demonstrate the efficacy of the proposed method in handling real-world irregular time series data. Limited data availability poses a major obstacle in training deep learning models for financial applications. Synthesizing financial time series to augment real-world data is challenging due to the irregular and scale-invariant patterns uniquely associated with financial time series - temporal dynamics that repeat with varying duration and magnitude. Such dynamics cannot be captured by existing approaches, which often assume regularity and uniformity in the underlying data. We develop a novel generative framework called FTS-Diffusion to model irregular and scale-invariant patterns that consists of three modules. First, we develop a scale-invariant pattern recognition algorithm to extract recurring patterns that vary in duration and magnitude. Second, we construct a diffusion-based generative network to synthesize segments of patterns. Third, we model the temporal transition of patterns in order to aggregate the generated segments. Extensive experiments show that FTSDiffusion generates synthetic financial time series highly resembling observed data, outperforming state-of-the-art alternatives. Two downstream experiments demonstrate that augmenting real-world data with synthetic data generated by FTS-Diffusion reduces the error of stock market prediction by up to 17.9%. To the best of our knowledge, this is the first work on generating intricate time series with irregular and scale-invariant patterns, addressing data limitation issues in finance. Surrogate neural network-based partial differential equation (PDE) solvers have the potential to solve PDEs in an accelerated manner, but they are largely limited to systems featuring predetermined problem sizes or fixed PDE parameters. We propose Specialized Neural AcceleratorPowered Domain Decomposition Methods (SNAP-DDM), a DDM-based approach to PDE solving in which subdomain problems containing arbitrary boundary conditions and geometric parameters are accurately solved using an ensemble of specialized neural operators. We tailor SNAP-DDM to 2D electromagnetics and fluidic flow problems and show how innovations in network architecture and loss function engineering can produce specialized surrogate subdomain solvers with near unity accuracy. We also show how these solvers can be used with standard DDM algorithms to accurately solve freeform electromagnetics and fluids problems with a wide range of domain sizes. Recent work provides promising evidence that PhysicsInformed Neural Networks (PINN) can efficiently solve partial differential equations (PDE). However, previous works have failed to provide guarantees on the worst-case residual error of a PINN across the spatio-temporal domain - a measure akin to the tolerance of numerical solvers - focusing instead on point-wise comparisons between their solution and the ones obtained by a solver on a set of inputs. In real-world applications, one cannot consider tests on a finite set of points to be sufficient grounds for deployment, as the performance could be substantially worse on a different set. To alleviate this issue, we establish tolerance-based correctness conditions for PINNs over the entire input domain. To verify the extent to which they hold, we introduce partial-CROWN: a general, efficient and scalable post-training framework to bound PINN residual errors. We demonstrate its effectiveness in obtaining tight certificates by applying it to two classically studied PDEs - Burgers' and Schrödinger's equations -, and two more challenging ones with real-world applications - the AllanCahn and Diffusion-Sorption equations. Continued on next page                      |                                        |      |

|                      |                                        |          | 24     |         |      |
|----------------------|----------------------------------------|----------|--------|---------|------|
|                      | Large language models (LLMs) are initially pretrained for broad capabilities and then finetuned with instructionfollowing datasets to improve their performance in interacting with humans. Despite advances in finetuning, a standardized guideline for selecting high-quality datasets to optimize this process remains elusive. In this paper, we first propose InstructMining, an innovative method designed for automatically selecting premium instruction-following data for finetuning LLMs. Specifically, InstructMining utilizes natural language indicators as a measure of data quality, applying them to evaluate unseen datasets. During experimentation, we discover that double descent phenomenon exists in large language model finetuning. Based on this observation, we further leverage BlendSearch to help find the best subset among the entire dataset (i.e., 2,532 out of 100,000). Experiment results show that InstructMining7B achieves state-of-the-art performance on two of the most popular benchmarks: LLM-as-a-judge and OpenLLM benchmark.                                        |          |        |         |      |
|                      | TABLE X - continued from previous page |          |        |         |      |
| AI-assisted Abstract | Human Abstract                         |          |        |         |      |
| Information retrieval (IR) plays a crucial role in locating relevant resources from vast amounts of data, and its applications have evolved from traditional knowledge bases to modern retrieval models (RMs). The emergence of large language models (LLMs) has further revolutionized the IR field by enabling users to interact with search systems in natural languages. In this paper, we explore the advantages and disadvantages of LLMs and RMs, highlighting their respective strengths in understanding user-issued queries and retrieving up-to-date information. To leverage the benefits of both paradigms while circumventing their limitations, we propose InteR, a novel framework that facilitates information refinement through synergy between RMs and LLMs. InteR allows RMs to expand knowledge in queries using LLM-generated knowledge collections and enables LLMs to enhance prompt formulation using retrieved documents. This iterative refinement process augments the inputs of RMs and LLMs, leading to more accurate retrieval. Experiments on large-scale retrieval benchmarks involving web search and low-resource retrieval tasks demonstrate that InteR achieves overall superior zero-shot retrieval performance compared to state-of-the-art methods, even those using relevance judgment. Retrieval-augmented                      |                                        | language | models | (RALMs) | hold |
|                      | promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding relevant passages. Thus, we propose a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages, using a mix of relevant and irrelevant contexts at training time. We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones.                                        |          |        |         |      |
| We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains.If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains.These corrected rationales can plausibly serve as a better foundation for the final answer consolidation.Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information.To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains.                      |                                        |          |        |         |      |
