# <span id="page-0-1"></span><span id="page-0-0"></span>Natural Language Processing

Jacob Eisenstein

November 13, 2018

# **Contents**

<span id="page-2-0"></span>

|   | Contents   |                                                                                                                                                                                                                                                                            |                                              |  |
|---|------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------|--|
|   | Preface    | Background<br><br>How to use this book                                                                                                                                                                                                                                     | i<br>i<br>ii                                 |  |
| 1 | 1.1<br>1.2 | Introduction<br>Natural language processing and its neighbors<br><br>Three themes in natural language processing<br><br>1.2.1<br>Learning and knowledge<br><br>1.2.2<br>Search and learning<br><br>1.2.3<br>Relational, compositional, and distributional perspectives<br> | 1<br>1<br>6<br>6<br>7<br>9                   |  |
| I |            | Learning<br>11                                                                                                                                                                                                                                                             |                                              |  |
| 2 | 2.1<br>2.2 | Linear text classification<br>The bag of words<br><br>Na¨ıve Bayes<br><br>2.2.1<br>Types and tokens<br>2.2.2<br>Prediction<br><br>2.2.3<br>Estimation<br>2.2.4<br>Smoothing<br>2.2.5<br>Setting hyperparameters                                                            | 13<br>13<br>17<br>19<br>20<br>21<br>22<br>23 |  |
|   | 2.3        | Discriminative learning<br><br>2.3.1<br>Perceptron<br>2.3.2<br>Averaged perceptron                                                                                                                                                                                         | 24<br>25<br>27                               |  |
|   | 2.4        | Loss functions and large-margin classification<br><br>2.4.1<br>Online large margin classification<br><br>2.4.2<br>*Derivation of the online support vector machine<br>                                                                                                     | 27<br>30                                     |  |
|   |            |                                                                                                                                                                                                                                                                            | 32                                           |  |

|   |     | 2.5.1 | Regularization<br><br>36                                 |
|---|-----|-------|----------------------------------------------------------|
|   |     | 2.5.2 | Gradients<br><br>37                                      |
|   | 2.6 |       | Optimization<br>37                                       |
|   |     | 2.6.1 | Batch optimization<br><br>38                             |
|   |     | 2.6.2 | Online optimization<br><br>39                            |
|   | 2.7 |       | *Additional topics in classification<br><br>41           |
|   |     | 2.7.1 | Feature selection by regularization<br>41                |
|   |     | 2.7.2 | Other views of logistic regression<br>41                 |
|   | 2.8 |       | Summary of learning algorithms<br><br>43                 |
| 3 |     |       | Nonlinear classification<br>47                           |
|   | 3.1 |       | Feedforward neural networks<br>48                        |
|   | 3.2 |       | Designing neural networks<br><br>50                      |
|   |     | 3.2.1 | Activation functions<br><br>50                           |
|   |     | 3.2.2 | Network structure<br><br>51                              |
|   |     | 3.2.3 | Outputs and loss functions<br><br>52                     |
|   |     | 3.2.4 | Inputs and lookup layers<br><br>53                       |
|   | 3.3 |       | Learning neural networks<br><br>53                       |
|   |     | 3.3.1 | Backpropagation<br><br>55                                |
|   |     | 3.3.2 | Regularization and dropout<br>57                         |
|   |     | 3.3.3 | *Learning theory<br><br>58                               |
|   |     | 3.3.4 | Tricks<br>59                                             |
|   | 3.4 |       | Convolutional neural networks<br><br>62                  |
| 4 |     |       | Linguistic applications of classification<br>69          |
|   | 4.1 |       | Sentiment and opinion analysis<br>69                     |
|   |     | 4.1.1 | Related problems<br>70                                   |
|   |     | 4.1.2 | Alternative approaches to sentiment analysis<br>72       |
|   | 4.2 |       | Word sense disambiguation<br><br>73                      |
|   |     | 4.2.1 | How many word senses?<br><br>74                          |
|   |     | 4.2.2 | Word sense disambiguation as classification<br><br>75    |
|   | 4.3 |       | Design decisions for text classification<br><br>76       |
|   |     | 4.3.1 | What is a word?<br>76                                    |
|   |     | 4.3.2 | How many words?<br>79                                    |
|   |     | 4.3.3 | Count or binary?<br><br>80                               |
|   |     |       |                                                          |
|   | 4.4 |       | Evaluating classifiers<br><br>80                         |
|   |     | 4.4.1 | Precision, recall, and F<br>-MEASURE<br><br>81           |
|   |     | 4.4.2 | Threshold-free metrics<br>83                             |
|   |     | 4.4.3 | Classifier comparison and statistical significance<br>84 |
|   |     | 4.4.4 | *Multiple comparisons<br>87                              |
|   | 4.5 |       | Building datasets<br><br>88                              |

<span id="page-4-0"></span>

|   |     | 4.5.1 | Metadata as labels<br><br>88                               |
|---|-----|-------|------------------------------------------------------------|
|   |     | 4.5.2 | Labeling data<br><br>88                                    |
| 5 |     |       | Learning without supervision<br>95                         |
|   | 5.1 |       | Unsupervised learning<br>95                                |
|   |     | 5.1.1 | K-means clustering<br><br>96                               |
|   |     | 5.1.2 | Expectation-Maximization (EM)<br><br>98                    |
|   |     | 5.1.3 | EM as an optimization algorithm 102                        |
|   |     | 5.1.4 | How many clusters?<br>103                                  |
|   | 5.2 |       | Applications of expectation-maximization 104               |
|   |     | 5.2.1 | Word sense induction<br>104                                |
|   |     | 5.2.2 | Semi-supervised learning<br>105                            |
|   |     | 5.2.3 | Multi-component modeling 106                               |
|   | 5.3 |       | Semi-supervised learning<br>107                            |
|   |     | 5.3.1 | Multi-view learning<br>108                                 |
|   |     | 5.3.2 | Graph-based algorithms 109                                 |
|   | 5.4 |       | Domain adaptation<br>110                                   |
|   |     | 5.4.1 | Supervised domain adaptation<br>111                        |
|   |     | 5.4.2 | Unsupervised domain adaptation<br>112                      |
|   | 5.5 |       | *Other approaches to learning with latent variables<br>114 |
|   |     | 5.5.1 | Sampling 115                                               |
|   |     | 5.5.2 | Spectral learning<br>117                                   |
|   |     |       |                                                            |

### **[II Sequences and trees](#page-140-0) 123**

| 6 |     | Language models                              | 125 |
|---|-----|----------------------------------------------|-----|
|   | 6.1 | N-gram language models<br>126                |     |
|   | 6.2 | Smoothing and discounting<br>129             |     |
|   |     | 6.2.1<br>Smoothing 129                       |     |
|   |     | 6.2.2<br>Discounting and backoff 130         |     |
|   |     | 6.2.3<br>*Interpolation 131                  |     |
|   |     | 6.2.4<br>*Kneser-Ney smoothing<br>133        |     |
|   | 6.3 | Recurrent neural network language models 133 |     |
|   |     | 6.3.1<br>Backpropagation through time<br>136 |     |
|   |     | 6.3.2<br>Hyperparameters 137                 |     |
|   |     | 6.3.3<br>Gated recurrent neural networks 137 |     |
|   | 6.4 | Evaluating language models 139               |     |
|   |     | 6.4.1<br>Held-out likelihood<br>139          |     |
|   |     | 6.4.2<br>Perplexity<br>140                   |     |
|   | 6.5 | Out-of-vocabulary words<br>141               |     |

| 7 |     | Sequence labeling                                                       | 145 |
|---|-----|-------------------------------------------------------------------------|-----|
|   | 7.1 | Sequence labeling as classification<br>145                              |     |
|   | 7.2 | Sequence labeling as structure prediction<br>147                        |     |
|   | 7.3 | The Viterbi algorithm 149                                               |     |
|   |     | 7.3.1<br>Example<br>152                                                 |     |
|   |     | 7.3.2<br>Higher-order features<br>153                                   |     |
|   | 7.4 | Hidden Markov Models<br>153                                             |     |
|   |     | 7.4.1<br>Estimation 155                                                 |     |
|   |     | 7.4.2<br>Inference 155                                                  |     |
|   | 7.5 | Discriminative sequence labeling with features 157                      |     |
|   |     | 7.5.1<br>Structured perceptron<br>160                                   |     |
|   |     | 7.5.2<br>Structured support vector machines<br>160                      |     |
|   |     | 7.5.3<br>Conditional random fields 162                                  |     |
|   | 7.6 | Neural sequence labeling 167                                            |     |
|   |     | 7.6.1<br>Recurrent neural networks<br>167                               |     |
|   |     | 7.6.2<br>Character-level models 169                                     |     |
|   |     | 7.6.3<br>Convolutional Neural Networks for Sequence Labeling 170        |     |
|   | 7.7 | *Unsupervised sequence labeling 170                                     |     |
|   |     | 7.7.1<br>Linear dynamical systems 172                                   |     |
|   |     | 7.7.2<br>Alternative unsupervised learning methods<br>172               |     |
|   |     | 7.7.3<br>Semiring notation and the generalized viterbi algorithm<br>172 |     |
|   |     |                                                                         |     |
| 8 |     | Applications of sequence labeling                                       | 175 |
|   | 8.1 | Part-of-speech tagging<br>175                                           |     |
|   |     | 8.1.1<br>Parts-of-Speech<br>176                                         |     |
|   |     | 8.1.2<br>Accurate part-of-speech tagging<br>180                         |     |
|   | 8.2 | Morphosyntactic Attributes<br>182                                       |     |
|   | 8.3 | Named Entity Recognition 183                                            |     |
|   | 8.4 | Tokenization 185                                                        |     |
|   | 8.5 | Code switching<br>186                                                   |     |
|   | 8.6 | Dialogue acts<br>187                                                    |     |
| 9 |     | Formal language theory                                                  | 191 |
|   | 9.1 | Regular languages<br>192                                                |     |
|   |     | 9.1.1<br>Finite state acceptors 193                                     |     |
|   |     | 9.1.2<br>Morphology as a regular language 194                           |     |
|   |     | 9.1.3<br>Weighted finite state acceptors<br>196                         |     |
|   |     | 9.1.4<br>Finite state transducers<br>201                                |     |
|   |     | 9.1.5<br>*Learning weighted finite state automata<br>206                |     |
|   | 9.2 | Context-free languages 207                                              |     |
|   |     | 9.2.1<br>Context-free grammars<br>208                                   |     |
|   |     |                                                                         |     |

<span id="page-5-0"></span>

<span id="page-6-0"></span>

|     | 9.2.2          | Natural language syntax as a context-free language<br>211 |
|-----|----------------|-----------------------------------------------------------|
|     | 9.2.3          | A phrase-structure grammar for English<br>213             |
|     | 9.2.4          | Grammatical ambiguity<br>218                              |
| 9.3 |                | *Mildly context-sensitive languages<br>218                |
|     | 9.3.1          | Context-sensitive phenomena in natural language 219       |
|     | 9.3.2          | Combinatory categorial grammar<br>220                     |
|     |                | 10 Context-free parsing<br>225                            |
|     |                | 10.1 Deterministic bottom-up parsing<br>226               |
|     | 10.1.1         | Recovering the parse tree<br>227                          |
|     | 10.1.2         | Non-binary productions 227                                |
|     | 10.1.3         | Complexity<br>229                                         |
|     | 10.2 Ambiguity | 229                                                       |
|     | 10.2.1         | Parser evaluation 230                                     |
|     | 10.2.2         | Local solutions<br>231                                    |
|     |                | 10.3 Weighted Context-Free Grammars<br>232                |
|     | 10.3.1         | Parsing with weighted context-free grammars<br>234        |
|     | 10.3.2         | Probabilistic context-free grammars<br>235                |
|     | 10.3.3         | *Semiring weighted context-free grammars 237              |
|     |                | 10.4 Learning weighted context-free grammars 238          |
|     | 10.4.1         | Probabilistic context-free grammars<br>238                |
|     | 10.4.2         | Feature-based parsing<br>239                              |
|     | 10.4.3         | *Conditional random field parsing 240                     |
|     | 10.4.4         | Neural context-free grammars<br>242                       |
|     |                | 10.5 Grammar refinement<br>242                            |
|     | 10.5.1         | Parent annotations and other tree transformations 243     |
|     | 10.5.2         | Lexicalized context-free grammars 244                     |
|     | 10.5.3         | *Refinement grammars<br>248                               |
|     |                | 10.6 Beyond context-free parsing 249                      |
|     | 10.6.1         | Reranking<br>250                                          |
|     | 10.6.2         | Transition-based parsing 251                              |
|     |                | 11 Dependency parsing<br>257                              |
|     |                | 11.1 Dependency grammar<br>257                            |
|     | 11.1.1         | Heads and dependents 258                                  |
|     | 11.1.2         | Labeled dependencies<br>259                               |
|     | 11.1.3         | Dependency subtrees and constituents<br>260               |
|     |                | 11.2 Graph-based dependency parsing<br>262                |
|     | 11.2.1         | Graph-based parsing algorithms<br>264                     |
|     | 11.2.2         | Computing scores for dependency arcs<br>265               |
|     | 11.2.3         | Learning 267                                              |
|     |                |                                                           |

<span id="page-7-0"></span>

| 11.3 Transition-based dependency parsing<br>268                 |  |
|-----------------------------------------------------------------|--|
| 11.3.1<br>Transition systems for dependency parsing 269         |  |
| 11.3.2<br>Scoring functions for transition-based parsers<br>273 |  |
| 11.3.3<br>Learning to parse 274                                 |  |
| 11.4 Applications 277                                           |  |

| III Meaning | 283 |
|-------------|-----|
|             |     |

| 12 Logical semantics                                                | 285 |
|---------------------------------------------------------------------|-----|
| 12.1 Meaning and denotation 286                                     |     |
| 12.2 Logical representations of meaning 287                         |     |
| 12.2.1<br>Propositional logic<br>287                                |     |
| 12.2.2<br>First-order logic 288                                     |     |
| 12.3 Semantic parsing and the lambda calculus 291                   |     |
| 12.3.1<br>The lambda calculus<br>292                                |     |
| 12.3.2<br>Quantification 293                                        |     |
| 12.4 Learning semantic parsers 296                                  |     |
| 12.4.1<br>Learning from derivations 297                             |     |
| 12.4.2<br>Learning from logical forms 299                           |     |
| 12.4.3<br>Learning from denotations<br>301                          |     |
|                                                                     |     |
| 13 Predicate-argument semantics                                     | 305 |
| 13.1 Semantic roles 307                                             |     |
| 13.1.1<br>VerbNet<br>308                                            |     |
| 13.1.2<br>Proto-roles and PropBank<br>309                           |     |
| 13.1.3<br>FrameNet<br>310                                           |     |
| 13.2 Semantic role labeling<br>312                                  |     |
| 13.2.1<br>Semantic role labeling as classification 312              |     |
| 13.2.2<br>Semantic role labeling as constrained optimization<br>315 |     |
| 13.2.3<br>Neural semantic role labeling 317                         |     |
| 13.3 Abstract Meaning Representation 318                            |     |
| 13.3.1<br>AMR Parsing<br>321                                        |     |
| 14 Distributional and distributed semantics                         | 325 |
| 14.1 The distributional hypothesis<br>325                           |     |
| 14.2 Design decisions for word representations 327                  |     |
| 14.2.1<br>Representation<br>327                                     |     |
| 14.2.2<br>Context 328                                               |     |
| 14.2.3<br>Estimation 329                                            |     |
| 14.3 Latent semantic analysis<br>329                                |     |
|                                                                     |     |

<span id="page-8-0"></span>

|  | 14.4 Brown clusters 331                                               |     |
|--|-----------------------------------------------------------------------|-----|
|  | 14.5 Neural word embeddings<br>334                                    |     |
|  | 14.5.1<br>Continuous bag-of-words (CBOW) 334                          |     |
|  | 14.5.2<br>Skipgrams 335                                               |     |
|  | 14.5.3<br>Computational complexity<br>335                             |     |
|  | 14.5.4<br>Word embeddings as matrix factorization 337                 |     |
|  | 14.6 Evaluating word embeddings 338                                   |     |
|  | 14.6.1<br>Intrinsic evaluations<br>339                                |     |
|  | 14.6.2<br>Extrinsic evaluations 339                                   |     |
|  | 14.6.3<br>Fairness and bias<br>340                                    |     |
|  | 14.7 Distributed representations beyond distributional statistics 341 |     |
|  | 14.7.1<br>Word-internal structure<br>341                              |     |
|  | 14.7.2<br>Lexical semantic resources 343                              |     |
|  | 14.8 Distributed representations of multiword units 344               |     |
|  | 14.8.1<br>Purely distributional methods<br>344                        |     |
|  | 14.8.2<br>Distributional-compositional hybrids<br>345                 |     |
|  | 14.8.3<br>Supervised compositional methods<br>346                     |     |
|  | 14.8.4<br>Hybrid distributed-symbolic representations 346             |     |
|  | 15 Reference Resolution                                               | 351 |
|  | 15.1 Forms of referring expressions<br>352                            |     |
|  | 15.1.1<br>Pronouns<br>352                                             |     |
|  | 15.1.2<br>Proper Nouns 357                                            |     |
|  | 15.1.3<br>Nominals<br>357                                             |     |
|  | 15.2 Algorithms for coreference resolution<br>358                     |     |
|  | 15.2.1<br>Mention-pair models 359                                     |     |
|  | 15.2.2<br>Mention-ranking models<br>360                               |     |
|  | 15.2.3<br>Transitive closure in mention-based models 361              |     |
|  | 15.2.4<br>Entity-based models<br>362                                  |     |
|  | 15.3 Representations for coreference resolution 367                   |     |
|  | 15.3.1<br>Features<br>367                                             |     |
|  | 15.3.2<br>Distributed representations of mentions and entities 370    |     |
|  | 15.4 Evaluating coreference resolution 373                            |     |
|  |                                                                       |     |
|  | 16 Discourse                                                          | 379 |
|  | 16.1 Segments 379                                                     |     |
|  | 16.1.1<br>Topic segmentation 380                                      |     |
|  | 16.1.2<br>Functional segmentation 381                                 |     |
|  | 16.2 Entities and reference 381                                       |     |
|  | 16.2.1<br>Centering theory<br>382<br>16.2.2<br>The entity grid<br>383 |     |
|  |                                                                       |     |

<span id="page-9-0"></span>

| 16.2.3         | *Formal semantics beyond the sentence level 384 |  |
|----------------|-------------------------------------------------|--|
| 16.3 Relations | 385                                             |  |
| 16.3.1         | Shallow discourse relations<br>385              |  |
| 16.3.2         | Hierarchical discourse relations 389            |  |
| 16.3.3         | Argumentation<br>392                            |  |
| 16.3.4         | Applications of discourse relations 393         |  |

### **[IV Applications](#page-418-0) 401**

|  | 17 Information extraction                                                             | 403 |
|--|---------------------------------------------------------------------------------------|-----|
|  | 17.1 Entities<br>405                                                                  |     |
|  | 17.1.1<br>Entity linking by learning to rank<br>406                                   |     |
|  | 17.1.2<br>Collective entity linking<br>408                                            |     |
|  | 17.1.3<br>*Pairwise ranking loss functions<br>409                                     |     |
|  | 17.2 Relations<br>411                                                                 |     |
|  | 17.2.1<br>Pattern-based relation extraction<br>412                                    |     |
|  | 17.2.2<br>Relation extraction as a classification task 413                            |     |
|  | 17.2.3<br>Knowledge base population 416                                               |     |
|  | 17.2.4<br>Open information extraction<br>419                                          |     |
|  | 17.3 Events<br>420                                                                    |     |
|  | 17.4 Hedges, denials, and hypotheticals 422                                           |     |
|  | 17.5 Question answering and machine reading 424                                       |     |
|  | 17.5.1<br>Formal semantics 424                                                        |     |
|  | 17.5.2<br>Machine reading<br>425                                                      |     |
|  | 18 Machine translation                                                                | 431 |
|  |                                                                                       |     |
|  |                                                                                       |     |
|  | 18.1 Machine translation as a task<br>431<br>18.1.1<br>Evaluating translations<br>433 |     |
|  | 18.1.2<br>Data<br>435                                                                 |     |
|  | 18.2 Statistical machine translation 436                                              |     |
|  | 18.2.1<br>Statistical translation modeling 437                                        |     |
|  | 18.2.2<br>Estimation 438                                                              |     |
|  | 18.2.3<br>Phrase-based translation 439                                                |     |
|  | 18.2.4<br>*Syntax-based translation<br>441                                            |     |
|  | 18.3 Neural machine translation<br>442                                                |     |
|  | 18.3.1<br>Neural attention<br>444                                                     |     |
|  | 18.3.2<br>*Neural machine translation without recurrence<br>446                       |     |
|  | 18.3.3<br>Out-of-vocabulary words<br>448                                              |     |
|  | 18.4 Decoding 449                                                                     |     |
|  | 18.5 Training towards the evaluation metric<br>451                                    |     |

<span id="page-10-0"></span>

|   |     | 19 Text generation                                              | 457 |
|---|-----|-----------------------------------------------------------------|-----|
|   |     | 19.1 Data-to-text generation 457                                |     |
|   |     | 19.1.1<br>Latent data-to-text alignment<br>459                  |     |
|   |     | 19.1.2<br>Neural data-to-text generation<br>460                 |     |
|   |     | 19.2 Text-to-text generation<br>464                             |     |
|   |     | 19.2.1<br>Neural abstractive summarization<br>464               |     |
|   |     | 19.2.2<br>Sentence fusion for multi-document summarization 465  |     |
|   |     | 19.3 Dialogue<br>466                                            |     |
|   |     | 19.3.1<br>Finite-state and agenda-based dialogue systems<br>467 |     |
|   |     | 19.3.2<br>Markov decision processes<br>468                      |     |
|   |     | 19.3.3<br>Neural chatbots 470                                   |     |
| A |     | Probability                                                     | 475 |
|   | A.1 | Probabilities of event combinations 475                         |     |
|   |     | A.1.1<br>Probabilities of disjoint events<br>476                |     |
|   |     | A.1.2<br>Law of total probability<br>477                        |     |
|   | A.2 | Conditional probability and Bayes' rule<br>477                  |     |
|   | A.3 | Independence<br>479                                             |     |
|   | A.4 | Random variables 480                                            |     |
|   | A.5 | Expectations 481                                                |     |
|   | A.6 | Modeling and estimation 482                                     |     |
| B |     | Numerical optimization                                          | 485 |
|   | B.1 | Gradient descent<br>486                                         |     |
|   | B.2 | Constrained optimization<br>486                                 |     |
|   | B.3 | Example: Passive-aggressive online learning<br>487              |     |
|   |     | Bibliography                                                    | 489 |

# <span id="page-12-0"></span>**Preface**

The goal of this text is focus on a core subset of the natural language processing, unified by the concepts of learning and search. A remarkable number of problems in natural language processing can be solved by a compact set of methods:

- **Search.** Viterbi, CKY, minimum spanning tree, shift-reduce, integer linear programming, beam search.
- **Learning.** Maximum-likelihood estimation, logistic regression, perceptron, expectationmaximization, matrix factorization, backpropagation.

This text explains how these methods work, and how they can be applied to a wide range of tasks: document classification, word sense disambiguation, part-of-speech tagging, named entity recognition, parsing, coreference resolution, relation extraction, discourse analysis, language modeling, and machine translation.

# <span id="page-12-1"></span>**Background**

Because natural language processing draws on many different intellectual traditions, almost everyone who approaches it feels underprepared in one way or another. Here is a summary of what is expected, and where you can learn more:

**Mathematics and machine learning.** The text assumes a background in multivariate calculus and linear algebra: vectors, matrices, derivatives, and partial derivatives. You should also be familiar with probability and statistics. A review of basic probability is found in [Appendix A,](#page-492-0) and a minimal review of numerical optimization is found in [Appendix B.](#page-502-0) For linear algebra, the online course and textbook from [Strang](#page-564-0) [\(2016\)](#page-564-0) provide an excellent review. [Deisenroth et al.](#page-524-0) [\(2018\)](#page-524-0) are currently preparing a textbook on *Mathematics for Machine Learning*, a draft can be found online.[1](#page-12-2) For an introduction to probabilistic modeling and estimation, see [James et al.](#page-537-0) [\(2013\)](#page-537-0); for

<span id="page-12-2"></span><sup>1</sup><https://mml-book.github.io/>

a more advanced and comprehensive discussion of the same material, the classic reference is [Hastie et al.](#page-533-0) [\(2009\)](#page-533-0).

- **Linguistics.** This book assumes no formal training in linguistics, aside from elementary concepts likes nouns and verbs, which you have probably encountered in the study of English grammar. Ideas from linguistics are introduced throughout the text as needed, including discussions of morphology and syntax [\(chapter 9\)](#page-208-0), semantics (chapters [12](#page-302-0) and [13\)](#page-322-0), and discourse [\(chapter 16\)](#page-396-0). Linguistic issues also arise in the application-focused chapters [4,](#page-86-0) [8,](#page-192-0) and [18.](#page-448-0) A short guide to linguistics for students of natural language processing is offered by [Bender](#page-513-0) [\(2013\)](#page-513-0); you are encouraged to start there, and then pick up a more comprehensive introductory textbook (e.g., [Ak](#page-510-0)[majian et al., 2010;](#page-510-0) [Fromkin et al., 2013\)](#page-529-0).
- **Computer science.** The book is targeted at computer scientists, who are assumed to have taken introductory courses on the analysis of algorithms and complexity theory. In particular, you should be familiar with asymptotic analysis of the time and memory costs of algorithms, and with the basics of dynamic programming. The classic text on algorithms is offered by [Cormen et al.](#page-522-0) [\(2009\)](#page-522-0); for an introduction to the theory of computation, see [Arora and Barak](#page-511-0) [\(2009\)](#page-511-0) and [Sipser](#page-561-0) [\(2012\)](#page-561-0).

#### <span id="page-13-0"></span>**How to use this book**

After the introduction, the textbook is organized into four main units:

- **Learning.** This section builds up a set of machine learning tools that will be used throughout the other sections. Because the focus is on machine learning, the text representations and linguistic phenomena are mostly simple: "bag-of-words" text classification is treated as a model example. Chapter [4](#page-86-0) describes some of the more linguistically interesting applications of word-based text analysis.
- **Sequences and trees.** This section introduces the treatment of language as a structured phenomena. It describes sequence and tree representations and the algorithms that they facilitate, as well as the limitations that these representations impose. Chapter [9](#page-208-0) introduces finite state automata and briefly overviews a context-free account of English syntax.
- **Meaning.** This section takes a broad view of efforts to represent and compute meaning from text, ranging from formal logic to neural word embeddings. It also includes two topics that are closely related to semantics: resolution of ambiguous references, and analysis of multi-sentence discourse structure.
- **Applications.** The final section offers chapter-length treatments on three of the most prominent applications of natural language processing: information extraction, machine

translation, and text generation. Each of these applications merits a textbook length treatment of its own [\(Koehn, 2009;](#page-541-0) [Grishman, 2012;](#page-532-0) [Reiter and Dale, 2000\)](#page-557-0); the chapters here explain some of the most well known systems using the formalisms and methods built up earlier in the book, while introducing methods such as neural attention.

Each chapter contains some advanced material, which is marked with an asterisk. This material can be safely omitted without causing misunderstandings later on. But even without these advanced sections, the text is too long for a single semester course, so instructors will have to pick and choose among the chapters.

Chapters [1-](#page-18-0)[3](#page-64-0) provide building blocks that will be used throughout the book, and [chap](#page-86-0)[ter 4](#page-86-0) describes some critical aspects of the practice of language technology. Language models [\(chapter 6\)](#page-142-0), sequence labeling [\(chapter 7\)](#page-162-0), and parsing [\(chapter 10](#page-242-0) and [11\)](#page-274-0) are canonical topics in natural language processing, and distributed word embeddings [\(chap](#page-342-0)[ter 14\)](#page-342-0) have become ubiquitous. Of the applications, machine translation [\(chapter 18\)](#page-448-0) is the best choice: it is more cohesive than information extraction, and more mature than text generation. Many students will benefit from the review of probability in [Appendix A.](#page-492-0)

- A course focusing on machine learning should add the chapter on unsupervised learning [\(chapter 5\)](#page-112-0). The chapters on predicate-argument semantics [\(chapter 13\)](#page-322-0), reference resolution [\(chapter 15\)](#page-368-0), and text generation [\(chapter 19\)](#page-474-0) are particularly influenced by recent progress in machine learning, including deep neural networks and learning to search.
- A course with a more linguistic orientation should add the chapters on applications of sequence labeling [\(chapter 8\)](#page-192-0), formal language theory [\(chapter 9\)](#page-208-0), semantics [\(chapter 12](#page-302-0) and [13\)](#page-322-0), and discourse [\(chapter 16\)](#page-396-0).
- For a course with a more applied focus, I recommend the chapters on applications of sequence labeling [\(chapter 8\)](#page-192-0), predicate-argument semantics [\(chapter 13\)](#page-322-0), information extraction [\(chapter 17\)](#page-420-0), and text generation [\(chapter 19\)](#page-474-0).

### **Acknowledgments**

Several colleagues, students, and friends read early drafts of chapters in their areas of expertise, including Yoav Artzi, Kevin Duh, Heng Ji, Jessy Li, Brendan O'Connor, Yuval Pinter, Shawn Ling Ramirez, Nathan Schneider, Pamela Shapiro, Noah A. Smith, Sandeep Soni, and Luke Zettlemoyer. I also thank the anonymous reviewers, particularly reviewer 4, who provided detailed line-by-line edits and suggestions. The text benefited from highlevel discussions with my editor Marie Lufkin Lee, as well as Kevin Murphy, Shawn Ling Ramirez, and Bonnie Webber. In addition, there are many students, colleagues, friends, and family who found mistakes in early drafts, or who recommended key references.

These include: Parminder Bhatia, Kimberly Caras, Jiahao Cai, Justin Chen, Rodolfo Delmonte, Murtaza Dhuliawala, Yantao Du, Barbara Eisenstein, Luiz C. F. Ribeiro, Chris Gu, Joshua Killingsworth, Jonathan May, Taha Merghani, Gus Monod, Raghavendra Murali, Nidish Nair, Brendan O'Connor, Dan Oneata, Brandon Peck, Yuval Pinter, Nathan Schneider, Jianhao Shen, Zhewei Sun, Rubin Tsui, Ashwin Cunnapakkam Vinjimur, Denny Vrandeciˇ c, William Yang Wang, Clay Washington, Ishan Waykul, Aobo Yang, Xavier Yao, ´ Yuyu Zhang, and several anonymous commenters. Clay Washington tested some of the programming exercises, and Varun Gupta tested some of the written exercises. Thanks to Kelvin Xu for sharing a high-resolution version of [Figure 19.3.](#page-478-0)

Most of the book was written while I was at Georgia Tech's School of Interactive Computing. I thank the School for its support of this project, and I thank my colleagues there for their help and support at the beginning of my faculty career. I also thank (and apologize to) the many students in Georgia Tech's CS 4650 and 7650 who suffered through early versions of the text. The book is dedicated to my parents.

# Notation

As a general rule, words, word counts, and other types of observations are indicated with Roman letters (a, b, c); parameters are indicated with Greek letters ( $\alpha$ ,  $\beta$ ,  $\theta$ ). Vectors are indicated with bold script for both random variables x and parameters  $\theta$ . Other useful notations are indicated in the table below.

| Basics                                        |                                  |
|-----------------------------------------------|----------------------------------|
| $\exp x$                                      | the base-2 exponent, $2^x$       |
| $\log x$                                      | the base-2 logarithm, $\log_2 x$ |
| ${x_n}_{n=1}^N$                               | the set $\{x_1, x_2,, x_N\}$     |
| $x_i^j$                                       | $x_i$ raised to the power $j$    |
| $ \{ x_n \}_{n=1}^N $ $ x_i^j $ $ x_i^{(j)} $ | indexing by both $i$ and $j$     |

| 0                                           |                                                                                                                                 |
|---------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|
| $oldsymbol{x}^{(i)}$                        | a column vector of feature counts for instance <i>i</i> , often word counts                                                     |
| $oldsymbol{x}_{j:k}$                        | elements $j$ through $k$ (inclusive) of a vector $\boldsymbol{x}$                                                               |
| $[oldsymbol{x};oldsymbol{y}]$               | vertical concatenation of two column vectors                                                                                    |
| $[\boldsymbol{x}, \boldsymbol{y}]$          | horizontal concatenation of two column vectors                                                                                  |
| $e_n$                                       | a "one-hot" vector with a value of 1 at position $n$ , and zero everywhere                                                      |
| $oldsymbol{	heta}^	op$                      | else the transpose of a column vector $\boldsymbol{\theta}$                                                                     |
| $oldsymbol{	heta} \cdot oldsymbol{x}^{(i)}$ | the dot product $\sum_{i=1}^{N} \theta_j \times x_i^{(i)}$                                                                      |
| $\mathbf{X}$                                | a matrix                                                                                                                        |
| $x_{i,j}$                                   | row <i>i</i> , column <i>j</i> of matrix $\mathbf{X}$                                                                           |
|                                             | $\begin{pmatrix} x_1 & 0 & 0 \end{pmatrix}$                                                                                     |
| $Diag(\boldsymbol{x})$                      | a matrix with $\boldsymbol{x}$ on the diagonal, e.g., $\begin{pmatrix} x_1 & 0 & 0 \\ 0 & x_2 & 0 \\ 0 & 0 & x_2 \end{pmatrix}$ |
| $\mathbf{X}^{-1}$                           | the inverse of matrix <b>X</b>                                                                                                  |

| Text dataset                | S                                                                 |
|-----------------------------|-------------------------------------------------------------------|
| $w_m$                       | word token at position <i>m</i>                                   |
| N                           | number of training instances                                      |
| M                           | length of a sequence (of words or tags)                           |
| V                           | number of words in vocabulary                                     |
| $y^{(i)}$                   | the true label for instance <i>i</i>                              |
| $\hat{\hat{y}}$             | a predicted label                                                 |
| ${\cal Y}$                  | the set of all possible labels                                    |
| K                           | number of possible labels $K =  \mathcal{Y} $                     |
|                             | the start token                                                   |
|                             | the stop token                                                    |
| $oldsymbol{y}^{(i)}$        | a structured label for instance <i>i</i> , such as a tag sequence |
| $\mathcal{Y}(oldsymbol{w})$ | the set of possible labelings for the word sequence $w$           |
| $\diamond$                  | the start tag                                                     |
| <b>♦</b>                    | the stop tag                                                      |

| Probabilitie        | S                                                                                                                                                   |
|---------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|
| $\Pr(A)$            | probability of event A                                                                                                                              |
| $\Pr(A \mid B)$     | probability of event $A$ , conditioned on event $B$                                                                                                 |
| $p_B(b)$            | the marginal probability of random variable <i>B</i> taking value <i>b</i> ; written                                                                |
|                     | p(b) when the choice of random variable is clear from context                                                                                       |
| $p_{B A}(b \mid a)$ | the probability of random variable $B$ taking value $b$ , conditioned on $A$                                                                        |
| -                   | taking value $a$ ; written $p(b \mid a)$ when clear from context                                                                                    |
| $A \sim p$          | the random variable $A$ is distributed according to distribution $p$ . For                                                                          |
|                     | example, $X \sim \mathcal{N}(0, 1)$ states that the random variable X is drawn from                                                                 |
| $A \mid B \sim p$   | a normal distribution with zero mean and unit variance. conditioned on the random variable $B$ , $A$ is distributed according to $p$ . <sup>2</sup> |

# Machine learning

| $\Psi(oldsymbol{x}^{(i)},y)$         | the score for assigning label $y$ to instance $i$  |
|--------------------------------------|----------------------------------------------------|
| $oldsymbol{f}(oldsymbol{x}^{(i)},y)$ | the feature vector for instance $i$ with label $y$ |
| $oldsymbol{	heta}$                   | a (column) vector of weights                       |
| $\ell^{(i)}$                         | loss on an individual instance <i>i</i>            |
| L                                    | objective function for an entire dataset           |
| $\mathcal{L}$                        | log-likelihood of a dataset                        |
| $\lambda$                            | the amount of regularization                       |

# <span id="page-18-0"></span>**Chapter 1 Introduction**

Natural language processing is the set of methods for making human language accessible to computers. In the past decade, natural language processing has become embedded in our daily lives: automatic machine translation is ubiquitous on the web and in social media; text classification keeps our email inboxes from collapsing under a deluge of spam; search engines have moved beyond string matching and network analysis to a high degree of linguistic sophistication; dialog systems provide an increasingly common and effective way to get and share information.

These diverse applications are based on a common set of ideas, drawing on algorithms, linguistics, logic, statistics, and more. The goal of this text is to provide a survey of these foundations. The technical fun starts in the next chapter; the rest of this current chapter situates natural language processing with respect to other intellectual disciplines, identifies some high-level themes in contemporary natural language processing, and advises the reader on how best to approach the subject.

# <span id="page-18-1"></span>**1.1 Natural language processing and its neighbors**

Natural language processing draws on many other intellectual traditions, from formal linguistics to statistical physics. This section briefly situates natural language processing with respect to some of its closest neighbors.

**Computational Linguistics** Most of the meetings and journals that host natural language processing research bear the name "computational linguistics", and the terms may be thought of as essentially synonymous. But while there is substantial overlap, there is an important difference in focus. In linguistics, language is the object of study. Computational methods may be brought to bear, just as in scientific disciplines like computational biology and computational astronomy, but they play only a supporting role. In contrast, natural language processing is focused on the design and analysis of computational algorithms and representations for processing natural human language. The goal of natural language processing is to provide new computational capabilities around human language: for example, extracting information from texts, translating between languages, answering questions, holding a conversation, taking instructions, and so on. Fundamental linguistic insights may be crucial for accomplishing these tasks, but success is ultimately measured by whether and how well the job gets done.

**Machine Learning** Contemporary approaches to natural language processing rely heavily on machine learning, which makes it possible to build complex computer programs from examples. Machine learning provides an array of general techniques for tasks like converting a sequence of discrete tokens in one vocabulary to a sequence of discrete tokens in another vocabulary — a generalization of what one might informally call "translation." Much of today's natural language processing research can be thought of as applied machine learning. However, natural language processing has characteristics that distinguish it from many of machine learning's other application domains.

- Unlike images or audio, text data is fundamentally discrete, with meaning created by combinatorial arrangements of symbolic units. This is particularly consequential for applications in which text is the output, such as translation and summarization, because it is not possible to gradually approach an optimal solution.
- Although the set of words is discrete, new words are always being created. Furthermore, the distribution over words (and other linguistic elements) resembles that of a **power law**[1](#page-19-0) [\(Zipf, 1949\)](#page-573-0): there will be a few words that are very frequent, and a long tail of words that are rare. A consequence is that natural language processing algorithms must be especially robust to observations that do not occur in the training data.
- Language is **compositional**: units such as words can combine to create phrases, which can combine by the very same principles to create larger phrases. For example, a **noun phrase** can be created by combining a smaller noun phrase with a **prepositional phrase**, as in *the whiteness of the whale*. The prepositional phrase is created by combining a preposition (in this case, *of*) with another noun phrase (*the whale*). In this way, it is possible to create arbitrarily long phrases, such as,
  - (1.1) . . . huge globular pieces of the whale of the bigness of a human head.[2](#page-19-1)

The meaning of such a phrase must be analyzed in accord with the underlying hierarchical structure. In this case, *huge globular pieces of the whale* acts as a single noun

<span id="page-19-0"></span><sup>1</sup>Throughout the text, **boldface** will be used to indicate keywords that appear in the index.

<span id="page-19-1"></span><sup>2</sup>Throughout the text, this notation will be used to introduce linguistic examples.

#### 1.1. NATURAL LANGUAGE PROCESSING AND ITS NEIGHBORS 3

phrase, which is conjoined with the prepositional phrase *of the bigness of a human head*. The interpretation would be different if instead, *huge globular pieces* were conjoined with the prepositional phrase *of the whale of the bigness of a human head* implying a disappointingly small whale. Even though text appears as a sequence, machine learning methods must account for its implicit recursive structure.

**Artificial Intelligence** The goal of artificial intelligence is to build software and robots with the same range of abilities as humans [\(Russell and Norvig, 2009\)](#page-559-0). Natural language processing is relevant to this goal in several ways. On the most basic level, the capacity for language is one of the central features of human intelligence, and is therefore a prerequisite for artificial intelligence.[3](#page-20-0) Second, much of artificial intelligence research is dedicated to the development of systems that can reason from premises to a conclusion, but such algorithms are only as good as what they know [\(Dreyfus, 1992\)](#page-525-0). Natural language processing is a potential solution to the "knowledge bottleneck", by acquiring knowledge from texts, and perhaps also from conversations. This idea goes all the way back to Turing's 1949 paper *Computing Machinery and Intelligence*, which proposed the **Turing test** for determining whether artificial intelligence had been achieved [\(Turing, 2009\)](#page-566-0).

Conversely, reasoning is sometimes essential for basic tasks of language processing, such as resolving a pronoun. **Winograd schemas** are examples in which a single word changes the likely referent of a pronoun, in a way that seems to require knowledge and reasoning to decode [\(Levesque et al., 2011\)](#page-543-0). For example,

(1.2) The trophy doesn't fit into the brown suitcase because **it** is too [small/large].

When the final word is *small*, then the pronoun *it* refers to the suitcase; when the final word is *large*, then *it* refers to the trophy. Solving this example requires spatial reasoning; other schemas require reasoning about actions and their effects, emotions and intentions, and social conventions.

Such examples demonstrate that natural language understanding cannot be achieved in isolation from knowledge and reasoning. Yet the history of artificial intelligence has been one of increasing specialization: with the growing volume of research in subdisciplines such as natural language processing, machine learning, and computer vision, it is

<span id="page-20-0"></span><sup>3</sup>This view is shared by some, but not all, prominent researchers in artificial intelligence. Michael Jordan, a specialist in machine learning, has said that if he had a billion dollars to spend on any large research project, he would spend it on natural language processing ([https://www.reddit.com/r/](https://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/) [MachineLearning/comments/2fxi6v/ama\\_michael\\_i\\_jordan/](https://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/)). On the other hand, in a public discussion about the future of artificial intelligence in February 2018, computer vision researcher Yann Lecun argued that despite its many practical applications, language is perhaps "number 300" in the priority list for artificial intelligence research, and that it would be a great achievement if AI could attain the capabilities of an orangutan, which do not include language ([http://www.abigailsee.com/2018/02/21/](http://www.abigailsee.com/2018/02/21/deep-learning-structure-and-innate-priors.html) [deep-learning-structure-and-innate-priors.html](http://www.abigailsee.com/2018/02/21/deep-learning-structure-and-innate-priors.html)).

difficult for anyone to maintain expertise across the entire field. Still, recent work has demonstrated interesting connections between natural language processing and other areas of AI, including computer vision (e.g., [Antol et al., 2015\)](#page-511-1) and game playing (e.g., [Branavan et al., 2009\)](#page-516-0). The dominance of machine learning throughout artificial intelligence has led to a broad consensus on representations such as graphical models and computation graphs, and on algorithms such as backpropagation and combinatorial optimization. Many of the algorithms and representations covered in this text are part of this consensus.

**Computer Science** The discrete and recursive nature of natural language invites the application of theoretical ideas from computer science. Linguists such as Chomsky and Montague have shown how formal language theory can help to explain the syntax and semantics of natural language. Theoretical models such as finite-state and pushdown automata are the basis for many practical natural language processing systems. Algorithms for searching the combinatorial space of analyses of natural language utterances can be analyzed in terms of their computational complexity, and theoretically motivated approximations can sometimes be applied.

The study of computer systems is also relevant to natural language processing. Large datasets of unlabeled text can be processed more quickly by parallelization techniques like MapReduce [\(Dean and Ghemawat, 2008;](#page-524-1) [Lin and Dyer, 2010\)](#page-544-0); high-volume data sources such as social media can be summarized efficiently by approximate streaming and sketching techniques [\(Goyal et al., 2009\)](#page-531-0). When deep neural networks are implemented in production systems, it is possible to eke out speed gains using techniques such as reduced-precision arithmetic [\(Wu et al., 2016\)](#page-570-0). Many classical natural language processing algorithms are not naturally suited to graphics processing unit (GPU) parallelization, suggesting directions for further research at the intersection of natural language processing and computing hardware [\(Yi et al., 2011\)](#page-571-0).

**Speech Processing** Natural language is often communicated in spoken form, and speech recognition is the task of converting an audio signal to text. From one perspective, this is a signal processing problem, which might be viewed as a preprocessing step before natural language processing can be applied. However, context plays a critical role in speech recognition by human listeners: knowledge of the surrounding words influences perception and helps to correct for noise [\(Miller et al., 1951\)](#page-549-0). For this reason, speech recognition is often integrated with text analysis, particularly with statistical **language models**, which quantify the probability of a sequence of text (see [chapter 6\)](#page-142-0). Beyond speech recognition, the broader field of speech processing includes the study of speech-based dialogue systems, which are briefly discussed in [chapter 19.](#page-474-0) Historically, speech processing has often been pursued in electrical engineering departments, while natural language processing

has been the purview of computer scientists. For this reason, the extent of interaction between these two disciplines is less than it might otherwise be.

**Ethics** As machine learning and artificial intelligence become increasingly ubiquitous, it is crucial to understand how their benefits, costs, and risks are distributed across different kinds of people. Natural language processing raises some particularly salient issues around **ethics, fairness, and accountability**:

- **Access.** Who is natural language processing designed to serve? For example, whose language is translated *from*, and whose language is translated *to*?
- **Bias.** Does language technology learn to replicate social biases from text corpora, and does it reinforce these biases as seemingly objective computational conclusions?
- **Labor.** Whose text and speech comprise the datasets that power natural language processing, and who performs the annotations? Are the benefits of this technology shared with all the people whose work makes it possible?
- **Privacy and internet freedom.** What is the impact of large-scale text processing on the right to free and private communication? What is the potential role of natural language processing in regimes of censorship or surveillance?

This text lightly touches on issues related to fairness and bias in § [14.6.3](#page-357-0) and § [18.1.1,](#page-450-0) but these issues are worthy of a book of their own. For more from within the field of computational linguistics, see the papers from the annual workshop on Ethics in Natural Language Processing [\(Hovy et al., 2017;](#page-535-0) [Alfano et al., 2018\)](#page-510-1). For an outside perspective on ethical issues relating to data science at large, see [boyd and Crawford](#page-516-1) [\(2012\)](#page-516-1).

**Others** Natural language processing plays a significant role in emerging interdisciplinary fields like **computational social science** and the **digital humanities**. Text classification [\(chapter 4\)](#page-86-0), clustering [\(chapter 5\)](#page-112-0), and information extraction [\(chapter 17\)](#page-420-0) are particularly useful tools; another is **probabilistic topic models** [\(Blei, 2012\)](#page-514-0), which are not covered in this text. **Information retrieval** [\(Manning et al., 2008\)](#page-546-0) makes use of similar tools, and conversely, techniques such as latent semantic analysis (§ [14.3\)](#page-346-1) have roots in information retrieval. **Text mining** is sometimes used to refer to the application of data mining techniques, especially classification and clustering, to text. While there is no clear distinction between text mining and natural language processing (nor between data mining and machine learning), text mining is typically less concerned with linguistic structure, and more interested in fast, scalable algorithms.

# <span id="page-23-0"></span>1.2 Three themes in natural language processing

Natural language processing covers a diverse range of tasks, methods, and linguistic phenomena. But despite the apparent incommensurability between, say, the summarization of scientific articles (§ 16.3.4) and the identification of suffix patterns in Spanish verbs (§ 9.1.4), some general themes emerge. The remainder of the introduction focuses on these themes, which will recur in various forms through the text. Each theme can be expressed as an opposition between two extreme viewpoints on how to process natural language. The methods discussed in the text can usually be placed somewhere on the continuum between these two extremes.

## <span id="page-23-1"></span>1.2.1 Learning and knowledge

A recurring topic of debate is the relative importance of machine learning and linguistic knowledge. On one extreme, advocates of "natural language processing from scratch" (Collobert et al., 2011) propose to use machine learning to train end-to-end systems that transmute raw text into any desired output structure: e.g., a summary, database, or translation. On the other extreme, the core work of natural language processing is sometimes taken to be transforming text into a stack of general-purpose linguistic structures: from subword units called **morphemes**, to word-level **parts-of-speech**, to tree-structured representations of grammar, and beyond, to logic-based representations of meaning. In theory, these general-purpose structures should then be able to support any desired application.

The end-to-end approach has been buoyed by recent results in computer vision and speech recognition, in which advances in machine learning have swept away expertengineered representations based on the fundamentals of optics and phonology (Krizhevsky et al., 2012; Graves and Jaitly, 2014). But while machine learning is an element of nearly every contemporary approach to natural language processing, linguistic representations such as syntax trees have not yet gone the way of the visual edge detector or the auditory triphone. Linguists have argued for the existence of a "language faculty" in all human beings, which encodes a set of abstractions specially designed to facilitate the understanding and production of language. The argument for the existence of such a language faculty is based on the observation that children learn language faster and from fewer examples than would be possible if language was learned from experience alone.<sup>4</sup> From a practical standpoint, linguistic structure seems to be particularly important in scenarios where training data is limited.

There are a number of ways in which knowledge and learning can be combined in natural language processing. Many supervised learning systems make use of carefully engineered **features**, which transform the data into a representation that can facilitate

<span id="page-23-2"></span><sup>&</sup>lt;sup>4</sup>*The Language Instinct* (Pinker, 2003) articulates these arguments in an engaging and popular style. For arguments against the innateness of language, see Elman et al. (1998).

#### 1.2. THREE THEMES IN NATURAL LANGUAGE PROCESSING 7

learning. For example, in a task like search, it may be useful to identify each word's **stem**, so that a system can more easily generalize across related terms such as *whale*, *whales*, *whalers*, and *whaling*. (This issue is relatively benign in English, as compared to the many other languages which include much more elaborate systems of prefixed and suffixes.) Such features could be obtained from a hand-crafted resource, like a dictionary that maps each word to a single root form. Alternatively, features can be obtained from the output of a general-purpose language processing system, such as a parser or part-of-speech tagger, which may itself be built on supervised machine learning.

Another synthesis of learning and knowledge is in model structure: building machine learning models whose architectures are inspired by linguistic theories. For example, the organization of sentences is often described as **compositional**, with meaning of larger units gradually constructed from the meaning of their smaller constituents. This idea can be built into the architecture of a deep neural network, which is then trained using contemporary deep learning techniques [\(Dyer et al., 2016\)](#page-526-1).

The debate about the relative importance of machine learning and linguistic knowledge sometimes becomes heated. No machine learning specialist likes to be told that their engineering methodology is unscientific alchemy;[5](#page-24-1) nor does a linguist want to hear that the search for general linguistic principles and structures has been made irrelevant by big data. Yet there is clearly room for both types of research: we need to know how far we can go with end-to-end learning alone, while at the same time, we continue the search for linguistic representations that generalize across applications, scenarios, and languages. For more on the history of this debate, see [Church](#page-520-0) [\(2011\)](#page-520-0); for an optimistic view of the potential symbiosis between computational linguistics and deep learning, see [Manning](#page-546-1) [\(2015\)](#page-546-1).

#### <span id="page-24-0"></span>**1.2.2 Search and learning**

Many natural language processing problems can be written mathematically in the form of optimization,[6](#page-24-2)

$$\hat{\boldsymbol{y}} = \operatorname*{argmax}_{\boldsymbol{y} \in \mathcal{Y}(\boldsymbol{x})} \Psi(\boldsymbol{x}, \boldsymbol{y}; \boldsymbol{\theta}),$$
[1.1]

where,

- x is the input, which is an element of a set X ;
- <span id="page-24-1"></span>• y is the output, which is an element of a set Y(x);

<sup>5</sup>Ali Rahimi argued that much of deep learning research was similar to "alchemy" in a presentation at the 2017 conference on Neural Information Processing Systems. He was advocating for more learning theory, not more linguistics.

<span id="page-24-2"></span><sup>6</sup>Throughout this text, equations will be numbered by square brackets, and linguistic examples will be numbered by parentheses.

- Ψ is a scoring function (also called the **model**), which maps from the set X × Y to the real numbers;
- *θ* is a vector of parameters for Ψ;
- $\hat{y}$  is the predicted output, which is chosen to maximize the scoring function.

This basic structure can be applied to a huge range of problems. For example, the input x might be a social media post, and the output y might be a labeling of the emotional sentiment expressed by the author (chapter 4); or x could be a sentence in French, and the output y could be a sentence in Tamil (chapter 18); or x might be a sentence in English, and y might be a representation of the syntactic structure of the sentence (chapter 10); or x might be a news article and y might be a structured record of the events that the article describes (chapter 17).

This formulation reflects an implicit decision that language processing algorithms will have two distinct modules:

- **Search.** The search module is responsible for computing the argmax of the function  $\Psi$ . In other words, it finds the output  $\hat{y}$  that gets the best score with respect to the input x. This is easy when the search space  $\mathcal{Y}(x)$  is small enough to enumerate, or when the scoring function  $\Psi$  has a convenient decomposition into parts. In many cases, we will want to work with scoring functions that do not have these properties, motivating the use of more sophisticated search algorithms, such as bottom-up dynamic programming (§ 10.1) and beam search (§ 11.3.1). Because the outputs are usually discrete in language processing problems, search often relies on the machinery of **combinatorial optimization**.
- **Learning.** The learning module is responsible for finding the parameters  $\theta$ . This is typically (but not always) done by processing a large dataset of labeled examples,  $\{(x^{(i)}, y^{(i)})\}_{i=1}^N$ . Like search, learning is also approached through the framework of optimization, as we will see in chapter 2. Because the parameters are usually continuous, learning algorithms generally rely on **numerical optimization** to identify vectors of real-valued parameters that optimize some function of the model and the labeled data. Some basic principles of numerical optimization are reviewed in Appendix B.

The division of natural language processing into separate modules for search and learning makes it possible to reuse generic algorithms across many tasks and models. Much of the work of natural language processing can be focused on the design of the model  $\Psi$  — identifying and formalizing the linguistic phenomena that are relevant to the task at hand — while reaping the benefits of decades of progress in search, optimization, and learning. This textbook will describe several classes of scoring functions, and the corresponding algorithms for search and learning.

#### 1.2. THREE THEMES IN NATURAL LANGUAGE PROCESSING 9

When a model is capable of making subtle linguistic distinctions, it is said to be *expressive*. Expressiveness is often traded off against efficiency of search and learning. For example, a word-to-word translation model makes search and learning easy, but it is not expressive enough to distinguish good translations from bad ones. Many of the most important problems in natural language processing seem to require expressive models, in which the complexity of search grows exponentially with the size of the input. In these models, exact search is usually impossible. Intractability threatens the neat modular decomposition between search and learning: if search requires a set of heuristic approximations, then it may be advantageous to learn a model that performs well under these specific heuristics. This has motivated some researchers to take a more integrated approach to search and learning, as briefly mentioned in chapters [11](#page-274-0) and [15.](#page-368-0)

#### <span id="page-26-0"></span>**1.2.3 Relational, compositional, and distributional perspectives**

Any element of language — a word, a phrase, a sentence, or even a sound — can be described from at least three perspectives. Consider the word *journalist*. A *journalist* is a subcategory of a *profession*, and an *anchorwoman* is a subcategory of *journalist*; furthermore, a *journalist* performs *journalism*, which is often, but not always, a subcategory of *writing*. This relational perspective on meaning is the basis for semantic **ontologies** such as **WORDNET** [\(Fellbaum, 2010\)](#page-527-0), which enumerate the relations that hold between words and other elementary semantic units. The power of the relational perspective is illustrated by the following example:

(1.3) Umashanthi interviewed Ana. She works for the college newspaper.

Who works for the college newspaper? The word *journalist*, while not stated in the example, implicitly links the *interview* to the *newspaper*, making *Umashanthi* the most likely referent for the pronoun. (A general discussion of how to resolve pronouns is found in [chapter 15.](#page-368-0))

Yet despite the inferential power of the relational perspective, it is not easy to formalize computationally. Exactly which elements are to be related? Are *journalists* and *reporters* distinct, or should we group them into a single unit? Is the kind of *interview* performed by a journalist the same as the kind that one undergoes when applying for a job? Ontology designers face many such thorny questions, and the project of ontology design hearkens back to Borges' [\(1993\)](#page-515-0) *Celestial Emporium of Benevolent Knowledge*, which divides animals into:

(a) belonging to the emperor; (b) embalmed; (c) tame; (d) suckling pigs; (e) sirens; (f) fabulous; (g) stray dogs; (h) included in the present classification; (i) frenzied; (j) innumerable; (k) drawn with a very fine camelhair brush; (l) et cetera; (m) having just broken the water pitcher; (n) that from a long way off resemble flies.

Difficulties in ontology construction have led some linguists to argue that there is no taskindependent way to partition up word meanings [\(Kilgarriff, 1997\)](#page-539-0).

Some problems are easier. Each member in a group of *journalists* is a *journalist*: the *-s* suffix distinguishes the plural meaning from the singular in most of the nouns in English. Similarly, a *journalist* can be thought of, perhaps colloquially, as someone who produces or works on a *journal*. (Taking this approach even further, the word *journal* derives from the French *jour+nal*, or *day+ly = daily*.) In this way, the meaning of a word is constructed from the constituent parts — the principle of **compositionality**. This principle can be applied to larger units: phrases, sentences, and beyond. Indeed, one of the great strengths of the compositional view of meaning is that it provides a roadmap for understanding entire texts and dialogues through a single analytic lens, grounding out in the smallest parts of individual words.

But alongside *journalists* and *anti-parliamentarians*, there are many words that seem to be linguistic atoms: think, for example, of *whale*, *blubber*, and *Nantucket*. Idiomatic phrases like *kick the bucket* and *shoot the breeze* have meanings that are quite different from the sum of their parts [\(Sag et al., 2002\)](#page-559-1). Composition is of little help for such words and expressions, but their meanings can be ascertained — or at least approximated — from the contexts in which they appear. Take, for example, *blubber*, which appears in such contexts as:

- (1.4) a. The blubber served them as fuel.
  - b. . . . extracting it from the blubber of the large fish . . .
  - c. Amongst oily substances, blubber has been employed as a manure.

These contexts form the **distributional properties** of the word *blubber*, and they link it to words which can appear in similar constructions: *fat*, *pelts*, and *barnacles*. This distributional perspective makes it possible to learn about meaning from unlabeled data alone; unlike relational and compositional semantics, no manual annotation or expert knowledge is required. Distributional semantics is thus capable of covering a huge range of linguistic phenomena. However, it lacks precision: *blubber* is similar to *fat* in one sense, to *pelts* in another sense, and to *barnacles* in still another. The question of *why* all these words tend to appear in the same contexts is left unanswered.

The relational, compositional, and distributional perspectives all contribute to our understanding of linguistic meaning, and all three appear to be critical to natural language processing. Yet they are uneasy collaborators, requiring seemingly incompatible representations and algorithmic approaches. This text presents some of the best known and most successful methods for working with each of these representations, but future research may reveal new ways to combine them.

# **Part I**

# <span id="page-28-0"></span>**Learning**

# <span id="page-30-2"></span><span id="page-30-0"></span>Chapter 2

# Linear text classification

We begin with the problem of **text classification**: given a text document, assign it a discrete label  $y \in \mathcal{Y}$ , where  $\mathcal{Y}$  is the set of possible labels. Text classification has many applications, from spam filtering to the analysis of electronic health records. This chapter describes some of the most well known and effective algorithms for text classification, from a mathematical perspective that should help you understand what they do and why they work. Text classification is also a building block in more elaborate natural language processing tasks. For readers without a background in machine learning or statistics, the material in this chapter will take more time to digest than most of the subsequent chapters. But this investment will pay off as the mathematical principles behind these basic classification algorithms reappear in other contexts throughout the book.

# <span id="page-30-1"></span>2.1 The bag of words

To perform text classification, the first question is how to represent each document, or instance. A common approach is to use a column vector of word counts, e.g.,  $\boldsymbol{x} = [0, 1, 1, 0, 0, 2, 0, 1, 13, 0...]^{\top}$ , where  $x_j$  is the count of word j. The length of  $\boldsymbol{x}$  is  $V \triangleq |\mathcal{V}|$ , where  $\mathcal{V}$  is the set of possible words in the vocabulary. In linear classification, the classification decision is based on a weighted sum of individual feature counts, such as word counts.

The object x is a vector, but it is often called a **bag of words**, because it includes only information about the count of each word, and not the order in which the words appear. With the bag of words representation, we are ignoring grammar, sentence boundaries, paragraphs — everything but the words. Yet the bag of words model is surprisingly effective for text classification. If you see the word *whale* in a document, is it fiction or non-fiction? What if you see the word *molybdenum*? For many labeling problems, individual words can be strong predictors.

<span id="page-31-4"></span>To predict a label from a bag-of-words, we can assign a score to each word in the vocabulary, measuring the compatibility with the label. For example, for the label FICTION, we might assign a positive score to the word *whale*, and a negative score to the word *molybdenum*. These scores are called **weights**, and they are arranged in a column vector θ.

Suppose that you want a multiclass classifier, where <sup>K</sup> , |Y| <sup>&</sup>gt; <sup>2</sup>. For example, you might want to classify news stories about sports, celebrities, music, and business. The goal is to predict a label yˆ, given the bag of words x, using the weights θ. For each label y ∈ Y, we compute a score Ψ(x, y), which is a scalar measure of the compatibility between the bag-of-words x and the label y. In a linear bag-of-words classifier, this score is the vector inner product between the weights θ and the output of a **feature function** f(x, y),

<span id="page-31-1"></span>
$$\Psi(\boldsymbol{x}, y) = \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}, y) = \sum_{j} \theta_{j} f_{j}(\boldsymbol{x}, y).$$
[2.1]

As the notation suggests, f is a function of two arguments, the word counts x and the label y, and it returns a vector output. For example, given arguments x and y, element j of this feature vector might be,

$$f_j(\boldsymbol{x}, y) = \begin{cases} x_{whale}, & \text{if } y = \text{FICTION} \\ 0, & \text{otherwise} \end{cases}$$
[2.2]

This function returns the count of the word *whale* if the label is FICTION, and it returns zero otherwise. The index j depends on the position of *whale* in the vocabulary, and of FICTION in the set of possible labels. The corresponding weight θ<sup>j</sup> then scores the compatibility of the word *whale* with the label FICTION. [1](#page-31-0) A positive score means that this word makes the label more likely.

The output of the feature function can be formalized as a vector:

<span id="page-31-2"></span>
$$f(x, y = 1) = [x; \underbrace{0; 0; \dots; 0}_{(K-1) \times V}]$$
[2.3]

<span id="page-31-3"></span>
$$\boldsymbol{f}(\boldsymbol{x}, y = 2) = [\underbrace{0; 0; \dots; 0}_{V}; \boldsymbol{x}; \underbrace{0; 0; \dots; 0}_{(K-2) \times V}]$$
[2.4]

$$f(x, y = K) = [\underbrace{0; 0; \dots; 0}_{(K-1) \times V}; x],$$
 [2.5]

where [0; 0; . . . ; 0 | {z } (K−1)×V ] is a column vector of (K − 1) × V zeros, and the semicolon indicates

vertical concatenation. For each of the K possible labels, the feature function returns a

<span id="page-31-0"></span><sup>1</sup> In practice, both f and θ may be implemented as a dictionary rather than vectors, so that it is not necessary to explicitly identify j. In such an implementation, the tuple (*whale*, FICTION) acts as a key in both dictionaries; the values in f are feature counts, and the values in θ are weights.

#### <span id="page-32-1"></span>2.1. THE BAG OF WORDS 15

vector that is mostly zeros, with a column vector of word counts x inserted in a location that depends on the specific label y. This arrangement is shown in [Figure 2.1.](#page-33-0) The notation may seem awkward at first, but it generalizes to an impressive range of learning settings, particularly **structure prediction**, which is the focus of Chapters [7](#page-162-0)[-11.](#page-274-0)

Given a vector of weights, θ ∈ R V K, we can now compute the score Ψ(x, y) by [Equa](#page-31-1)[tion 2.1.](#page-31-1) This inner product gives a scalar measure of the compatibility of the observation x with label y. [2](#page-32-0) For any document x, we predict the label yˆ,

$$\hat{y} = \underset{y \in \mathcal{Y}}{\operatorname{argmax}} \Psi(\boldsymbol{x}, y)$$
[2.6]

$$\Psi(\boldsymbol{x}, y) = \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}, y).$$
 [2.7]

This inner product notation gives a clean separation between the *data* (x and y) and the *parameters* (θ).

While vector notation is used for presentation and analysis, in code the weights and feature vector can be implemented as dictionaries. The inner product can then be computed as a loop. In python:

```
def compute_score(x,y,weights):
    total = 0
    for feature,count in feature_function(x,y).items():
        total += weights[feature] * count
    return total
```

This representation is advantageous because it avoids storing and iterating over the many features whose counts are zero.

It is common to add an **offset feature** at the end of the vector of word counts x, which is always 1. We then have to also add an extra zero to each of the zero vectors, to make the vector lengths match. This gives the entire feature vector f(x, y) a length of (V + 1) × K. The weight associated with this offset feature can be thought of as a bias for or against each label. For example, if we expect most emails to be spam, then the weight for the offset feature for y = SPAM should be larger than the weight for the offset feature for y = NOT-SPAM.

Returning to the weights θ, where do they come from? One possibility is to set them by hand. If we wanted to distinguish, say, English from Spanish, we can use English and Spanish dictionaries, and set the weight to one for each word that appears in the

<span id="page-32-0"></span><sup>2</sup>Only V × (K − 1) features and weights are necessary. By stipulating that Ψ(x, y = K) = 0 regardless of x, it is possible to implement any classification rule that can be achieved with V × K features and weights. This is the approach taken in binary classification rules like y = Sign(β·x+a), where β is a vector of weights, a is an offset, and the label set is Y = {−1, 1}. However, for multiclass classification, it is more concise to write θ · f(x, y) for all y ∈ Y.

<span id="page-33-2"></span><span id="page-33-0"></span>![](_page_33_Figure_1.jpeg)

Figure 2.1: The bag-of-words and feature vector representations, for a hypothetical text classification task.

associated dictionary. For example,[3](#page-33-1)

$$\begin{array}{ll} \theta_{(E,bicycle)} = 1 & \theta_{(S,bicycle)} = 0 \\ \theta_{(E,bicicleta)} = 0 & \theta_{(S,bicicleta)} = 1 \\ \theta_{(E,con)} = 1 & \theta_{(S,con)} = 1 \\ \theta_{(E,ordinateur)} = 0 & \theta_{(S,ordinateur)} = 0. \end{array}$$

Similarly, if we want to distinguish positive and negative sentiment, we could use positive and negative **sentiment lexicons** (see § [4.1.2\)](#page-89-0), which are defined by social psychologists [\(Tausczik and Pennebaker, 2010\)](#page-565-0).

But it is usually not easy to set classification weights by hand, due to the large number of words and the difficulty of selecting exact numerical weights. Instead, we will learn the weights from data. Email users manually label messages as SPAM; newspapers label their own articles as BUSINESS or STYLE. Using such **instance labels**, we can automatically acquire weights using **supervised machine learning**. This chapter will discuss several machine learning approaches for classification. The first is based on probability. For a review of probability, consult [Appendix A.](#page-492-0)

<span id="page-33-1"></span><sup>3</sup> In this notation, each tuple (language, word) indexes an element in θ, which remains a vector.

## <span id="page-34-5"></span><span id="page-34-0"></span>2.2 Naïve Bayes

The **joint probability** of a bag of words x and its true label y is written p(x, y). Suppose we have a dataset of N labeled instances,  $\{(x^{(i)}, y^{(i)})\}_{i=1}^N$ , which we assume are **independent and identically distributed (IID)** (see § A.3). Then the joint probability of the entire dataset, written  $p(x^{(1:N)}, y^{(1:N)})$ , is equal to  $\prod_{i=1}^N p_{X,Y}(x^{(i)}, y^{(i)})$ .<sup>4</sup>

What does this have to do with classification? One approach to classification is to set the weights  $\theta$  so as to maximize the joint probability of a **training set** of labeled documents. This is known as **maximum likelihood estimation**:

$$\hat{\boldsymbol{\theta}} = \operatorname*{argmax}_{\boldsymbol{\theta}} p(\boldsymbol{x}^{(1:N)}, y^{(1:N)}; \boldsymbol{\theta})$$
[2.8]

$$= \underset{\boldsymbol{\theta}}{\operatorname{argmax}} \prod_{i=1}^{N} p(\boldsymbol{x}^{(i)}, y^{(i)}; \boldsymbol{\theta})$$
[2.9]

$$= \underset{\boldsymbol{\theta}}{\operatorname{argmax}} \sum_{i=1}^{N} \log p(\boldsymbol{x}^{(i)}, y^{(i)}; \boldsymbol{\theta}).$$
[2.10]

The notation  $p(\boldsymbol{x}^{(i)}, y^{(i)}; \boldsymbol{\theta})$  indicates that  $\boldsymbol{\theta}$  is a *parameter* of the probability function. The product of probabilities can be replaced by a sum of log-probabilities because the log function is monotonically increasing over positive arguments, and so the same  $\boldsymbol{\theta}$  will maximize both the probability and its logarithm. Working with logarithms is desirable because of numerical stability: on a large dataset, multiplying many probabilities can **underflow** to zero.<sup>5</sup>

The probability  $p(x^{(i)}, y^{(i)}; \theta)$  is defined through a **generative model** — an idealized random process that has generated the observed data.<sup>6</sup> Algorithm 1 describes the generative model underlying the **Naïve Bayes** classifier, with parameters  $\theta = {\mu, \phi}$ .

• The first line of this generative model encodes the assumption that the instances are mutually independent: neither the label nor the text of document *i* affects the label or text of document *j*.<sup>7</sup> Furthermore, the instances are identically distributed: the

<span id="page-34-1"></span><sup>&</sup>lt;sup>4</sup>The notation  $p_{X,Y}(\boldsymbol{x}^{(i)}, y^{(i)})$  indicates the joint probability that random variables *X* and *Y* take the specific values  $\boldsymbol{x}^{(i)}$  and  $y^{(i)}$  respectively. The subscript will often be omitted when it is clear from context. For a review of random variables, see Appendix A.

<span id="page-34-2"></span><sup>&</sup>lt;sup>5</sup>Throughout this text, you may assume all logarithms and exponents are base 2, unless otherwise indicated. Any reasonable base will yield an identical classifier, and base 2 is most convenient for working out examples by hand.

<span id="page-34-3"></span><sup>&</sup>lt;sup>6</sup>Generative models will be used throughout this text. They explicitly define the assumptions underlying the form of a probability distribution over observed and latent variables. For a readable introduction to generative models in statistics, see Blei (2014).

<span id="page-34-4"></span><sup>&</sup>lt;sup>7</sup>Can you think of any cases in which this assumption is too strong?

<span id="page-35-3"></span><span id="page-35-0"></span>Algorithm 1 Generative process for the Naïve Bayes classification model

for Instance  $i \in \{1, 2, ..., N\}$  do: Draw the label  $y^{(i)} \sim \text{Categorical}(\mu)$ ; Draw the word counts  $\boldsymbol{x}^{(i)} \mid y^{(i)} \sim \text{Multinomial}(\phi_{y^{(i)}})$ .

distributions over the label  $y^{(i)}$  and the text  $x^{(i)}$  (conditioned on  $y^{(i)}$ ) are the same for all instances *i*. In other words, we make the assumption that every document has the same distribution over labels, and that each document's distribution over words depends only on the label, and not on anything else about the document. We also assume that the documents don't affect each other: if the word *whale* appears in document i = 7, that does not make it any more or less likely that it will appear again in document i = 8.

- The second line of the generative model states that the random variable y<sup>(i)</sup> is drawn from a categorical distribution with parameter μ. Categorical distributions are like weighted dice: the column vector μ = [μ<sub>1</sub>; μ<sub>2</sub>; ...; μ<sub>K</sub>] gives the probabilities of each label, so that the probability of drawing label y is equal to μ<sub>y</sub>. For example, if
- The third line describes how the bag-of-words counts  $x^{(i)}$  are generated. By writing  $x^{(i)} \mid y^{(i)}$ , this line indicates that the word counts are conditioned on the label, so that the joint probability is factored using the chain rule,

$$\mathbf{p}_{X,Y}(\boldsymbol{x}^{(i)}, y^{(i)}) = \mathbf{p}_{X|Y}(\boldsymbol{x}^{(i)} \mid y^{(i)}) \times \mathbf{p}_{Y}(y^{(i)}).$$
[2.11]

The specific distribution  $p_{X|Y}$  is the **multinomial**, which is a probability distribution over vectors of non-negative counts. The probability mass function for this distribution is:

$$p_{\text{mult}}(\boldsymbol{x}; \boldsymbol{\phi}) = B(\boldsymbol{x}) \prod_{j=1}^{V} \phi_j^{x_j}$$
[2.12]

<span id="page-35-2"></span>
$$B(\boldsymbol{x}) = \frac{\left(\sum_{j=1}^{V} x_j\right)!}{\prod_{j=1}^{V} (x_j!)}.$$
[2.13]

<span id="page-35-1"></span><sup>&</sup>lt;sup>8</sup>Formally, we require  $\mu \in \Delta^{K-1}$ , where  $\Delta^{K-1}$  is the K-1 **probability simplex**, the set of all vectors of K nonnegative numbers that sum to one. Because of the sum-to-one constraint, there are K-1 degrees of freedom for a vector of size K.

<span id="page-36-3"></span>As in the categorical distribution, the parameter  $\phi_j$  can be interpreted as a probability: specifically, the probability that any given token in the document is the word j. The multinomial distribution involves a product over words, with each term in the product equal to the probability  $\phi_j$ , exponentiated by the count  $x_j$ . Words that have zero count play no role in this product, because  $\phi_j^0 = 1$ . The term  $B(\mathbf{x})$  is called the **multinomial coefficient**. It doesn't depend on  $\phi$ , and can usually be ignored. Can you see why we need this term at all?<sup>9</sup>

The notation  $p(x \mid y; \phi)$  indicates the conditional probability of word counts x given label y, with parameter  $\phi$ , which is equal to  $p_{mult}(x; \phi_y)$ . By specifying the multinomial distribution, we describe the **multinomial Naïve Bayes** classifier. Why "naïve"? Because the multinomial distribution treats each word token independently, conditioned on the class: the probability mass function factorizes across the counts.<sup>10</sup>

#### <span id="page-36-0"></span>2.2.1 Types and tokens

A slight modification to the generative model of Naïve Bayes is shown in Algorithm 2. Instead of generating a vector of counts of **types**, x, this model generates a *sequence* of **tokens**,  $w = (w_1, w_2, \ldots, w_M)$ . The distinction between types and tokens is critical:  $x_j \in \{0, 1, 2, \ldots, M\}$  is the count of word type j in the vocabulary, e.g., the number of times the word *cannibal* appears;  $w_m \in \mathcal{V}$  is the identity of token m in the document, e.g.  $w_m = cannibal$ .

The probability of the sequence w is a product of categorical probabilities. Algorithm 2 makes a conditional independence assumption: each token  $w_m^{(i)}$  is independent of all other tokens  $w_{n\neq m}^{(i)}$ , conditioned on the label  $y^{(i)}$ . This is identical to the "naïve" independence assumption implied by the multinomial distribution, and as a result, the optimal parameters for this model are identical to those in multinomial Naïve Bayes. For any instance, the probability assigned by this model is proportional to the probability under multinomial Naïve Bayes. The constant of proportionality is the multinomial coefficient B(x). Because  $B(x) \ge 1$ , the probability for a vector of counts x is at least as large as the probability for a list of words w that induces the same counts: there can be many word sequences that correspond to a single vector of counts. For example, *man bites dog* and *dog bites man* correspond to an identical count vector, {*bites* : 1, *dog* : 1, *man* : 1}, and B(x) is equal to the total number of possible word orderings for count vector x.

<span id="page-36-1"></span><sup>&</sup>lt;sup>9</sup>Technically, a multinomial distribution requires a second parameter, the total number of word counts in x. In the bag-of-words representation is equal to the number of words in the document. However, this parameter is irrelevant for classification.

<span id="page-36-2"></span><sup>&</sup>lt;sup>10</sup>You can plug in any probability distribution to the generative story and it will still be Naïve Bayes, as long as you are making the "naïve" assumption that the features are conditionally independent, given the label. For example, a multivariate Gaussian with diagonal covariance is naïve in exactly the same sense.

<span id="page-37-1"></span>Algorithm 2 Alternative generative process for the Naïve Bayes classification model

$$\begin{split} & \text{for Instance } i \in \{1, 2, \dots, N\} \text{ do:} \\ & \text{Draw the label } y^{(i)} \sim \text{Categorical}(\boldsymbol{\mu}); \\ & \text{for Token } m \in \{1, 2, \dots, M_i\} \text{ do:} \\ & \text{Draw the token } w_m^{(i)} \mid y^{(i)} \sim \text{Categorical}(\phi_{y^{(i)}}). \end{split}$$

Sometimes it is useful to think of instances as counts of types, x; other times, it is better to think of them as sequences of tokens, w. If the tokens are generated from a model that assumes conditional independence, then these two views lead to probability models that are identical, except for a scaling factor that does not depend on the label or the parameters.

#### <span id="page-37-0"></span>2.2.2 Prediction

The Naïve Bayes prediction rule is to choose the label *y* which maximizes  $\log p(x, y; \mu, \phi)$ :

$$\hat{y} = \underset{y}{\operatorname{argmax}} \log p(\boldsymbol{x}, y; \boldsymbol{\mu}, \boldsymbol{\phi})$$
[2.14]

$$= \underset{y}{\operatorname{argmax}} \log p(\boldsymbol{x} \mid y; \boldsymbol{\phi}) + \log p(y; \boldsymbol{\mu})$$
[2.15]

Now we can plug in the probability distributions from the generative story.

$$\log \mathbf{p}(\boldsymbol{x} \mid \boldsymbol{y}; \boldsymbol{\phi}) + \log \mathbf{p}(\boldsymbol{y}; \boldsymbol{\mu}) = \log \left[ B(\boldsymbol{x}) \prod_{j=1}^{V} \boldsymbol{\phi}_{\boldsymbol{y}, j}^{\boldsymbol{x}_{j}} \right] + \log \mu_{\boldsymbol{y}}$$
[2.16]

$$= \log B(\boldsymbol{x}) + \sum_{j=1}^{V} x_j \log \phi_{y,j} + \log \mu_y$$
 [2.17]

$$= \log B(\boldsymbol{x}) + \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}, y), \qquad [2.18]$$

where

$$\boldsymbol{\theta} = [\boldsymbol{\theta}^{(1)}; \boldsymbol{\theta}^{(2)}; \dots; \boldsymbol{\theta}^{(K)}]$$
[2.19]

$$\boldsymbol{\theta}^{(y)} = [\log \phi_{y,1}; \log \phi_{y,2}; \dots; \log \phi_{y,V}; \log \mu_y]$$
[2.20]

The feature function f(x, y) is a vector of *V* word counts and an offset, padded by zeros for the labels not equal to *y* (see Equations 2.3-2.5, and Figure 2.1). This construction ensures that the inner product  $\theta \cdot f(x, y)$  only activates the features whose weights are in  $\theta^{(y)}$ . These features and weights are all we need to compute the joint log-probability  $\log p(x, y)$  for each *y*. This is a key point: through this notation, we have converted the problem of computing the log-likelihood for a document-label pair (x, y) into the computation of a vector inner product.

#### <span id="page-38-2"></span><span id="page-38-0"></span>2.2.3 Estimation

The parameters of the categorical and multinomial distributions have a simple interpretation: they are vectors of expected frequencies for each possible event. Based on this interpretation, it is tempting to set the parameters empirically,

<span id="page-38-1"></span>
$$\phi_{y,j} = \frac{\operatorname{count}(y,j)}{\sum_{j'=1}^{V} \operatorname{count}(y,j')} = \frac{\sum_{i:y^{(i)}=y} x_j^{(i)}}{\sum_{j'=1}^{V} \sum_{i:y^{(i)}=y} x_{j'}^{(i)}},$$
[2.21]

where count(y, j) refers to the count of word j in documents with label y.

Equation 2.21 defines the **relative frequency estimate** for  $\phi$ . It can be justified as a **maximum likelihood estimate**: the estimate that maximizes the probability  $p(\boldsymbol{x}^{(1:N)}, y^{(1:N)}; \boldsymbol{\theta})$ . Based on the generative model in Algorithm 1, the log-likelihood is,

$$\mathcal{L}(\boldsymbol{\phi}, \boldsymbol{\mu}) = \sum_{i=1}^{N} \log p_{\text{mult}}(\boldsymbol{x}^{(i)}; \boldsymbol{\phi}_{y^{(i)}}) + \log p_{\text{cat}}(y^{(i)}; \boldsymbol{\mu}),$$
 [2.22]

which is now written as a function  $\mathcal{L}$  of the parameters  $\phi$  and  $\mu$ . Let's continue to focus on the parameters  $\phi$ . Since p(y) is constant with respect to  $\phi$ , we can drop it:

$$\mathcal{L}(\boldsymbol{\phi}) = \sum_{i=1}^{N} \log p_{\text{mult}}(\boldsymbol{x}^{(i)}; \boldsymbol{\phi}_{y^{(i)}}) = \sum_{i=1}^{N} \log B(\boldsymbol{x}^{(i)}) + \sum_{j=1}^{V} x_{j}^{(i)} \log \phi_{y^{(i)}, j}, \quad [2.23]$$

where  $B(\boldsymbol{x}^{(i)})$  is constant with respect to  $\phi$ .

Maximum-likelihood estimation chooses  $\phi$  to maximize the log-likelihood  $\mathcal{L}$ . However, the solution must obey the following constraints:

$$\sum_{j=1}^{V} \phi_{y,j} = 1 \quad \forall y$$
[2.24]

These constraints can be incorporated by adding a set of Lagrange multipliers to the objective (see Appendix B for more details). To solve for each  $\theta_y$ , we maximize the Lagrangian,

$$\ell(\phi_y) = \sum_{i:y^{(i)}=y} \sum_{j=1}^{V} x_j^{(i)} \log \phi_{y,j} - \lambda(\sum_{j=1}^{V} \phi_{y,j} - 1).$$
[2.25]

Differentiating with respect to the parameter  $\phi_{y,j}$  yields,

$$\frac{\partial \ell(\phi_y)}{\partial \phi_{y,j}} = \sum_{i:y^{(i)}=y} x_j^{(i)} / \phi_{y,j} - \lambda.$$
[2.26]

<span id="page-39-3"></span>The solution is obtained by setting each element in this vector of derivatives equal to zero,

$$\lambda \phi_{y,j} = \sum_{i:y^{(i)}=y} x_j^{(i)}$$
[2.27]

$$\phi_{y,j} \propto \sum_{i:y^{(i)}=y} x_j^{(i)} = \sum_{i=1}^N \delta\left(y^{(i)}=y\right) x_j^{(i)} = \operatorname{count}(y,j),$$
[2.28]

where  $\delta(y^{(i)} = y)$  is a **delta function**, also sometimes called an indicator function, which returns one if  $y^{(i)} = y$ . The symbol  $\propto$  indicates that  $\phi_{y,j}$  is **proportional to** the right-hand side of the equation.

Equation 2.28 shows three different notations for the same thing: a sum over the word counts for all documents *i* such that the label  $y^{(i)} = y$ . This gives a solution for each  $\phi_y$  up to a constant of proportionality. Now recall the constraint  $\sum_{j=1}^{V} \phi_{y,j} = 1$ , which arises because  $\phi_y$  represents a vector of probabilities for each word in the vocabulary. This constraint leads to an exact solution, which does not depend on  $\lambda$ :

<span id="page-39-1"></span>
$$\phi_{y,j} = \frac{\text{count}(y,j)}{\sum_{j'=1}^{V} \text{count}(y,j')}.$$
[2.29]

This is equal to the relative frequency estimator from Equation 2.21. A similar derivation gives  $\mu_y \propto \sum_{i=1}^N \delta(y^{(i)} = y)$ .

#### <span id="page-39-0"></span>2.2.4 Smoothing

With text data, there are likely to be pairs of labels and words that never appear in the training set, leaving  $\phi_{y,j} = 0$ . For example, the word *molybdenum* may have never yet appeared in a work of fiction. But choosing a value of  $\phi_{\text{FICTION}, molybdenum} = 0$  would allow this single feature to completely veto a label, since  $p(\text{FICTION} | \boldsymbol{x}) = 0$  if  $\boldsymbol{x}_{molybdenum} > 0$ .

This is undesirable, because it imposes high **variance**: depending on what data happens to be in the training set, we could get vastly different classification rules. One solution is to **smooth** the probabilities, by adding a "pseudocount" of  $\alpha$  to each count, and then normalizing.

$$\phi_{y,j} = \frac{\alpha + \operatorname{count}(y,j)}{V\alpha + \sum_{j'=1}^{V} \operatorname{count}(y,j')}$$
[2.30]

This is called **Laplace smoothing**.<sup>11</sup> The pseudocount  $\alpha$  is a **hyperparameter**, because it controls the form of the log-likelihood function, which in turn drives the estimation of  $\phi$ .

<span id="page-39-2"></span><sup>&</sup>lt;sup>11</sup>Laplace smoothing has a Bayesian justification, in which the generative model is extended to include  $\phi$  as a random variable. The resulting distribution over  $\phi$  depends on both the data (x and y) and the **prior probability**  $p(\phi; \alpha)$ . The corresponding estimate of  $\phi$  is called **maximum a posteriori**, or MAP. This is in contrast with maximum likelihood, which depends only on the data.

#### <span id="page-40-1"></span>2.2. NAÏVE BAYES

Smoothing reduces variance, but moves us away from the maximum likelihood estimate: it imposes a **bias**. In this case, the bias points towards uniform probabilities. Machine learning theory shows that errors on heldout data can be attributed to the sum of bias and variance (Mohri et al., 2012). In general, techniques for reducing variance often increase the bias, leading to a **bias-variance tradeoff**.

- Unbiased classifiers may **overfit** the training data, yielding poor performance on unseen data.
- But if the smoothing is too large, the resulting classifier can underfit instead. In the limit of *α* → ∞, there is zero variance: you get the same classifier, regardless of the data. However, the bias is likely to be large.

Similar issues arise throughout machine learning. Later in this chapter we will encounter **regularization**, which controls the bias-variance tradeoff for logistic regression and large-margin classifiers (§ 2.5.1); § 3.3.2 describes techniques for controlling variance in deep learning; chapter 6 describes more elaborate methods for smoothing empirical probabilities.

#### <span id="page-40-0"></span>2.2.5 Setting hyperparameters

Returning to Naïve Bayes, how should we choose the best value of hyperparameters like  $\alpha$ ? Maximum likelihood will not work: the maximum likelihood estimate of  $\alpha$  on the training set will always be  $\alpha = 0$ . In many cases, what we really want is **accuracy**: the number of correct predictions, divided by the total number of predictions. (Other measures of classification performance are discussed in § 4.4.) As we will see, it is hard to optimize for accuracy directly. But for scalar hyperparameters like  $\alpha$ , tuning can be performed by a simple heuristic called **grid search**: try a set of values (e.g.,  $\alpha \in \{0.001, 0.01, 0.1, 1, 10\}$ ), compute the accuracy for each value, and choose the setting that maximizes the accuracy.

The goal is to tune  $\alpha$  so that the classifier performs well on *unseen* data. For this reason, the data used for hyperparameter tuning should not overlap the training set, where very small values of  $\alpha$  will be preferred. Instead, we hold out a **development set** (also called a **tuning set**) for hyperparameter selection. This development set may consist of a small fraction of the labeled data, such as 10%.

We also want to predict the performance of our classifier on unseen data. To do this, we must hold out a separate subset of data, called the **test set**. It is critical that the test set not overlap with either the training or development sets, or else we will overestimate the performance that the classifier will achieve on unlabeled data in the future. The test set should also not be used when making modeling decisions, such as the form of the feature function, the size of the vocabulary, and so on (these decisions are reviewed in chapter 4.) The ideal practice is to use the test set only once — otherwise, the test set is used to guide

<span id="page-41-2"></span>the classifier design, and test set accuracy will diverge from accuracy on truly unseen data. Because annotated data is expensive, this ideal can be hard to follow in practice, and many test sets have been used for decades. But in some high-impact applications like machine translation and information extraction, new test sets are released every year.

When only a small amount of labeled data is available, the test set accuracy can be unreliable. K-fold **cross-validation** is one way to cope with this scenario: the labeled data is divided into K folds, and each fold acts as the test set, while training on the other folds. The test set accuracies are then aggregated. In the extreme, each fold is a single data point; this is called **leave-one-out cross-validation**. To perform hyperparameter tuning in the context of cross-validation, another fold can be used for grid search. It is important not to repeatedly evaluate the cross-validated accuracy while making design decisions about the classifier, or you will overstate the accuracy on truly unseen data.

# <span id="page-41-0"></span>**2.3 Discriminative learning**

Na¨ıve Bayes is easy to work with: the weights can be estimated in closed form, and the probabilistic interpretation makes it relatively easy to extend. However, the assumption that features are independent can seriously limit its accuracy. Thus far, we have defined the feature function f(x, y) so that it corresponds to bag-of-words features: one feature per word in the vocabulary. In natural language, bag-of-words features violate the assumption of conditional independence — for example, the probability that a document will contain the word *na¨ıve* is surely higher given that it also contains the word *Bayes* but this violation is relatively mild.

However, good performance on text classification often requires features that are richer than the bag-of-words:

- To better handle out-of-vocabulary terms, we want features that apply to multiple words, such as prefixes and suffixes (e.g., *anti-*, *un-*, *-ing*) and capitalization.
- We also want n**-gram** features that apply to multi-word units: **bigrams** (e.g., *not good*, *not bad*), **trigrams** (e.g., *not so bad*, *lacking any decency*, *never before imagined*), and beyond.

These features flagrantly violate the Na¨ıve Bayes independence assumption. Consider what happens if we add a prefix feature. Under the Na¨ıve Bayes assumption, the joint probability of a word and its prefix are computed with the following approximation:[12](#page-41-1)

Pr(word = *unfit*, prefix = *un-* | y) ≈ Pr(prefix = *un-* | y) × Pr(word = *unfit* | y).

<span id="page-41-1"></span><sup>12</sup>The notation Pr(·) refers to the probability of an event, and p(·) refers to the probability density or mass for a random variable (see [Appendix A\)](#page-492-0).

<span id="page-42-1"></span>To test the quality of the approximation, we can manipulate the left-hand side by applying the chain rule,

$$Pr(word = unfit, prefix = un - | y) = Pr(prefix = un - | word = unfit, y)$$
[2.31]

$$\times \Pr(\text{word} = unfit \mid y)$$
 [2.32]

But Pr(prefix = *un-* | word = *unfit*, y) = 1, since *un-* is guaranteed to be the prefix for the word *unfit*. Therefore,

$$Pr(word = unfit, prefix = un - | y) = 1 \times Pr(word = unfit | y)$$
[2.33]  
$$\gg Pr(prefix = un - | y) \times Pr(word = unfit | y),$$
[2.34]

because the probability of any given word starting with the prefix *un-* is much less than one. Na¨ıve Bayes will systematically underestimate the true probabilities of conjunctions of positively correlated features. To use such features, we need learning algorithms that do not rely on an independence assumption.

The origin of the Na¨ıve Bayes independence assumption is the learning objective, p(x (1:N) , y(1:N) ), which requires modeling the probability of the observed text. In classification problems, we are always given x, and are only interested in predicting the label y. In this setting, modeling the probability of the text x seems like a difficult and unnecessary task. **Discriminative learning** algorithms avoid this task, and focus directly on the problem of predicting y.

#### <span id="page-42-0"></span>**2.3.1 Perceptron**

In Na¨ıve Bayes, the weights can be interpreted as parameters of a probabilistic model. But this model requires an independence assumption that usually does not hold, and limits our choice of features. Why not forget about probability and learn the weights in an errordriven way? The **perceptron** algorithm, shown in [Algorithm 3,](#page-43-0) is one way to do this.

The algorithm is simple: if you make a mistake, increase the weights for features that are active with the correct label y (i) , and decrease the weights for features that are active with the guessed label yˆ. Perceptron is an **online learning** algorithm, since the classifier weights change after every example. This is different from Na¨ıve Bayes, which is a **batch learning** algorithm: it computes statistics over the entire dataset, and then sets the weights in a single operation. [Algorithm 3](#page-43-0) is vague about when this online learning procedure terminates. We will return to this issue shortly.

The perceptron algorithm may seem like an unprincipled heuristic: Na¨ıve Bayes has a solid foundation in probability, but the perceptron is just adding and subtracting constants from the weights every time there is a mistake. Will this really work? In fact, there is some nice theory for the perceptron, based on the concept of **linear separability**. Informally, a dataset with binary labels (y ∈ {0, 1}) is linearly separable if it is possible to draw a

<span id="page-43-3"></span><span id="page-43-0"></span>**Algorithm 3** Perceptron learning algorithm

```
1: procedure PERCEPTRON(x^{(1:N)}, y^{(1:N)})
                 t \leftarrow 0
  2:
                 \boldsymbol{\theta}^{(0)} \leftarrow \mathbf{0}
  3:
                 repeat
  4:
                         t \leftarrow t + 1
  5:
                         Select an instance i
  6.
                         \hat{y} \leftarrow \operatorname{argmax}_{u} \boldsymbol{\theta}^{(t-1)} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y)
 7:
                         if \hat{y} \neq y^{(i)} then
  8:
                                 \boldsymbol{\theta}^{(t)} \leftarrow \boldsymbol{\theta}^{(t-1)} + \boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) - \boldsymbol{f}(\boldsymbol{x}^{(i)}, \hat{y})
  9:
10:
                         else
                                  \boldsymbol{\theta}^{(t)} \leftarrow \boldsymbol{\theta}^{(t-1)}
11:
                 until tired
12.
                 return \theta^{(t)}
13:
```

hyperplane (a line in many dimensions), such that on each side of the hyperplane, all instances have the same label. This definition can be formalized and extended to multiple labels:

<span id="page-43-2"></span>**Definition 1** (Linear separability). The dataset  $\mathcal{D} = \{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^{N}$  is linearly separable iff (if and only if) there exists some weight vector  $\boldsymbol{\theta}$  and some margin  $\rho$  such that for every instance  $(\mathbf{x}^{(i)}, y^{(i)})$ , the inner product of  $\boldsymbol{\theta}$  and the feature function for the true label,  $\boldsymbol{\theta} \cdot f(\mathbf{x}^{(i)}, y^{(i)})$ , is at least  $\rho$  greater than inner product of  $\boldsymbol{\theta}$  and the feature function for every other possible label,  $\boldsymbol{\theta} \cdot f(\mathbf{x}^{(i)}, y^{(i)})$ , is

$$\exists \boldsymbol{\theta}, \rho > 0 : \forall (\boldsymbol{x}^{(i)}, y^{(i)}) \in \mathcal{D}, \quad \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) \ge \rho + \max_{\boldsymbol{y}' \neq \boldsymbol{y}^{(i)}} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y').$$
[2.35]

Linear separability is important because of the following guarantee: if your data is linearly separable, then the perceptron algorithm will find a separator (Novikoff, 1962).<sup>13</sup> So while the perceptron may seem heuristic, it is guaranteed to succeed, if the learning problem is easy enough.

How useful is this proof? Minsky and Papert (1969) famously proved that the simple logical function of *exclusive-or* is not separable, and that a perceptron is therefore incapable of learning this function. But this is not just an issue for the perceptron: any linear classification algorithm, including Naïve Bayes, will fail on this task. Text classification problems usually involve high dimensional feature spaces, with thousands or millions of

<span id="page-43-1"></span><sup>&</sup>lt;sup>13</sup>It is also possible to prove an upper bound on the number of training iterations required to find the separator. Proofs like this are part of the field of **machine learning theory** (Mohri et al., 2012).

<span id="page-44-2"></span>features. For these problems, it is very likely that the training data is indeed separable. And even if the dataset is not separable, it is still possible to place an upper bound on the number of errors that the perceptron algorithm will make (Freund and Schapire, 1999).

#### <span id="page-44-0"></span>2.3.2 Averaged perceptron

The perceptron iterates over the data repeatedly — until "tired", as described in Algorithm 3. If the data is linearly separable, the perceptron will eventually find a separator, and we can stop once all training instances are classified correctly. But if the data is not linearly separable, the perceptron can *thrash* between two or more weight settings, never converging. In this case, how do we know that we can stop training, and how should we choose the final weights? An effective practical solution is to *average* the perceptron weights across all iterations.

This procedure is shown in Algorithm 4. The learning algorithm is nearly identical, but we also maintain a vector of the sum of the weights, m. At the end of the learning procedure, we divide this sum by the total number of updates t, to compute the average weights,  $\overline{\theta}$ . These average weights are then used for prediction. In the algorithm sketch, the average is computed from a running sum,  $m \leftarrow m + \theta$ . However, this is inefficient, because it requires  $|\theta|$  operations to update the running sum. When f(x, y) is sparse,  $|\theta| \gg |f(x, y)|$  for any individual (x, y). This means that computing the running sum will be much more expensive than computing of the update to  $\theta$  itself, which requires only  $2 \times |f(x, y)|$  operations. One of the exercises is to sketch a more efficient algorithm for computing the averaged weights.

Even if the dataset is not separable, the averaged weights will eventually converge. One possible stopping criterion is to check the difference between the average weight vectors after each pass through the data: if the norm of the difference falls below some predefined threshold, we can stop training. Another stopping criterion is to hold out some data, and to measure the predictive accuracy on this heldout data. When the accuracy on the heldout data starts to decrease, the learning algorithm has begun to **overfit** the training set. At this point, it is probably best to stop; this stopping criterion is known as **early stopping**.

**Generalization** is the ability to make good predictions on instances that are not in the training data. Averaging can be proven to improve generalization, by computing an upper bound on the generalization error (Freund and Schapire, 1999; Collins, 2002).

# <span id="page-44-1"></span>2.4 Loss functions and large-margin classification

Naïve Bayes chooses the weights  $\theta$  by maximizing the joint log-likelihood log  $p(x^{(1:N)}, y^{(1:N)})$ . By convention, optimization problems are generally formulated as minimization of a **loss function**. The input to a loss function is the vector of weights  $\theta$ , and the output is a

<span id="page-45-1"></span><span id="page-45-0"></span>Algorithm 4 Averaged perceptron learning algorithm

```
1: procedure AVG-PERCEPTRON(m{x}^{(1:N)},m{y}^{(1:N)})
  2:
                  t \leftarrow 0
                  \boldsymbol{\theta}^{(0)} \leftarrow 0
  3:
  4:
                  repeat
                           t \leftarrow t + 1
  5:
                           Select an instance i
  6:
                           \hat{y} \leftarrow \operatorname{argmax}_{\boldsymbol{y}} \boldsymbol{\theta}^{(t-1)} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, \boldsymbol{y})
  7:
                           if \hat{y} \neq y^{(i)} then
  8:
                                    \boldsymbol{\theta}^{(t)} \leftarrow \boldsymbol{\theta}^{(t-1)} + \boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) - \boldsymbol{f}(\boldsymbol{x}^{(i)}, \hat{y})
  9:
10:
                           else
                                    \boldsymbol{\theta}^{(t)} \leftarrow \boldsymbol{\theta}^{(t-1)}
11:
                           \boldsymbol{m} \leftarrow \boldsymbol{m} + \boldsymbol{\theta}^{(t)}
12:
                  until tired
13:
                  \overline{	heta} \leftarrow rac{1}{t}m
14:
                  return \overline{\theta}
15:
```

non-negative number, measuring the performance of the classifier on a training instance. Formally, the loss  $\ell(\theta; x^{(i)}, y^{(i)})$  is then a measure of the performance of the weights  $\theta$  on the instance  $(x^{(i)}, y^{(i)})$ . The goal of learning is to minimize the sum of the losses across all instances in the training set.

We can trivially reformulate maximum likelihood as a loss function, by defining the loss function to be the *negative* log-likelihood:

$$\log p(\boldsymbol{x}^{(1:N)}, y^{(1:N)}; \boldsymbol{\theta}) = \sum_{i=1}^{N} \log p(\boldsymbol{x}^{(i)}, y^{(i)}; \boldsymbol{\theta})$$
[2.36]

$$\ell_{\text{NB}}(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, y^{(i)}) = -\log p(\boldsymbol{x}^{(i)}, y^{(i)}; \boldsymbol{\theta})$$
[2.37]

$$\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta}}{\operatorname{argmin}} \sum_{i=1}^{N} \ell_{\text{NB}}(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, y^{(i)})$$
[2.38]

$$= \underset{\boldsymbol{\theta}}{\operatorname{argmax}} \sum_{i=1}^{N} \log p(\boldsymbol{x}^{(i)}, y^{(i)}; \boldsymbol{\theta}).$$
 [2.39]

The problem of minimizing  $\ell_{\rm NB}$  is thus identical to maximum-likelihood estimation.

Loss functions provide a general framework for comparing learning objectives. For example, an alternative loss function is the **zero-one loss**,

$$\ell_{0-1}(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, y^{(i)}) = \begin{cases} 0, & y^{(i)} = \operatorname{argmax}_{y} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y) \\ 1, & \text{otherwise} \end{cases}$$
[2.40]

<span id="page-46-3"></span>The zero-one loss is zero if the instance is correctly classified, and one otherwise. The sum of zero-one losses is proportional to the error rate of the classifier on the training data. Since a low error rate is often the ultimate goal of classification, this may seem ideal. But the zero-one loss has several problems. One is that it is **non-convex**,<sup>14</sup> which means that there is no guarantee that gradient-based optimization will be effective. A more serious problem is that the derivatives are useless: the partial derivative with respect to any parameter is zero everywhere, except at the points where  $\theta \cdot f(x^{(i)}, y) = \theta \cdot f(x^{(i)}, \hat{y})$  for some  $\hat{y}$ . At those points, the loss is discontinuous, and the derivative is undefined.

The perceptron optimizes a loss function that has better properties for learning:

<span id="page-46-1"></span>
$$\ell_{\text{PERCEPTRON}}(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, y^{(i)}) = \max_{y \in \mathcal{Y}} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y) - \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}), \quad [2.41]$$

When  $\hat{y} = y^{(i)}$ , the loss is zero; otherwise, it increases linearly with the gap between the score for the predicted label  $\hat{y}$  and the score for the true label  $y^{(i)}$ . Plotting this loss against the input  $\max_{y \in \mathcal{Y}} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y) - \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)})$  gives a hinge shape, motivating the name **hinge loss**.

To see why this is the loss function optimized by the perceptron, take the derivative with respect to  $\theta$ ,

$$\frac{\partial}{\partial \boldsymbol{\theta}} \ell_{\text{PERCEPTRON}}(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, y^{(i)}) = \boldsymbol{f}(\boldsymbol{x}^{(i)}, \hat{y}) - \boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}).$$
[2.42]

At each instance, the perceptron algorithm takes a step of magnitude one in the opposite direction of this **gradient**,  $\nabla_{\theta} \ell_{\text{PERCEPTRON}} = \frac{\partial}{\partial \theta} \ell_{\text{PERCEPTRON}}(\theta; x^{(i)}, y^{(i)})$ . As we will see in § 2.6, this is an example of the optimization algorithm **stochastic gradient descent**, applied to the objective in Equation 2.41.

\*Breaking ties with subgradient descent <sup>15</sup> Careful readers will notice the tacit assumption that there is a unique  $\hat{y}$  that maximizes  $\theta \cdot f(x^{(i)}, y)$ . What if there are two or more labels that maximize this function? Consider binary classification: if the maximizer is  $y^{(i)}$ , then the gradient is zero, and so is the perceptron update; if the maximizer is  $\hat{y} \neq y^{(i)}$ , then the update is the difference  $f(x^{(i)}, y^{(i)}) - f(x^{(i)}, \hat{y})$ . The underlying issue is that the perceptron loss is not smooth, because the first derivative has a discontinuity at the hinge point, where the score for the true label  $y^{(i)}$  is equal to the score for some other label  $\hat{y}$ . At this point, there is no unique gradient; rather, there is a set of subgradients. A vector v is

<span id="page-46-0"></span><sup>&</sup>lt;sup>14</sup>A function *f* is **convex** iff  $\alpha f(x_i) + (1-\alpha)f(x_j) \ge f(\alpha x_i + (1-\alpha)x_j)$ , for all  $\alpha \in [0, 1]$  and for all  $x_i$  and  $x_j$  on the domain of the function. In words, any weighted average of the output of *f* applied to any two points is larger than the output of *f* when applied to the weighted average of the same two points. Convexity implies that any local minimum is also a global minimum, and there are many effective techniques for optimizing convex functions (Boyd and Vandenberghe, 2004). See Appendix B for a brief review.

<span id="page-46-2"></span><sup>&</sup>lt;sup>15</sup>Throughout this text, advanced topics will be marked with an asterisk.

<span id="page-47-1"></span>a subgradient of the function g at  $u_0$  iff  $g(u) - g(u_0) \ge v \cdot (u - u_0)$  for all u. Graphically, this defines the set of hyperplanes that include  $g(u_0)$  and do not intersect g at any other point. As we approach the hinge point from the left, the gradient is  $f(x, \hat{y}) - f(x, y)$ ; as we approach from the right, the gradient is 0. At the hinge point, the subgradients include all vectors that are bounded by these two extremes. In subgradient descent, *any* subgradient can be used (Bertsekas, 2012). Since both 0 and  $f(x, \hat{y}) - f(x, y)$  are subgradients at the hinge point, either one can be used in the perceptron update. This means that if multiple labels maximize  $\theta \cdot f(x^{(i)}, y)$ , any of them can be used in the perceptron update.

**Perceptron versus Naïve Bayes** The perceptron loss function has some pros and cons with respect to the negative log-likelihood loss implied by Naïve Bayes.

- Both  $\ell_{\text{NB}}$  and  $\ell_{\text{PERCEPTRON}}$  are convex, making them relatively easy to optimize. However,  $\ell_{\text{NB}}$  can be optimized in closed form, while  $\ell_{\text{PERCEPTRON}}$  requires iterating over the dataset multiple times.
- $\ell_{\text{NB}}$  can suffer *infinite* loss on a single example, since the logarithm of zero probability is negative infinity. Naïve Bayes will therefore overemphasize some examples, and underemphasize others.
- The Naïve Bayes classifier assumes that the observed features are conditionally independent, given the label, and the performance of the classifier depends on the extent to which this assumption holds. The perceptron requires no such assumption.
- $\ell_{\text{PERCEPTRON}}$  treats all correct answers equally. Even if  $\theta$  only gives the correct answer by a tiny margin, the loss is still zero.

#### <span id="page-47-0"></span>2.4.1 Online large margin classification

This last comment suggests a potential problem with the perceptron. Suppose a test example is very close to a training example, but not identical. If the classifier only gets the correct answer on the training example by a small amount, then it may give a different answer on the nearby test instance. To formalize this intuition, define the **margin** as,

$$\gamma(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, y^{(i)}) = \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) - \max_{y \neq y^{(i)}} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y).$$
[2.43]

The margin represents the difference between the score for the correct label  $y^{(i)}$ , and the score for the highest-scoring incorrect label. The intuition behind **large margin classification** is that it is not enough to label the training data correctly — the correct label should be separated from other labels by a comfortable margin. This idea can be encoded

<span id="page-48-2"></span><span id="page-48-0"></span>![](_page_48_Figure_1.jpeg)

Figure 2.2: Margin, zero-one, and logistic loss functions.

into a loss function,

$$\ell_{\text{MARGIN}}(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, y^{(i)}) = \begin{cases} 0, & \gamma(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, y^{(i)}) \ge 1, \\ 1 - \gamma(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, y^{(i)}), & \text{otherwise} \end{cases}$$
[2.44]

$$= \left(1 - \gamma(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, y^{(i)})\right)_{+}, \qquad [2.45]$$

where  $(x)_+ = \max(0, x)$ . The loss is zero if there is a margin of at least 1 between the score for the true label and the best-scoring alternative  $\hat{y}$ . This is almost identical to the perceptron loss, but the hinge point is shifted to the right, as shown in Figure 2.2. The margin loss is a convex upper bound on the zero-one loss.

The margin loss can be minimized using an online learning rule that is similar to perceptron. We will call this learning rule the **online support vector machine**, for reasons that will be discussed in the derivation. Let us first generalize the notion of a classification error with a **cost function**  $c(y^{(i)}, y)$ . We will focus on the simple cost function,

<span id="page-48-1"></span>
$$c(y^{(i)}, y) = \begin{cases} 1, & y^{(i)} \neq \hat{y} \\ 0, & \text{otherwise,} \end{cases}$$
[2.46]

but it is possible to design specialized cost functions that assign heavier penalties to especially undesirable errors (Tsochantaridis et al., 2004). This idea is revisited in chapter 7.

Using the cost function, we can now define the online support vector machine as the

<span id="page-49-1"></span>following classification rule:

$$\hat{y} = \operatorname*{argmax}_{y \in \mathcal{Y}} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y) + c(y^{(i)}, y)$$
[2.47]

$$\boldsymbol{\theta}^{(t)} \leftarrow (1-\lambda)\boldsymbol{\theta}^{(t-1)} + \boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) - \boldsymbol{f}(\boldsymbol{x}^{(i)}, \hat{y})$$
[2.48]

This update is similar in form to the perceptron, with two key differences.

- Rather than selecting the label ŷ that maximizes the score of the current classification model, the argmax searches for labels that are both *strong*, as measured by θ · f(x<sup>(i)</sup>, y), and *wrong*, as measured by c(y<sup>(i)</sup>, y). This maximization is known as cost-augmented decoding, because it augments the maximization objective to favor high-cost labels. If the highest-scoring label is y = y<sup>(i)</sup>, then the margin loss for this instance is zero, and no update is needed. If not, then an update is required to reduce the margin loss even if the current model classifies the instance correctly. Cost augmentation is only done while learning; it is not applied when making predictions on unseen data.
- The previous weights θ<sup>(t-1)</sup> are scaled by (1 − λ), with λ ∈ (0, 1). The effect of this term is to cause the weights to "decay" back towards zero. In the support vector machine, this term arises from the minimization of a specific form of the margin, as described below. However, it can also be viewed as a form of **regularization**, which can help to prevent overfitting (see § 2.5.1). In this sense, it plays a role that is similar to smoothing in Naïve Bayes (see § 2.2.4).

#### <span id="page-49-0"></span>2.4.2 \*Derivation of the online support vector machine

The derivation of the online support vector machine is somewhat involved, but gives further intuition about why the method works. Begin by returning the idea of linear separability (Definition 1): if a dataset is linearly separable, then there is some hyperplane  $\theta$  that correctly classifies all training instances with margin  $\rho$ . This margin can be increased to any desired value by multiplying the weights by a constant.

Now, for any datapoint  $(\boldsymbol{x}^{(i)}, \boldsymbol{y}^{(i)})$ , the geometric distance to the separating hyperplane is given by  $\frac{\gamma(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, \boldsymbol{y}^{(i)})}{||\boldsymbol{\theta}||_2}$ , where the denominator is the norm of the weights,  $||\boldsymbol{\theta}||_2 = \sqrt{\sum_j \theta_j^2}$ . The geometric distance is sometimes called the **geometric margin**, in contrast to the **functional margin**  $\gamma(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, \boldsymbol{y}^{(i)})$ . Both are shown in Figure 2.3. The geometric margin is a good measure of the robustness of the separator: if the functional margin is large, but the norm  $||\boldsymbol{\theta}||_2$  is also large, then a small change in  $\boldsymbol{x}^{(i)}$  could cause it to be misclassified. We therefore seek to maximize the minimum geometric margin across the dataset, subject

<span id="page-50-1"></span><span id="page-50-0"></span>![](_page_50_Figure_1.jpeg)

Figure 2.3: Functional and geometric margins for a binary classification problem. All separators that satisfy the margin constraint are shown. The separator with the largest geometric margin is shown in bold.

to the constraint that the margin loss is always zero:

$$\max_{\boldsymbol{\theta}} \qquad \min_{i=1,2,\dots N} \quad \frac{\gamma(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, y^{(i)})}{||\boldsymbol{\theta}||_2} \\ \text{s.t.} \qquad \gamma(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, y^{(i)}) \ge 1, \quad \forall i.$$
 [2.49]

This is a **constrained optimization** problem, where the second line describes constraints on the space of possible solutions  $\theta$ . In this case, the constraint is that the functional margin always be at least one, and the objective is that the minimum geometric margin be as large as possible.

Constrained optimization is reviewed in Appendix B. In this case, further manipulation yields an unconstrained optimization problem. First, note that the norm  $||\theta||_2$  scales linearly:  $||a\theta||_2 = a||\theta||_2$ . Furthermore, the functional margin  $\gamma$  is a linear function of  $\theta$ , so that  $\gamma(a\theta, x^{(i)}, y^{(i)}) = a\gamma(\theta, x^{(i)}, y^{(i)})$ . As a result, any scaling factor on  $\theta$  will cancel in the numerator and denominator of the geometric margin. If the data is linearly separable at any  $\rho > 0$ , it is always possible to rescale the functional margin to 1 by multiplying  $\theta$  by a scalar constant. We therefore need only minimize the denominator  $||\theta||_2$ , subject to the constraint on the functional margin. The minimizer of  $||\theta||_2$  is also the minimizer of  $\frac{1}{2}||\theta||_2^2 = \frac{1}{2}\sum_{i=1}^{2} \theta_i^2$ , which is easier to work with. This yields a simpler optimization prob-

<span id="page-51-3"></span>lem:

$$\min_{\boldsymbol{\theta}} \quad \frac{1}{2} ||\boldsymbol{\theta}||_2^2$$
s.t.  $\gamma(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, y^{(i)}) \ge 1, \quad \forall_i.$ 
[2.50]

This problem is a **quadratic program**: the objective is a quadratic function of the parameters, and the constraints are all linear inequalities. One solution to this problem is to incorporate the constraints through Lagrange multipliers  $\alpha_i \ge 0, i = 1, 2, ..., N$ . The instances for which  $\alpha_i > 0$  are called **support vectors**; other instances are irrelevant to the classification boundary. This motivates the name **support vector machine**.

Thus far we have assumed linear separability, but many datasets of interest are not linearly separable. In this case, there is no  $\theta$  that satisfies the margin constraint. To add more flexibility, we can introduce a set of **slack variables**  $\xi_i \ge 0$ . Instead of requiring that the functional margin be greater than or equal to one, we require that it be greater than or equal to  $1 - \xi_i$ . Ideally there would not be any slack, so the slack variables are penalized in the objective function:

<span id="page-51-0"></span>
$$\min_{\boldsymbol{\theta},\boldsymbol{\xi}} \quad \frac{1}{2} ||\boldsymbol{\theta}||_2^2 + C \sum_{i=1}^N \xi_i$$
s.t. 
$$\gamma(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, \boldsymbol{y}^{(i)}) + \xi_i \ge 1, \quad \forall i$$

$$\xi_i \ge 0, \quad \forall_i.$$
[2.51]

The hyperparameter *C* controls the tradeoff between violations of the margin constraint and the preference for a low norm of  $\theta$ . As  $C \to \infty$ , slack is infinitely expensive, and there is only a solution if the data is separable. As  $C \to 0$ , slack becomes free, and there is a trivial solution at  $\theta = 0$ . Thus, *C* plays a similar role to the smoothing parameter in Naïve Bayes (§ 2.2.4), trading off between a close fit to the training data and better generalization. Like the smoothing parameter of Naïve Bayes, *C* must be set by the user, typically by maximizing performance on a heldout development set.

To solve the constrained optimization problem defined in Equation 2.51, we can first solve for the slack variables,

<span id="page-51-2"></span><span id="page-51-1"></span>
$$\xi_i \ge (1 - \gamma(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, y^{(i)}))_+.$$
 [2.52]

The inequality is tight: the optimal solution is to make the slack variables as small as possible, while still satisfying the constraints (Ratliff et al., 2007; Smith, 2011). By plugging in the minimum slack variables back into Equation 2.51, the problem can be transformed into the unconstrained optimization,

$$\min_{\boldsymbol{\theta}} \quad \frac{\lambda}{2} ||\boldsymbol{\theta}||_2^2 + \sum_{i=1}^N (1 - \gamma(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, y^{(i)}))_+,$$
[2.53]

#### <span id="page-52-3"></span>2.5. LOGISTIC REGRESSION

where each  $\xi_i$  has been substituted by the right-hand side of Equation 2.52, and the factor of *C* on the slack variables has been replaced by an equivalent factor of  $\lambda = \frac{1}{C}$  on the norm of the weights.

Equation 2.53 can be rewritten by expanding the margin,

$$\min_{\boldsymbol{\theta}} \quad \frac{\lambda}{2} ||\boldsymbol{\theta}||_2^2 + \sum_{i=1}^N \left( \max_{y \in \mathcal{Y}} \left( \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y) + c(y^{(i)}, y) \right) - \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) \right)_+, \quad [2.54]$$

where  $c(y, y^{(i)})$  is the cost function defined in Equation 2.46. We can now differentiate with respect to the weights,

<span id="page-52-1"></span>
$$\nabla_{\boldsymbol{\theta}} L_{\text{SVM}} = \lambda \boldsymbol{\theta} + \sum_{i=1}^{N} \boldsymbol{f}(\boldsymbol{x}^{(i)}, \hat{y}) - \boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}), \qquad [2.55]$$

where  $L_{\text{SVM}}$  refers to minimization objective in Equation 2.54 and  $\hat{y} = \operatorname{argmax}_{y \in \mathcal{Y}} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y) + c(y^{(i)}, y)$ . The online support vector machine update arises from the application of **stochastic gradient descent** (described in § 2.6.2) to this gradient.

# <span id="page-52-0"></span>2.5 Logistic regression

Thus far, we have seen two broad classes of learning algorithms. Naïve Bayes is a probabilistic method, where learning is equivalent to estimating a joint probability distribution. The perceptron and support vector machine are discriminative, error-driven algorithms: the learning objective is closely related to the number of errors on the training data. Probabilistic and error-driven approaches each have advantages: probability makes it possible to quantify uncertainty about the predicted labels, but the probability model of Naïve Bayes makes unrealistic independence assumptions that limit the features that can be used.

**Logistic regression** combines advantages of discriminative and probabilistic classifiers. Unlike Naïve Bayes, which starts from the **joint probability**  $p_{X,Y}$ , logistic regression defines the desired **conditional probability**  $p_{Y|X}$  directly. Think of  $\theta \cdot f(x, y)$  as a scoring function for the compatibility of the base features x and the label y. To convert this score into a probability, we first exponentiate, obtaining  $\exp(\theta \cdot f(x, y))$ , which is guaranteed to be non-negative. Next, we normalize, dividing over all possible labels  $y' \in \mathcal{Y}$ . The resulting conditional probability is defined as,

<span id="page-52-2"></span>
$$p(y \mid \boldsymbol{x}; \boldsymbol{\theta}) = \frac{\exp\left(\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}, y)\right)}{\sum_{y' \in \mathcal{Y}} \exp\left(\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}, y')\right)}.$$
[2.56]

<span id="page-53-2"></span>Given a dataset  $\mathcal{D} = \{(x^{(i)}, y^{(i)})\}_{i=1}^N$ , the weights  $\theta$  are estimated by maximum conditional likelihood,

$$\log p(\boldsymbol{y}^{(1:N)} \mid \boldsymbol{x}^{(1:N)}; \boldsymbol{\theta}) = \sum_{i=1}^{N} \log p(\boldsymbol{y}^{(i)} \mid \boldsymbol{x}^{(i)}; \boldsymbol{\theta})$$
[2.57]

$$=\sum_{i=1}^{N}\boldsymbol{\theta}\cdot\boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) - \log\sum_{y'\in\mathcal{Y}}\exp\left(\boldsymbol{\theta}\cdot\boldsymbol{f}(\boldsymbol{x}^{(i)}, y')\right). \quad [2.58]$$

The final line is obtained by plugging in Equation 2.56 and taking the logarithm.<sup>16</sup> Inside the sum, we have the (additive inverse of the) **logistic loss**,

$$\ell_{\text{LOGREG}}(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, y^{(i)}) = -\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) + \log \sum_{y' \in \mathcal{Y}} \exp(\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y'))$$
[2.59]

The logistic loss is shown in Figure 2.2 on page 31. A key difference from the zero-one and hinge losses is that logistic loss is never zero. This means that the objective function can always be improved by assigning higher confidence to the correct label.

#### <span id="page-53-0"></span>2.5.1 Regularization

As with the support vector machine, better generalization can be obtained by penalizing the norm of  $\theta$ . This is done by adding a multiple of the squared norm  $\frac{\lambda}{2}||\theta||_2^2$  to the minimization objective. This is called  $L_2$  regularization, because  $||\theta||_2^2$  is the squared  $L_2$ norm of the vector  $\theta$ . Regularization forces the estimator to trade off performance on the training data against the norm of the weights, and this can help to prevent overfitting. Consider what would happen to the unregularized weight for a base feature j that is active in only one instance  $x^{(i)}$ : the conditional log-likelihood could always be improved by increasing the weight for this feature, so that  $\theta_{(j,y^{(i)})} \to \infty$  and  $\theta_{(j,\tilde{y}\neq y^{(i)})} \to -\infty$ , where (j, y) is the index of feature associated with  $x_i^{(i)}$  and label y in  $f(x^{(i)}, y)$ .

In § 2.2.4 (footnote 11), we saw that smoothing the probabilities of a Naïve Bayes classifier can be justified as a form of maximum a posteriori estimation, in which the parameters of the classifier are themselves random variables, drawn from a **prior distribution**. The same justification applies to  $L_2$  regularization. In this case, the prior is a zero-mean Gaussian on each term of  $\theta$ . The log-likelihood under a zero-mean Gaussian is,

$$\log N(\theta_j; 0, \sigma^2) \propto -\frac{1}{2\sigma^2} \theta_j^2, \qquad [2.60]$$

so that the regularization weight  $\lambda$  is equal to the inverse variance of the prior,  $\lambda = \frac{1}{\sigma^2}$ .

<span id="page-53-1"></span><sup>&</sup>lt;sup>16</sup>The log-sum-exp term is a common pattern in machine learning. It is numerically unstable, because it will underflow if the inner product is small, and overflow if the inner product is large. Scientific computing libraries usually contain special functions for computing logsumexp, but with some thought, you should be able to see how to create an implementation that is numerically stable.

#### <span id="page-54-0"></span>2.5.2 Gradients

Logistic loss is minimized by optimization along the gradient. Specific algorithms are described in the next section, but first let's compute the gradient with respect to the logistic loss of a single example:

$$\ell_{\text{LOGREG}} = -\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) + \log \sum_{y' \in \mathcal{Y}} \exp\left(\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y')\right)$$
[2.61]

$$\frac{\partial \ell}{\partial \boldsymbol{\theta}} = -\boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) + \frac{1}{\sum_{y'' \in \mathcal{Y}} \exp\left(\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y'')\right)} \times \sum_{y' \in \mathcal{Y}} \exp\left(\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y')\right) \times \boldsymbol{f}(\boldsymbol{x}^{(i)}, y')$$
[2.62]

$$= -\boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) + \sum_{\boldsymbol{y}' \in \mathcal{Y}} \frac{\exp\left(\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y')\right)}{\sum_{\boldsymbol{y}'' \in \mathcal{Y}} \exp\left(\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y'')\right)} \times \boldsymbol{f}(\boldsymbol{x}^{(i)}, y')$$
[2.63]

$$= -\boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) + \sum_{\boldsymbol{y}' \in \mathcal{Y}} p(\boldsymbol{y}' \mid \boldsymbol{x}^{(i)}; \boldsymbol{\theta}) \times \boldsymbol{f}(\boldsymbol{x}^{(i)}, \boldsymbol{y}')$$
[2.64]

<span id="page-54-2"></span>
$$= -f(x^{(i)}, y^{(i)}) + E_{Y|X}[f(x^{(i)}, y)].$$
[2.65]

The final step employs the definition of a conditional expectation (§ A.5). The gradient of the logistic loss is equal to the difference between the expected counts under the current model,  $E_{Y|X}[f(\mathbf{x}^{(i)}, y)]$ , and the observed feature counts  $f(\mathbf{x}^{(i)}, y^{(i)})$ . When these two vectors are equal for a single instance, there is nothing more to learn from it; when they are equal in sum over the entire dataset, there is nothing more to learn from the dataset as a whole. The gradient of the hinge loss is nearly identical, but it involves the features of the predicted label under the current model,  $f(\mathbf{x}^{(i)}, \hat{y})$ , rather than the expected features  $E_{Y|X}[f(\mathbf{x}^{(i)}, y)]$  under the conditional distribution  $p(y | \mathbf{x}; \boldsymbol{\theta})$ .

The regularizer contributes  $\lambda \theta$  to the overall gradient:

$$L_{\text{LOGREG}} = \frac{\lambda}{2} ||\boldsymbol{\theta}||_2^2 - \sum_{i=1}^N \left( \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) - \log \sum_{y' \in \mathcal{Y}} \exp \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y') \right)$$
[2.66]

$$\nabla_{\boldsymbol{\theta}} L_{\text{LOGREG}} = \lambda \boldsymbol{\theta} - \sum_{i=1}^{N} \left( \boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) - E_{y|\boldsymbol{x}}[\boldsymbol{f}(\boldsymbol{x}^{(i)}, y)] \right).$$
[2.67]

#### <span id="page-54-1"></span>2.6 **Optimization**

Each of the classification algorithms in this chapter can be viewed as an optimization problem:

• In Naïve Bayes, the objective is the joint likelihood  $\log p(x^{(1:N)}, y^{(1:N)})$ . Maximum likelihood estimation yields a closed-form solution for  $\theta$ .

<span id="page-55-2"></span>• In the support vector machine, the objective is the regularized margin loss,

$$L_{\text{SVM}} = \frac{\lambda}{2} ||\boldsymbol{\theta}||_{2}^{2} + \sum_{i=1}^{N} (\max_{y \in \mathcal{Y}} (\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y) + c(y^{(i)}, y)) - \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}))_{+}, \quad [2.68]$$

There is no closed-form solution, but the objective is convex. The perceptron algorithm minimizes a similar objective.

In logistic regression, the objective is the regularized negative log-likelihood,

$$L_{\text{LOGREG}} = \frac{\lambda}{2} ||\boldsymbol{\theta}||_2^2 - \sum_{i=1}^N \left( \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) - \log \sum_{y \in \mathcal{Y}} \exp\left(\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y)\right) \right)$$
[2.69]

Again, there is no closed-form solution, but the objective is convex.

These learning algorithms are distinguished by *what* is being optimized, rather than *how* the optimal weights are found. This decomposition is an essential feature of contemporary machine learning. The domain expert's job is to design an objective function — or more generally, a **model** of the problem. If the model has certain characteristics, then generic optimization algorithms can be used to find the solution. In particular, if an objective function is differentiable, then gradient-based optimization can be employed; if it is also convex, then gradient-based optimization is guaranteed to find the globally optimal solution. The support vector machine and logistic regression have both of these properties, and so are amenable to generic **convex optimization** techniques (Boyd and Vandenberghe, 2004).

#### <span id="page-55-0"></span>2.6.1 Batch optimization

In **batch optimization**, each update to the weights is based on a computation involving the entire dataset. One such algorithm is **gradient descent**, which iteratively updates the weights,

$$\boldsymbol{\theta}^{(t+1)} \leftarrow \boldsymbol{\theta}^{(t)} - \eta^{(t)} \nabla_{\boldsymbol{\theta}} L, \qquad [2.70]$$

where  $\nabla_{\theta} L$  is the gradient computed over the entire training set, and  $\eta^{(t)}$  is the **learning** rate at iteration *t*. If the objective *L* is a convex function of  $\theta$ , then this procedure is guaranteed to terminate at the global optimum, for appropriate schedule of learning rates,  $\eta^{(t)}$ .<sup>17</sup>

<span id="page-55-1"></span><sup>&</sup>lt;sup>17</sup>Convergence proofs typically require the learning rate to satisfy the following conditions:  $\sum_{t=1}^{\infty} \eta^{(t)} = \infty$  and  $\sum_{t=1}^{\infty} (\eta^{(t)})^2 < \infty$  (Bottou et al., 2016). These properties are satisfied by any learning rate schedule  $\eta^{(t)} = \eta^{(0)}t^{-\alpha}$  for  $\alpha \in [1, 2]$ .

#### <span id="page-56-1"></span>2.6. OPTIMIZATION 39

In practice, gradient descent can be slow to converge, as the gradient can become infinitesimally small. Faster convergence can be obtained by second-order Newton optimization, which incorporates the inverse of the **Hessian matrix**,

$$H_{i,j} = \frac{\partial^2 L}{\partial \theta_i \partial \theta_j}$$
[2.71]

The size of the Hessian matrix is quadratic in the number of features. In the bag-of-words representation, this is usually too big to store, let alone invert. **Quasi-Network optimization** techniques maintain a low-rank approximation to the inverse of the Hessian matrix. Such techniques usually converge more quickly than gradient descent, while remaining computationally tractable even for large feature sets. A popular quasi-Newton algorithm is L-BFGS [\(Liu and Nocedal, 1989\)](#page-545-0), which is implemented in many scientific computing environments, such as SCIPY and MATLAB.

For any gradient-based technique, the user must set the learning rates η (t) . While convergence proofs usually employ a decreasing learning rate, in practice, it is common to fix η (t) to a small constant, like 10−<sup>3</sup> . The specific constant can be chosen by experimentation, although there is research on determining the learning rate automatically [\(Schaul et al.,](#page-560-0) [2013;](#page-560-0) [Wu et al., 2018\)](#page-570-1).

#### <span id="page-56-0"></span>**2.6.2 Online optimization**

Batch optimization computes the objective on the entire training set before making an update. This may be inefficient, because at early stages of training, a small number of training examples could point the learner in the correct direction. **Online learning** algorithms make updates to the weights while iterating through the training data. The theoretical basis for this approach is a stochastic approximation to the true objective function,

$$\sum_{i=1}^{N} \ell(\boldsymbol{\theta}; \boldsymbol{x}^{(i)}, y^{(i)}) \approx N \times \ell(\boldsymbol{\theta}; \boldsymbol{x}^{(j)}, y^{(j)}), \qquad (\boldsymbol{x}^{(j)}, y^{(j)}) \sim \{(\boldsymbol{x}^{(i)}, y^{(i)})\}_{i=1}^{N}, \quad [2.72]$$

where the instance (x (j) , y(j) ) is sampled at random from the full dataset.

In **stochastic gradient descent**, the approximate gradient is computed by randomly sampling a single instance, and an update is made immediately. This is similar to the perceptron algorithm, which also updates the weights one instance at a time. In **minibatch** stochastic gradient descent, the gradient is computed over a small set of instances. A typical approach is to set the minibatch size so that the entire batch fits in memory on a graphics processing unit (GPU; [Neubig et al., 2017\)](#page-551-0). It is then possible to speed up learning by parallelizing the computation of the gradient over each instance in the minibatch.

[Algorithm 5](#page-57-0) offers a generalized view of gradient descent. In standard gradient descent, the batcher returns a single batch with all the instances. In stochastic gradient de-

<span id="page-57-0"></span>**Algorithm 5** Generalized gradient descent. The function BATCHER partitions the training set into *B* batches such that each instance appears in exactly one batch. In gradient descent, B = 1; in stochastic gradient descent, B = N; in minibatch stochastic gradient descent, 1 < B < N.

```
1: procedure GRADIENT-DESCENT(\boldsymbol{x}^{(1:N)}, \boldsymbol{y}^{(1:N)}, L, \eta^{(1...\infty)}, BATCHER, T_{max})
               oldsymbol{	heta} \leftarrow \mathbf{0}
 2:
               t \leftarrow 0
 3:
               repeat
  4:
                       (\boldsymbol{b}^{(1)}, \boldsymbol{b}^{(2)}, \dots, \boldsymbol{b}^{(B)}) \leftarrow \text{BATCHER}(N)
  5:
                       for n \in \{1, 2, ..., B\} do
  6:
                              t \leftarrow t + 1
  7:
                              \boldsymbol{\theta}^{(t)} \leftarrow \boldsymbol{\theta}^{(t-1)} - \eta^{(t)} \nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}^{(t-1)}; \boldsymbol{x}^{(b_1^{(n)}, b_2^{(n)}, \ldots)}, \boldsymbol{y}^{(b_1^{(n)}, b_2^{(n)}, \ldots)})
  8.
                              if Converged(\theta^{(1,2,\ldots,t)}) then
  9:
                                      return \theta^{(t)}
10:
               until t > T_{max}
11:
               return \theta^{(t)}
12:
```

scent, it returns N batches with one instance each. In mini-batch settings, the batcher returns B minibatches, 1 < B < N.

There are many other techniques for online learning, and research in this area is ongoing (Bottou et al., 2016). Some algorithms use an adaptive learning rate, which can be different for every feature (Duchi et al., 2011). Features that occur frequently are likely to be updated frequently, so it is best to use a small learning rate; rare features will be updated infrequently, so it is better to take larger steps. The **AdaGrad** (adaptive gradient) algorithm achieves this behavior by storing the sum of the squares of the gradients for each feature, and rescaling the learning rate by its inverse:

$$\boldsymbol{g}_t = \nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}^{(t)}; \boldsymbol{x}^{(i)}, y^{(i)})$$
[2.73]

$$\theta_{j}^{(t+1)} \leftarrow \theta_{j}^{(t)} - \frac{\eta^{(t)}}{\sqrt{\sum_{t'=1}^{t} g_{t,j}^{2}}} g_{t,j}, \qquad [2.74]$$

where *j* iterates over features in f(x, y).

In most cases, the number of active features for any instance is much smaller than the number of weights. If so, the computation cost of online optimization will be dominated by the update from the regularization term,  $\lambda \theta$ . The solution is to be "lazy", updating each  $\theta_j$  only as it is used. To implement lazy updating, store an additional parameter  $\tau_j$ , which is the iteration at which  $\theta_j$  was last updated. If  $\theta_j$  is needed at time t, the  $t - \tau$  regularization updates can be performed all at once. This strategy is described in detail by Kummerfeld et al. (2015).

Jacob Eisenstein. Draft of November 13, 2018.

<span id="page-57-1"></span>40

# <span id="page-58-3"></span><span id="page-58-0"></span>2.7 \*Additional topics in classification

This section presents some additional topics in classification that are particularly relevant for natural language processing, especially for understanding the research literature.

#### <span id="page-58-1"></span>2.7.1 Feature selection by regularization

In logistic regression and large-margin classification, generalization can be improved by regularizing the weights towards 0, using the  $L_2$  norm. But rather than encouraging weights to be small, it might be better for the model to be **sparse**: it should assign weights of exactly zero to most features, and only assign non-zero weights to features that are clearly necessary. This idea can be formalized by the  $L_0$  norm,  $L_0 = ||\theta||_0 = \sum_j \delta (\theta_j \neq 0)$ , which applies a constant penalty for each non-zero weight. This norm can be thought of as a form of **feature selection**: optimizing the  $L_0$ -regularized conditional likelihood is equivalent to trading off the log-likelihood against the number of active features. Reducing the number of active features is desirable because the resulting model will be fast, low-memory, and should generalize well, since irrelevant features will be pruned away. Unfortunately, the  $L_0$  norm is non-convex and non-differentiable. Optimization under  $L_0$  regularization is **NP-hard**, meaning that it can be solved efficiently only if P=NP (Ge et al., 2011).

A useful alternative is the  $L_1$  norm, which is equal to the sum of the absolute values of the weights,  $||\theta||_1 = \sum_j |\theta_j|$ . The  $L_1$  norm is convex, and can be used as an approximation to  $L_0$  (Tibshirani, 1996). Conveniently, the  $L_1$  norm also performs feature selection, by driving many of the coefficients to zero; it is therefore known as a **sparsity inducing regularizer**. The  $L_1$  norm does not have a gradient at  $\theta_j = 0$ , so we must instead optimize the  $L_1$ -regularized objective using **subgradient** methods. The associated stochastic subgradient descent algorithms are only somewhat more complex than conventional SGD; Sra et al. (2012) survey approaches for estimation under  $L_1$  and other regularizers.

Gao et al. (2007) compare  $L_1$  and  $L_2$  regularization on a suite of NLP problems, finding that  $L_1$  regularization generally gives similar accuracy to  $L_2$  regularization, but that  $L_1$ regularization produces models that are between ten and fifty times smaller, because more than 90% of the feature weights are set to zero.

#### <span id="page-58-2"></span>2.7.2 Other views of logistic regression

In binary classification, we can dispense with the feature function, and choose *y* based on the inner product of  $\theta \cdot x$ . The conditional probability  $p_{Y|X}$  is obtained by passing this

<span id="page-59-1"></span>inner product through a logistic function,

$$\sigma(a) \triangleq \frac{\exp(a)}{1 + \exp(a)} = (1 + \exp(-a))^{-1}$$
[2.75]

$$\mathbf{p}(y \mid \boldsymbol{x}; \boldsymbol{\theta}) = \sigma(\boldsymbol{\theta} \cdot \boldsymbol{x}).$$
[2.76]

This is the origin of the name "logistic regression." Logistic regression can be viewed as part of a larger family of **generalized linear models** (GLMs), in which various other **link functions** convert between the inner product  $\theta \cdot x$  and the parameter of a conditional probability distribution.

Logistic regression and related models are sometimes referred to as **log-linear**, because the log-probability is a linear function of the features. But in the early NLP literature, logistic regression was often called **maximum entropy** classification (Berger et al., 1996). This name refers to an alternative formulation, in which the goal is to find the maximum entropy probability function that satisfies **moment-matching** constraints. These constraints specify that the empirical counts of each feature should match the expected counts under the induced probability distribution  $p_{Y|X;\theta'}$ 

<span id="page-59-0"></span>
$$\sum_{i=1}^{N} f_j(\boldsymbol{x}^{(i)}, y^{(i)}) = \sum_{i=1}^{N} \sum_{y \in \mathcal{Y}} p(y \mid \boldsymbol{x}^{(i)}; \boldsymbol{\theta}) f_j(\boldsymbol{x}^{(i)}, y), \quad \forall j$$
[2.77]

The moment-matching constraint is satisfied exactly when the derivative of the conditional log-likelihood function (Equation 2.65) is equal to zero. However, the constraint can be met by many values of  $\theta$ , so which should we choose?

The **entropy** of the conditional probability distribution  $p_{Y|X}$  is,

$$H(\mathbf{p}_{Y|X}) = -\sum_{\boldsymbol{x}\in\mathcal{X}} \mathbf{p}_X(\boldsymbol{x}) \sum_{\boldsymbol{y}\in\mathcal{Y}} \mathbf{p}_{Y|X}(\boldsymbol{y} \mid \boldsymbol{x}) \log \mathbf{p}_{Y|X}(\boldsymbol{y} \mid \boldsymbol{x}),$$
[2.78]

where  $\mathcal{X}$  is the set of all possible feature vectors, and  $p_X(x)$  is the probability of observing the base features x. The distribution  $p_X$  is unknown, but it can be estimated by summing over all the instances in the training set,

$$\tilde{H}(\mathbf{p}_{Y|X}) = -\frac{1}{N} \sum_{i=1}^{N} \sum_{y \in \mathcal{Y}} \mathbf{p}_{Y|X}(y \mid \boldsymbol{x}^{(i)}) \log \mathbf{p}_{Y|X}(y \mid \boldsymbol{x}^{(i)}).$$
[2.79]

If the entropy is large, the likelihood function is smooth across possible values of y; if it is small, the likelihood function is sharply peaked at some preferred value; in the limiting case, the entropy is zero if p(y | x) = 1 for some y. The maximum-entropy criterion chooses to make the weakest commitments possible, while satisfying the moment-matching constraints from Equation 2.77. The solution to this constrained optimization problem is identical to the maximum conditional likelihood (logistic-loss) formulation that was presented in § 2.5.

#### <span id="page-60-1"></span>2.8. SUMMARY OF LEARNING ALGORITHMS 43

# <span id="page-60-0"></span>**2.8 Summary of learning algorithms**

It is natural to ask which learning algorithm is best, but the answer depends on what characteristics are important to the problem you are trying to solve.

- **Na¨ıve Bayes** *Pros*: easy to implement; estimation is fast, requiring only a single pass over the data; assigns probabilities to predicted labels; controls overfitting with smoothing parameter. *Cons*: often has poor accuracy, especially with correlated features.
- **Perceptron** *Pros*: easy to implement; online; error-driven learning means that accuracy is typically high, especially after averaging. *Cons*: not probabilistic; hard to know when to stop learning; lack of margin can lead to overfitting.
- **Support vector machine** *Pros*: optimizes an error-based metric, usually resulting in high accuracy; overfitting is controlled by a regularization parameter. *Cons*: not probabilistic.
- **Logistic regression** *Pros*: error-driven and probabilistic; overfitting is controlled by a regularization parameter. *Cons*: batch learning requires black-box optimization; logistic loss can "overtrain" on correctly labeled examples.

One of the main distinctions is whether the learning algorithm offers a probability over labels. This is useful in modular architectures, where the output of one classifier is the input for some other system. In cases where probability is not necessary, the support vector machine is usually the right choice, since it is no more difficult to implement than the perceptron, and is often more accurate. When probability is necessary, logistic regression is usually more accurate than Na¨ıve Bayes.

# **Additional resources**

A machine learning textbook will offer more classifiers and more details (e.g., [Murphy,](#page-551-1) [2012\)](#page-551-1), although the notation will differ slightly from what is typical in natural language processing. Probabilistic methods are surveyed by [Hastie et al.](#page-533-0) [\(2009\)](#page-533-0), and [Mohri et al.](#page-550-0) [\(2012\)](#page-550-0) emphasize theoretical considerations. [Bottou et al.](#page-515-1) [\(2016\)](#page-515-1) surveys the rapidly moving field of online learning, and [Kummerfeld et al.](#page-541-2) [\(2015\)](#page-541-2) empirically review several optimization algorithms for large-margin learning. The python toolkit SCIKIT-LEARN includes implementations of all of the algorithms described in this chapter [\(Pedregosa et al., 2011\)](#page-554-0).

[Appendix B](#page-502-0) describes an alternative large-margin classifier, called **passive-aggressive**. Passive-aggressive is an online learner that seeks to make the smallest update that satisfies the margin constraint at the current instance. It is closely related to MIRA, which was used widely in NLP in the 2000s [\(Crammer and Singer, 2003\)](#page-522-2).

### **Exercises**

There will be exercises at the end of each chapter. In this chapter, the exercises are mostly mathematical, matching the subject material. In other chapters, the exercises will emphasize linguistics or programming.

- 1. Let x be a bag-of-words vector such that  $\sum_{j=1}^{V} x_j = 1$ . Verify that the multinomial probability  $p_{mult}(x; \phi)$ , as defined in Equation 2.12, is identical to the probability of the same document under a categorical distribution,  $p_{cat}(w; \phi)$ .
- 2. Suppose you have a single feature *x*, with the following conditional distribution:

$$\mathbf{p}(x \mid y) = \begin{cases} \alpha, & X = 0, Y = 0\\ 1 - \alpha, & X = 1, Y = 0\\ 1 - \beta, & X = 0, Y = 1\\ \beta, & X = 1, Y = 1. \end{cases}$$
[2.80]

Further suppose that the prior is uniform,  $Pr(Y = 0) = Pr(Y = 1) = \frac{1}{2}$ , and that both  $\alpha > \frac{1}{2}$  and  $\beta > \frac{1}{2}$ . Given a Naïve Bayes classifier with accurate parameters, what is the probability of making an error?

- 3. Derive the maximum-likelihood estimate for the parameter  $\mu$  in Naïve Bayes.
- 4. The classification models in the text have a vector of weights for each possible label. While this is notationally convenient, it is overdetermined: for any linear classifier that can be obtained with  $K \times V$  weights, an equivalent classifier can be constructed using  $(K 1) \times V$  weights.
  - a) Describe how to construct this classifier. Specifically, if given a set of weights  $\theta$  and a feature function f(x, y), explain how to construct alternative weights and feature function  $\theta'$  and f'(x, y), such that,

$$\forall y, y' \in \mathcal{Y}, \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}, y) - \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}, y') = \boldsymbol{\theta}' \cdot \boldsymbol{f}'(\boldsymbol{x}, y) - \boldsymbol{\theta}' \cdot \boldsymbol{f}'(\boldsymbol{x}, y').$$
[2.81]

- b) Explain how your construction justifies the well-known alternative form for binary logistic regression,  $Pr(Y = 1 | \boldsymbol{x}; \boldsymbol{\theta}) = \frac{1}{1 + \exp(-\boldsymbol{\theta}' \cdot \boldsymbol{x})} = \sigma(\boldsymbol{\theta}' \cdot \boldsymbol{x})$ , where  $\sigma$  is the sigmoid function.
- 5. Suppose you have two labeled datasets  $D_1$  and  $D_2$ , with the same features and labels.
  - Let θ<sup>(1)</sup> be the unregularized logistic regression (LR) coefficients from training on dataset D<sub>1</sub>.

- Let θ<sup>(2)</sup> be the unregularized LR coefficients (same model) from training on dataset D<sub>2</sub>.
- Let θ<sup>\*</sup> be the unregularized LR coefficients from training on the combined dataset D<sub>1</sub> ∪ D<sub>2</sub>.

Under these conditions, prove that for any feature *j*,

$$\theta_j^* \ge \min(\theta_j^{(1)}, \theta_j^{(2)})$$
$$\theta_j^* \le \max(\theta_j^{(1)}, \theta_j^{(2)}).$$

- 6. Let  $\hat{\theta}$  be the solution to an unregularized logistic regression problem, and let  $\theta^*$  be the solution to the same problem, with  $L_2$  regularization. Prove that  $||\theta^*||_2^2 \le ||\hat{\theta}||_2^2$ .
- 7. As noted in the discussion of averaged perceptron in § 2.3.2, the computation of the running sum  $m \leftarrow m + \theta$  is unnecessarily expensive, requiring  $K \times V$  operations. Give an alternative way to compute the averaged weights  $\overline{\theta}$ , with complexity that is independent of V and linear in the sum of feature sizes  $\sum_{i=1}^{N} |f(x^{(i)}, y^{(i)})|$ .
- 8. Consider a dataset that is comprised of two identical instances  $x^{(1)} = x^{(2)}$  with distinct labels  $y^{(1)} \neq y^{(2)}$ . Assume all features are binary,  $x_i \in \{0, 1\}$  for all j.

Now suppose that the averaged perceptron always trains on the instance  $(x^{i(t)}, y^{i(t)})$ , where  $i(t) = 2 - (t \mod 2)$ , which is 1 when the training iteration *t* is odd, and 2 when *t* is even. Further suppose that learning terminates under the following condition:

$$\epsilon \ge \max_{j} \left| \frac{1}{t} \sum_{t} \theta_{j}^{(t)} - \frac{1}{t-1} \sum_{t} \theta_{j}^{(t-1)} \right|.$$
[2.82]

In words, the algorithm stops when the largest change in the averaged weights is less than or equal to  $\epsilon$ . Compute the number of iterations before the averaged perceptron terminates.

9. Prove that the margin loss is convex in  $\theta$ . Use this definition of the margin loss:

$$L(\boldsymbol{\theta}) = -\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}, y^*) + \max_{\boldsymbol{y}} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}, y) + c(y^*, y), \qquad [2.83]$$

where  $y^*$  is the gold label. As a reminder, a function *f* is convex iff,

$$f(\alpha x_1 + (1 - \alpha)x_2) \le \alpha f(x_1) + (1 - \alpha)f(x_2),$$
[2.84]

for any  $x_1, x_2$  and  $\alpha \in [0, 1]$ .

10. If a function f is m-strongly convex, then for some m > 0, the following inequality holds for all x and x <sup>0</sup> on the domain of the function:

$$f(x') \le f(x) + (\nabla_x f) \cdot (x' - x) + \frac{m}{2} ||x' - x||_2^2.$$
 [2.85]

Let f(x) = L(θ (t) ), representing the loss of the classifier at iteration t of gradient descent; let f(x 0 ) = L(θ (t+1)). Assuming the loss function is m-convex, prove that L(θ (t+1)) <sup>≤</sup> <sup>L</sup>(<sup>θ</sup> (t) ) for an appropriate constant learning rate η, which will depend on m. Explain why this implies that gradient descent converges when applied to an m-strongly convex loss function with a unique minimum.

# <span id="page-64-1"></span><span id="page-64-0"></span>**Chapter 3 Nonlinear classification**

Linear classification may seem like all we need for natural language processing. The bagof-words representation is inherently high dimensional, and the number of features is often larger than the number of labeled training instances. This means that it is usually possible to find a linear classifier that perfectly fits the training data, or even to fit any arbitrary labeling of the training instances! Moving to nonlinear classification may therefore only increase the risk of overfitting. Furthermore, for many tasks, **lexical features** (words) are meaningful in isolation, and can offer independent evidence about the instance label — unlike computer vision, where individual pixels are rarely informative, and must be evaluated holistically to make sense of an image. For these reasons, natural language processing has historically focused on linear classification.

But in recent years, nonlinear classifiers have swept through natural language processing, and are now the default approach for many tasks [\(Manning, 2015\)](#page-546-1). There are at least three reasons for this change.

- There have been rapid advances in **deep learning**, a family of nonlinear methods that learn complex functions of the input through multiple layers of computation [\(Goodfellow et al., 2016\)](#page-531-1).
- Deep learning facilitates the incorporation of **word embeddings**, which are dense vector representations of words. Word embeddings can be learned from large amounts of unlabeled data, and enable generalization to words that do not appear in the annotated training data (word embeddings are discussed in detail in [chapter 14\)](#page-342-0).
- While CPU speeds have plateaued, there have been rapid advances in specialized hardware called graphics processing units (GPUs), which have become faster, cheaper, and easier to program. Many deep learning models can be implemented efficiently on GPUs, offering substantial performance improvements over CPU-based computing.

<span id="page-65-2"></span>This chapter focuses on **neural networks**, which are the dominant approach for nonlinear classification in natural language processing today.[1](#page-65-1) Historically, a few other nonlinear learning methods have been applied to language data.

- **Kernel methods** are generalizations of the **nearest-neighbor** classification rule, which classifies each instance by the label of the most similar example in the training set. The application of the **kernel support vector machine** to information extraction is described in [chapter 17.](#page-420-0)
- **Decision trees** classify instances by checking a set of conditions. Scaling decision trees to bag-of-words inputs is difficult, but decision trees have been successful in problems such as coreference resolution [\(chapter 15\)](#page-368-0), where more compact feature sets can be constructed [\(Soon et al., 2001\)](#page-562-0).
- **Boosting** and related **ensemble methods** work by combining the predictions of several "weak" classifiers, each of which may consider only a small subset of features. Boosting has been successfully applied to text classification [\(Schapire and Singer,](#page-560-1) [2000\)](#page-560-1) and syntactic analysis [\(Abney et al., 1999\)](#page-509-0), and remains one of the most successful methods on machine learning competition sites such as Kaggle [\(Chen and](#page-519-0) [Guestrin, 2016\)](#page-519-0).

[Hastie et al.](#page-533-0) [\(2009\)](#page-533-0) provide an excellent overview of these techniques.

# <span id="page-65-0"></span>**3.1 Feedforward neural networks**

Consider the problem of building a classifier for movie reviews. The goal is to predict a label y ∈ {GOOD, BAD, OKAY} from a representation of the text of each document, x. But what makes a good movie? The story, acting, cinematography, editing, soundtrack, and so on. Now suppose the training set contains labels for each of these additional features, z = [z1, z2, . . . , zK<sup>z</sup> ] <sup>&</sup>gt;. With a training set of such information, we could build a two-step classifier:

- 1. **Use the text** x **to predict the features** z. Specifically, train a logistic regression classifier to compute p(z<sup>k</sup> | x), for each k ∈ {1, 2, . . . , Kz}.
- 2. **Use the features** z **to predict the label** y. Again, train a logistic regression classifier to compute p(y | z). On test data, z is unknown, so we will use the probabilities p(z | x) from the first layer as the features.

This setup is shown in [Figure 3.1,](#page-66-0) which describes the proposed classifier in a **computation graph**: the text features x are connected to the middle layer z, which is connected to the label y.

<span id="page-65-1"></span><sup>1</sup> I will use "deep learning" and "neural networks" interchangeably.

#### <span id="page-66-2"></span><span id="page-66-0"></span>3.1. FEEDFORWARD NEURAL NETWORKS 49

![](_page_66_Figure_1.jpeg)

Figure 3.1: A feedforward neural network. Shaded circles indicate observed features, usually words; squares indicate nodes in the computation graph, which are computed from the information carried over the incoming arrows.

If we assume that each z<sup>k</sup> is binary, z<sup>k</sup> ∈ {0, 1}, then the probability p(z<sup>k</sup> | x) can be modeled using binary logistic regression:

$$\Pr(z_k = 1 \mid \boldsymbol{x}; \boldsymbol{\Theta}^{(x \to z)}) = \sigma(\boldsymbol{\theta}_k^{(x \to z)} \cdot \boldsymbol{x}) = (1 + \exp(-\boldsymbol{\theta}_k^{(x \to z)} \cdot \boldsymbol{x}))^{-1}, \quad [3.1]$$

where <sup>σ</sup> is the **sigmoid** function (shown in [Figure 3.2\)](#page-67-2), and the matrix <sup>Θ</sup>(x→z) <sup>∈</sup> <sup>R</sup> Kz×V is constructed by stacking the weight vectors for each zk,

$$\boldsymbol{\Theta}^{(x \to z)} = [\boldsymbol{\theta}_1^{(x \to z)}, \boldsymbol{\theta}_2^{(x \to z)}, \dots, \boldsymbol{\theta}_{K_z}^{(x \to z)}]^\top.$$
[3.2]

We will assume that x contains a term with a constant value of 1, so that a corresponding offset parameter is included in each θ (x→z) k .

The output layer is computed by the multi-class logistic regression probability,

<span id="page-66-1"></span>
$$\Pr(y = j \mid \boldsymbol{z}; \boldsymbol{\Theta}^{(z \to y)}, \boldsymbol{b}) = \frac{\exp(\boldsymbol{\theta}_j^{(z \to y)} \cdot \boldsymbol{z} + b_j)}{\sum_{j' \in \mathcal{Y}} \exp(\boldsymbol{\theta}_{j'}^{(z \to y)} \cdot \boldsymbol{z} + b_{j'})},$$
[3.3]

where <sup>b</sup><sup>j</sup> is an offset for label <sup>j</sup>, and the output weight matrix <sup>Θ</sup>(z→y) <sup>∈</sup> <sup>R</sup> Ky×K<sup>z</sup> is again constructed by concatenation,

$$\boldsymbol{\Theta}^{(z \to y)} = [\boldsymbol{\theta}_1^{(z \to y)}, \boldsymbol{\theta}_2^{(z \to y)}, \dots, \boldsymbol{\theta}_{K_y}^{(z \to y)}]^\top.$$
[3.4]

The vector of probabilities over each possible value of y is denoted,

$$p(\boldsymbol{y} \mid \boldsymbol{z}; \boldsymbol{\Theta}^{(z \to y)}, \boldsymbol{b}) = \text{SoftMax}(\boldsymbol{\Theta}^{(z \to y)} \boldsymbol{z} + \boldsymbol{b}),$$
[3.5]

where element j in the output of the **SoftMax** function is computed as in [Equation 3.3.](#page-66-1)

This set of equations defines a multilayer classifier, which can be summarized as,

$$p(\boldsymbol{z} \mid \boldsymbol{x}; \boldsymbol{\Theta}^{(x \to z)}) = \sigma(\boldsymbol{\Theta}^{(x \to z)} \boldsymbol{x})$$
[3.6]

$$p(\boldsymbol{y} \mid \boldsymbol{z}; \boldsymbol{\Theta}^{(z \to y)}, \boldsymbol{b}) = \text{SoftMax}(\boldsymbol{\Theta}^{(z \to y)} \boldsymbol{z} + \boldsymbol{b}),$$
[3.7]

<span id="page-67-5"></span><span id="page-67-2"></span>![](_page_67_Figure_1.jpeg)

Figure 3.2: The sigmoid, tanh, and ReLU activation functions

where the function  $\sigma$  is now applied **elementwise** to the vector of inner products,

$$\sigma(\boldsymbol{\Theta}^{(x \to z)} \boldsymbol{x}) = [\sigma(\boldsymbol{\theta}_1^{(x \to z)} \cdot \boldsymbol{x}), \sigma(\boldsymbol{\theta}_2^{(x \to z)} \cdot \boldsymbol{x}), \dots, \sigma(\boldsymbol{\theta}_{K_z}^{(x \to z)} \cdot \boldsymbol{x})]^{\top}.$$
 [3.8]

Now suppose that the hidden features z are never observed, even in the training data. We can still construct the architecture in Figure 3.1. Instead of predicting y from a discrete vector of predicted values z, we use the probabilities  $\sigma(\theta_k \cdot x)$ . The resulting classifier is barely changed:

<span id="page-67-4"></span>
$$\boldsymbol{z} = \sigma(\boldsymbol{\Theta}^{(x \to z)} \boldsymbol{x})$$
[3.9]

$$p(y \mid \boldsymbol{x}; \boldsymbol{\Theta}^{(z \to y)}, \boldsymbol{b}) = \text{SoftMax}(\boldsymbol{\Theta}^{(z \to y)} \boldsymbol{z} + \boldsymbol{b}).$$
[3.10]

This defines a classification model that predicts the label  $y \in \mathcal{Y}$  from the base features x, through a "hidden layer" z. This is a **feedforward neural network**.<sup>2</sup>

# <span id="page-67-0"></span>3.2 Designing neural networks

There several ways to generalize the feedforward neural network.

#### <span id="page-67-1"></span>3.2.1 Activation functions

If the hidden layer is viewed as a set of latent features, then the sigmoid function in Equation 3.9 represents the extent to which each of these features is "activated" by a given input. However, the hidden layer can be regarded more generally as a nonlinear transformation of the input. This opens the door to many other activation functions, some of which are shown in Figure 3.2. At the moment, the choice of activation functions is more art than science, but a few points can be made about the most popular varieties:

<span id="page-67-3"></span><sup>&</sup>lt;sup>2</sup>The architecture is sometimes called a **multilayer perceptron**, but this is misleading, because each layer is not a perceptron as defined in the previous chapter.

#### <span id="page-68-2"></span>3.2. DESIGNING NEURAL NETWORKS

- The range of the sigmoid function is (0, 1). The bounded range ensures that a cascade of sigmoid functions will not "blow up" to a huge output, and this is important for deep networks with several hidden layers. The derivative of the sigmoid is  $\frac{\partial}{\partial a}\sigma(a) = \sigma(a)(1 \sigma(a))$ . This derivative becomes small at the extremes, which can make learning slow; this is called the **vanishing gradient** problem.
- The range of the **tanh activation function** is (-1,1): like the sigmoid, the range is bounded, but unlike the sigmoid, it includes negative values. The derivative is  $\frac{\partial}{\partial a} \tanh(a) = 1 \tanh(a)^2$ , which is steeper than the logistic function near the origin (LeCun et al., 1998). The tanh function can also suffer from vanishing gradients at extreme values.
- The **rectified linear unit (ReLU)** is zero for negative inputs, and linear for positive inputs (Glorot et al., 2011),

$$\operatorname{ReLU}(a) = \begin{cases} a, & a \ge 0\\ 0, & \text{otherwise.} \end{cases}$$
[3.11]

The derivative is a step function, which is 1 if the input is positive, and zero otherwise. Once the activation is zero, the gradient is also zero. This can lead to the problem of "dead neurons", where some ReLU nodes are zero for all inputs, throughout learning. A solution is the **leaky ReLU**, which has a small positive slope for negative inputs (Maas et al., 2013),

Leaky-ReLU(a) = 
$$\begin{cases} a, & a \ge 0\\ .0001a, & \text{otherwise.} \end{cases}$$
 [3.12]

Sigmoid and tanh are sometimes described as **squashing functions**, because they squash an unbounded input into a bounded range. Glorot and Bengio (2010) recommend against the use of the sigmoid activation in deep networks, because its mean value of  $\frac{1}{2}$  can cause the next layer of the network to be saturated, leading to small gradients on its own parameters. Several other activation functions are reviewed in the textbook by Goodfellow et al. (2016), who recommend ReLU as the "default option."

#### <span id="page-68-0"></span>3.2.2 Network structure

Deep networks stack up several hidden layers, with each  $z^{(d)}$  acting as the input to the next layer,  $z^{(d+1)}$ . As the total number of nodes in the network increases, so does its capacity to learn complex functions of the input. Given a fixed number of nodes, one must decide whether to emphasize width (large  $K_z$  at each layer) or depth (many layers). At present, this tradeoff is not well understood.<sup>3</sup>

<span id="page-68-1"></span><sup>&</sup>lt;sup>3</sup>With even a single hidden layer, a neural network can approximate any continuous function on a closed and bounded subset of  $\mathbb{R}^N$  to an arbitrarily small non-zero error; see section 6.4.1 of Goodfellow et al. (2016)

<span id="page-69-1"></span>It is also possible to "short circuit" a hidden layer, by propagating information directly from the input to the next higher level of the network. This is the idea behind **residual networks**, which propagate information directly from the input to the subsequent layer [\(He](#page-534-0) [et al., 2016\)](#page-534-0),

$$\boldsymbol{z} = f(\boldsymbol{\Theta}^{(x \to z)} \boldsymbol{x}) + \boldsymbol{x}, \qquad [3.13]$$

where f is any nonlinearity, such as sigmoid or ReLU. A more complex architecture is the **highway network** [\(Srivastava et al., 2015;](#page-563-1) [Kim et al., 2016\)](#page-539-1), in which an addition **gate** controls an interpolation between f(Θ(x→z)x) and x,

$$\boldsymbol{t} = \sigma(\boldsymbol{\Theta}^{(t)}\boldsymbol{x} + \boldsymbol{b}^{(t)})$$
[3.14]

$$\boldsymbol{z} = \boldsymbol{t} \odot f(\boldsymbol{\Theta}^{(x \to z)} \boldsymbol{x}) + (\boldsymbol{1} - \boldsymbol{t}) \odot \boldsymbol{x}, \qquad [3.15]$$

where  refers to an elementwise vector product, and 1 is a column vector of ones. As before, the sigmoid function is applied elementwise to its input; recall that the output of this function is restricted to the range (0, 1). Gating is also used in the **long short-term memory (LSTM)**, which is discussed in [chapter 6.](#page-142-0) Residual and highway connections address a problem with deep architectures: repeated application of a nonlinear activation function can make it difficult to learn the parameters of the lower levels of the network, which are too distant from the supervision signal.

#### <span id="page-69-0"></span>**3.2.3 Outputs and loss functions**

In the multi-class classification example, a softmax output produces probabilities over each possible label. This aligns with a negative **conditional log-likelihood**,

$$-\mathcal{L} = -\sum_{i=1}^{N} \log p(y^{(i)} \mid \boldsymbol{x}^{(i)}; \Theta).$$
[3.16]

where Θ = {Θ(x→z) , Θ(z→y) , b} is the entire set of parameters.

This loss can be written alternatively as follows:

$$\tilde{y}_j \triangleq \Pr(y = j \mid \boldsymbol{x}^{(i)}; \Theta)$$
[3.17]

$$-\mathcal{L} = -\sum_{i=1}^{N} \boldsymbol{e}_{y^{(i)}} \cdot \log \tilde{\boldsymbol{y}}$$
[3.18]

for a survey of these theoretical results. However, depending on the function to be approximated, the width of the hidden layer may need to be arbitrarily large. Furthermore, the fact that a network has the *capacity* to approximate any given function does not imply that it is possible to *learn* the function using gradient-based optimization.

#### <span id="page-70-2"></span>3.3. LEARNING NEURAL NETWORKS

where  $e_{y^{(i)}}$  is a **one-hot vector** of zeros with a value of 1 at position  $y^{(i)}$ . The inner product between  $e_{y^{(i)}}$  and  $\log \tilde{y}$  is also called the multinomial **cross-entropy**, and this terminology is preferred in many neural networks papers and software packages.

It is also possible to train neural networks from other objectives, such as a margin loss. In this case, it is not necessary to use softmax at the output layer: an affine transformation of the hidden layer is enough:

$$\Psi(y; \boldsymbol{x}^{(i)}, \Theta) = \boldsymbol{\theta}_{y}^{(z \to y)} \cdot \boldsymbol{z} + b_{y}$$
[3.19]

$$\ell_{\text{MARGIN}}(\Theta; \boldsymbol{x}^{(i)}, y^{(i)}) = \max_{y \neq y^{(i)}} \left( 1 + \Psi(y; \boldsymbol{x}^{(i)}, \Theta) - \Psi(y^{(i)}; \boldsymbol{x}^{(i)}, \Theta) \right)_{+}.$$
 [3.20]

In regression problems, the output is a scalar or vector (see § 4.1.2). For these problems, a typical loss function is the squared error  $(y - \hat{y})^2$  or squared norm  $||\boldsymbol{y} - \hat{\boldsymbol{y}}||_2^2$ .

#### <span id="page-70-0"></span>3.2.4 Inputs and lookup layers

In text classification, the input layer x can refer to a bag-of-words vector, where  $x_j$  is the count of word j. The input to the hidden unit  $z_k$  is then  $\sum_{j=1}^{V} \theta_{j,k}^{(x \to z)} x_j$ , and word j is represented by the vector  $\theta_j^{(x \to z)}$ . This vector is sometimes described as the **embedding** of word j, and can be learned from unlabeled data, using techniques discussed in chapter 14. The columns of  $\Theta^{(x \to z)}$  are each  $K_z$ -dimensional word embeddings.

Chapter 2 presented an alternative view of text documents, as a sequence of word tokens,  $w_1, w_2, \ldots, w_M$ . In a neural network, each word token  $w_m$  is represented with a one-hot vector,  $e_{w_m}$ , with dimension V. The matrix-vector product  $\Theta^{(x \to z)} e_{w_m}$  returns the embedding of word  $w_m$ . The complete document can represented by horizontally concatenating these one-hot vectors,  $\mathbf{W} = [e_{w_1}, e_{w_2}, \ldots, e_{w_M}]$ , and the bag-of-words representation can be recovered from the matrix-vector product  $\mathbf{W}[1, 1, \ldots, 1]^{\top}$ , which sums each row over the tokens  $m = \{1, 2, \ldots, M\}$ . The matrix product  $\Theta^{(x \to z)}\mathbf{W}$  contains the horizontally concatenated embeddings of each word in the document, which will be useful as the starting point for **convolutional neural networks** (see § 3.4). This is sometimes called a **lookup layer**, because the first step is to lookup the embeddings for each word in the input text.

#### <span id="page-70-1"></span>3.3 Learning neural networks

The feedforward network in Figure 3.1 can now be written as,

$$\boldsymbol{z} \leftarrow f(\boldsymbol{\Theta}^{(x \to z)} \boldsymbol{x}^{(i)})$$
[3.21]

$$\tilde{\boldsymbol{y}} \leftarrow \text{SoftMax}\left(\boldsymbol{\Theta}^{(z \to y)} \boldsymbol{z} + \boldsymbol{b}\right)$$
 [3.22]

$$\ell^{(i)} \leftarrow -\boldsymbol{e}_{u^{(i)}} \cdot \log \tilde{\boldsymbol{y}}, \tag{3.23}$$

where f is an elementwise activation function, such as σ or ReLU, and ` (i) is the loss at instance i. The parameters Θ(x→z) , Θ(z→y) , and b can be estimated using online gradientbased optimization. The simplest such algorithm is stochastic gradient descent, which was discussed in § [2.6.](#page-54-1) Each parameter is updated by the gradient of the loss,

$$\boldsymbol{b} \leftarrow \boldsymbol{b} - \eta^{(t)} \nabla_{\boldsymbol{b}} \ell^{(i)}$$
[3.24]

$$\boldsymbol{\theta}_{k}^{(z \to y)} \leftarrow \boldsymbol{\theta}_{k}^{(z \to y)} - \eta^{(t)} \nabla_{\boldsymbol{\theta}_{k}^{(z \to y)}} \ell^{(i)}$$

$$[3.25]$$

$$\boldsymbol{\theta}_{n}^{(x \to z)} \leftarrow \boldsymbol{\theta}_{n}^{(x \to z)} - \eta^{(t)} \nabla_{\boldsymbol{\theta}_{n}^{(x \to z)}} \ell^{(i)}, \qquad [3.26]$$

where η (t) is the learning rate on iteration t, ` (i) is the loss on instance (or minibatch) i, and θ (x→z) <sup>n</sup> is column n of the matrix Θ(x→z) , and θ (z→y) k is column k of Θ(z→y) .

The gradients of the negative log-likelihood on b and θ (z→y) k are similar to the gradients in logistic regression. For θ (z→y) , the gradient is,

$$\nabla_{\boldsymbol{\theta}_{k}^{(z \to y)}} \ell^{(i)} = \left[ \frac{\partial \ell^{(i)}}{\partial \theta_{k,1}^{(z \to y)}}, \frac{\partial \ell^{(i)}}{\partial \theta_{k,2}^{(z \to y)}}, \dots, \frac{\partial \ell^{(i)}}{\partial \theta_{k,K_{y}}^{(z \to y)}} \right]^{\top}$$
[3.27]

$$\frac{\partial \ell^{(i)}}{\partial \theta_{k,j}^{(z \to y)}} = -\frac{\partial}{\partial \theta_{k,j}^{(z \to y)}} \left( \boldsymbol{\theta}_{y^{(i)}}^{(z \to y)} \cdot \boldsymbol{z} - \log \sum_{y \in \mathcal{Y}} \exp \boldsymbol{\theta}_{y}^{(z \to y)} \cdot \boldsymbol{z} \right)$$
[3.28]

<span id="page-71-0"></span>
$$= \left( \Pr(y = j \mid \boldsymbol{z}; \boldsymbol{\Theta}^{(z \to y)}, \boldsymbol{b}) - \delta\left(j = y^{(i)}\right) \right) z_k,$$
[3.29]

where δ j = y (i) is a function that returns one when j = y (i) , and zero otherwise. The gradient ∇b` (i) is similar to [Equation 3.29.](#page-71-0)

The gradients on the input layer weights Θ(x→z) are obtained by the chain rule of differentiation:

$$\frac{\partial \ell^{(i)}}{\partial \theta_{n,k}^{(x \to z)}} = \frac{\partial \ell^{(i)}}{\partial z_k} \frac{\partial z_k}{\partial \theta_{n,k}^{(x \to z)}}$$
[3.30]

$$= \frac{\partial \ell^{(i)}}{\partial z_k} \frac{\partial f(\boldsymbol{\theta}_k^{(x \to z)} \cdot \boldsymbol{x})}{\partial \boldsymbol{\theta}_{n,k}^{(x \to z)}}$$
[3.31]

$$=\frac{\partial \ell^{(i)}}{\partial z_k} \times f'(\boldsymbol{\theta}_k^{(x \to z)} \cdot \boldsymbol{x}) \times x_n, \qquad [3.32]$$

where f 0 (θ (x→z) k · x) is the derivative of the activation function f, applied at the input

#### <span id="page-72-1"></span>3.3. LEARNING NEURAL NETWORKS

 $\boldsymbol{\theta}_{k}^{(x \to z)} \cdot \boldsymbol{x}$ . For example, if *f* is the sigmoid function, then the derivative is,

$$\frac{\partial \ell^{(i)}}{\partial \theta_{n,k}^{(x \to z)}} = \frac{\partial \ell^{(i)}}{\partial z_k} \times \sigma(\boldsymbol{\theta}_k^{(x \to z)} \cdot \boldsymbol{x}) \times (1 - \sigma(\boldsymbol{\theta}_k^{(x \to z)} \cdot \boldsymbol{x})) \times x_n$$
[3.33]

$$=\frac{\partial \ell^{(i)}}{\partial z_k} \times z_k \times (1-z_k) \times x_n.$$
[3.34]

For intuition, consider each of the terms in the product.

- If the negative log-likelihood  $\ell^{(i)}$  does not depend much on  $z_k$ , then  $\frac{\partial \ell^{(i)}}{\partial z_k} \approx 0$ . In this case it doesn't matter how  $z_k$  is computed, and so  $\frac{\partial \ell^{(i)}}{\partial \theta_n^{(x \to z)}} \approx 0$ .
- If  $z_k$  is near 1 or 0, then the curve of the sigmoid function is nearly flat (Figure 3.2), and changing the inputs will make little local difference. The term  $z_k \times (1 z_k)$  is maximized at  $z_k = \frac{1}{2}$ , where the slope of the sigmoid function is steepest.
- If  $x_n = 0$ , then it does not matter how we set the weights  $\theta_{n,k}^{(x \to z)}$ , so  $\frac{\partial \ell^{(i)}}{\partial \theta_{n,k}^{(x \to z)}} = 0$ .

#### <span id="page-72-0"></span>3.3.1 Backpropagation

The equations above rely on the chain rule to compute derivatives of the loss with respect to each parameter of the model. Furthermore, local derivatives are frequently reused: for example,  $\frac{\partial \ell^{(i)}}{\partial z_k}$  is reused in computing the derivatives with respect to each  $\theta_{n,k}^{(x\to z)}$ . These terms should therefore be computed once, and then cached. Furthermore, we should only compute any derivative once we have already computed all of the necessary "inputs" demanded by the chain rule of differentiation. This combination of sequencing, caching, and differentiation is known as **backpropagation**. It can be generalized to any directed acyclic **computation graph**.

A computation graph is a declarative representation of a computational process. At each node t, compute a value  $v_t$  by applying a function  $f_t$  to a (possibly empty) list of parent nodes,  $\pi_t$ . Figure 3.3 shows the computation graph for a feedforward network with one hidden layer. There are nodes for the input  $x^{(i)}$ , the hidden layer z, the predicted output  $\hat{y}$ , and the parameters  $\Theta$ . During training, there is also a node for the ground truth label  $y^{(i)}$  and the loss  $\ell^{(i)}$ . The predicted output  $\hat{y}$  is one of the parents of the loss (the other is the label  $y^{(i)}$ ); *its* parents include  $\Theta$  and z, and so on.

Computation graphs include three types of nodes:

**Variables.** In the feedforward network of Figure 3.3, the variables include the inputs x, the hidden nodes z, the outputs y, and the loss function. Inputs are variables that do not have parents. Backpropagation computes the gradients with respect to all

<span id="page-73-1"></span><span id="page-73-0"></span>**Algorithm 6** General backpropagation algorithm. In the computation graph *G*, every node contains a function  $f_t$  and a set of parent nodes  $\pi_t$ ; the inputs to the graph are  $x^{(i)}$ .

1: **procedure** BACKPROP( $G = \{f_t, \pi_t\}_{t=1}^T\}, x^{(i)}$ )  $v_{t(n)} \leftarrow x_n^{(i)}$  for all *n* and associated computation nodes t(n). 2: for  $t \in \text{TOPOLOGICALSORT}(G)$  do  $\triangleright$  Forward pass: compute value at each node 3: if  $|\boldsymbol{\pi}_t| > 0$  then 4:  $v_t \leftarrow f_t(v_{\pi_{t,1}}, v_{\pi_{t,2}}, \dots, v_{\pi_{t,N_t}})$ 5: ▷ Backward pass: compute gradients at each node  $g_{\text{objective}} = 1$ 6: for  $t \in \text{REVERSE}(\text{TOPOLOGICALSORT}(G))$  do 7:  $g_t \leftarrow \sum_{t':t \in \pi_{t'}} g_{t'} \times \nabla_{v_t} v_{t'} \quad \triangleright$  Sum over all t' that are children of t, propagating 8: the gradient  $g_{t'}$ , scaled by the local gradient  $\nabla_{v_t} v_{t'}$ **return**  $\{g_1, g_2, ..., g_T\}$ 9:

variables except the inputs, and propagates these gradients backwards to the parameters.

- **Parameters.** In a feedforward network, the parameters include the weights and offsets. In Figure 3.3, the parameters are summarized in the node  $\Theta$ , but we could have separate nodes for  $\Theta^{(x \to z)}$ ,  $\Theta^{(z \to y)}$ , and any offset parameters. Parameter nodes do not have parents; they are not computed from other nodes, but rather, are learned by gradient descent.
- **Loss.** The loss  $\ell^{(i)}$  is the quantity that is to be minimized during training. The node representing the loss in the computation graph is not the parent of any other node; its parents are typically the predicted label  $\hat{y}$  and the true label  $y^{(i)}$ . Backpropagation begins by computing the gradient of the loss, and then propagating this gradient backwards to its immediate parents.

If the computation graph is a directed acyclic graph, then it is possible to order the nodes with a topological sort, so that if node t is a parent of node t', then t < t'. This means that the values  $\{v_t\}_{t=1}^T$  can be computed in a single forward pass. The topological sort is reversed when computing gradients: each gradient  $g_t$  is computed from the gradients of the children of t, implementing the chain rule of differentiation. The general backpropagation algorithm for computation graphs is shown in Algorithm 6.

While the gradients with respect to each parameter may be complex, they are composed of products of simple parts. For many networks, all gradients can be computed through **automatic differentiation**. This means that you need only specify the feedforward computation, and the gradients necessary for learning can be obtained automatically. There are many software libraries that perform automatic differentiation on compu-

<span id="page-74-2"></span><span id="page-74-1"></span>![](_page_74_Figure_1.jpeg)

Figure 3.3: A computation graph for the feedforward neural network shown in Figure 3.1.

tation graphs, such as TORCH (Collobert et al., 2011), TENSORFLOW (Abadi et al., 2016), and DYNET (Neubig et al., 2017). One important distinction between these libraries is whether they support **dynamic computation graphs**, in which the structure of the computation graph varies across instances. Static computation graphs are compiled in advance, and can be applied to fixed-dimensional data, such as bag-of-words vectors. In many natural language processing problems, each input has a distinct structure, requiring a unique computation graph. A simple case occurs in recurrent neural network language models (see chapter 6), in which there is one node for each word in a sentence. More complex cases include recursive neural networks (see chapter 14), in which the network is a tree structure matching the syntactic organization of the input.

#### <span id="page-74-0"></span>3.3.2 Regularization and dropout

In linear classification, overfitting was addressed by augmenting the objective with a regularization term,  $\lambda ||\boldsymbol{\theta}||_2^2$ . This same approach can be applied to feedforward neural networks, penalizing each matrix of weights:

$$L = \sum_{i=1}^{N} \ell^{(i)} + \lambda_{z \to y} || \Theta^{(z \to y)} ||_{F}^{2} + \lambda_{x \to z} || \Theta^{(x \to z)} ||_{F}^{2},$$
 [3.35]

where  $||\Theta||_F^2 = \sum_{i,j} \theta_{i,j}^2$  is the squared **Frobenius norm**, which generalizes the  $L_2$  norm to matrices. The bias parameters b are not regularized, as they do not contribute to the sensitivity of the classifier to the inputs. In gradient-based optimization, the practical effect of Frobenius norm regularization is that the weights "decay" towards zero at each update, motivating the alternative name **weight decay**.

Another approach to controlling model complexity is **dropout**, which involves randomly setting some computation nodes to zero during training (Srivastava et al., 2014). For example, in the feedforward network, on each training instance, with probability  $\rho$  we

<span id="page-75-1"></span>set each input  $x_n$  and each hidden layer node  $z_k$  to zero. Srivastava et al. (2014) recommend  $\rho = 0.5$  for hidden units, and  $\rho = 0.2$  for input units. Dropout is also incorporated in the gradient computation, so if node  $z_k$  is dropped, then none of the weights  $\theta_k^{(x \to z)}$  will be updated for this instance. Dropout prevents the network from learning to depend too much on any one feature or hidden node, and prevents **feature co-adaptation**, in which a hidden unit is only useful in combination with one or more other hidden units. Dropout is a special case of **feature noising**, which can also involve adding Gaussian noise to inputs or hidden units (Holmstrom and Koistinen, 1992). Wager et al. (2013) show that dropout is approximately equivalent to "adaptive"  $L_2$  regularization, with a separate regularization penalty for each feature.

#### <span id="page-75-0"></span>3.3.3 \*Learning theory

Chapter 2 emphasized the importance of **convexity** for learning: for convex objectives, the global optimum can be found efficiently. The negative log-likelihood and hinge loss are convex functions of the parameters of the output layer. However, the output of a feed-forward network is generally not a convex function of the parameters of the input layer,  $\Theta^{(x \to z)}$ . Feedforward networks can be viewed as function composition, where each layer is a function that is applied to the output of the previous layer. Convexity is generally not preserved in the composition of two convex functions — and furthermore, "squashing" activation functions like tanh and sigmoid are not convex.

The non-convexity of hidden layer neural networks can also be seen by permuting the elements of the hidden layer, from  $\mathbf{z} = [z_1, z_2, ..., z_{K_z}]$  to  $\tilde{\mathbf{z}} = [z_{\pi(1)}, z_{\pi(2)}, ..., z_{\pi(K_z)}]$ . This corresponds to applying  $\pi$  to the rows of  $\mathbf{\Theta}^{(x \to z)}$  and the columns of  $\mathbf{\Theta}^{(z \to y)}$ , resulting in permuted parameter matrices  $\mathbf{\Theta}_{\pi}^{(x \to z)}$  and  $\mathbf{\Theta}_{\pi}^{(z \to y)}$ . As long as this permutation is applied consistently, the loss will be identical,  $L(\mathbf{\Theta}) = L(\mathbf{\Theta}_{\pi})$ : it is *invariant* to this permutation. However, the loss of the linear combination  $L(\alpha \mathbf{\Theta} + (1 - \alpha)\mathbf{\Theta}_{\pi})$  will generally not be identical to the loss under  $\mathbf{\Theta}$  or its permutations. If  $L(\mathbf{\Theta})$  is better than the loss at any points in the immediate vicinity, and if  $L(\mathbf{\Theta}) = L(\mathbf{\Theta}_{\pi})$ , then the loss function does not satisfy the definition of convexity (see § 2.4). One of the exercises asks you to prove this more rigorously.

In practice, the existence of multiple optima is not necessary problematic, if all such optima are permutations of the sort described in the previous paragraph. In contrast, "bad" local optima are better than their neighbors, but much worse than the global optimum. Fortunately, in large feedforward neural networks, most local optima are nearly as good as the global optimum (Choromanska et al., 2015). More generally, a **critical point** is one at which the gradient is zero. Critical points may be local optima, but they may also be **saddle points**, which are local minima in some directions, but local *maxima* in other directions. For example, the equation  $x_1^2 - x_2^2$  has a saddle point at  $\mathbf{x} = (0, 0)$ . In large networks, the overwhelming majority of critical points are saddle points, rather

#### <span id="page-76-1"></span>3.3. LEARNING NEURAL NETWORKS 59

than local minima or maxima [\(Dauphin et al., 2014\)](#page-523-0). Saddle points can pose problems for gradient-based optimization, since learning will slow to a crawl as the gradient goes to zero. However, the noise introduced by stochastic gradient descent, and by feature noising techniques such as dropout, can help online optimization to escape saddle points and find high-quality optima [\(Ge et al., 2015\)](#page-530-2). Other techniques address saddle points directly, using local reconstructions of the Hessian matrix [\(Dauphin et al., 2014\)](#page-523-0) or higherorder derivatives [\(Anandkumar and Ge, 2016\)](#page-510-2).

Another theoretical puzzle about neural networks is how they are able to **generalize** to unseen data. Given enough parameters, a two-layer feedforward network can "memorize" its training data, attaining perfect accuracy on any training set. A particularly salient demonstration was provided by [Zhang et al.](#page-572-0) [\(2017\)](#page-572-0), who showed that neural networks can learn to perfectly classify a training set of images, even when the labels are replaced with random values! Of course, this network attains only chance accuracy when applied to heldout data. The concern is that when such a powerful learner is applied to real training data, it may learn a pathological classification function, which exploits irrelevant details of the training data and fails to generalize. Yet this extreme **overfitting** is rarely encountered in practice, and can usually be prevented by regularization, dropout, and early stopping (see § [3.3.4\)](#page-76-0). Recent papers have derived generalization guarantees for specific classes of neural networks (e.g., [Kawaguchi et al., 2017;](#page-539-2) [Brutzkus et al., 2018\)](#page-517-0), but theoretical work in this area is ongoing.

#### <span id="page-76-0"></span>**3.3.4 Tricks**

Getting neural networks to work sometimes requires heuristic "tricks" [\(Bottou, 2012;](#page-515-2) [Goodfellow et al., 2016;](#page-531-1) [Goldberg, 2017b\)](#page-531-4). This section presents some tricks that are especially important.

**Initialization** Initialization is not especially important for linear classifiers, since convexity ensures that the global optimum can usually be found quickly. But for multilayer neural networks, it is helpful to have a good starting point. One reason is that if the magnitude of the initial weights is too large, a sigmoid or tanh nonlinearity will be saturated, leading to a small gradient, and slow learning. Large gradients can cause training to diverge, with the parameters taking increasingly extreme values until reaching the limits of the floating point representation.

Initialization can help avoid these problems by ensuring that the variance over the initial gradients is constant and bounded throughout the network. For networks with tanh activation functions, this can be achieved by sampling the initial weights from the

<span id="page-77-0"></span>following uniform distribution (Glorot and Bengio, 2010),

$$\theta_{i,j} \sim U\left[-\frac{\sqrt{6}}{\sqrt{d_{\text{in}}(n) + d_{\text{out}}(n)}}, \frac{\sqrt{6}}{\sqrt{d_{\text{in}}(n) + d_{\text{out}}(n)}}\right],$$
[3.36]

[3.37]

For the weights leading to a ReLU activation function, He et al. (2015) use similar argumentation to justify sampling from a zero-mean Gaussian distribution,

$$\theta_{i,j} \sim N(0, \sqrt{2/d_{\text{in}}(n)})$$
 [3.38]

Rather than initializing the weights independently, it can be beneficial to initialize each layer jointly as an **orthonormal matrix**, ensuring that  $\Theta^{\top}\Theta = \mathbb{I}$  (Saxe et al., 2014). Orthonormal matrices preserve the norm of the input, so that  $||\Theta x|| = ||x||$ , which prevents the gradients from exploding or vanishing. Orthogonality ensures that the hidden units are uncorrelated, so that they correspond to different features of the input. Orthonormal initialization can be performed by applying **singular value decomposition** to a matrix of values sampled from a standard normal distribution:

$$a_{i,j} \sim N(0,1)$$
 [3.39]

$$\mathbf{A} = \{a_{i,j}\}_{i=1}^{d_{\text{in}}(j), d_{\text{out}}(j)}$$
[3.40]

$$\mathbf{A} = \{a_{i,j}\}_{i=1,j=1}^{+n(j),-bar(j)}$$
[3.40]  
$$\mathbf{U}, \mathbf{S}, \mathbf{V}^{\top} = \text{SVD}(\mathbf{A})$$
[3.41]

$$\Theta^{(j)} \leftarrow \mathbf{U}.$$
 [3.42]

The matrix  $\mathbf{U}$  contains the **singular vectors** of  $\mathbf{A}_{i}$  and is guaranteed to be orthonormal. For more on singular value decomposition, see chapter 14.

Even with careful initialization, there can still be significant variance in the final results. It can be useful to make multiple training runs, and select the one with the best performance on a heldout development set.

**Clipping and normalization** Learning can be sensitive to the magnitude of the gradient: too large, and learning can diverge, with successive updates thrashing between increasingly extreme values; too small, and learning can grind to a halt. Several heuristics have been proposed to address this issue.

• In gradient clipping (Pascanu et al., 2013), an upper limit is placed on the norm of the gradient, and the gradient is rescaled when this limit is exceeded,

$$CLIP(\tilde{\boldsymbol{g}}) = \begin{cases} \boldsymbol{g} & ||\hat{\boldsymbol{g}}|| < \tau \\ \frac{\tau}{||\boldsymbol{g}||} \boldsymbol{g} & \text{otherwise.} \end{cases}$$
[3.43]

#### <span id="page-78-0"></span>3.3. LEARNING NEURAL NETWORKS

• In **batch normalization** (Ioffe and Szegedy, 2015), the inputs to each computation node are recentered by their mean and variance across all of the instances in the minibatch  $\mathcal{B}$  (see § 2.6.2). For example, in a feedforward network with one hidden layer, batch normalization would tranform the inputs to the hidden layer as follows:

$$\boldsymbol{\mu}^{(\mathcal{B})} = \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \boldsymbol{x}^{(i)}$$
[3.44]

$$\boldsymbol{s}^{(\mathcal{B})} = \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} (\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{(\mathcal{B})})^2$$
[3.45]

$$\overline{\boldsymbol{x}}^{(i)} = (\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{(\mathcal{B})}) / \sqrt{\boldsymbol{s}^{(\mathcal{B})}}.$$
[3.46]

Empirically, this speeds convergence of deep architectures. One explanation is that it helps to correct for changes in the distribution of activations during training.

• In **layer normalization** (Ba et al., 2016), the inputs to each nonlinear activation function are recentered across the layer:

....

$$\boldsymbol{a} = \boldsymbol{\Theta}^{(x \to z)} \boldsymbol{x}$$
 [3.47]

$$\mu = \frac{1}{K_z} \sum_{k=1}^{K_z} a_k$$
[3.48]

$$s = \frac{1}{K_z} \sum_{k=1}^{K_z} (a_k - \mu)^2$$
[3.49]

$$\boldsymbol{z} = (\boldsymbol{a} - \boldsymbol{\mu}) / \sqrt{s}.$$
 [3.50]

Layer normalization has similar motivations to batch normalization, but it can be applied across a wider range of architectures and training conditions.

**Online optimization** There is a cottage industry of online optimization algorithms that attempt to improve on stochastic gradient descent. **AdaGrad** was reviewed in § 2.6.2; its main innovation is to set adaptive learning rates for each parameter by storing the sum of squared gradients. Rather than using the sum over the entire training history, we can keep a running estimate,

$$v_j^{(t)} = \beta v_j^{(t-1)} + (1-\beta)g_{t,j}^2, \qquad [3.51]$$

where  $g_{t,j}$  is the gradient with respect to parameter j at time t, and  $\beta \in [0, 1]$ . This term places more emphasis on recent gradients, and is employed in the AdaDelta (Zeiler, 2012) and Adam (Kingma and Ba, 2014) optimizers. Online optimization and its theoretical background are reviewed by Bottou et al. (2016). **Early stopping**, mentioned in § 2.3.2, can help to avoid overfitting by terminating training after reaching a plateau in the performance on a heldout validation set.

<span id="page-79-1"></span>**Practical advice** The bag of tricks for training neural networks continues to grow, and it is likely that there will be several new ones by the time you read this. Today, it is standard practice to use gradient clipping, early stopping, and a sensible initialization of parameters to small random values. More bells and whistles can be added as solutions to specific problems — for example, if it is difficult to find a good learning rate for stochastic gradient descent, then it may help to try a fancier optimizer with an adaptive learning rate. Alternatively, if a method such as layer normalization is used by related models in the research literature, you should probably consider it, especially if you are having trouble matching published results. As with linear classifiers, it is important to evaluate these decisions on a held-out development set, and not on the test set that will be used to provide the final measure of the model's performance (see § 2.2.5).

# <span id="page-79-0"></span>3.4 Convolutional neural networks

A basic weakness of the bag-of-words model is its inability to account for the ways in which words combine to create meaning, including even simple reversals such as *not pleasant, hardly a generous offer,* and *I wouldn't mind missing the flight*. Computer vision faces the related challenge of identifying the semantics of images from pixel features that are uninformative in isolation. An earlier generation of computer vision research focused on designing *filters* to aggregate local pixel-level features into more meaningful representations, such as edges and corners (e.g., Canny, 1987). Similarly, earlier NLP research attempted to capture multiword linguistic phenomena by hand-designed lexical patterns (Hobbs et al., 1997). In both cases, the output of the filters and patterns could then act as base features in a linear classifier. But rather than designing these feature extractors by hand, a better approach is to learn them, using the magic of backpropagation. This is the idea behind **convolutional neural networks**.

Following  $\S$  3.2.4, define the base layer of a neural network as,

$$\mathbf{X}^{(0)} = \mathbf{\Theta}^{(x \to z)}[\boldsymbol{e}_{w_1}, \boldsymbol{e}_{w_2}, \dots, \boldsymbol{e}_{w_M}], \qquad [3.52]$$

where  $e_{w_m}$  is a column vector of zeros, with a 1 at position  $w_m$ . The base layer has dimension  $\mathbf{X}^{(0)} \in \mathbb{R}^{K_e \times M}$ , where  $K_e$  is the size of the word embeddings. To merge information across adjacent words, we *convolve*  $\mathbf{X}^{(0)}$  with a set of filter matrices  $\mathbf{C}^{(k)} \in \mathbb{R}^{K_e \times h}$ . Convolution is indicated by the symbol \*, and is defined,

$$\mathbf{X}^{(1)} = f(\mathbf{b} + \mathbf{C} * \mathbf{X}^{(0)}) \implies x_{k,m}^{(1)} = f\left(b_k + \sum_{k'=1}^{K_e} \sum_{n=1}^h c_{k',n}^{(k)} \times x_{k',m+n-1}^{(0)}\right), \quad [3.53]$$

where *f* is an activation function such as tanh or ReLU, and *b* is a vector of offsets. The convolution operation slides the matrix  $\mathbf{C}^{(k)}$  across the columns of  $\mathbf{X}^{(0)}$ . At each position *m*, we compute the elementwise product  $\mathbf{C}^{(k)} \odot \mathbf{X}^{(0)}_{m:m+h-1}$ , and take the sum.

<span id="page-80-1"></span><span id="page-80-0"></span>![](_page_80_Figure_1.jpeg)

Figure 3.4: A convolutional neural network for text classification

A simple filter might compute a weighted average over nearby words,

$$\mathbf{C}^{(k)} = \begin{bmatrix} 0.5 & 1 & 0.5 \\ 0.5 & 1 & 0.5 \\ \dots & \dots & \dots \\ 0.5 & 1 & 0.5 \end{bmatrix},$$
[3.54]

thereby representing trigram units like *not so unpleasant*. In **one-dimensional convolution**, each filter matrix  $\mathbf{C}^{(k)}$  is constrained to have non-zero values only at row k (Kalchbrenner et al., 2014). This means that each dimension of the word embedding is processed by a separate filter, and it implies that  $K_f = K_e$ .

To deal with the beginning and end of the input, the base matrix  $\mathbf{X}^{(0)}$  may be padded with *h* column vectors of zeros at the beginning and end; this is known as **wide convolution**. If padding is not applied, then the output from each layer will be h - 1 units smaller than the input; this is known as **narrow convolution**. The filter matrices need not have identical filter widths, so more generally we could write  $h_k$  to indicate to width of filter  $\mathbf{C}^{(k)}$ . As suggested by the notation  $\mathbf{X}^{(0)}$ , multiple layers of convolution may be applied, so that  $\mathbf{X}^{(d)}$  is the input to  $\mathbf{X}^{(d+1)}$ .

After *D* convolutional layers, we obtain a matrix representation of the document  $\mathbf{X}^{(D)} \in \mathbb{R}^{K_z \times M}$ . If the instances have variable lengths, it is necessary to aggregate over all *M* word positions to obtain a fixed-length representation. This can be done by a **pooling** operation,

<span id="page-81-2"></span><span id="page-81-0"></span>![](_page_81_Figure_1.jpeg)

Figure 3.5: A dilated convolutional neural network captures progressively larger context through recursive application of the convolutional operator

such as max-pooling [\(Collobert et al., 2011\)](#page-522-1) or average-pooling,

$$\boldsymbol{z} = \operatorname{MaxPool}(\mathbf{X}^{(D)}) \implies z_k = \max\left(x_{k,1}^{(D)}, x_{k,2}^{(D)}, \dots, x_{k,M}^{(D)}\right)$$
[3.55]

<span id="page-81-1"></span>
$$\boldsymbol{z} = \operatorname{AvgPool}(\mathbf{X}^{(D)}) \implies z_k = \frac{1}{M} \sum_{m=1}^M x_{k,m}^{(D)}.$$
 [3.56]

The vector z can now act as a layer in a feedforward network, culminating in a prediction yˆ and a loss ` (i) . The setup is shown in [Figure 3.4.](#page-80-0)

Just as in feedforward networks, the parameters (C(k) , b, Θ) can be learned by backpropagating from the classification loss. This requires backpropagating through the maxpooling operation, which is a discontinuous function of the input. But because we need only a local gradient, backpropagation flows only through the argmax m:

$$\frac{\partial z_k}{\partial x_{k,m}^{(D)}} = \begin{cases} 1, & x_{k,m}^{(D)} = \max\left(x_{k,1}^{(D)}, x_{k,2}^{(D)}, \dots, x_{k,M}^{(D)}\right) \\ 0, & \text{otherwise.} \end{cases}$$
[3.57]

The computer vision literature has produced a huge variety of convolutional architectures, and many of these innovations can be applied to text data. One avenue for improvement is more complex pooling operations, such as k-max pooling [\(Kalchbrenner](#page-539-4) [et al., 2014\)](#page-539-4), which returns a matrix of the k largest values for each filter. Another innovation is the use of **dilated convolution** to build multiscale representations [\(Yu and Koltun,](#page-571-2) [2016\)](#page-571-2). At each layer, the convolutional operator applied in *strides*, skipping ahead by s steps after each feature. As we move up the hierarchy, each layer is s times smaller than the layer below it, effectively summarizing the input [\(Kalchbrenner et al., 2016;](#page-539-5) [Strubell](#page-564-1) [et al., 2017\)](#page-564-1). This idea is shown in [Figure 3.5.](#page-81-0) Multi-layer convolutional networks can also be augmented with "shortcut" connections, as in the residual network from § [3.2.2](#page-68-0) [\(John](#page-538-0)[son and Zhang, 2017\)](#page-538-0).

#### <span id="page-82-0"></span>3.4. CONVOLUTIONAL NEURAL NETWORKS 65

# **Additional resources**

The deep learning textbook by [Goodfellow et al.](#page-531-1) [\(2016\)](#page-531-1) covers many of the topics in this chapter in more detail. For a comprehensive review of neural networks in natural language processing, see [Goldberg](#page-531-4) [\(2017b\)](#page-531-4). A seminal work on deep learning in natural language processing is the aggressively titled "Natural Language Processing (Almost) from Scratch", which uses convolutional neural networks to perform a range of language processing tasks [\(Collobert et al., 2011\)](#page-522-1), although there is earlier work (e.g., [Henderson,](#page-534-1) [2004\)](#page-534-1). This chapter focuses on feedforward and convolutional neural networks, but recurrent neural networks are one of the most important deep learning architectures for natural language processing. They are covered extensively in chapters [6](#page-142-0) and [7.](#page-162-0)

The role of deep learning in natural language processing research has caused angst in some parts of the natural language processing research community (e.g., [Goldberg,](#page-531-5) [2017a\)](#page-531-5), especially as some of the more zealous deep learning advocates have argued that end-to-end learning from "raw" text can eliminate the need for linguistic constructs such as sentences, phrases, and even words [\(Zhang et al., 2015,](#page-572-1) originally titled "Text understanding from scratch"). These developments were surveyed by [Manning](#page-546-1) [\(2015\)](#page-546-1). While reports of the demise of linguistics in natural language processing remain controversial at best, deep learning and backpropagation have become ubiquitous in both research and applications.

# **Exercises**

- 1. [Figure 3.3](#page-74-1) shows the computation graph for a feedforward neural network with one layer.
  - a) Update the computation graph to include a residual connection between x and z.
  - b) Update the computation graph to include a highway connection between x and z.
- 2. Prove that the softmax and sigmoid functions are equivalent when the number of possible labels is two. Specifically, for any Θ(z→y) (omitting the offset b for simplicity), show how to construct a vector of weights θ such that,

SoftMax
$$(\boldsymbol{\Theta}^{(z \to y)} \boldsymbol{z})[0] = \sigma(\boldsymbol{\theta} \cdot \boldsymbol{z}).$$
 [3.58]

3. Convolutional neural networks often aggregate across words by using max-**pooling** [\(Equation 3.55](#page-81-1) in § [3.4\)](#page-79-0). A potential concern is that there is zero gradient with respect to the parts of the input that are not included in the maximum. The following

questions consider the gradient with respect to an element of the input,  $x_{m,k}^{(0)}$ , and they assume that all parameters are independently distributed.

- a) First consider a minimal network, with  $z = \text{MaxPool}(\mathbf{X}^{(0)})$ . What is the probability that the gradient  $\frac{\partial \ell}{\partial x_{m,k}^{(0)}}$  is non-zero?
- b) Now consider a two-level network, with  $\mathbf{X}^{(1)} = f(\mathbf{b} + \mathbf{C} * \mathbf{X}^{(0)})$ . Express the probability that the gradient  $\frac{\partial \ell}{\partial x_{m,k}^{(0)}}$  is non-zero, in terms of the input length M, the filter size n, and the number of filters  $K_f$ .
- c) Using a calculator, work out the probability for the case  $M = 128, n = 4, K_f = 32$ .
- d) Now consider a three-level network,  $\mathbf{X}^{(2)} = f(\mathbf{b} + \mathbf{C} * \mathbf{X}^{(1)})$ . Give the general equation for the probability that  $\frac{\partial \ell}{\partial x_{m,k}^{(0)}}$  is non-zero, and compute the numerical probability for the scenario in the previous part, assuming  $K_f = 32$  and n = 4 at both levels.
- 4. Design a feedforward network to compute the XOR function:

$$f(x_1, x_2) = \begin{cases} -1, & x_1 = 1, x_2 = 1\\ 1, & x_1 = 1, x_2 = 0\\ 1, & x_1 = 0, x_2 = 1\\ -1, & x_1 = 0, x_2 = 0 \end{cases}$$
[3.59]

Your network should have a single output node which uses the Sign activation function,  $f(x) = \begin{cases} 1, & x > 0 \\ -1, & x \le 0. \end{cases}$ . Use a single hidden layer, with ReLU activation functions. Describe all unights and effects

tions. Describe all weights and offsets.

- 5. Consider the same network as above (with ReLU activations for the hidden layer), with an arbitrary differentiable loss function  $\ell(y^{(i)}, \tilde{y})$ , where  $\tilde{y}$  is the activation of the output node. Suppose all weights and offsets are initialized to zero. Show that gradient descent will not learn the desired function from this initialization.
- 6. The simplest solution to the previous problem relies on the use of the ReLU activation function at the hidden layer. Now consider a network with arbitrary activations on the hidden layer. Show that if the initial weights are any uniform constant, then gradient descent will not learn the desired function from this initialization.
- 7. Consider a network in which: the base features are all binary,  $x \in \{0,1\}^M$ ; the hidden layer activation function is sigmoid,  $z_k = \sigma(\theta_k \cdot x)$ ; and the initial weights are sampled independently from a standard normal distribution,  $\theta_{j,k} \sim N(0,1)$ .

#### 3.4. CONVOLUTIONAL NEURAL NETWORKS

• Show how the probability of a small initial gradient on any weight,  $\frac{\partial z_k}{\partial \theta_{j,k}} < \alpha$ , depends on the size of the input *M*. **Hint**: use the lower bound,

$$\Pr(\sigma(\boldsymbol{\theta}_k \cdot \boldsymbol{x}) \times (1 - \sigma(\boldsymbol{\theta}_k \cdot \boldsymbol{x})) < \alpha) \geq 2\Pr(\sigma(\boldsymbol{\theta}_k \cdot \boldsymbol{x}) < \alpha), \quad [3.60]$$

and relate this probability to the variance  $V[\boldsymbol{\theta}_k \cdot \boldsymbol{x}]$ .

- Design an alternative initialization that removes this dependence.
- 8. The ReLU activation function can lead to "dead neurons", which can never be activated on any input. Consider the following two-layer feedforward network with a scalar output *y*:

$$z_i = \operatorname{ReLU}(\boldsymbol{\theta}_i^{(x \to z)} \cdot \boldsymbol{x} + b_i)$$
[3.61]

$$y = \boldsymbol{\theta}^{(z \to y)} \cdot \boldsymbol{z}.$$
 [3.62]

Suppose that the input is a binary vector of observations,  $x \in \{0, 1\}^D$ .

- a) Under what condition is node  $z_i$  "dead"? Your answer should be expressed in terms of the parameters  $\theta_i^{(x \to z)}$  and  $b_i$ .
- b) Suppose that the gradient of the loss on a given instance is  $\frac{\partial \ell}{\partial y} = 1$ . Derive the gradients  $\frac{\partial \ell}{\partial b_i}$  and  $\frac{\partial \ell}{\partial \theta_{i}^{(\alpha \to z)}}$  for such an instance.
- c) Using your answers to the previous two parts, explain why a dead neuron can never be brought back to life during gradient-based learning.
- 9. Suppose that the parameters  $\Theta = \{\Theta^{(x \to z)}, \Theta(z \to y), b\}$  are a local optimum of a feedforward network in the following sense: there exists some  $\epsilon > 0$  such that,

$$\left( ||\tilde{\Theta}^{(x \to z)} - \Theta^{(x \to z)}||_{F}^{2} + ||\tilde{\Theta}^{(z \to y)} - \Theta^{(z \to y)}||_{F}^{2} + ||\tilde{\boldsymbol{b}} - \boldsymbol{b}||_{2}^{2} < \epsilon \right)$$
  
$$\Rightarrow \left( L(\tilde{\Theta}) > L(\Theta) \right)$$
 [3.63]

Define the function  $\pi$  as a permutation on the hidden units, as described in § 3.3.3, so that for any  $\Theta$ ,  $L(\Theta) = L(\Theta_{\pi})$ . Prove that if a feedforward network has a local optimum in the sense of Equation 3.63, then its loss is not a convex function of the parameters  $\Theta$ , using the definition of convexity from § 2.4

10. Consider a network with a single hidden layer, and a single output,

<span id="page-84-0"></span>
$$y = \boldsymbol{\theta}^{(z \to y)} \cdot g(\boldsymbol{\Theta}^{(x \to z)} \boldsymbol{x}).$$
[3.64]

Assume that *g* is the ReLU function. Show that for any matrix of weights  $\Theta^{(x \to z)}$ , it is permissible to rescale each row to have a norm of one, because an identical output can be obtained by finding a corresponding rescaling of  $\theta^{(z \to y)}$ .

# <span id="page-86-3"></span><span id="page-86-0"></span>**Chapter 4**

# **Linguistic applications of classification**

Having covered several techniques for classification, this chapter shifts the focus from mathematics to linguistic applications. Later in the chapter, we will consider the design decisions involved in text classification, as well as best practices for evaluation.

# <span id="page-86-1"></span>**4.1 Sentiment and opinion analysis**

A popular application of text classification is to automatically determine the **sentiment** or **opinion polarity** of documents such as product reviews and social media posts. For example, marketers are interested to know how people respond to advertisements, services, and products [\(Hu and Liu, 2004\)](#page-536-1); social scientists are interested in how emotions are affected by phenomena such as the weather [\(Hannak et al., 2012\)](#page-533-2), and how both opinions and emotions spread over social networks [\(Coviello et al., 2014;](#page-522-4) [Miller et al., 2011\)](#page-549-2). In the field of **digital humanities**, literary scholars track plot structures through the flow of sentiment across a novel [\(Jockers, 2015\)](#page-538-1).[1](#page-86-2)

Sentiment analysis can be framed as a direct application of document classification, assuming reliable labels can be obtained. In the simplest case, sentiment analysis is a two or three-class problem, with sentiments of POSITIVE, NEGATIVE, and possibly NEU-TRAL. Such annotations could be annotated by hand, or obtained automatically through a variety of means:

• Tweets containing happy emoticons can be marked as positive, sad emoticons as negative [\(Read, 2005;](#page-557-2) [Pak and Paroubek, 2010\)](#page-553-0).

<span id="page-86-2"></span><sup>1</sup>Comprehensive surveys on sentiment analysis and related problems are offered by [Pang and Lee](#page-553-1) [\(2008\)](#page-553-1) and [Liu](#page-545-1) [\(2015\)](#page-545-1).

- <span id="page-87-3"></span>• Reviews with four or more stars can be marked as positive, three or fewer stars as negative [\(Pang et al., 2002\)](#page-553-2).
- Statements from politicians who are voting for a given bill are marked as positive (towards that bill); statements from politicians voting against the bill are marked as negative [\(Thomas et al., 2006\)](#page-565-2).

The bag-of-words model is a good fit for sentiment analysis at the document level: if the document is long enough, we would expect the words associated with its true sentiment to overwhelm the others. Indeed, **lexicon-based sentiment analysis** avoids machine learning altogether, and classifies documents by counting words against positive and negative sentiment word lists [\(Taboada et al., 2011\)](#page-564-2).

Lexicon-based classification is less effective for short documents, such as single-sentence reviews or social media posts. In these documents, linguistic issues like **negation** and **irrealis** [\(Polanyi and Zaenen, 2006\)](#page-555-1) — events that are hypothetical or otherwise non-factual — can make bag-of-words classification ineffective. Consider the following examples:

- <span id="page-87-1"></span>(4.1) a. That's not bad for the first day.
  - b. This is not the worst thing that can happen.
  - c. It would be nice if you acted like you understood.
  - d. There is no reason at all to believe that the polluters are suddenly going to become reasonable. [\(Wilson et al., 2005\)](#page-569-0)
  - e. This film should be brilliant. The actors are first grade. Stallone plays a happy, wonderful man. His sweet wife is beautiful and adores him. He has a fascinating gift for living life fully. It sounds like a great plot, **however**, the film is a failure. [\(Pang et al., 2002\)](#page-553-2)

<span id="page-87-2"></span>A minimal solution is to move from a bag-of-words model to a bag-of-**bigrams** model, where each base feature is a pair of adjacent words, e.g.,

$$(that's, not), (not, bad), (bad, for), \dots$$
 [4.1]

Bigrams can handle relatively straightforward cases, such as when an adjective is immediately negated; trigrams would be required to extend to larger contexts (e.g., *not the worst*). But this approach will not scale to more complex examples like [\(4.1d\)](#page-87-1) and [\(4.1e\)](#page-87-2). More sophisticated solutions try to account for the syntactic structure of the sentence [\(Wilson](#page-569-0) [et al., 2005;](#page-569-0) [Socher et al., 2013\)](#page-562-1), or apply more complex classifiers such as convolutional neural networks [\(Kim, 2014\)](#page-539-6), which are described in [chapter 3.](#page-64-0)

#### <span id="page-87-0"></span>**4.1.1 Related problems**

**Subjectivity** Closely related to sentiment analysis is **subjectivity detection**, which requires identifying the parts of a text that express subjective opinions, as well as other non-

#### <span id="page-88-1"></span>4.1. SENTIMENT AND OPINION ANALYSIS 71

factual content such as speculation and hypotheticals [\(Riloff and Wiebe, 2003\)](#page-558-0). This can be done by treating each sentence as a separate document, and then applying a bag-of-words classifier: indeed, [Pang and Lee](#page-553-3) [\(2004\)](#page-553-3) do exactly this, using a training set consisting of (mostly) subjective sentences gathered from movie reviews, and (mostly) objective sentences gathered from plot descriptions. They augment this bag-of-words model with a graph-based algorithm that encourages nearby sentences to have the same subjectivity label.

**Stance classification** In debates, each participant takes a side: for example, advocating for or against proposals like adopting a vegetarian lifestyle or mandating free college education. The problem of stance classification is to identify the author's position from the text of the argument. In some cases, there is training data available for each position, so that standard document classification techniques can be employed. In other cases, it suffices to classify each document as whether it is in support or opposition of the argument advanced by a previous document [\(Anand et al., 2011\)](#page-510-3). In the most challenging case, there is no labeled data for any of the stances, so the only possibility is group documents that advocate the same position [\(Somasundaran and Wiebe, 2009\)](#page-562-2). This is a form of **unsupervised learning**, discussed in [chapter 5.](#page-112-0)

**Targeted sentiment analysis** The expression of sentiment is often more nuanced than a simple binary label. Consider the following examples:

- <span id="page-88-0"></span>(4.2) a. The vodka was good, but the meat was rotten.
  - b. Go to Heaven for the climate, Hell for the company. –*Mark Twain*

These statements display a mixed overall sentiment: positive towards some entities (e.g., *the vodka*), negative towards others (e.g., *the meat*). **Targeted sentiment analysis** seeks to identify the writer's sentiment towards specific entities [\(Jiang et al., 2011\)](#page-537-1). This requires identifying the entities in the text and linking them to specific sentiment words — much more than we can do with the classification-based approaches discussed thus far. For example, [Kim and Hovy](#page-539-7) [\(2006\)](#page-539-7) analyze sentence-internal structure to determine the topic of each sentiment expression.

**Aspect-based opinion mining** seeks to identify the sentiment of the author of a review towards predefined aspects such as PRICE and SERVICE, or, in the case of [\(4.2b\)](#page-88-0), CLIMATE and COMPANY [\(Hu and Liu, 2004\)](#page-536-1). If the aspects are not defined in advance, it may again be necessary to employ unsupervised learning methods to identify them (e.g., [Branavan](#page-516-3) [et al., 2009\)](#page-516-3).

**Emotion classification** While sentiment analysis is framed in terms of positive and negative categories, psychologists generally regard **emotion** as more multifaceted. For example, [Ekman](#page-526-2) [\(1992\)](#page-526-2) argues that there are six basic emotions — happiness, surprise, fear,

<span id="page-89-1"></span>sadness, anger, and contempt — and that they are universal across human cultures. Alm et al. (2005) build a linear classifier for recognizing the emotions expressed in children's stories. The ultimate goal of this work was to improve text-to-speech synthesis, so that stories could be read with intonation that reflected the emotional content. They used bagof-words features, as well as features capturing the story type (e.g., jokes, folktales), and structural features that reflect the position of each sentence in the story. The task is difficult: even human annotators frequently disagreed with each other, and the best classifiers achieved accuracy between 60-70%.

#### <span id="page-89-0"></span>4.1.2 Alternative approaches to sentiment analysis

**Regression** A more challenging version of sentiment analysis is to determine not just the class of a document, but its rating on a numerical scale (Pang and Lee, 2005). If the scale is continuous, it is most natural to apply **regression**, identifying a set of weights  $\theta$ that minimize the squared error of a predictor  $\hat{y} = \theta \cdot x + b$ , where *b* is an offset. This approach is called **linear regression**, and sometimes **least squares**, because the regression coefficients  $\theta$  are determined by minimizing the squared error,  $(y - \hat{y})^2$ . If the weights are regularized using a penalty  $\lambda ||\theta||_2^2$ , then it is **ridge regression**. Unlike logistic regression, both linear regression and ridge regression can be solved in closed form as a system of linear equations.

**Ordinal ranking** In many problems, the labels are ordered but discrete: for example, product reviews are often integers on a scale of 1 - 5, and grades are on a scale of A - F. Such problems can be solved by discretizing the score  $\theta \cdot x$  into "ranks",

$$\hat{y} = \operatorname*{argmax}_{r: \boldsymbol{\theta} \cdot \boldsymbol{x} \ge b_r} \boldsymbol{x}.$$
[4.2]

where  $\boldsymbol{b} = [b_1 = -\infty, b_2, b_3, \dots, b_K]$  is a vector of boundaries. It is possible to learn the weights and boundaries simultaneously, using a perceptron-like algorithm (Crammer and Singer, 2001).

**Lexicon-based classification** Sentiment analysis is one of the only NLP tasks where hand-crafted feature weights are still widely employed. In **lexicon-based classification** (Taboada et al., 2011), the user creates a list of words for each label, and then classifies each document based on how many of the words from each list are present. In our linear classification tion framework, this is equivalent to choosing the following weights:

$$\theta_{y,j} = \begin{cases} 1, & j \in \mathcal{L}_y \\ 0, & \text{otherwise,} \end{cases}$$
[4.3]

<span id="page-90-2"></span>where L<sup>y</sup> is the lexicon for label y. Compared to the machine learning classifiers discussed in the previous chapters, lexicon-based classification may seem primitive. However, supervised machine learning relies on large annotated datasets, which are time-consuming and expensive to produce. If the goal is to distinguish two or more categories in a new domain, it may be simpler to start by writing down a list of words for each category.

An early lexicon was the *General Inquirer* [\(Stone, 1966\)](#page-563-3). Today, popular sentiment lexicons include SENTIWORDNET [\(Esuli and Sebastiani, 2006\)](#page-527-1) and an evolving set of lexicons from [Liu](#page-545-1) [\(2015\)](#page-545-1). For emotions and more fine-grained analysis, *Linguistic Inquiry and Word Count* (LIWC) provides a set of lexicons [\(Tausczik and Pennebaker, 2010\)](#page-565-0). The MPQA lexicon indicates the polarity (positive or negative) of 8221 terms, as well as whether they are strongly or weakly subjective [\(Wiebe et al., 2005\)](#page-569-1). A comprehensive comparison of sentiment lexicons is offered by [Ribeiro et al.](#page-558-1) [\(2016\)](#page-558-1). Given an initial **seed lexicon**, it is possible to automatically expand the lexicon by looking for words that frequently co-occur with words in the seed set [\(Hatzivassiloglou and McKeown, 1997;](#page-533-3) [Qiu et al., 2011\)](#page-556-0).

# <span id="page-90-0"></span>**4.2 Word sense disambiguation**

Consider the the following headlines:

- (4.3) a. Iraqi head seeks arms
  - b. Prostitutes appeal to Pope
  - c. Drunk gets nine years in violin case[2](#page-90-1)

These headlines are ambiguous because they contain words that have multiple meanings, or **senses**. Word sense disambiguation is the problem of identifying the intended sense of each word token in a document. Word sense disambiguation is part of a larger field of research called **lexical semantics**, which is concerned with meanings of the words.

At a basic level, the problem of word sense disambiguation is to identify the correct sense for each word token in a document. Part-of-speech ambiguity (e.g., noun versus verb) is usually considered to be a different problem, to be solved at an earlier stage. From a linguistic perspective, senses are not properties of words, but of **lemmas**, which are canonical forms that stand in for a set of inflected words. For example, *arm*/N is a lemma that includes the inflected form *arms*/N — the /N indicates that it we are referring to the noun, and not its **homonym** *arm*/V, which is another lemma that includes the inflected verbs (*arm*/V, *arms*/V, *armed*/V, *arming*/V). Therefore, word sense disambiguation requires first identifying the correct part-of-speech and lemma for each token,

<span id="page-90-1"></span><sup>2</sup>These examples, and many more, can be found at [http://www.ling.upenn.edu/˜beatrice/](http://www.ling.upenn.edu/~beatrice/humor/headlines.html) [humor/headlines.html](http://www.ling.upenn.edu/~beatrice/humor/headlines.html)

<span id="page-91-3"></span>and then choosing the correct sense from the inventory associated with the corresponding lemma.<sup>3</sup> (Part-of-speech tagging is discussed in  $\S$  8.1.)

#### <span id="page-91-0"></span>4.2.1 How many word senses?

Words sometimes have many more than two senses, as exemplified by the word *serve*:

- [FUNCTION]: *The tree stump served as a table*
- [CONTRIBUTE TO]: *His evasive replies only served to heighten suspicion*
- [PROVIDE]: We serve only the rawest fish
- [ENLIST]: She served in an elite combat unit
- [JAIL]: He served six years for a crime he didn't commit
- [LEGAL]: They were served with subpoenas<sup>4</sup>

These sense distinctions are annotated in **WORDNET** (http://wordnet.princeton. edu), a lexical semantic database for English. WORDNET consists of roughly 100,000 **synsets**, which are groups of lemmas (or phrases) that are synonymous. An example synset is { $chump^1, fool^2, sucker^1, mark^9$ }, where the superscripts index the sense of each lemma that is included in the synset: for example, there are at least eight other senses of *mark* that have different meanings, and are not part of this synset. A lemma is **polysemous** if it participates in multiple synsets.

WORDNET defines the scope of the word sense disambiguation problem, and, more generally, formalizes lexical semantic knowledge of English. (WordNets have been created for a few dozen other languages, at varying levels of detail.) Some have argued that WordNet's sense granularity is too fine (Ide and Wilks, 2006); more fundamentally, the premise that word senses can be differentiated in a task-neutral way has been criticized as linguistically naïve (Kilgarriff, 1997). One way of testing this question is to ask whether people tend to agree on the appropriate sense for example sentences: according to Mihalcea et al. (2004), people agree on roughly 70% of examples using WordNet senses; far better than chance, but less than agreement on other tasks, such as sentiment annotation (Wilson et al., 2005).

**\*Other lexical semantic relations** Besides **synonymy**, WordNet also describes many other lexical semantic relationships, including:

• **antonymy**: *x* means the opposite of *y*, e.g. FRIEND-ENEMY;

<span id="page-91-2"></span><span id="page-91-1"></span><sup>&</sup>lt;sup>3</sup>Navigli (2009) provides a survey of approaches for word-sense disambiguation. <sup>4</sup>Several of the examples are adapted from WORDNET (Fellbaum, 2010).

- <span id="page-92-2"></span>• **hyponymy**: x is a special case of y, e.g. RED-COLOR; the inverse relationship is **hypernymy**;
- **meronymy**: x is a part of y, e.g., WHEEL-BICYCLE; the inverse relationship is **holonymy**.

Classification of these relations can be performed by searching for characteristic patterns between pairs of words, e.g., *X, such as Y*, which signals hyponymy [\(Hearst, 1992\)](#page-534-2), or *X but Y*, which signals antonymy [\(Hatzivassiloglou and McKeown, 1997\)](#page-533-3). Another approach is to analyze each term's **distributional statistics** (the frequency of its neighboring words). Such approaches are described in detail in [chapter 14.](#page-342-0)

#### <span id="page-92-0"></span>**4.2.2 Word sense disambiguation as classification**

How can we tell living *plants* from manufacturing *plants*? The context is often critical:

- (4.4) a. Town officials are hoping to attract new manufacturing plants through weakened environmental regulations.
  - b. The endangered plants play an important role in the local ecosystem.

It is possible to build a feature vector using the bag-of-words representation, by treating each context as a pseudo-document. The feature function is then,

> f((*plant*, *The endangered plants play an . . .*), y) = {(*the*, y) : 1,(*endangered*, y) : 1,(*play*, y) : 1,(*an*, y) : 1, . . .}

As in document classification, many of these features are irrelevant, but a few are very strong predictors. In this example, the context word *endangered* is a strong signal that the intended sense is biology rather than manufacturing. We would therefore expect a learning algorithm to assign high weight to (*endangered*, BIOLOGY), and low weight to (*endangered*, MANUFACTURING). [5](#page-92-1)

It may also be helpful to go beyond the bag-of-words: for example, one might encode the position of each context word with respect to the target, e.g.,

> f((*bank*, *I went to the bank to deposit my paycheck*), y) = {(i − 3, *went*, y) : 1,(i + 2, *deposit*, y) : 1,(i + 4, *paycheck*, y) : 1}

These are called **collocation features**, and they give more information about the specific role played by each context word. This idea can be taken further by incorporating additional syntactic information about the grammatical role played by each context feature, such as the **dependency path** (see [chapter 11\)](#page-274-0).

<span id="page-92-1"></span><sup>5</sup>The context bag-of-words can be also used be used to perform word-sense disambiguation without machine learning: the [Lesk](#page-543-1) [\(1986\)](#page-543-1) algorithm selects the word sense whose dictionary definition best overlaps the local context.

<span id="page-93-2"></span>Using such features, a classifier can be trained from labeled data. A **semantic concordance** is a corpus in which each open-class word (nouns, verbs, adjectives, and adverbs) is tagged with its word sense from the target dictionary or thesaurus. SemCor is a semantic concordance built from 234K tokens of the Brown corpus (Francis and Kucera, 1982), annotated as part of the WORDNET project (Fellbaum, 2010). SemCor annotations look like this:

(4.5) As of Sunday<sup>1</sup><sub>N</sub> night<sup>1</sup><sub>N</sub> there was<sup>4</sup><sub>V</sub> no word<sup>2</sup><sub>N</sub> ...,

with the superscripts indicating the annotated sense of each polysemous word, and the subscripts indicating the part-of-speech.

As always, supervised classification is only possible if enough labeled examples can be accumulated. This is difficult in word sense disambiguation, because each polysemous lemma requires its own training set: having a good classifier for the senses of *serve* is no help towards disambiguating *plant*. For this reason, unsupervised and **semi-supervised** methods are particularly important for word sense disambiguation (e.g., Yarowsky, 1995). These methods will be discussed in chapter 5. Unsupervised methods typically lean on the heuristic of "one sense per discourse", which means that a lemma will usually have a single, consistent sense throughout any given document (Gale et al., 1992). Based on this heuristic, we can propagate information from high-confidence instances to lowerconfidence instances in the same document (Yarowsky, 1995). Semi-supervised methods combine labeled and unlabeled data, and are discussed in more detail in chapter 5.

# <span id="page-93-0"></span>4.3 Design decisions for text classification

Text classification involves a number of design decisions. In some cases, the design decision is clear from the mathematics: if you are using regularization, then a regularization weight  $\lambda$  must be chosen. Other decisions are more subtle, arising only in the low level "plumbing" code that ingests and processes the raw data. Such decision can be surprisingly consequential for classification accuracy.

#### <span id="page-93-1"></span>4.3.1 What is a word?

The bag-of-words representation presupposes that extracting a vector of word counts from text is unambiguous. But text documents are generally represented as a sequences of characters (in an encoding such as ascii or unicode), and the conversion to bag-of-words presupposes a definition of the "words" that are to be counted.

<span id="page-94-1"></span><span id="page-94-0"></span>

| Whitespace                | Isn't | Ahab, | Ahab? | ;)   |      |      |   |   |   |
|---------------------------|-------|-------|-------|------|------|------|---|---|---|
| Treebank                  | Is    | n't   | Ahab  | ,    | Ahab | ?    | ; | ) |   |
| Tweet                     | Isn't | Ahab  | ,     | Ahab | ?    | ;)   |   |   |   |
| TokTok<br>(Dehdari, 2014) | Isn   | '     | t     | Ahab | ,    | Ahab | ? | ; | ) |

Figure 4.1: The output of four NLTK tokenizers, applied to the string *Isn't Ahab, Ahab? ;)*

#### **Tokenization**

The first subtask for constructing a bag-of-words vector is **tokenization**: converting the text from a sequence of characters to a sequence of **word!tokens**. A simple approach is to define a subset of characters as whitespace, and then split the text on these tokens. However, whitespace-based tokenization is not ideal: we may want to split conjunctions like *isn't* and hyphenated phrases like *prize-winning* and *half-asleep*, and we likely want to separate words from commas and periods that immediately follow them. At the same time, it would be better not to split abbreviations like *U.S.* and *Ph.D.* In languages with Roman scripts, tokenization is typically performed using regular expressions, with modules designed to handle each of these cases. For example, the NLTK package includes a number of tokenizers [\(Loper and Bird, 2002\)](#page-545-2); the outputs of four of the better-known tokenizers are shown in [Figure 4.1.](#page-94-0) Social media researchers have found that emoticons and other forms of orthographic variation pose new challenges for tokenization, leading to the development of special purpose tokenizers to handle these phenomena [\(O'Connor et al.,](#page-552-1) [2010\)](#page-552-1).

Tokenization is a language-specific problem, and each language poses unique challenges. For example, Chinese does not include spaces between words, nor any other consistent orthographic markers of word boundaries. A "greedy" approach is to scan the input for character substrings that are in a predefined lexicon. However, [Xue et al.](#page-570-2) [\(2003\)](#page-570-2) notes that this can be ambiguous, since many character sequences could be segmented in multiple ways. Instead, he trains a classifier to determine whether each Chinese character, or **hanzi**, is a word boundary. More advanced sequence labeling methods for word segmentation are discussed in § [8.4.](#page-202-0) Similar problems can occur in languages with alphabetic scripts, such as German, which does not include whitespace in compound nouns, yielding examples such as *Freundschaftsbezeigungen* (demonstration of friendship) and *Dilettantenaufdringlichkeiten* (the importunities of dilettantes). As [Twain](#page-566-2) [\(1997\)](#page-566-2) argues, "*These things are not words, they are alphabetic processions.*" Social media raises similar problems for English and other languages, with hashtags such as *#TrueLoveInFourWords* requiring decomposition for analysis [\(Brun and Roux, 2014\)](#page-516-4).

<span id="page-95-2"></span><span id="page-95-0"></span>

| Original           | The | Williams | sisters | are | leaving | this | tennis | centre |
|--------------------|-----|----------|---------|-----|---------|------|--------|--------|
| Porter stemmer     | the | william  | sister  | are | leav    | thi  | tenni  | centr  |
| Lancaster stemmer  | the | william  | sist    | ar  | leav    | thi  | ten    | cent   |
| WordNet lemmatizer | The | Williams | sister  | are | leaving | this | tennis | centre |

Figure 4.2: Sample outputs of the [Porter](#page-556-1) [\(1980\)](#page-556-1) and Lancaster [\(Paice, 1990\)](#page-553-5) stemmers, and the WORDNET lemmatizer

#### <span id="page-95-1"></span>**Text normalization**

After splitting the text into tokens, the next question is which tokens are really distinct. Is it necessary to distinguish *great*, *Great*, and *GREAT*? Sentence-initial capitalization may be irrelevant to the classification task. Going further, the complete elimination of case distinctions will result in a smaller vocabulary, and thus smaller feature vectors. However, case distinctions might be relevant in some situations: for example, *apple* is a delicious pie filling, while *Apple* is a company that specializes in proprietary dongles and power adapters.

For Roman script, case conversion can be performed using unicode string libraries. Many scripts do not have case distinctions (e.g., the Devanagari script used for South Asian languages, the Thai alphabet, and Japanese kana), and case conversion for all scripts may not be available in every programming environment. (Unicode support is an important distinction between Python's versions 2 and 3, and is a good reason for migrating to Python 3 if you have not already done so. Compare the output of the code "\`a l\'hˆotel".upper() in the two language versions.)

Case conversion is a type of **text normalization**, which refers to string transformations that remove distinctions that are irrelevant to downstream applications [\(Sproat et al.,](#page-563-4) [2001\)](#page-563-4). Other forms of normalization include the standardization of numbers (e.g., *1,000* to *1000*) and dates (e.g., *August 11, 2015* to *2015/11/08*). Depending on the application, it may even be worthwhile to convert all numbers and dates to special tokens, !NUM and !DATE. In social media, there are additional orthographic phenomena that may be normalized, such as expressive lengthening, e.g., *cooooool* [\(Aw et al., 2006;](#page-512-1) [Yang and Eisenstein, 2013\)](#page-571-4). Similarly, historical texts feature spelling variations that may need to be normalized to a contemporary standard form [\(Baron and Rayson, 2008\)](#page-512-2).

A more extreme form of normalization is to eliminate **inflectional affixes**, such as the *-ed* and *-s* suffixes in English. On this view, *whale*, *whales*, and *whaling* all refer to the same underlying concept, so they should be grouped into a single feature. A **stemmer** is a program for eliminating affixes, usually by applying a series of regular expression substitutions. Character-based stemming algorithms are necessarily approximate, as shown in [Figure 4.2:](#page-95-0) the Lancaster stemmer incorrectly identifies *-ers* as an inflectional suffix of

<span id="page-96-2"></span><span id="page-96-1"></span>![](_page_96_Figure_1.jpeg)

![](_page_96_Figure_2.jpeg)

(b) News articles in Brazilian Portuguese

Figure 4.3: Tradeoff between token coverage (y-axis) and vocabulary size, on the NLTK movie review dataset, after sorting the vocabulary by decreasing frequency. The red dashed lines indicate 80%, 90%, and 95% coverage.

*sisters* (by analogy to *fix/fixers*), and both stemmers incorrectly identify *-s* as a suffix of *this* and *Williams*. Fortunately, even inaccurate stemming can improve bag-of-words classification models, by merging related strings and thereby reducing the vocabulary size.

Accurately handling irregular orthography requires word-specific rules. **Lemmatizers** are systems that identify the underlying lemma of a given wordform. They must avoid the over-generalization errors of the stemmers in Figure 4.2, and also handle more complex transformations, such as *geese* $\rightarrow$ *goose*. The output of the WordNet lemmatizer is shown in the final line of Figure 4.2. Both stemming and lemmatization are language-specific: an English stemmer or lemmatizer is of little use on a text written in another language. The discipline of **morphology** relates to the study of word-internal structure, and is described in more detail in § 9.1.2.

The value of normalization depends on the data and the task. Normalization reduces the size of the feature space, which can help in generalization. However, there is always the risk of merging away linguistically meaningful distinctions. In supervised machine learning, regularization and smoothing can play a similar role to normalization — preventing the learner from overfitting to rare features — while avoiding the languagespecific engineering required for accurate normalization. In unsupervised scenarios, such as content-based information retrieval (Manning et al., 2008) and topic modeling (Blei et al., 2003), normalization is more critical.

#### <span id="page-96-0"></span>4.3.2 How many words?

Limiting the size of the feature vector reduces the memory footprint of the resulting models, and increases the speed of prediction. Normalization can help to play this role, but a more direct approach is simply to limit the vocabulary to the *N* most frequent words in the dataset. For example, in the MOVIE-REVIEWS dataset provided with NLTK (originally from Pang et al., 2002), there are 39,768 word types, and 1.58M tokens. As shown

<span id="page-97-2"></span>in Figure 4.3a, the most frequent 4000 word types cover 90% of all tokens, offering an order-of-magnitude reduction in the model size. Such ratios are language-specific: in for example, in the Brazilian Portuguese Mac-Morpho corpus (Aluísio et al., 2003), attaining 90% coverage requires more than 10000 word types (Figure 4.3b). This reflects the morphological complexity of Portuguese, which includes many more inflectional suffixes than English.

Eliminating rare words is not always advantageous for classification performance: for example, names, which are typically rare, play a large role in distinguishing topics of news articles. Another way to reduce the size of the feature space is to eliminate **stopwords** such as *the*, *to*, and *and*, which may seem to play little role in expressing the topic, sentiment, or stance. This is typically done by creating a **stoplist** (e.g., NLTK.CORPUS.STOPWORDS), and then ignoring all terms that match the list. However, corpus linguists and social psychologists have shown that seemingly inconsequential words can offer surprising insights about the author or nature of the text (Biber, 1991; Chung and Pennebaker, 2007). Furthermore, high-frequency words are unlikely to cause overfitting in discriminative classifiers. As with normalization, stopword filtering is more important for unsupervised problems, such as term-based document retrieval.

Another alternative for controlling model size is **feature hashing** (Weinberger et al., 2009). Each feature is assigned an index using a hash function. If a hash function that permits collisions is chosen (typically by taking the hash output modulo some integer), then the model can be made arbitrarily small, as multiple features share a single weight. Because most features are rare, accuracy is surprisingly robust to such collisions (Ganchev and Dredze, 2008).

#### <span id="page-97-0"></span>4.3.3 Count or binary?

Finally, we may consider whether we want our feature vector to include the *count* of each word, or its *presence*. This gets at a subtle limitation of linear classification: it's worse to have two *failures* than one, but is it really twice as bad? Motivated by this intuition, Pang et al. (2002) use binary indicators of presence or absence in the feature vector:  $f_j(x, y) \in \{0, 1\}$ . They find that classifiers trained on these binary vectors tend to outperform feature vectors based on word counts. One explanation is that words tend to appear in clumps: if a word has appeared once in a document, it is likely to appear again (Church, 2000). These subsequent appearances can be attributed to this tendency towards repetition, and thus provide little additional information about the class label of the document.

# <span id="page-97-1"></span>4.4 Evaluating classifiers

In any supervised machine learning application, it is critical to reserve a held-out test set. This data should be used for only one purpose: to evaluate the overall accuracy of a single

#### <span id="page-98-1"></span>4.4. EVALUATING CLASSIFIERS 81

classifier. Using this data more than once would cause the estimated accuracy to be overly optimistic, because the classifier would be customized to this data, and would not perform as well as on unseen data in the future. It is usually necessary to set hyperparameters or perform feature selection, so you may need to construct a **tuning** or **development set** for this purpose, as discussed in § [2.2.5.](#page-40-0)

There are a number of ways to evaluate classifier performance. The simplest is **accuracy**: the number of correct predictions, divided by the total number of instances,

$$\operatorname{acc}(\boldsymbol{y}, \hat{\boldsymbol{y}}) = \frac{1}{N} \sum_{i}^{N} \delta(y^{(i)} = \hat{y}).$$
[4.4]

Exams are usually graded by accuracy. Why are other metrics necessary? The main reason is **class imbalance**. Suppose you are building a classifier to detect whether an electronic health record (EHR) describes symptoms of a rare disease, which appears in only 1% of all documents in the dataset. A classifier that reports yˆ = NEGATIVE for all documents would achieve 99% accuracy, but would be practically useless. We need metrics that are capable of detecting the classifier's ability to discriminate between classes, even when the distribution is skewed.

One solution is to build a **balanced test set**, in which each possible label is equally represented. But in the EHR example, this would mean throwing away 98% of the original dataset! Furthermore, the detection threshold itself might be a design consideration: in health-related applications, we might prefer a very sensitive classifier, which returned a positive prediction if there is even a small chance that y (i) = POSITIVE. In other applications, a positive result might trigger a costly action, so we would prefer a classifier that only makes positive predictions when absolutely certain. We need additional metrics to capture these characteristics.

#### <span id="page-98-0"></span>**4.4.1 Precision, recall, and** F **-MEASURE**

For any label (e.g., positive for presence of symptoms of a disease), there are two possible errors:

- **False positive**: the system incorrectly predicts the label.
- **False negative**: the system incorrectly fails to predict the label.

Similarly, for any label, there are two ways to be correct:

- **True positive**: the system correctly predicts the label.
- **True negative**: the system correctly predicts that the label does not apply to this instance.

<span id="page-99-1"></span>Classifiers that make a lot of false positives have low **precision**: they predict the label even when it isn't there. Classifiers that make a lot of false negatives have low **recall**: they fail to predict the label, even when it is there. These metrics distinguish these two sources of error, and are defined formally as:

$$\operatorname{RECALL}(\boldsymbol{y}, \hat{\boldsymbol{y}}, k) = \frac{\operatorname{TP}}{\operatorname{TP} + \operatorname{FN}}$$
[4.5]

$$PRECISION(\boldsymbol{y}, \hat{\boldsymbol{y}}, k) = \frac{TP}{TP + FP}.$$
[4.6]

Recall and precision are both conditional likelihoods of a correct prediction, which is why their numerators are the same. Recall is conditioned on k being the correct label, y (i) = k, so the denominator sums over true positive and false negatives. Precision is conditioned on k being the prediction, so the denominator sums over true positives and false positives. Note that true negatives are not considered in either statistic. The classifier that labels every document as "negative" would achieve zero recall; precision would be <sup>0</sup> 0 .

Recall and precision are complementary. A high-recall classifier is preferred when false positives are cheaper than false negatives: for example, in a preliminary screening for symptoms of a disease, the cost of a false positive might be an additional test, while a false negative would result in the disease going untreated. Conversely, a high-precision classifier is preferred when false positives are more expensive: for example, in spam detection, a false negative is a relatively minor inconvenience, while a false positive might mean that an important message goes unread.

The F **-MEASURE** combines recall and precision into a single metric, using the harmonic mean:

$$F-\text{MEASURE}(\boldsymbol{y}, \hat{\boldsymbol{y}}, k) = \frac{2rp}{r+p},$$
[4.7]

where r is recall and p is precision.[6](#page-99-0)

**Evaluating multi-class classification** Recall, precision, and F -MEASURE are defined with respect to a specific label k. When there are multiple labels of interest (e.g., in word sense disambiguation or emotion classification), it is necessary to combine the F -MEASURE across each class. **Macro** F **-MEASURE** is the average F -MEASURE across several classes,

Macro-
$$F(\boldsymbol{y}, \hat{\boldsymbol{y}}) = \frac{1}{|\mathcal{K}|} \sum_{k \in \mathcal{K}} F$$
-MEASURE $(\boldsymbol{y}, \hat{\boldsymbol{y}}, k)$  [4.8]

<span id="page-99-0"></span><sup>6</sup>F -MEASURE is sometimes called F1, and generalizes to F<sup>β</sup> = (1+β <sup>2</sup>)rp β2p+r . The β parameter can be tuned to emphasize recall or precision.

<span id="page-100-3"></span><span id="page-100-2"></span>![](_page_100_Figure_1.jpeg)

Figure 4.4: ROC curves for three classifiers of varying discriminative power, measured by AUC (area under the curve)

In multi-class problems with unbalanced class distributions, the macro F-MEASURE is a balanced measure of how well the classifier recognizes each class. In **micro** F-MEASURE, we compute true positives, false positives, and false negatives for each class, and then add them up to compute a single recall, precision, and F-MEASURE. This metric is balanced across instances rather than classes, so it weights each class in proportion to its frequency — unlike macro F-MEASURE, which weights each class equally.

#### <span id="page-100-0"></span>4.4.2 Threshold-free metrics

In binary classification problems, it is possible to trade off between recall and precision by adding a constant "threshold" to the output of the scoring function. This makes it possible to trace out a curve, where each point indicates the performance at a single threshold. In the **receiver operating characteristic (ROC)** curve,<sup>7</sup> the *x*-axis indicates the **false positive rate**,  $\frac{FP}{FP+TN}$ , and the y-axis indicates the recall, or **true positive rate**. A perfect classifier attains perfect recall without any false positives, tracing a "curve" from the origin (0,0) to the upper left corner (0,1), and then to (1,1). In expectation, a non-discriminative classifier traces a diagonal line from the origin (0,0) to the upper right corner (1,1). Real classifiers tend to fall between these two extremes. Examples are shown in Figure 4.4.

The ROC curve can be summarized in a single number by taking its integral, the **area under the curve (AUC)**. The AUC can be interpreted as the probability that a randomly-selected positive example will be assigned a higher score by the classifier than a randomly-

<span id="page-100-1"></span><sup>&</sup>lt;sup>7</sup>The name "receiver operator characteristic" comes from the metric's origin in signal processing applications (Peterson et al., 1954). Other threshold-free metrics include **precision-recall curves**, **precision-at**-*k*, and **balanced** *F*-**MEASURE**; see Manning et al. (2008) for more details.

<span id="page-101-4"></span>selected negative example. A perfect classifier has AUC = 1 (all positive examples score higher than all negative examples); a non-discriminative classifier has AUC = 0.5 (given a randomly selected positive and negative example, either could score higher with equal probability); a perfectly wrong classifier would have AUC = 0 (all negative examples score higher than all positive examples). One advantage of AUC in comparison to F -MEASURE is that the baseline rate of 0.5 does not depend on the label distribution.

#### <span id="page-101-0"></span>**4.4.3 Classifier comparison and statistical significance**

Natural language processing research and engineering often involves comparing different classification techniques. In some cases, the comparison is between algorithms, such as logistic regression versus averaged perceptron, or L<sup>2</sup> regularization versus L1. In other cases, the comparison is between feature sets, such as the bag-of-words versus positional bag-of-words (see § [4.2.2\)](#page-92-0). **Ablation testing** involves systematically removing (ablating) various aspects of the classifier, such as feature groups, and testing the **null hypothesis** that the ablated classifier is as good as the full model.

A full treatment of hypothesis testing is beyond the scope of this text, but this section contains a brief summary of the techniques necessary to compare classifiers. The main aim of hypothesis testing is to determine whether the difference between two statistics — for example, the accuracies of two classifiers — is likely to arise by chance. We will be concerned with chance fluctuations that arise due to the finite size of the test set.[8](#page-101-1) An improvement of 10% on a test set with ten instances may reflect a random fluctuation that makes the test set more favorable to classifier c<sup>1</sup> than c2; on another test set with a different ten instances, we might find that c<sup>2</sup> does better than c1. But if we observe the same 10% improvement on a test set with 1000 instances, this is highly unlikely to be explained by chance. Such a finding is said to be **statistically significant** at a level p, which is the probability of observing an effect of equal or greater magnitude when the null hypothesis is true. The notation p < .05 indicates that the likelihood of an equal or greater effect is less than 5%, assuming the null hypothesis is true.[9](#page-101-2)

#### **The binomial test**

The statistical significance of a difference in accuracy can be evaluated using classical tests, such as the **binomial test**. [10](#page-101-3) Suppose that classifiers c<sup>1</sup> and c<sup>2</sup> disagree on N instances in a

<span id="page-101-1"></span><sup>8</sup>Other sources of variance include the initialization of non-convex classifiers such as neural networks, and the ordering of instances in online learning such as stochastic gradient descent and perceptron.

<span id="page-101-2"></span><sup>9</sup> Statistical hypothesis testing is useful only to the extent that the existing test set is representative of the instances that will be encountered in the future. If, for example, the test set is constructed from news documents, no hypothesis test can predict which classifier will perform best on documents from another domain, such as electronic health records.

<span id="page-101-3"></span><sup>10</sup>A well-known alternative to the binomial test is **McNemar's test**, which computes a **test statistic** based on the number of examples that are correctly classified by one system and incorrectly classified by the other.

<span id="page-102-1"></span><span id="page-102-0"></span>![](_page_102_Figure_1.jpeg)

Figure 4.5: Probability mass function for the binomial distribution. The pink highlighted areas represent the cumulative probability for a significance test on an observation of k = 10 and N = 30.

test set with binary labels, and that  $c_1$  is correct on k of those instances. Under the null hypothesis that the classifiers are equally accurate, we would expect k/N to be roughly equal to 1/2, and as N increases, k/N should be increasingly close to this expected value. These properties are captured by the **binomial distribution**, which is a probability over counts of binary random variables. We write  $k \sim \text{Binom}(\theta, N)$  to indicate that k is drawn from a binomial distribution, with parameter N indicating the number of random "draws", and  $\theta$  indicating the probability of "success" on each draw. Each draw is an example on which the two classifiers disagree, and a "success" is a case in which  $c_1$  is right and  $c_2$  is wrong. (The label space is assumed to be binary, so if the classifiers disagree, exactly one of them is correct. The test can be generalized to multi-class classification by focusing on the examples in which exactly one classifier is correct.)

The probability mass function (PMF) of the binomial distribution is,

$$\mathbf{p}_{\mathsf{Binom}}(k; N, \theta) = \binom{N}{k} \theta^k (1-\theta)^{N-k},$$
[4.9]

with  $\theta^k$  representing the probability of the k successes,  $(1 - \theta)^{N-k}$  representing the probability of the N - k unsuccessful draws. The expression  $\binom{N}{k} = \frac{N!}{k!(N-k)!}$  is a binomial coefficient, representing the number of possible orderings of events; this ensures that the distribution sums to one over all  $k \in \{0, 1, 2, ..., N\}$ .

Under the null hypothesis, when the classifiers disagree, each classifier is equally likely to be right, so  $\theta = \frac{1}{2}$ . Now suppose that among *N* disagreements,  $c_1$  is correct  $k < \frac{N}{2}$  times. The probability of  $c_1$  being correct *k* or fewer times is the **one-tailed** *p***-value**,

The null hypothesis distribution for this test statistic is known to be drawn from a chi-squared distribution with a single degree of freedom, so a *p*-value can be computed from the cumulative density function of this distribution (Dietterich, 1998). Both tests give similar results in most circumstances, but the binomial test is easier to understand from first principles.

<span id="page-103-0"></span>because it is computed from the area under the binomial probability mass function from 0 to k, as shown in the left tail of Figure 4.5. This **cumulative probability** is computed as a sum over all values  $i \le k$ ,

$$\Pr_{\text{Binom}}\left(\text{count}(\hat{y}_{2}^{(i)} = y^{(i)} \neq \hat{y}_{1}^{(i)}) \le k; N, \theta = \frac{1}{2}\right) = \sum_{i=0}^{k} p_{\text{Binom}}\left(i; N, \theta = \frac{1}{2}\right).$$
[4.10]

The one-tailed p-value applies only to the asymmetric null hypothesis that  $c_1$  is at least as accurate as  $c_2$ . To test the **two-tailed** null hypothesis that  $c_1$  and  $c_2$  are equally accurate, we would take the sum of one-tailed *p*-values, where the second term is computed from the right tail of Figure 4.5. The binomial distribution is symmetric, so this can be computed by simply doubling the one-tailed p-value.

Two-tailed tests are more stringent, but they are necessary in cases in which there is no prior intuition about whether  $c_1$  or  $c_2$  is better. For example, in comparing logistic regression versus averaged perceptron, a two-tailed test is appropriate. In an ablation test,  $c_2$  may contain a superset of the features available to  $c_1$ . If the additional features are thought to be likely to improve performance, then a one-tailed test would be appropriate, if chosen in advance. However, such a test can only prove that  $c_2$  is more accurate than  $c_1$ , and not the reverse.

#### \*Randomized testing

The binomial test is appropriate for accuracy, but not for more complex metrics such as F-MEASURE. To compute statistical significance for arbitrary metrics, we can apply randomization. Specifically, draw a set of M bootstrap samples (Efron and Tibshirani, 1993), by resampling instances from the original test set with replacement. Each bootstrap sample is itself a test set of size N. Some instances from the original test set will not appear in any given bootstrap sample, while others will appear multiple times; but overall, the sample will be drawn from the same distribution as the original test set. We can then compute any desired evaluation on each bootstrap sample, which gives a distribution over the value of the metric. Algorithm 7 shows how to perform this computation.

To compare the *F*-MEASURE of two classifiers  $c_1$  and  $c_2$ , we set the function  $\delta(\cdot)$  to compute the difference in *F*-MEASURE on the bootstrap sample. If the difference is less than or equal to zero in at least 5% of the samples, then we cannot reject the one-tailed null hypothesis that  $c_2$  is at least as good as  $c_1$  (Berg-Kirkpatrick et al., 2012). We may also be interested in the 95% **confidence interval** around a metric of interest, such as the *F*-MEASURE of a single classifier. This can be computed by sorting the output of Algorithm 7, and then setting the top and bottom of the 95% confidence interval to the values at the 2.5% and 97.5% percentiles of the sorted outputs. Alternatively, you can fit a normal distribution to the set of differences across bootstrap samples, and compute a Gaussian confidence interval from the mean and variance.

<span id="page-104-2"></span><span id="page-104-1"></span>**Algorithm 7** Bootstrap sampling for classifier evaluation. The original test set is  $\{x^{(1:N)}, y^{(1:N)}\}$ , the metric is  $\delta(\cdot)$ , and the number of samples is M.

```
procedure BOOTSTRAP-SAMPLE(\boldsymbol{x}^{(1:N)}, \boldsymbol{y}^{(1:N)}, \delta(\cdot), M)

for t \in \{1, 2, ..., M\} do

for i \in \{1, 2, ..., N\} do

j \sim UniformInteger(1, N)

\tilde{\boldsymbol{x}}^{(i)} \leftarrow \boldsymbol{x}^{(j)}

\tilde{\boldsymbol{y}}^{(i)} \leftarrow \boldsymbol{y}^{(j)}

d^{(t)} \leftarrow \delta(\tilde{\boldsymbol{x}}^{(1:N)}, \tilde{\boldsymbol{y}}^{(1:N)})

return \{d^{(t)}\}_{t=1}^{M}
```

As the number of bootstrap samples goes to infinity,  $M \to \infty$ , the bootstrap estimate is increasingly accurate. A typical choice for M is  $10^4$  or  $10^5$ ; larger numbers of samples are necessary for smaller p-values. One way to validate your choice of M is to run the test multiple times, and ensure that the p-values are similar; if not, increase M by an order of magnitude. This is a heuristic measure of the **variance** of the test, which can decreases with the square root  $\sqrt{M}$  (Robert and Casella, 2013).

#### <span id="page-104-0"></span>4.4.4 \*Multiple comparisons

Sometimes it is necessary to perform multiple hypothesis tests, such as when comparing the performance of several classifiers on multiple datasets. Suppose you have five datasets, and you compare four versions of your classifier against a baseline system, for a total of 20 comparisons. Even if none of your classifiers is better than the baseline, there will be some chance variation in the results, and in expectation you will get one statistically significant improvement at  $p = 0.05 = \frac{1}{20}$ . It is therefore necessary to adjust the *p*-values when reporting the results of multiple comparisons.

One approach is to require a threshold of  $\frac{\alpha}{m}$  to report a p value of  $p < \alpha$  when performing m tests. This is known as the **Bonferroni correction**, and it limits the overall probability of incorrectly rejecting the null hypothesis at  $\alpha$ . Another approach is to bound the **false discovery rate** (FDR), which is the fraction of null hypothesis rejections that are incorrect. Benjamini and Hochberg (1995) propose a p-value correction that bounds the fraction of false discoveries at  $\alpha$ : sort the p-values of each individual test in ascending order, and set the significance threshold equal to largest k such that  $p_k \leq \frac{k}{m} \alpha$ . If k > 1, the FDR adjustment is more permissive than the Bonferroni correction.

# <span id="page-105-0"></span>**4.5 Building datasets**

Sometimes, if you want to build a classifier, you must first build a dataset of your own. This includes selecting a set of documents or instances to annotate, and then performing the annotations. The scope of the dataset may be determined by the application: if you want to build a system to classify electronic health records, then you must work with a corpus of records of the type that your classifier will encounter when deployed. In other cases, the goal is to build a system that will work across a broad range of documents. In this case, it is best to have a *balanced* corpus, with contributions from many styles and genres. For example, the Brown corpus draws from texts ranging from government documents to romance novels [\(Francis, 1964\)](#page-529-5), and the Google Web Treebank includes annotations for five "domains" of web documents: question answers, emails, newsgroups, reviews, and blogs [\(Petrov and McDonald, 2012\)](#page-555-2).

#### <span id="page-105-1"></span>**4.5.1 Metadata as labels**

Annotation is difficult and time-consuming, and most people would rather avoid it. It is sometimes possible to exploit existing metadata to obtain labels for training a classifier. For example, reviews are often accompanied by a numerical rating, which can be converted into a classification label (see § [4.1\)](#page-86-1). Similarly, the nationalities of social media users can be estimated from their profiles [\(Dredze et al., 2013\)](#page-525-2) or even the time zones of their posts [\(Gouws et al., 2011\)](#page-531-6). More ambitiously, we may try to classify the political affiliations of social media profiles based on their social network connections to politicians and major political parties [\(Rao et al., 2010\)](#page-557-3).

The convenience of quickly constructing large labeled datasets without manual annotation is appealing. However this approach relies on the assumption that unlabeled instances — for which metadata is unavailable — will be similar to labeled instances. Consider the example of labeling the political affiliation of social media users based on their network ties to politicians. If a classifier attains high accuracy on such a test set, is it safe to assume that it accurately predicts the political affiliation of all social media users? Probably not. Social media users who establish social network ties to politicians may be more likely to mention politics in the text of their messages, as compared to the average user, for whom no political metadata is available. If so, the accuracy on a test set constructed from social network metadata would give an overly optimistic picture of the method's true performance on unlabeled data.

#### <span id="page-105-2"></span>**4.5.2 Labeling data**

In many cases, there is no way to get ground truth labels other than manual annotation. An annotation protocol should satisfy several criteria: the annotations should be *expressive* enough to capture the phenomenon of interest; they should be *replicable*, meaning that

#### 4.5. BUILDING DATASETS 89

another annotator or team of annotators would produce very similar annotations if given the same data; and they should be *scalable*, so that they can be produced relatively quickly. [Hovy and Lavid](#page-535-3) [\(2010\)](#page-535-3) propose a structured procedure for obtaining annotations that meet these criteria, which is summarized below.

- 1. **Determine what to annotate**. This is usually based on some theory of the underlying phenomenon: for example, if the goal is to produce annotations about the emotional state of a document's author, one should start with a theoretical account of the types or dimensions of emotion (e.g., [Mohammad and Turney, 2013\)](#page-550-1). At this stage, the tradeoff between expressiveness and scalability should be considered: a full instantiation of the underlying theory might be too costly to annotate at scale, so reasonable approximations should be considered.
- 2. Optionally, one may **design or select a software tool to support the annotation effort**. Existing general-purpose annotation tools include BRAT [\(Stenetorp et al.,](#page-563-5) [2012\)](#page-563-5) and MMAX2 [\(Muller and Strube, 2006\)](#page-551-4). ¨
- 3. **Formalize the instructions for the annotation task**. To the extent that the instructions are not explicit, the resulting annotations will depend on the intuitions of the annotators. These intuitions may not be shared by other annotators, or by the users of the annotated data. Therefore explicit instructions are critical to ensuring the annotations are replicable and usable by other researchers.
- 4. **Perform a pilot annotation** of a small subset of data, with multiple annotators for each instance. This will give a preliminary assessment of both the replicability and scalability of the current annotation instructions. Metrics for computing the rate of agreement are described below. Manual analysis of specific disagreements should help to clarify the instructions, and may lead to modifications of the annotation task itself. For example, if two labels are commonly conflated by annotators, it may be best to merge them.
- 5. **Annotate the data**. After finalizing the annotation protocol and instructions, the main annotation effort can begin. Some, if not all, of the instances should receive multiple annotations, so that inter-annotator agreement can be computed. In some annotation projects, instances receive many annotations, which are then aggregated into a "consensus" label (e.g., [Danescu-Niculescu-Mizil et al., 2013\)](#page-523-1). However, if the annotations are time-consuming or require significant expertise, it may be preferable to maximize scalability by obtaining multiple annotations for only a small subset of examples.
- 6. **Compute and report inter-annotator agreement, and release the data**. In some cases, the raw text data cannot be released, due to concerns related to copyright or

<span id="page-107-2"></span>privacy. In these cases, one solution is to publicly release **stand-off annotations**, which contain links to document identifiers. The documents themselves can be released under the terms of a licensing agreement, which can impose conditions on how the data is used. It is important to think through the potential consequences of releasing data: people may make personal data publicly available without realizing that it could be redistributed in a dataset and publicized far beyond their expectations [\(boyd and Crawford, 2012\)](#page-516-1).

#### <span id="page-107-1"></span>**Measuring inter-annotator agreement**

To measure the replicability of annotations, a standard practice is to compute the extent to which annotators agree with each other. If the annotators frequently disagree, this casts doubt on either their reliability or on the annotation system itself. For classification, one can compute the frequency with which the annotators agree; for rating scales, one can compute the average distance between ratings. These raw agreement statistics must then be compared with the rate of agreement by chance — the expected level of agreement that would be obtained between two annotators who ignored the data.

**Cohen's Kappa** is widely used for quantifying the agreement on discrete labeling tasks [\(Cohen, 1960;](#page-521-1) [Carletta, 1996\)](#page-518-0),[11](#page-107-0)

$$\kappa = \frac{\text{agreement} - E[\text{agreement}]}{1 - E[\text{agreement}]}.$$
[4.11]

The numerator is the difference between the observed agreement and the chance agreement, and the denominator is the difference between perfect agreement and chance agreement. Thus, κ = 1 when the annotators agree in every case, and κ = 0 when the annotators agree only as often as would happen by chance. Various heuristic scales have been proposed for determining when κ indicates "moderate", "good", or "substantial" agreement; for reference, [Lee and Narayanan](#page-542-1) [\(2005\)](#page-542-1) report κ ≈ 0.45 − 0.47 for annotations of emotions in spoken dialogues, which they describe as "moderate agreement"; [Stolcke](#page-563-6) [et al.](#page-563-6) [\(2000\)](#page-563-6) report κ = 0.8 for annotations of **dialogue acts**, which are labels for the purpose of each turn in a conversation.

When there are two annotators, the expected chance agreement is computed as,

$$E[\text{agreement}] = \sum_{k} \hat{\Pr}(Y = k)^2, \qquad [4.12]$$

where k is a sum over labels, and Pr( ˆ Y = k) is the empirical probability of label k across all annotations. The formula is derived from the expected number of agreements if the annotations were randomly shuffled. Thus, in a binary labeling task, if one label is applied to 90% of instances, chance agreement is .9 <sup>2</sup> + .1 <sup>2</sup> = .82.

<span id="page-107-0"></span><sup>11</sup> For other types of annotations, Krippendorf's alpha is a popular choice [\(Hayes and Krippendorff, 2007;](#page-533-4) [Artstein and Poesio, 2008\)](#page-511-2).

#### <span id="page-108-0"></span>Crowdsourcing

Crowdsourcing is often used to rapidly obtain annotations for classification problems. For example, **Amazon Mechanical Turk** makes it possible to define "human intelligence tasks (hits)", such as labeling data. The researcher sets a price for each set of annotations and a list of minimal qualifications for annotators, such as their native language and their satisfaction rate on previous tasks. The use of relatively untrained "crowdworkers" contrasts with earlier annotation efforts, which relied on professional linguists (Marcus et al., 1993). However, crowdsourcing has been found to produce reliable annotations for many language-related tasks (Snow et al., 2008). Crowdsourcing is part of the broader field of **human computation** (Law and Ahn, 2011).For a critical examination of ethical issues related to crowdsourcing, see Fort et al. (2011).

# Additional resources

Many of the preprocessing issues discussed in this chapter also arise in information retrieval. See Manning et al. (2008) for discussion of tokenization and related algorithms. For more on hypothesis testing in particular and replicability in general, see (Dror et al., 2017, 2018).

# Exercises

1. As noted in § 4.3.3, words tend to appear in clumps, with subsequent occurrences of a word being more probable. More concretely, if word *j* has probability  $\phi_{y,j}$  of appearing in a document with label *y*, then the probability of two appearances  $(x_i^{(i)} = 2)$  is greater than  $\phi_{u,j}^2$ .

Suppose you are applying Naïve Bayes to a binary classification. Focus on a word j which is more probable under label y = 1, so that,

$$\Pr(w = j \mid y = 1) > \Pr(w = j \mid y = 0).$$
[4.13]

Now suppose that  $x_j^{(i)} > 1$ . All else equal, will the classifier overestimate or underestimate the posterior Pr(y = 1 | x)?

- 2. Prove that F-measure is never greater than the arithmetic mean of recall and precision,  $\frac{r+p}{2}$ . Your solution should also show that F-measure is equal to  $\frac{r+p}{2}$  iff r = p.
- 3. Given a binary classification problem in which the probability of the "positive" label is equal to  $\alpha$ , what is the expected *F*-MEASURE of a random classifier which ignores the data, and selects  $\hat{y} = +1$  with probability  $\frac{1}{2}$ ? (Assume that  $p(\hat{y}) \perp p(y)$ .) What is the expected *F*-MEASURE of a classifier that selects  $\hat{y} = +1$  with probability  $\alpha$  (also independent of  $y^{(i)}$ )? Depending on  $\alpha$ , which random classifier will score better?

- <span id="page-109-2"></span>4. Suppose that binary classifiers  $c_1$  and  $c_2$  disagree on N = 30 cases, and that  $c_1$  is correct in k = 10 of those cases.
  - Write a program that uses primitive functions such as exp and factorial to compute the **two-tailed** *p*-value — you may use an implementation of the "choose" function if one is available. Verify your code against the output of a library for computing the binomial test or the binomial CDF, such as SCIPY.STATS.BINOM in Python.
  - Then use a randomized test to try to obtain the same *p*-value. In each sample, draw from a binomial distribution with N = 30 and  $\theta = \frac{1}{2}$ . Count the fraction of samples in which  $k \leq 10$ . This is the one-tailed *p*-value; double this to compute the two-tailed *p*-value.
  - Try this with varying numbers of bootstrap samples:  $M \in \{100, 1000, 5000, 10000\}$ . For M = 100 and M = 1000, run the test 10 times, and plot the resulting *p*-values.
  - Finally, perform the same tests for N = 70 and k = 25.
- 5. SemCor 3.0 is a labeled dataset for word sense disambiguation. You can download it,<sup>12</sup> or access it in NLTK.CORPORA.SEMCOR.

Choose a word that appears at least ten times in SemCor (*find*), and annotate its WordNet senses across ten randomly-selected examples, without looking at the ground truth. Use online WordNet to understand the definition of each of the senses.<sup>13</sup> Have a partner do the same annotations, and compute the raw rate of agreement, expected chance rate of agreement, and Cohen's kappa.

6. Download the Pang and Lee movie review data, currently available from http: //www.cs.cornell.edu/people/pabo/movie-review-data/. Hold out a randomly-selected 400 reviews as a test set.

Download a sentiment lexicon, such as the one currently available from Bing Liu, https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html. Tokenize the data, and classify each document as positive iff it has more positive sentiment words than negative sentiment words. Compute the accuracy and *F*-MEASURE on detecting positive reviews on the test set, using this lexicon-based classifier.

Then train a discriminative classifier (averaged perceptron or logistic regression) on the training set, and compute its accuracy and *F*-MEASURE on the test set.

Determine whether the differences are statistically significant, using two-tailed hypothesis tests: Binomial for the difference in accuracy, and bootstrap for the difference in macro-F-MEASURE.

<span id="page-109-0"></span><sup>&</sup>lt;sup>12</sup>e.g., https://github.com/google-research-datasets/word\_sense\_disambigation\_ corpora or http://globalwordnet.org/wordnet-annotated-corpora/

<span id="page-109-1"></span><sup>&</sup>lt;sup>13</sup>http://wordnetweb.princeton.edu/perl/webwn

#### 4.5. BUILDING DATASETS 93

The remaining problems will require you to build a classifier and test its properties. Pick a multi-class text classification dataset that is not already tokenized. One example is a dataset of New York Times headlines and topics [\(Boydstun, 2013\)](#page-516-5).[14](#page-110-0) Divide your data into training (60%), development (20%), and test sets (20%), if no such division already exists. If your dataset is very large, you may want to focus on a few thousand instances at first.

- 7. Compare various vocabulary sizes of 10<sup>2</sup> , 10<sup>3</sup> , 10<sup>4</sup> , 10<sup>5</sup> , using the most frequent words in each case (you may use any reasonable tokenizer). Train logistic regression classifiers for each vocabulary size, and apply them to the development set. Plot the accuracy and Macro-F -MEASURE with the increasing vocabulary size. For each vocabulary size, tune the regularizer to maximize accuracy on a subset of data that is held out from the training set.
- 8. Compare the following tokenization algorithms:
  - Whitespace, using a regular expression;
  - The Penn Treebank tokenizer from NLTK;
  - Splitting the input into non-overlapping five-character units, regardless of whitespace or punctuation.

Compute the token/type ratio for each tokenizer on the training data, and explain what you find. Train your classifier on each tokenized dataset, tuning the regularizer on a subset of data that is held out from the training data. Tokenize the development set, and report accuracy and Macro-F -MEASURE.

- 9. Apply the Porter and Lancaster stemmers to the training set, using any reasonable tokenizer, and compute the token/type ratios. Train your classifier on the stemmed data, and compute the accuracy and Macro-F -MEASURE on stemmed development data, again using a held-out portion of the training data to tune the regularizer.
- 10. Identify the best combination of vocabulary filtering, tokenization, and stemming from the previous three problems. Apply this preprocessing to the test set, and compute the test set accuracy and Macro-F -MEASURE. Compare against a baseline system that applies no vocabulary filtering, whitespace tokenization, and no stemming.

Use the binomial test to determine whether your best-performing system is significantly more accurate than the baseline.

<span id="page-110-0"></span><sup>14</sup>Available as a CSV file at [http://www.amber-boydstun.com/](http://www.amber-boydstun.com/supplementary-information-for-making-the-news.html) [supplementary-information-for-making-the-news.html](http://www.amber-boydstun.com/supplementary-information-for-making-the-news.html). Use the field TOPIC 2DIGIT for this problem.

Use the bootstrap test with M = 10<sup>4</sup> to determine whether your best-performing system achieves significantly higher macro-F -MEASURE.

# <span id="page-112-2"></span><span id="page-112-0"></span>**Chapter 5**

# **Learning without supervision**

So far, we have assumed the following setup:

- a **training set** where you get observations x and labels y;
- a **test set** where you only get observations x.

Without labeled data, is it possible to learn anything? This scenario is known as **unsupervised learning**, and we will see that indeed it is possible to learn about the underlying structure of unlabeled observations. This chapter will also explore some related scenarios: **semi-supervised learning**, in which only some instances are labeled, and **domain adaptation**, in which the training data differs from the data on which the trained system will be deployed.

# <span id="page-112-1"></span>**5.1 Unsupervised learning**

To motivate unsupervised learning, consider the problem of word sense disambiguation (§ [4.2\)](#page-90-0). The goal is to classify each instance of a word, such as *bank* into a sense,

- bank#1: a financial institution
- bank#2: the land bordering a river

It is difficult to obtain sufficient training data for word sense disambiguation, because even a large corpus will contain only a few instances of all but the most common words. Is it possible to learn anything about these different senses without labeled data?

Word sense disambiguation is usually performed using feature vectors constructed from the local context of the word to be disambiguated. For example, for the word

<span id="page-113-2"></span><span id="page-113-1"></span>![](_page_113_Figure_1.jpeg)

Figure 5.1: Counts of words from two different context groups

*bank*, the immediate context might typically include words from one of the following two groups:

- 1. *financial, deposits, credit, lending, capital, markets, regulated, reserve, liquid, assets*
- 2. *land, water, geography, stream, river, flow, deposits, discharge, channel, ecology*

Now consider a scatterplot, in which each point is a document containing the word *bank*. The location of the document on the x-axis is the count of words in group 1, and the location on the y-axis is the count for group 2. In such a plot, shown in [Figure 5.1,](#page-113-1) two "blobs" might emerge, and these blobs correspond to the different senses of *bank*.

Here's a related scenario, from a different problem. Suppose you download thousands of news articles, and make a scatterplot, where each point corresponds to a document: the x-axis is the frequency of the group of words (*hurricane, winds, storm*); the y-axis is the frequency of the group (*election, voters, vote*). This time, three blobs might emerge: one for documents that are largely about a hurricane, another for documents largely about a election, and a third for documents about neither topic.

These clumps represent the underlying structure of the data. But the two-dimensional scatter plots are based on groupings of context words, and in real scenarios these word lists are unknown. Unsupervised learning applies the same basic idea, but in a highdimensional space with one dimension for every context word. This space can't be directly visualized, but the goal is the same: try to identify the underlying structure of the observed data, such that there are a few clusters of points, each of which is internally coherent. **Clustering** algorithms are capable of finding such structure automatically.

#### <span id="page-113-0"></span>**5.1.1** K**-means clustering**

Clustering algorithms assign each data point to a discrete cluster, z<sup>i</sup> ∈ 1, 2, . . . K. One of the best known clustering algorithms is K**-means**, an iterative algorithm that maintains

<span id="page-114-1"></span><span id="page-114-0"></span>

| Alg  | orithm 8 K-means clustering algorithm                                                                     |                                          |
|------|-----------------------------------------------------------------------------------------------------------|------------------------------------------|
| 1: ] | procedure $K$ -MEANS $(oldsymbol{x}_{1:N},K)$                                                             |                                          |
| 2:   | for $i \in 1 \dots N$ do                                                                                  | initialize cluster memberships           |
| 3:   | $z^{(i)} \leftarrow RANDOMINT(1, K)$                                                                      |                                          |
| 4:   | repeat                                                                                                    |                                          |
| 5:   | for $k \in 1 \dots K$ do                                                                                  | recompute cluster centers                |
| 6:   | $oldsymbol{ u}_k \leftarrow rac{1}{\delta(z^{(i)}=k)} \sum_{i=1}^N \delta(z^{(i)}=k) oldsymbol{x}^{(i)}$ |                                          |
| 7:   | for $i \in 1 \dots N$ do                                                                                  | ▷ reassign instances to nearest clusters |
| 8:   | $z^{(i)} \leftarrow \operatorname{argmin}_k   oldsymbol{x}^{(i)} - oldsymbol{ u}_k  ^2$                   |                                          |
| 9:   | until converged                                                                                           |                                          |
| 10:  | return $\{z^{(i)}\}$                                                                                      | ▷ return cluster assignments             |

a cluster assignment for each instance, and a central ("mean") location for each cluster. *K*-means iterates between updates to the assignments and the centers:

- 1. each instance is placed in the cluster with the closest center;
- 2. each center is recomputed as the average over points in the cluster.

This procedure is formalized in Algorithm 8. The term  $||\boldsymbol{x}^{(i)} - \boldsymbol{\nu}||^2$  refers to the squared Euclidean norm,  $\sum_{j=1}^{V} (x_j^{(i)} - \nu_j)^2$ . An important property of *K*-means is that the converged solution depends on the initialization, and a better clustering can sometimes be found simply by re-running the algorithm from a different random starting point.

**Soft** *K*-means is a particularly relevant variant. Instead of directly assigning each point to a specific cluster, soft *K*-means assigns to each point a *distribution* over clusters  $q^{(i)}$ , so that  $\sum_{k=1}^{K} q^{(i)}(k) = 1$ , and  $\forall_k, q^{(i)}(k) \ge 0$ . The soft weight  $q^{(i)}(k)$  is computed from the distance of  $x^{(i)}$  to the cluster center  $\nu_k$ . In turn, the center of each cluster is computed from a weighted average of the points in the cluster,

$$\boldsymbol{\nu}_{k} = \frac{1}{\sum_{i=1}^{N} q^{(i)}(k)} \sum_{i=1}^{N} q^{(i)}(k) \boldsymbol{x}^{(i)}.$$
[5.1]

We will now explore a probablistic version of soft *K*-means clustering, based on **expectation**-**maximization** (EM). Because EM clustering can be derived as an approximation to maximumlikelihood estimation, it can be extended in a number of useful ways.

#### <span id="page-115-2"></span><span id="page-115-0"></span>5.1.2 Expectation-Maximization (EM)

Expectation-maximization combines the idea of soft *K*-means with Naïve Bayes classification. To review, Naïve Bayes defines a probability distribution over the data,

$$\log p(\boldsymbol{x}, \boldsymbol{y}; \boldsymbol{\phi}, \boldsymbol{\mu}) = \sum_{i=1}^{N} \log \left( p(\boldsymbol{x}^{(i)} \mid y^{(i)}; \boldsymbol{\phi}) \times p(y^{(i)}; \boldsymbol{\mu}) \right)$$
[5.2]

Now suppose that you never observe the labels. To indicate this, we'll refer to the label of each instance as  $z^{(i)}$ , rather than  $y^{(i)}$ , which is usually reserved for observed variables. By marginalizing over the **latent variables** z, we obtain the marginal probability of the observed instances x:

$$\log p(\boldsymbol{x}; \boldsymbol{\phi}, \boldsymbol{\mu}) = \sum_{i=1}^{N} \log p(\boldsymbol{x}^{(i)}; \boldsymbol{\phi}, \boldsymbol{\mu})$$
[5.3]

$$=\sum_{i=1}^{N} \log \sum_{z=1}^{K} p(\boldsymbol{x}^{(i)}, z; \boldsymbol{\phi}, \boldsymbol{\mu})$$
 [5.4]

<span id="page-115-1"></span>
$$=\sum_{i=1}^{N}\log\sum_{z=1}^{K}\mathbf{p}(\boldsymbol{x}^{(i)} \mid z; \boldsymbol{\phi}) \times \mathbf{p}(z; \boldsymbol{\mu}).$$
[5.5]

The parameters  $\phi$  and  $\mu$  can be obtained by maximizing the marginal likelihood in Equation 5.5. Why is this the right thing to maximize? Without labels, discriminative learning is impossible — there's nothing to discriminate. So maximum likelihood is all we have.

When the labels are observed, we can estimate the parameters of the Naïve Bayes probability model separately for each label. But marginalizing over the labels couples these parameters, making direct optimization of  $\log p(x)$  intractable. We will approximate the log-likelihood by introducing an auxiliary variable  $q^{(i)}$ , which is a distribution over the label set  $\mathcal{Z} = \{1, 2, ..., K\}$ . The optimization procedure will alternate between updates to q and updates to the parameters  $(\phi, \mu)$ . Thus,  $q^{(i)}$  plays here as in soft *K*-means.

To derive the updates for this optimization, multiply the right side of Equation 5.5 by

<span id="page-116-2"></span>the ratio <sup>q</sup> (i) (z) q (i)(z) = 1,

$$\log \mathbf{p}(\boldsymbol{x}; \boldsymbol{\phi}, \boldsymbol{\mu}) = \sum_{i=1}^{N} \log \sum_{z=1}^{K} \mathbf{p}(\boldsymbol{x}^{(i)} \mid z; \boldsymbol{\phi}) \times \mathbf{p}(z; \boldsymbol{\mu}) \times \frac{q^{(i)}(z)}{q^{(i)}(z)}$$
[5.6]

$$= \sum_{i=1}^{N} \log \sum_{z=1}^{K} q^{(i)}(z) \times \mathbf{p}(\boldsymbol{x}^{(i)} \mid z; \boldsymbol{\phi}) \times \mathbf{p}(z; \boldsymbol{\mu}) \times \frac{1}{q^{(i)}(z)}$$
[5.7]

$$= \sum_{i=1}^{N} \log E_{q^{(i)}} \left[ \frac{\mathbf{p}(\boldsymbol{x}^{(i)} \mid z; \boldsymbol{\phi}) \mathbf{p}(z; \boldsymbol{\mu})}{q^{(i)}(z)} \right],$$
[5.8]

where E<sup>q</sup> (i) [f(z)] = P<sup>K</sup> <sup>z</sup>=1 q (i) (z) × f(z) refers to the expectation of the function f under the distribution z ∼ q (i) .

**Jensen's inequality** says that because log is a concave function, we can push it inside the expectation, and obtain a lower bound.

$$\log \mathbf{p}(\boldsymbol{x}; \boldsymbol{\phi}, \boldsymbol{\mu}) \ge \sum_{i=1}^{N} E_{\boldsymbol{q}^{(i)}} \left[ \log \frac{\mathbf{p}(\boldsymbol{x}^{(i)} \mid \boldsymbol{z}; \boldsymbol{\phi}) \mathbf{p}(\boldsymbol{z}; \boldsymbol{\mu})}{q^{(i)}(\boldsymbol{z})} \right]$$
[5.9]

$$J \triangleq \sum_{i=1}^{N} E_{\boldsymbol{q}^{(i)}} \left[ \log p(\boldsymbol{x}^{(i)} \mid z; \boldsymbol{\phi}) + \log p(z; \boldsymbol{\mu}) - \log q^{(i)}(z) \right]$$
[5.10]

<span id="page-116-1"></span><span id="page-116-0"></span>
$$=\sum_{i=1}^{N} E_{\boldsymbol{q}^{(i)}} \left[ \log p(\boldsymbol{x}^{(i)}, z; \boldsymbol{\phi}, \boldsymbol{\mu}) \right] + H(\boldsymbol{q}^{(i)})$$
[5.11]

We will focus on [Equation 5.10,](#page-116-0) which is the lower bound on the marginal log-likelihood of the observed data, log p(x). [Equation 5.11](#page-116-1) shows the connection to the information theoretic concept of **entropy**, H(q (i) ) = − P<sup>K</sup> <sup>z</sup>=1 q (i) (z) log q (i) (z), which measures the average amount of information produced by a draw from the distribution q (i) . The lower bound J is a function of two groups of arguments:

- the distributions q (i) for each instance;
- the parameters µ and φ.

The expectation-maximization (EM) algorithm maximizes the bound with respect to each of these arguments in turn, while holding the other fixed.

#### **The E-step**

The step in which we update q (i) is known as the **E-step**, because it updates the distribution under which the expectation is computed. To derive this update, first write out the

expectation in the lower bound as a sum,

$$J = \sum_{i=1}^{N} \sum_{z=1}^{K} q^{(i)}(z) \left[ \log p(\boldsymbol{x}^{(i)} \mid z; \boldsymbol{\phi}) + \log p(z; \boldsymbol{\mu}) - \log q^{(i)}(z) \right].$$
 [5.12]

When optimizing this bound, we must also respect a set of "sum-to-one" constraints,  $\sum_{z=1}^{K} q^{(i)}(z) = 1$  for all *i*. Just as in Naïve Bayes, this constraint can be incorporated into a Lagrangian:

$$J_q = \sum_{i=1}^N \sum_{z=1}^K q^{(i)}(z) \left( \log \mathbf{p}(\mathbf{x}^{(i)} \mid z; \phi) + \log \mathbf{p}(z; \mu) - \log q^{(i)}(z) \right) + \lambda^{(i)} (1 - \sum_{z=1}^K q^{(i)}(z)),$$
[5.13]

where  $\lambda^{(i)}$  is the Lagrange multiplier for instance *i*.

The Lagrangian is maximized by taking the derivative and solving for  $q^{(i)}$ :

$$\frac{\partial J_q}{\partial q^{(i)}(z)} = \log p(\boldsymbol{x}^{(i)} \mid z; \boldsymbol{\phi}) + \log p(z; \boldsymbol{\theta}) - \log q^{(i)}(z) - 1 - \lambda^{(i)}$$
[5.14]

$$\log q^{(i)}(z) = \log p(\boldsymbol{x}^{(i)} \mid z; \boldsymbol{\phi}) + \log p(z; \mu) - 1 - \lambda^{(i)}$$
[5.15]

$$q^{(i)}(z) \propto \mathbf{p}(\boldsymbol{x}^{(i)} \mid z; \boldsymbol{\phi}) \times \mathbf{p}(z; \boldsymbol{\mu}).$$

$$[5.16]$$

Applying the sum-to-one constraint gives an exact solution,

$$q^{(i)}(z) = \frac{p(\boldsymbol{x}^{(i)} \mid z; \boldsymbol{\phi}) \times p(z; \boldsymbol{\mu})}{\sum_{z'=1}^{K} p(\boldsymbol{x}^{(i)} \mid z'; \boldsymbol{\phi}) \times p(z'; \boldsymbol{\mu})}$$
[5.17]

$$=\mathbf{p}(z \mid \boldsymbol{x}^{(i)}; \boldsymbol{\phi}, \boldsymbol{\mu}).$$
[5.18]

After normalizing, each  $q^{(i)}$  — which is the soft distribution over clusters for data  $x^{(i)}$  — is set to the posterior probability  $p(z | x^{(i)}; \phi, \mu)$  under the current parameters. Although the Lagrange multipliers  $\lambda^{(i)}$  were introduced as additional parameters, they drop out during normalization.

#### The M-step

Next, we hold fixed the soft assignments  $q^{(i)}$ , and **m**aximize with respect to the parameters,  $\phi$  and  $\mu$ . Let's focus on the parameter  $\phi$ , which parametrizes the likelihood  $p(x \mid z; \phi)$ , and leave  $\mu$  for an exercise. The parameter  $\phi$  is a distribution over words for each cluster, so it is optimized under the constraint that  $\sum_{j=1}^{V} \phi_{z,j} = 1$ . To incorporate this

#### 5.1. UNSUPERVISED LEARNING

constraint, we introduce a set of Lagrange multiplers  $\{\lambda_z\}_{z=1}^K$ , and from the Lagrangian,

$$J_{\phi} = \sum_{i=1}^{N} \sum_{z=1}^{K} q^{(i)}(z) \left( \log \mathbf{p}(\boldsymbol{x}^{(i)} \mid z; \boldsymbol{\phi}) + \log \mathbf{p}(z; \mu) - \log q^{(i)}(z) \right) + \sum_{z=1}^{K} \lambda_{z} (1 - \sum_{j=1}^{V} \phi_{z,j}).$$
[5.19]

The term  $\log p(x^{(i)} | z; \phi)$  is the conditional log-likelihood for the multinomial, which expands to,

$$\log p(\mathbf{x}^{(i)} \mid z, \phi) = C + \sum_{j=1}^{V} x_j \log \phi_{z,j},$$
[5.20]

where *C* is a constant with respect to  $\phi$  — see Equation 2.12 in § 2.2 for more discussion of this probability function.

Setting the derivative of  $J_{\phi}$  equal to zero,

$$\frac{\partial J_{\phi}}{\partial \phi_{z,j}} = \sum_{i=1}^{N} q^{(i)}(z) \times \frac{x_j^{(i)}}{\phi_{z,j}} - \lambda_z$$
[5.21]

$$\phi_{z,j} \propto \sum_{i=1}^{N} q^{(i)}(z) \times x_j^{(i)}.$$
 [5.22]

Because  $\phi_z$  is constrained to be a probability distribution, the exact solution is computed as,

$$\phi_{z,j} = \frac{\sum_{i=1}^{N} q^{(i)}(z) \times x_j^{(i)}}{\sum_{j'=1}^{V} \sum_{i=1}^{N} q^{(i)}(z) \times x_{j'}^{(i)}} = \frac{E_q \left[ \text{count}(z,j) \right]}{\sum_{j'=1}^{V} E_q \left[ \text{count}(z,j') \right]},$$
[5.23]

where the counter  $j \in \{1, 2, ..., V\}$  indexes over base features, such as words.

This update sets  $\phi_z$  equal to the relative frequency estimate of the *expected counts* under the distribution q. As in supervised Naïve Bayes, we can smooth these counts by adding a constant  $\alpha$ . The update for  $\mu$  is similar:  $\mu_z \propto \sum_{i=1}^N q^{(i)}(z) = E_q [\text{count}(z)]$ , which is the expected frequency of cluster z. These probabilities can also be smoothed. In sum, the M-step is just like Naïve Bayes, but with expected counts rather than observed counts.

The multinomial likelihood  $p(x \mid z)$  can be replaced with other probability distributions: for example, for continuous observations, a Gaussian distribution can be used. In some cases, there is no closed-form update to the parameters of the likelihood. One approach is to run gradient-based optimization at each M-step; another is to simply take a single step along the gradient step and then return to the E-step (Berg-Kirkpatrick et al., 2010).

<span id="page-119-3"></span><span id="page-119-1"></span>![](_page_119_Figure_1.jpeg)

Figure 5.2: Sensitivity of expectation-maximization to initialization. Each line shows the progress of optimization from a different random initialization.

#### <span id="page-119-0"></span>5.1.3 EM as an optimization algorithm

Algorithms that update a global objective by alternating between updates to subsets of the parameters are called **coordinate ascent** algorithms. The objective J (the lower bound on the marginal likelihood of the data) is separately convex in q and  $(\mu, \phi)$ , but it is not jointly convex in all terms; this condition is known as **biconvexity**. Each step of the expectation-maximization algorithm is guaranteed not to decrease the lower bound J, which means that EM will converge towards a solution at which no nearby points yield further improvements. This solution is a **local optimum** — it is as good or better than any of its immediate neighbors, but is *not* guaranteed to be optimal among all possible configurations of  $(q, \mu, \phi)$ .

The fact that there is no guarantee of global optimality means that initialization is important: where you start can determine where you finish. To illustrate this point, Figure 5.2 shows the objective function for EM with ten different random initializations: while the objective function improves monotonically in each run, it converges to several different values.<sup>1</sup> For the convex objectives that we encountered in chapter 2, it was not necessary to worry about initialization, because gradient-based optimization guaranteed to reach the global minimum. But in expectation-maximization — as in the deep neural networks from chapter 3 — initialization matters.

In hard EM, each  $q^{(i)}$  distribution assigns probability of 1 to a single label  $\hat{z}^{(i)}$ , and zero probability to all others (Neal and Hinton, 1998). This is similar in spirit to *K*-means clustering, and can outperform standard EM in some cases (Spitkovsky et al., 2010). Another variant of expectation-maximization incorporates stochastic gradient descent (SGD): after performing a local E-step at each instance  $x^{(i)}$ , we immediately make a gradient update to the parameters ( $\mu$ ,  $\phi$ ). This algorithm has been called **incremental expectation maximization** (Neal and Hinton, 1998) and **online expectation maximization** (Sato and Ishii,

<span id="page-119-2"></span><sup>&</sup>lt;sup>1</sup>The figure shows the upper bound on the *negative* log-likelihood, because optimization is typically framed as minimization rather than maximization.

<span id="page-120-1"></span>[2000;](#page-559-2) [Cappe and Moulines, 2009\)](#page-517-2), and is especially useful when there is no closed-form ´ optimum for the likelihood p(x | z), and in online settings where new data is constantly streamed in (see [Liang and Klein, 2009,](#page-544-1) for a comparison for online EM variants).

#### <span id="page-120-0"></span>**5.1.4 How many clusters?**

So far, we have assumed that the number of clusters K is given. In some cases, this assumption is valid. For example, a lexical semantic resource like WORDNET might define the number of senses for a word. In other cases, the number of clusters could be a parameter for the user to tune: some readers want a coarse-grained clustering of news stories into three or four clusters, while others want a fine-grained clustering into twenty or more. But many times there is little extrinsic guidance for how to choose K.

One solution is to choose the number of clusters to maximize a metric of clustering quality. The other parameters µ and φ are chosen to maximize the log-likelihood bound J, so this might seem a potential candidate for tuning K. However, J will never decrease with K: if it is possible to obtain a bound of J<sup>K</sup> with K clusters, then it is always possible to do at least as well with K + 1 clusters, by simply ignoring the additional cluster and setting its probability to zero in q and µ. It is therefore necessary to introduce a penalty for model complexity, so that fewer clusters are preferred. For example, the Akaike Information Crition (AIC; [Akaike, 1974\)](#page-510-6) is the linear combination of the number of parameters and the log-likelihood,

$$AIC = 2M - 2J, [5.24]$$

where M is the number of parameters. In an expectation-maximization clustering algorithm, M = K × V + K. Since the number of parameters increases with the number of clusters K, the AIC may prefer more parsimonious models, even if they do not fit the data quite as well.

Another choice is to maximize the **predictive likelihood** on heldout data. This data is not used to estimate the model parameters φ and µ, and so it is not the case that the likelihood on this data is guaranteed to increase with K. [Figure 5.3](#page-121-2) shows the negative log-likelihood on training and heldout data, as well as the AIC.

**\*Bayesian nonparametrics** An alternative approach is to treat the number of clusters as another latent variable. This requires statistical inference over a set of models with a variable number of clusters. This is not possible within the framework of expectationmaximization, but there are several alternative inference procedures which can be applied, including **Markov Chain Monte Carlo (MCMC)**, which is briefly discussed in § [5.5](#page-131-0) (for more details, see Chapter 25 of [Murphy, 2012\)](#page-551-1). Bayesian nonparametrics have been applied to the problem of unsupervised word sense induction, learning not only the word senses but also the number of senses per word [\(Reisinger and Mooney, 2010\)](#page-557-4).

<span id="page-121-3"></span><span id="page-121-2"></span>![](_page_121_Figure_1.jpeg)

Figure 5.3: The negative log-likelihood and AIC for several runs of expectationmaximization, on synthetic data. Although the data was generated from a model with K = 10, the optimal number of clusters is  $\hat{K} = 15$ , according to AIC and the heldout log-likelihood. The training set log-likelihood continues to improve as K increases.

# <span id="page-121-0"></span>5.2 Applications of expectation-maximization

EM is not really an "algorithm" like, say, quicksort. Rather, it is a framework for learning with missing data. The recipe for using EM on a problem of interest is:

- Introduce latent variables z, such that it is easy to write the probability P(x, z). It should also be easy to estimate the associated parameters, given knowledge of z.
- Derive the E-step updates for q(z), which is typically factored as  $q(z) = \prod_{i=1}^{N} q_{z^{(i)}}(z^{(i)})$ , where *i* is an index over instances.
- The M-step updates typically correspond to the soft version of a probabilistic supervised learning algorithm, like Naïve Bayes.

This section discusses a few of the many applications of this general framework.

#### <span id="page-121-1"></span>5.2.1 Word sense induction

The chapter began by considering the problem of word sense disambiguation when the senses are not known in advance. Expectation-maximization can be applied to this problem by treating each cluster as a word sense. Each instance represents the use of an ambiguous word, and  $x^{(i)}$  is a vector of counts for the other words that appear nearby: Schütze (1998) uses all words within a 50-word window. The probability  $p(x^{(i)} | z)$  can be set to the multinomial distribution, as in Naïve Bayes. The EM algorithm can be applied directly to this data, yielding clusters that (hopefully) correspond to the word senses.

Better performance can be obtained by first applying **singular value decomposition** (SVD) to the matrix of context-counts  $C_{ij} = count(i, j)$ , where count(i, j) is the count of word *j* in the context of instance *i*. **Truncated** singular value decomposition approximates

<span id="page-122-2"></span>the matrix C as a product of three matrices, U, S, V, under the constraint that U and V are orthonormal, and S is diagonal:

$$\min_{\mathbf{U},\mathbf{S},\mathbf{V}} \|\mathbf{C} - \mathbf{U}\mathbf{S}\mathbf{V}^{\top}\|_{F}$$

$$s.t.\mathbf{U} \in \mathbb{R}^{V \times K}, \mathbf{U}\mathbf{U}^{\top} = \mathbb{I}$$

$$\mathbf{S} = \text{Diag}(s_{1}, s_{2}, \dots, s_{K})$$

$$\mathbf{V}^{\top} \in \mathbb{R}^{N_{p} \times K}, \mathbf{V}\mathbf{V}^{\top} = \mathbb{I},$$

$$(5.25)$$

where  $|| \cdot ||_F$  is the **Frobenius norm**,  $||X||_F = \sqrt{\sum_{i,j} X_{i,j}^2}$ . The matrix U contains the left singular vectors of C, and the rows of this matrix can be used as low-dimensional representations of the count vectors  $c_i$ . EM clustering can be made more robust by setting the instance descriptions  $x^{(i)}$  equal to these rows, rather than using raw counts (Schütze, 1998). However, because the instances are now dense vectors of continuous numbers, the probability  $p(x^{(i)} | z)$  must be defined as a multivariate Gaussian distribution.

In truncated singular value decomposition, the hyperparameter K is the truncation limit: when K is equal to the rank of  $\mathbf{C}$ , the norm of the difference between the original matrix  $\mathbf{C}$  and its reconstruction  $\mathbf{USV}^{\top}$  will be zero. Lower values of K increase the reconstruction error, but yield vector representations that are smaller and easier to learn from. Singular value decomposition is discussed in more detail in chapter 14.

#### <span id="page-122-0"></span>5.2.2 Semi-supervised learning

Expectation-maximization can also be applied to the problem of **semi-supervised learning**: learning from both labeled and unlabeled data in a single model. Semi-supervised learning makes use of annotated examples, ensuring that each label y corresponds to the desired concept. By adding unlabeled examples, it is possible to cover a greater fraction of the features than would appear in labeled data alone. Other methods for semi-supervised learning are discussed in § 5.3, but for now, let's approach the problem within the framework of expectation-maximization (Nigam et al., 2000).

Suppose we have labeled data  $\{(x^{(i)}, y^{(i)})\}_{i=1}^{N_{\ell}}$ , and unlabeled data  $\{x^{(i)}\}_{i=N_{\ell}+1}^{N_{\ell}+N_{u}}$ , where  $N_{\ell}$  is the number of labeled instances and  $N_{u}$  is the number of unlabeled instances. We can learn from the combined data by maximizing a lower bound on the joint log-likelihood,

$$\mathcal{L} = \sum_{i=1}^{N_{\ell}} \log p(\boldsymbol{x}^{(i)}, y^{(i)}; \boldsymbol{\mu}, \boldsymbol{\phi}) + \sum_{i=N_{\ell}+1}^{N_{\ell}+N_{u}} \log p(\boldsymbol{x}^{(j)}; \boldsymbol{\mu}, \boldsymbol{\phi})$$
[5.26]

<span id="page-122-1"></span>
$$= \sum_{i=1}^{N_{\ell}} \left( \log p(\boldsymbol{x}^{(i)} \mid y^{(i)}; \boldsymbol{\phi}) + \log p(y^{(i)}; \boldsymbol{\mu}) \right) + \sum_{j=N_{\ell}+1}^{N_{\ell}+N_{u}} \log \sum_{y=1}^{K} p(\boldsymbol{x}^{(j)}, y; \boldsymbol{\mu}, \boldsymbol{\phi}).$$
 [5.27]

<span id="page-123-1"></span>

| Algorithm 9 Generative process for the Naïve Bayes classifier with hidden components |
|--------------------------------------------------------------------------------------|
|--------------------------------------------------------------------------------------|

for Instance  $i \in \{1, 2, ..., N\}$  do: Draw the label  $y^{(i)} \sim \text{Categorical}(\mu)$ ; Draw the component  $z^{(i)} \sim \text{Categorical}(\beta_{y^{(i)}})$ ; Draw the word counts  $x^{(i)} \mid y^{(i)}, z^{(i)} \sim \text{Multinomial}(\phi_{z^{(i)}})$ .

The left sum is identical to the objective in Naïve Bayes; the right sum is the marginal loglikelihood for expectation-maximization clustering, from Equation 5.5. We can construct a lower bound on this log-likelihood by introducing distributions  $q^{(j)}$  for all  $j \in \{N_{\ell} + 1, ..., N_{\ell} + N_u\}$ . The E-step updates these distributions; the M-step updates the parameters  $\phi$  and  $\mu$ , using the expected counts from the unlabeled data and the observed counts from the labeled data.

A critical issue in semi-supervised learning is how to balance the impact of the labeled and unlabeled data on the classifier weights, especially when the unlabeled data is much larger than the labeled dataset. The risk is that the unlabeled data will dominate, causing the parameters to drift towards a "natural clustering" of the instances — which may not correspond to a good classifier for the labeled data. One solution is to heuristically reweight the two components of Equation 5.26, tuning the weight of the two components on a heldout development set (Nigam et al., 2000).

#### <span id="page-123-0"></span>5.2.3 Multi-component modeling

As a final application, let's return to fully supervised classification. A classic dataset for text classification is 20 newsgroups, which contains posts to a set of online forums, called newsgroups. One of the newsgroups is comp.sys.mac.hardware, which discusses Apple computing hardware. Suppose that within this newsgroup there are two kinds of posts: reviews of new hardware, and question-answer posts about hardware problems. The language in these *components* of the mac.hardware class might have little in common; if so, it would be better to model these components separately, rather than treating their union as a single class. However, the component responsible for each instance is not directly observed.

Recall that Naïve Bayes is based on a generative process, which provides a stochastic explanation for the observed data. In Naïve Bayes, each label is drawn from a categorical distribution with parameter  $\mu$ , and each vector of word counts is drawn from a multinomial distribution with parameter  $\phi_y$ . For multi-component modeling, we envision a slightly different generative process, incorporating both the observed label  $y^{(i)}$  and the latent component  $z^{(i)}$ . This generative process is shown in Algorithm 9. A new parameter  $\beta_{y^{(i)}}$  defines the distribution of components, conditioned on the label  $y^{(i)}$ . The component, and not the class label, then parametrizes the distribution over words.

- <span id="page-124-1"></span>(5.1) © Villeneuve a bel et bien **réussi** son pari de changer de perspectives tout en assurant une cohérence à la franchise.<sup>2</sup>
- (5.2) © Il est également trop **long** et bancal dans sa narration, tiède dans ses intentions, et tiraillé entre deux personnages et directions qui ne parviennent pas à coexister en harmonie.<sup>3</sup>
- <span id="page-124-2"></span>(5.3) Denis Villeneuve a réussi une suite parfaitement maitrisée<sup>4</sup>
- <span id="page-124-4"></span>(5.4) **Long, bavard**, hyper design, à peine agité (le comble de l'action : une bagarre dans la flotte), métaphysique et, surtout, ennuyeux jusqu'à la catalepsie.<sup>5</sup>
- <span id="page-124-3"></span>(5.5) Une suite d'une écrasante puissance, mêlant **parfaitement** le contemplatif au narratif.<sup>6</sup>
- <span id="page-124-5"></span>(5.6) Le film impitoyablement bavard finit quand même par se taire quand se lève l'espèce de bouquet final où semble se déchaîner, comme en libre parcours de poulets décapités, l'armée des graphistes numériques griffant nerveusement la palette graphique entre agonie et orgasme.<sup>7</sup>

Table 5.1: Labeled and unlabeled reviews of the films *Blade Runner* 2049 and *Transformers: The Last Knight*.

The labeled data includes  $(x^{(i)}, y^{(i)})$ , but not  $z^{(i)}$ , so this is another case of missing data. Again, we sum over the missing data, applying Jensen's inequality to as to obtain a lower bound on the log-likelihood,

$$\log p(\boldsymbol{x}^{(i)}, y^{(i)}) = \log \sum_{z=1}^{K_z} p(\boldsymbol{x}^{(i)}, y^{(i)}, z; \boldsymbol{\mu}, \boldsymbol{\phi}, \boldsymbol{\beta})$$

$$\geq \log p(y^{(i)}; \boldsymbol{\mu}) + E_{q_{Z|Y}^{(i)}} [\log p(\boldsymbol{x}^{(i)} \mid z; \boldsymbol{\phi}) + \log p(z \mid y^{(i)}; \boldsymbol{\beta}) - \log q^{(i)}(z)].$$
[5.29]

We are now ready to apply expectation-maximization. As usual, the E-step updates the distribution over the missing data,  $q_{Z|Y}^{(i)}$ . The M-step updates the parameters,

$$\beta_{y,z} = \frac{E_q \left[ \text{count}(y, z) \right]}{\sum_{z'=1}^{K_z} E_q \left[ \text{count}(y, z') \right]}$$
[5.30]

$$\phi_{z,j} = \frac{E_q \left[ \text{count}(z, j) \right]}{\sum_{j'=1}^{V} E_q \left[ \text{count}(z, j') \right]}.$$
[5.31]

# <span id="page-124-0"></span>5.3 Semi-supervised learning

In semi-supervised learning, the learner makes use of both labeled and unlabeled data. To see how this could help, suppose you want to do sentiment analysis in French. In Ta-

<span id="page-125-1"></span>[ble 5.1,](#page-124-1) there are two labeled examples, one positive and one negative. From this data, a learner could conclude that *r´eussi* is positive and *long* is negative. This isn't much! However, we can propagate this information to the unlabeled data, and potentially learn more.

- If we are confident that *r´eussi* is positive, then we might guess that [\(5.3\)](#page-124-2) is also positive.
- That suggests that *parfaitement* is also positive.
- We can then propagate this information to [\(5.5\)](#page-124-3), and learn from the words in this example.
- Similarly, we can propagate from the labeled data to [\(5.4\)](#page-124-4), which we guess to be negative because it shares the word *long*. This suggests that *bavard* is also negative, which we propagate to [\(5.6\)](#page-124-5).

Instances [\(5.3\)](#page-124-2) and [\(5.4\)](#page-124-4) were "similar" to the labeled examples for positivity and negativity, respectively. By using these instances to expand the models for each class, it became possible to correctly label instances [\(5.5\)](#page-124-3) and [\(5.6\)](#page-124-5), which didn't share any important features with the original labeled data. This requires a key assumption: that similar instances will have similar labels.

In § [5.2.2,](#page-122-0) we discussed how expectation-maximization can be applied to semi-supervised learning. Using the labeled data, the initial parameters φ would assign a high weight for *r´eussi* in the positive class, and a high weight for *long* in the negative class. These weights helped to shape the distributions q for instances [\(5.3\)](#page-124-2) and [\(5.4\)](#page-124-4) in the E-step. In the next iteration of the M-step, the parameters φ are updated with counts from these instances, making it possible to correctly label the instances [\(5.5\)](#page-124-3) and [\(5.6\)](#page-124-5).

However, expectation-maximization has an important disadvantage: it requires using a generative classification model, which restricts the features that can be used for classification. In this section, we explore non-probabilistic approaches, which impose fewer restrictions on the classification model.

#### <span id="page-125-0"></span>**5.3.1 Multi-view learning**

EM semi-supervised learning can be viewed as **self-training**: the labeled data guides the initial estimates of the classification parameters; these parameters are used to compute a label distribution over the unlabeled instances, q (i) ; the label distributions are used to update the parameters. The risk is that self-training drifts away from the original labeled data. This problem can be ameliorated by **multi-view learning**. Here we take the assumption that the features can be decomposed into multiple "views", each of which is conditionally independent, given the label. For example, consider the problem of classifying a name as a person or location: one view is the name itself; another is the context in which it appears. This situation is illustrated in [Table 5.2.](#page-126-1)

<span id="page-126-1"></span>

|    | $oldsymbol{x}^{(1)}$ | $oldsymbol{x}^{(2)}$ | y                          |
|----|----------------------|----------------------|----------------------------|
| 1. | Peachtree Street     | located on           | LOC                        |
| 2. | Dr. Walker           | said                 | PER                        |
| 3. | Zanzibar             | located in           | $? \rightarrow \text{LOC}$ |
| 4. | Zanzibar             | flew to              | $? \rightarrow \text{LOC}$ |
| 5. | Dr. Robert           | recommended          | $? \rightarrow PER$        |
| 6. | Oprah                | recommended          | $? \rightarrow PER$        |

Table 5.2: Example of multiview learning for named entity classification

**Co-training** is an iterative multi-view learning algorithm, in which there are separate classifiers for each view (Blum and Mitchell, 1998). At each iteration of the algorithm, each classifier predicts labels for a subset of the unlabeled instances, using only the features available in its view. These predictions are then used as ground truth to train the classifiers associated with the other views. In the example shown in Table 5.2, the classifier on  $x^{(1)}$  might correctly label instance #5 as a person, because of the feature Dr; this instance would then serve as training data for the classifier on  $x^{(2)}$ , which would then be able to correctly label instance #6, thanks to the feature *recommended*. If the views are truly independent, this procedure is robust to drift. Furthermore, it imposes no restrictions on the classifiers that can be used for each view.

Word-sense disambiguation is particularly suited to multi-view learning, thanks to the heuristic of "one sense per discourse": if a polysemous word is used more than once in a given text or conversation, all usages refer to the same sense (Gale et al., 1992). This motivates a multi-view learning approach, in which one view corresponds to the local context (the surrounding words), and another view corresponds to the global context at the document level (Yarowsky, 1995). The local context view is first trained on a small seed dataset. We then identify its most confident predictions on unlabeled instances. The global context view is then used to extend these confident predictions to other instances within the same documents. These new instances are added to the training data to the local context classifier, which is retrained and then applied to the remaining unlabeled data.

#### <span id="page-126-0"></span>5.3.2 Graph-based algorithms

Another family of approaches to semi-supervised learning begins by constructing a graph, in which pairs of instances are linked with symmetric weights  $\omega_{i,j}$ , e.g.,

<span id="page-126-2"></span>
$$\omega_{i,j} = \exp(-\alpha \times || \boldsymbol{x}^{(i)} - \boldsymbol{x}^{(j)} ||^2).$$
[5.32]

The goal is to use this weighted graph to propagate labels from a small set of labeled instances to larger set of unlabeled instances.

<span id="page-127-3"></span>In **label propagation**, this is done through a series of matrix operations (Zhu et al., 2003). Let **Q** be a matrix of size  $N \times K$ , in which each row  $q^{(i)}$  describes the labeling of instance *i*. When ground truth labels are available, then  $q^{(i)}$  is an indicator vector, with  $q_{y^{(i)}}^{(i)} = 1$  and  $q_{y'\neq y^{(i)}}^{(i)} = 0$ . Let us refer to the submatrix of rows containing labeled instances as  $\mathbf{Q}_L$ , and the remaining rows as  $\mathbf{Q}_U$ . The rows of  $\mathbf{Q}_U$  are initialized to assign equal probabilities to all labels,  $q_{i,k} = \frac{1}{K}$ .

Now, let  $T_{i,j}$  represent the "transition" probability of moving from node *j* to node *i*,

$$T_{i,j} \triangleq \Pr(j \to i) = \frac{\omega_{i,j}}{\sum_{k=1}^{N} \omega_{k,j}}.$$
[5.33]

We compute values of  $T_{i,j}$  for all instances j and all *unlabeled* instances i, forming a matrix of size  $N_U \times N$ . If the dataset is large, this matrix may be expensive to store and manipulate; a solution is to sparsify it, by keeping only the  $\kappa$  largest values in each row, and setting all other values to zero. We can then "propagate" the label distributions to the unlabeled instances,

$$\mathbf{\hat{Q}}_U \leftarrow \mathbf{T}\mathbf{Q}$$
 [5.34]

<span id="page-127-2"></span><span id="page-127-1"></span>
$$s \leftarrow \mathbf{\hat{Q}}_U \mathbf{1}$$
 [5.35]

$$\mathbf{Q}_U \leftarrow \mathrm{Diag}(s)^{-1} \mathbf{\tilde{Q}}_U.$$
 [5.36]

The expression  $\tilde{\mathbf{Q}}_U \mathbf{1}$  indicates multiplication of  $\tilde{\mathbf{Q}}_U$  by a column vector of ones, which is equivalent to computing the sum of each row of  $\tilde{\mathbf{Q}}_U$ . The matrix Diag(s) is a diagonal matrix with the elements of s on the diagonals. The product  $\text{Diag}(s)^{-1}\tilde{\mathbf{Q}}_U$  has the effect of normalizing the rows of  $\tilde{\mathbf{Q}}_U$ , so that each row of  $\mathbf{Q}_U$  is a probability distribution over labels.

#### <span id="page-127-0"></span>5.4 Domain adaptation

In many practical scenarios, the labeled data differs in some key respect from the data to which the trained model is to be applied. A classic example is in consumer reviews: we may have labeled reviews of movies (the source domain), but we want to predict the reviews of appliances (the target domain). A similar issue arises with genre differences: most linguistically-annotated data is news text, but application domains range from social media to electronic health records. In general, there may be several source and target domains, each with their own properties; however, for simplicity, this discussion will focus mainly on the case of a single source and target domain.

The simplest approach is "direct transfer": train a classifier on the source domain, and apply it directly to the target domain. The accuracy of this approach depends on the extent to which features are shared across domains. In review text, words like *outstanding* and

<span id="page-128-1"></span>*disappointing* will apply across both movies and appliances; but others, like *terrifying*, may have meanings that are domain-specific. As a result, direct transfer performs poorly: for example, an out-of-domain classifier (trained on book reviews) suffers twice the error rate of an in-domain classifier on reviews of kitchen appliances [\(Blitzer et al., 2007\)](#page-514-7). **Domain adaptation** algorithms attempt to do better than direct transfer by learning from data in both domains. There are two main families of domain adaptation algorithms, depending on whether any labeled data is available in the target domain.

#### <span id="page-128-0"></span>**5.4.1 Supervised domain adaptation**

In supervised domain adaptation, there is a small amount of labeled data in the target domain, and a large amount of data in the source domain. The simplest approach would be to ignore domain differences, and simply merge the training data from the source and target domains. There are several other baseline approaches to dealing with this scenario [\(Daume III, 2007\)](#page-523-2): ´

**Interpolation.** Train a classifier for each domain, and combine their predictions, e.g.,

$$\hat{y} = \operatorname*{argmax}_{y} \lambda_s \Psi_s(\boldsymbol{x}, y) + (1 - \lambda_s) \Psi_t(\boldsymbol{x}, y),$$

$$[5.37]$$

where Ψ<sup>s</sup> and Ψ<sup>t</sup> are the scoring functions from the source and target domain classifiers respectively, and λ<sup>s</sup> is the interpolation weight.

**Prediction.** Train a classifier on the source domain data, use its prediction as an additional feature in a classifier trained on the target domain data,

$$\hat{y}_s = \underset{y}{\operatorname{argmax}} \Psi_s(\boldsymbol{x}, y)$$
[5.38]

$$\hat{y}_t = \underset{y}{\operatorname{argmax}} \Psi_t([\boldsymbol{x}; \hat{y}_S], y).$$
[5.39]

**Priors.** Train a classifier on the source domain data, and use its weights as a prior distribution on the weights of the classifier for the target domain data. This is equivalent to regularizing the target domain weights towards the weights of the source domain classifier [\(Chelba and Acero, 2006\)](#page-519-1),

$$\ell(\boldsymbol{\theta}_t) = \sum_{i=1}^N \ell^{(i)}(\boldsymbol{x}^{(i)}, y^{(i)}; \boldsymbol{\theta}_t) + \lambda ||\boldsymbol{\theta}_t - \boldsymbol{\theta}_s||_2^2,$$
 [5.40]

where ` (i) is the prediction loss on instance i, and λ is the regularization weight.

<span id="page-129-3"></span>An effective and "frustratingly simple" alternative is EASYADAPT (Daumé III, 2007), which creates copies of each feature: one for each domain and one for the cross-domain setting. For example, a negative review of the film *Wonder Woman* begins, *As boring and flavorless as a three-day-old grilled cheese sandwich....*<sup>8</sup> The resulting bag-of-words feature vector would be,

$$\begin{split} \boldsymbol{f}(\boldsymbol{x}, y, d) &= \{(\textit{boring}, \textcircled{\odot}, \textit{MOVIE}) : 1, (\textit{boring}, \textcircled{\odot}, *) : 1, \\ &\quad (\textit{flavorless}, \textcircled{\odot}, \textit{MOVIE}) : 1, (\textit{flavorless}, \textcircled{\odot}, *) : 1, \\ &\quad (\textit{three-day-old}, \textcircled{\odot}, \textit{MOVIE}) : 1, (\textit{three-day-old}, \textcircled{\odot}, *) : 1, \\ &\quad \dots\}, \end{split}$$

with (*boring*, o, MOVIE) indicating the word *boring* appearing in a negative labeled document in the MOVIE domain, and (*boring*, o, \*) indicating the same word in a negative labeled document in *any* domain. It is up to the learner to allocate weight between the domain-specific and cross-domain features: for words that facilitate prediction in both domains, the learner will use the cross-domain features; for words that are relevant only to a single domain, the domain-specific features will be used. Any discriminative classifier can be used with these augmented features.<sup>9</sup>

#### <span id="page-129-0"></span>5.4.2 Unsupervised domain adaptation

In unsupervised domain adaptation, there is no labeled data in the target domain. Unsupervised domain adaptation algorithms cope with this problem by trying to make the data from the source and target domains as similar as possible. This is typically done by learning a **projection function**, which puts the source and target data in a shared space, in which a learner can generalize across domains. This projection is learned from data in both domains, and is applied to the base features — for example, the bag-of-words in text classification. The projected features can then be used both for training and for prediction.

#### Linear projection

In linear projection, the cross-domain representation is constructed by a matrix-vector product,

$$g(x^{(i)}) = \mathbf{U}x^{(i)}.$$
 [5.41]

The projected vectors  $g(x^{(i)})$  can then be used as base features during both training (from the source domain) and prediction (on the target domain).

<span id="page-129-1"></span><sup>&</sup>lt;sup>8</sup>http://www.colesmithey.com/capsules/2017/06/wonder-woman.HTML, accessed October 9. 2017.

<span id="page-129-2"></span><sup>&</sup>lt;sup>9</sup>EASYADAPT can be explained as a hierarchical Bayesian model, in which the weights for each domain are drawn from a shared prior (Finkel and Manning, 2009).

#### <span id="page-130-0"></span>5.4. DOMAIN ADAPTATION

The projection matrix **U** can be learned in a number of different ways, but many approaches focus on compressing and reconstructing the base features (Ando and Zhang, 2005). For example, we can define a set of **pivot features**, which are typically chosen because they appear in both domains: in the case of review documents, pivot features might include evaluative adjectives like *outstanding* and *disappointing* (Blitzer et al., 2007). For each pivot feature *j*, we define an auxiliary problem of predicting whether the feature is present in each example, using the remaining base features. Let  $\phi_j$  denote the weights of this classifier, and us horizontally concatenate the weights for each of the  $N_p$  pivot features into a matrix  $\mathbf{\Phi} = [\phi_1, \phi_2, \dots, \phi_{N_p}]$ .

We then perform truncated singular value decomposition on  $\Phi$ , as described in § 5.2.1, obtaining  $\Phi \approx \mathbf{USV}^{\top}$ . The rows of the matrix  $\mathbf{U}$  summarize information about each base feature: indeed, the truncated singular value decomposition identifies a low-dimension basis for the weight matrix  $\Phi$ , which in turn links base features to pivot features. Suppose that a base feature *reliable* occurs only in the target domain of appliance reviews. Nonetheless, it will have a positive weight towards some pivot features (e.g., *outstanding, recommended*), and a negative weight towards others (e.g., *worthless, unpleasant*). A base feature such as *watchable* might have the same associations with the pivot features, and therefore,  $u_{reliable} \approx u_{watchable}$ . The matrix  $\mathbf{U}$  can thus project the base features into a space in which this information is shared.

#### Non-linear projection

Non-linear transformations of the base features can be accomplished by implementing the transformation function as a deep neural network, which is trained from an auxiliary objective.

**Denoising objectives** One possibility is to train a projection function to reconstruct a corrupted version of the original input. The original input can be corrupted in various ways: by the addition of random noise (Glorot et al., 2011; Chen et al., 2012), or by the deletion of features (Chen et al., 2012; Yang and Eisenstein, 2015). Denoising objectives share many properties of the linear projection method described above: they enable the projection function to be trained on large amounts of unlabeled data from the target domain, and allow information to be shared across the feature space, thereby reducing sensitivity to rare and domain-specific features.

**Adversarial objectives** The ultimate goal is for the transformed representations  $g(x^{(i)})$  to be domain-general. This can be made an explicit optimization criterion by computing the similarity of transformed instances both within and between domains (Tzeng et al., 2015), or by formulating an auxiliary classification task, in which the domain itself is treated as a label (Ganin et al., 2016). This setting is **adversarial**, because we want

<span id="page-131-1"></span>![](_page_131_Figure_1.jpeg)

Figure 5.4: A schematic view of adversarial domain adaptation. The loss  $\ell_y$  is computed only for instances from the source domain, where labels  $y^{(i)}$  are available.

to learn a representation that makes this classifier perform poorly. At the same time, we want  $g(x^{(i)})$  to enable accurate predictions of the labels  $y^{(i)}$ .

To formalize this idea, let  $d^{(i)}$  represent the domain of instance *i*, and let  $\ell_d(g(x^{(i)}), d^{(i)}; \theta_d)$ represent the loss of a classifier (typically a deep neural network) trained to predict  $d^{(i)}$ from the transformed representation  $g(x^{(i)})$ , using parameters  $\theta_d$ . Analogously, let  $\ell_y(g(x^{(i)}), y^{(i)}; \theta_y)$ represent the loss of a classifier trained to predict the label  $y^{(i)}$  from  $g(x^{(i)})$ , using parameters  $\theta_y$ . The transformation g can then be trained from two criteria: it should yield accurate predictions of the labels  $y^{(i)}$ , while making *inaccurate* predictions of the domains  $d^{(i)}$ . This can be formulated as a joint optimization problem,

$$\min_{\boldsymbol{\theta}_{g}\boldsymbol{\theta}_{y},\boldsymbol{\theta}_{d}} \sum_{i=1}^{N_{\ell}+N_{u}} \ell_{d}(\boldsymbol{g}(\boldsymbol{x}^{(i)};\boldsymbol{\theta}_{g}), d^{(i)};\boldsymbol{\theta}_{d}) - \sum_{i=1}^{N_{\ell}} \ell_{y}(\boldsymbol{g}(\boldsymbol{x}^{(i)};\boldsymbol{\theta}_{g}), y^{(i)};\boldsymbol{\theta}_{y}),$$
 [5.42]

where  $N_{\ell}$  is the number of labeled instances and  $N_u$  is the number of unlabeled instances, with the labeled instances appearing first in the dataset. This setup is shown in Figure 5.4. The loss can be optimized by stochastic gradient descent, jointly training the parameters of the non-linear transformation  $\theta_g$ , and the parameters of the prediction models  $\theta_d$  and  $\theta_y$ .

# <span id="page-131-0"></span>5.5 \*Other approaches to learning with latent variables

Expectation-maximization provides a general approach to learning with latent variables, but it has limitations. One is the sensitivity to initialization; in practical applications, considerable attention may need to be devoted to finding a good initialization. A second issue is that EM tends to be easiest to apply in cases where the latent variables have a clear decomposition (in the cases we have considered, they decompose across the instances). For these reasons, it is worth briefly considering some alternatives to EM.

#### <span id="page-132-2"></span><span id="page-132-0"></span>5.5.1 Sampling

In EM clustering, there is a distribution  $q^{(i)}$  for the missing data related to each instance. The M-step consists of updating the parameters of this distribution. An alternative is to draw samples of the latent variables. If the sampling distribution is designed correctly, this procedure will eventually converge to drawing samples from the true posterior over the missing data,  $p(z^{(1:N_z)} | x^{(1:N_x)})$ . For example, in the case of clustering, the missing data  $z^{(1:N_z)}$  is the set of cluster memberships,  $y^{(1:N)}$ , so we draw samples from the posterior distribution over clusterings of the data. If a single clustering is required, we can select the one with the highest conditional likelihood,  $\hat{z} = \operatorname{argmax}_{z} p(z^{(1:N_z)} | x^{(1:N_x)})$ .

This general family of algorithms is called **Markov Chain Monte Carlo (MCMC)**: "Monte Carlo" because it is based on a series of random draws; "Markov Chain" because the sampling procedure must be designed such that each sample depends only on the previous sample, and not on the entire sampling history. **Gibbs sampling** is an MCMC algorithm in which each latent variable is sampled from its posterior distribution,

$$z^{(n)} \mid \boldsymbol{x}, \boldsymbol{z}^{(-n)} \sim p(z^{(n)} \mid \boldsymbol{x}, \boldsymbol{z}^{(-n)}),$$
 [5.43]

where  $z^{(-n)}$  indicates  $\{z \setminus z^{(n)}\}$ , the set of all latent variables except for  $z^{(n)}$ . Repeatedly drawing samples over all latent variables constructs a Markov chain that is guaranteed to converge to a sequence of samples from  $p(z^{(1:N_z)} \mid x^{(1:N_x)})$ . In probabilistic clustering, the sampling distribution has the following form,

$$p(z^{(i)} \mid \boldsymbol{x}, \boldsymbol{z}^{(-i)}) = \frac{p(\boldsymbol{x}^{(i)} \mid z^{(i)}; \boldsymbol{\phi}) \times p(z^{(i)}; \boldsymbol{\mu})}{\sum_{z=1}^{K} p(\boldsymbol{x}^{(i)} \mid z; \boldsymbol{\phi}) \times p(z; \boldsymbol{\mu})}$$
[5.44]

$$\propto$$
Multinomial $(\boldsymbol{x}^{(i)}; \boldsymbol{\phi}_{z^{(i)}}) \times \boldsymbol{\mu}_{z^{(i)}}.$  [5.45]

In this case, the sampling distribution does not depend on the other instances: the posterior distribution over each  $z^{(i)}$  can be computed from  $x^{(i)}$  and the parameters given the parameters  $\phi$  and  $\mu$ .

In sampling algorithms, there are several choices for how to deal with the parameters. One possibility is to sample them too. To do this, we must add them to the generative story, by introducing a prior distribution. For the multinomial and categorical parameters in the EM clustering model, the **Dirichlet distribution** is a typical choice, since it defines a probability on exactly the set of vectors that can be parameters: vectors that sum to one and include only non-negative numbers.<sup>10</sup>

To incorporate this prior, the generative model must be augmented to indicate that each  $\phi_z \sim \text{Dirichlet}(\alpha_{\phi})$ , and  $\mu \sim \text{Dirichlet}(\alpha_{\mu})$ . The hyperparameters  $\alpha$  are typically set to a constant vector  $\alpha = [\alpha, \alpha, ..., \alpha]$ . When  $\alpha$  is large, the Dirichlet distribution tends to

<span id="page-132-1"></span><sup>&</sup>lt;sup>10</sup>If  $\sum_{i}^{K} \theta_{i} = 1$  and  $\theta_{i} \ge 0$  for all *i*, then  $\theta$  is said to be on the K - 1 simplex. A Dirichlet distribution with

<span id="page-133-0"></span>generate vectors that are nearly uniform; when  $\alpha$  is small, it tends to generate vectors that assign most of their probability mass to a few entries. Given prior distributions over  $\phi$  and  $\mu$ , we can now include them in Gibbs sampling, drawing values for these parameters from posterior distributions that are conditioned on the other variables in the model.

Unfortunately, sampling  $\phi$  and  $\mu$  usually leads to slow "mixing", meaning that adjacent samples tend to be similar, so that a large number of samples is required to explore the space of random variables. The reason is that the sampling distributions for the parameters are tightly constrained by the cluster memberships  $y^{(i)}$ , which in turn are tightly constrained by the parameters. There are two solutions that are frequently employed:

- Empirical Bayesian methods maintain  $\phi$  and  $\mu$  as parameters rather than latent variables. They still employ sampling in the E-step of the EM algorithm, but they update the parameters using expected counts that are computed from the samples rather than from parametric distributions. This EM-MCMC hybrid is also known as Monte Carlo Expectation Maximization (MCEM; Wei and Tanner, 1990), and is well-suited for cases in which it is difficult to compute  $q^{(i)}$  directly.
- In collapsed Gibbs sampling, we analytically integrate φ and μ out of the model. The cluster memberships y<sup>(i)</sup> are the only remaining latent variable; we sample them from the compound distribution,

$$p(y^{(i)} \mid \boldsymbol{x}^{(1:N)}, \boldsymbol{y}^{(-i)}; \alpha_{\phi}, \alpha_{\mu}) = \int_{\boldsymbol{\phi}, \boldsymbol{\mu}} p(\boldsymbol{\phi}, \boldsymbol{\mu} \mid \boldsymbol{y}^{(-i)}, \boldsymbol{x}^{(1:N)}; \alpha_{\phi}, \alpha_{\mu}) p(y^{(i)} \mid \boldsymbol{x}^{(1:N)}, \boldsymbol{y}^{(-i)}, \boldsymbol{\phi}, \boldsymbol{\mu}) d\boldsymbol{\phi} d\boldsymbol{\mu}$$
[5.48]

For multinomial and Dirichlet distributions, this integral can be computed in closed form.

MCMC algorithms are guaranteed to converge to the true posterior distribution over the latent variables, but there is no way to know how long this will take. In practice, the rate of convergence depends on initialization, just as expectation-maximization depends on initialization to avoid local optima. Thus, while Gibbs Sampling and other MCMC algorithms provide a powerful and flexible array of techniques for statistical inference in latent variable models, they are not a panacea for the problems experienced by EM.

parameter  $\boldsymbol{\alpha} \in \mathbb{R}^{K}_{+}$  has support over the K-1 simplex,

$$p_{\text{Dirichlet}}(\boldsymbol{\theta} \mid \boldsymbol{\alpha}) = \frac{1}{B(\boldsymbol{\alpha})} \prod_{i=1}^{K} \theta_i^{\alpha_i - 1}$$
[5.46]

$$B(\boldsymbol{\alpha}) = \frac{\prod_{i=1}^{K} \Gamma(\alpha_i)}{\Gamma(\sum_{i=1}^{K} \alpha_i)},$$
[5.47]

with  $\Gamma(\cdot)$  indicating the gamma function, a generalization of the factorial function to non-negative reals.

#### <span id="page-134-1"></span><span id="page-134-0"></span>**5.5.2 Spectral learning**

Another approach to learning with latent variables is based on the **method of moments**, which makes it possible to avoid the problem of non-convex log-likelihood. Write x (i) for the normalized vector of word counts in document i, so that x (i) = x (i)/ P<sup>V</sup> <sup>j</sup>=1 x (i) j . Then we can form a matrix of word-word co-occurrence probabilities,

$$\mathbf{C} = \sum_{i=1}^{N} \overline{\boldsymbol{x}}^{(i)} (\overline{\boldsymbol{x}}^{(i)})^{\top}.$$
[5.49]

The expected value of this matrix under p(x | φ, µ), as

$$E[\mathbf{C}] = \sum_{i=1}^{N} \sum_{k=1}^{K} \Pr(Z^{(i)} = k; \boldsymbol{\mu}) \boldsymbol{\phi}_k \boldsymbol{\phi}_k^{\top}$$
[5.50]

$$=\sum_{k}^{K} N \mu_{k} \boldsymbol{\phi}_{k} \boldsymbol{\phi}_{k}^{\top}$$
[5.51]

$$=\Phi \operatorname{Diag}(N\mu)\Phi^{\top},$$
 [5.52]

where Φ is formed by horizontally concatenating φ<sup>1</sup> . . . φK, and Diag(Nµ) indicates a diagonal matrix with values Nµ<sup>k</sup> at position (k, k). Setting C equal to its expectation gives,

$$\mathbf{C} = \mathbf{\Phi} \mathrm{Diag}(N\mu) \mathbf{\Phi}^{\top}, \qquad [5.53]$$

which is similar to the eigendecomposition C = QΛQ>. This suggests that simply by finding the eigenvectors and eigenvalues of C, we could obtain the parameters φ and µ, and this is what motivates the name **spectral learning**.

While moment-matching and eigendecomposition are similar in form, they impose different constraints on the solutions: eigendecomposition requires orthonormality, so that QQ<sup>&</sup>gt; = I; in estimating the parameters of a text clustering model, we require that µ and the columns of Φ are probability vectors. Spectral learning algorithms must therefore include a procedure for converting the solution into vectors that are non-negative and sum to one. One approach is to replace eigendecomposition (or the related singular value decomposition) with non-negative matrix factorization [\(Xu et al., 2003\)](#page-570-3), which guarantees that the solutions are non-negative [\(Arora et al., 2013\)](#page-511-4).

After obtaining the parameters φ and µ, the distribution over clusters can be computed from Bayes' rule:

$$\mathbf{p}(z^{(i)} \mid \boldsymbol{x}^{(i)}; \boldsymbol{\phi}, \boldsymbol{\mu}) \propto \mathbf{p}(\boldsymbol{x}^{(i)} \mid z^{(i)}; \boldsymbol{\phi}) \times \mathbf{p}(z^{(i)}; \boldsymbol{\mu}).$$
[5.54]

<span id="page-135-0"></span>Spectral learning yields provably good solutions without regard to initialization, and can be quite fast in practice. However, it is more difficult to apply to a broad family of generative models than EM and Gibbs Sampling. For more on applying spectral learning across a range of latent variable models, see [Anandkumar et al.](#page-511-5) [\(2014\)](#page-511-5).

# **Additional resources**

There are a number of other learning paradigms that deviate from supervised learning.

- **Active learning**: the learner selects unlabeled instances and requests annotations [\(Set](#page-560-4)[tles, 2012\)](#page-560-4).
- **Multiple instance learning**: labels are applied to bags of instances, with a positive label applied if at least one instance in the bag meets the criterion [\(Dietterich et al.,](#page-524-4) [1997;](#page-524-4) [Maron and Lozano-Perez, 1998\)](#page-547-1). ´
- **Constraint-driven learning**: supervision is provided in the form of explicit constraints on the learner [\(Chang et al., 2007;](#page-518-1) [Ganchev et al., 2010\)](#page-529-6).
- **Distant supervision**: noisy labels are generated from an external resource [\(Mintz](#page-549-3) [et al., 2009,](#page-549-3) also see § [17.2.3\)](#page-433-0).
- **Multitask learning**: the learner induces a representation that can be used to solve multiple classification tasks [\(Collobert et al., 2011\)](#page-522-1).
- **Transfer learning**: the learner must solve a classification task that differs from the labeled data [\(Pan and Yang, 2010\)](#page-553-6).

Expectation-maximization was introduced by [Dempster et al.](#page-524-5) [\(1977\)](#page-524-5), and is discussed in more detail by [Murphy](#page-551-1) [\(2012\)](#page-551-1). Like most machine learning treatments, Murphy focuses on continuous observations and Gaussian likelihoods, rather than the discrete observations typically encountered in natural language processing. [Murphy](#page-551-1) [\(2012\)](#page-551-1) also includes an excellent chapter on MCMC; for a textbook-length treatment, see [Robert and Casella](#page-558-2) [\(2013\)](#page-558-2). For still more on Bayesian latent variable models, see [Barber](#page-512-3) [\(2012\)](#page-512-3), and for applications of Bayesian models to natural language processing, see [Cohen](#page-521-2) [\(2016\)](#page-521-2). Surveys are available for semi-supervised learning [\(Zhu and Goldberg, 2009\)](#page-573-1) and domain adaptation [\(Søgaard, 2013\)](#page-562-4), although both pre-date the current wave of interest in deep learning.

# **Exercises**

1. Derive the expectation maximization update for the parameter µ in the EM clustering model.

- 2. Derive the E-step and M-step updates for the following generative model. You may assume that the labels  $y^{(i)}$  are observed, but  $z_m^{(i)}$  is not.
  - For each instance *i*,
    - Draw label  $y^{(i)} \sim \text{Categorical}(\boldsymbol{\mu})$
    - For each token  $m \in \{1, 2, \dots, M^{(i)}\}$ 
      - \* Draw  $z_m^{(i)} \sim \text{Categorical}(\pi)$
      - \* If  $z_m^{(i)} = 0$ , draw the current token from a label-specific distribution,  $w_m^{(i)} \sim \phi_{u^{(i)}}$
      - \* If  $z_m^{(i)} = 1$ , draw the current token from a document-specific distribution,  $w_m^{(i)} \sim \boldsymbol{\nu}^{(i)}$
- 3. Using the iterative updates in Equations 5.34-5.36, compute the outcome of the label propagation algorithm for the following examples.

![](_page_136_Figure_9.jpeg)

The value inside the node indicates the label,  $y^{(i)} \in \{0, 1\}$ , with  $y^{(i)} =$ ? for unlabeled nodes. The presence of an edge between two nodes indicates  $w_{i,j} = 1$ , and the absence of an edge indicates  $w_{i,j} = 0$ . For the third example, you need only compute the first three iterations, and then you can guess at the solution in the limit.

- 4. Use expectation-maximization clustering to train a word-sense induction system, applied to the word *say*.
  - Import NLTK, run NLTK.DOWNLOAD() and select SEMCOR. Import SEMCOR from NLTK.CORPUS.
  - The command SEMCOR.TAGGED\_SENTENCES(TAG='SENSE') returns an iterator over sense-tagged sentences in the corpus. Each sentence can be viewed as an iterator over TREE objects. For TREE objects that are sense-annotated words, you can access the annotation as TREE.LABEL(), and the word itself with TREE.LEAVES(). So SEMCOR.TAGGED\_SENTENCES(TAG='SENSE')[0][2].LABEL() would return the sense annotation of the third word in the first sentence.
  - Extract all sentences containing the senses SAY.V.01 and SAY.V.02.
  - Build bag-of-words vectors  $x^{(i)}$ , containing the counts of other words in those sentences, including all words that occur in at least two sentences.
  - Implement and run expectation-maximization clustering on the merged data.

<span id="page-137-0"></span>• Compute the frequency with which each cluster includes instances of SAY.V.01 and SAY.V.02.

In the remaining exercises, you will try out some approaches for semisupervised learning and domain adaptation. You will need datasets in multiple domains. You can obtain product reviews in multiple domains here: [https://www.cs.jhu.edu/˜mdredze/](https://www.cs.jhu.edu/~mdredze/datasets/sentiment/processed_acl.tar.gz) [datasets/sentiment/processed\\_acl.tar.gz](https://www.cs.jhu.edu/~mdredze/datasets/sentiment/processed_acl.tar.gz). Choose a source and target domain, e.g. dvds and books, and divide the data for the target domain into training and test sets of equal size.

- 5. First, quantify the cost of cross-domain transfer.
  - Train a logistic regression classifier on the source domain training set, and evaluate it on the target domain test set.
  - Train a logistic regression classifier on the target domain training set, and evaluate it on the target domain test set. This it the "direct transfer" baseline.

Compute the difference in accuracy, which is a measure of the transfer loss across domains.

6. Next, apply the **label propagation** algorithm from § [5.3.2.](#page-126-0)

As a baseline, using only 5% of the target domain training set, train a classifier, and compute its accuracy on the target domain test set.

Next, apply label propagation:

- Compute the label matrix Q<sup>L</sup> for the labeled data (5% of the target domain training set), with each row equal to an indicator vector for the label (positive or negative).
- Iterate through the target domain instances, including both test and training data. At each instance i, compute all wij , using [Equation 5.32,](#page-126-2) with α = 0.01. Use these values to fill in column i of the transition matrix T, setting all but the ten largest values to zero for each column i. Be sure to normalize the column so that the remaining values sum to one. You may need to use a sparse matrix for this to fit into memory.
- Apply the iterative updates from Equations [5.34-](#page-127-1)[5.36](#page-127-2) to compute the outcome of the label propagation algorithm for the unlabeled examples.

Select the test set instances from Q<sup>U</sup> , and compute the accuracy of this method. Compare with the supervised classifier trained only on the 5% sample of the target domain training set.

- 7. Using only 5% of the target domain training data (and all of the source domain training data), implement one of the supervised domain adaptation baselines in § 5.4.1. See if this improves on the "direct transfer" baseline from the previous problem
- 8. Implement EASYADAPT (§ 5.4.1), again using 5% of the target domain training data and all of the source domain data.
- 9. Now try unsupervised domain adaptation, using the "linear projection" method described in § 5.4.2. Specifically:
  - Identify 500 pivot features as the words with the highest frequency in the (complete) training data for the source and target domains. Specifically, let  $x_i^d$  be the count of the word *i* in domain *d*: choose the 500 words with the largest values of  $\min(x_i^{\text{source}}, x_i^{\text{target}})$ .
  - Train a classifier to predict each pivot feature from the remaining words in the document.
  - Arrange the features of these classifiers into a matrix  $\Phi$ , and perform truncated singular value decomposition, with k = 20
  - Train a classifier from the source domain data, using the combined features *x*<sup>(i)</sup> ⊕ U<sup>T</sup>*x*<sup>(i)</sup> — these include the original bag-of-words features, plus the projected features.
  - Apply this classifier to the target domain test set, and compute the accuracy.

# **Part II**

# <span id="page-140-0"></span>**Sequences and trees**

# <span id="page-142-1"></span><span id="page-142-0"></span>**Chapter 6 Language models**

In probabilistic classification, the problem is to compute the probability of a label, conditioned on the text. Let's now consider the inverse problem: computing the probability of text itself. Specifically, we will consider models that assign probability to a sequence of word tokens, p(w1, w2, . . . , wM), with w<sup>m</sup> ∈ V. The set V is a discrete vocabulary,

$$\mathcal{V} = \{aardvark, abacus, \dots, zither\}.$$
 [6.1]

Why would you want to compute the probability of a word sequence? In many applications, the goal is to produce word sequences as output:

- In **machine translation** [\(chapter 18\)](#page-448-0), we convert from text in a source language to text in a target language.
- In **speech recognition**, we convert from audio signal to text.
- In **summarization** (§ [16.3.4;](#page-410-1) § [19.2\)](#page-481-0), we convert from long texts into short texts.
- In **dialogue systems** (§ [19.3\)](#page-483-0), we convert from the user's input (and perhaps an external knowledge base) into a text response.

In many of the systems for performing these tasks, there is a subcomponent that computes the probability of the output text. The purpose of this component is to generate texts that are more **fluent**. For example, suppose we want to translate a sentence from Spanish to English.

(6.1) El cafe negro me gusta mucho.

Here is a literal word-for-word translation (a **gloss**):

(6.2) The coffee black me pleases much.

<span id="page-143-1"></span>A good language model of English will tell us that the probability of this translation is low, in comparison with more grammatical alternatives,

$$p(The \ coffee \ black \ me \ pleases \ much) < p(I \ love \ dark \ coffee).$$
 [6.2]

How can we use this fact? Warren Weaver, one of the early leaders in machine translation, viewed it as a problem of breaking a secret code [\(Weaver, 1955\)](#page-568-3):

When I look at an article in Russian, I say: 'This is really written in English, but it has been coded in some strange symbols. I will now proceed to decode.'

This observation motivates a generative model (like Na¨ıve Bayes):

- The English sentence <sup>w</sup>(e) is generated from a **language model**, p<sup>e</sup> (w(e) ).
- The Spanish sentence <sup>w</sup>(s) is then generated from a **translation model**, ps|<sup>e</sup> (w(s) <sup>|</sup> <sup>w</sup>(e) ).

Given these two distributions, translation can be performed by Bayes' rule:

$$\mathbf{p}_{e|s}(\boldsymbol{w}^{(e)} \mid \boldsymbol{w}^{(s)}) \propto \mathbf{p}_{e,s}(\boldsymbol{w}^{(e)}, \boldsymbol{w}^{(s)})$$
[6.3]

$$=\mathbf{p}_{s|e}(\boldsymbol{w}^{(s)} \mid \boldsymbol{w}^{(e)}) \times \mathbf{p}_{e}(\boldsymbol{w}^{(e)}).$$
[6.4]

This is sometimes called the **noisy channel model**, because it envisions English text turning into Spanish by passing through a noisy channel, ps|<sup>e</sup> . What is the advantage of modeling translation this way, as opposed to modeling pe|<sup>s</sup> directly? The crucial point is that the two distributions ps|<sup>e</sup> (the translation model) and p<sup>e</sup> (the language model) can be estimated from separate data. The translation model requires examples of correct translations, but the language model requires only text in English. Such monolingual data is much more widely available. Furthermore, once estimated, the language model p<sup>e</sup> can be reused in any application that involves generating English text, including translation from other languages.

# <span id="page-143-0"></span>**6.1** N**-gram language models**

A simple approach to computing the probability of a sequence of tokens is to use a **relative frequency estimate**. Consider the quote, attributed to Picasso, "*computers are useless, they can only give you answers.*" One way to estimate the probability of this sentence is,

$$p(Computers are useless, they can only give you answers) = \frac{count(Computers are useless, they can only give you answers)}{count(all sentences ever spoken)}$$
[6.5]

#### <span id="page-144-1"></span>6.1. N-GRAM LANGUAGE MODELS

This estimator is **unbiased**: in the theoretical limit of infinite data, the estimate will be correct. But in practice, we are asking for accurate counts over an infinite number of events, since sequences of words can be arbitrarily long. Even with an aggressive upper bound of, say, M = 20 tokens in the sequence, the number of possible sequences is  $V^{20}$ , where  $V = |\mathcal{V}|$ . A small vocabularly for English would have  $V = 10^5$ , so there are  $10^{100}$  possible sequences. Clearly, this estimator is very data-hungry, and suffers from high variance: even grammatical sentences will have probability zero if they have not occurred in the training data.<sup>1</sup> We therefore need to introduce bias to have a chance of making reliable estimates from finite training data. The language models that follow in this chapter introduce bias in various ways.

We begin with *n*-gram language models, which compute the probability of a sequence as the product of probabilities of subsequences. The probability of a sequence  $p(w) = p(w_1, w_2, ..., w_M)$  can be refactored using the chain rule (see § A.2):

$$\mathbf{p}(\boldsymbol{w}) = \mathbf{p}(w_1, w_2, \dots, w_M)$$
[6.6]

$$=\mathbf{p}(w_1) \times \mathbf{p}(w_2 \mid w_1) \times \mathbf{p}(w_3 \mid w_2, w_1) \times \ldots \times \mathbf{p}(w_M \mid w_{M-1}, \ldots, w_1)$$
[6.7]

Each element in the product is the probability of a word given all its predecessors. We can think of this as a *word prediction* task: given the context *Computers are*, we want to compute a probability over the next token. The relative frequency estimate of the probability of the word *useless* in this context is,

$$p(useless \mid computers are) = \frac{count(computers are useless)}{\sum_{x \in \mathcal{V}} count(computers are x)} = \frac{count(computers are useless)}{count(computers are)}.$$

We haven't made any approximations yet, and we could have just as well applied the chain rule in reverse order,

$$\mathbf{p}(\boldsymbol{w}) = \mathbf{p}(w_M) \times \mathbf{p}(w_{M-1} \mid w_M) \times \ldots \times \mathbf{p}(w_1 \mid w_2, \ldots, w_M),$$
 [6.8]

or in any other order. But this means that we also haven't really made any progress: to compute the conditional probability  $p(w_M | w_{M-1}, w_{M-2}, ..., w_1)$ , we would need to model  $V^{M-1}$  contexts. Such a distribution cannot be estimated from any realistic sample of text.

To solve this problem, *n*-gram models make a crucial simplifying approximation: they condition on only the past n - 1 words.

$$\mathbf{p}(w_m \mid w_{m-1} \dots w_1) \approx \mathbf{p}(w_m \mid w_{m-1}, \dots, w_{m-n+1})$$
[6.9]

<span id="page-144-0"></span><sup>&</sup>lt;sup>1</sup>Chomsky famously argued that this is evidence against the very concept of probabilistic language models: no such model could distinguish the grammatical sentence *colorless green ideas sleep furiously* from the ungrammatical permutation *furiously sleep ideas green colorless*.

This means that the probability of a sentence *w* can be approximated as

$$p(w_1, \dots, w_M) \approx \prod_{m=1}^M p(w_m \mid w_{m-1}, \dots, w_{m-n+1})$$
 [6.10]

To compute the probability of an entire sentence, it is convenient to pad the beginning and end with special symbols  $\Box$  and  $\blacksquare$ . Then the bigram (n = 2) approximation to the probability of *l like black coffee* is:

$$p(I \ like \ black \ coffee) = p(I \mid \Box) \times p(like \mid I) \times p(black \mid like) \times p(coffee \mid black) \times p(\blacksquare \mid coffee).$$
[6.11]

This model requires estimating and storing the probability of only  $V^n$  events, which is exponential in the order of the *n*-gram, and not  $V^M$ , which is exponential in the length of the sentence. The *n*-gram probabilities can be computed by relative frequency estimation,

$$p(w_m \mid w_{m-1}, w_{m-2}) = \frac{\operatorname{count}(w_{m-2}, w_{m-1}, w_m)}{\sum_{w'} \operatorname{count}(w_{m-2}, w_{m-1}, w')}$$
[6.12]

The hyperparameter n controls the size of the context used in each conditional probability. If this is misspecified, the language model will perform poorly. Let's consider the potential problems concretely.

When *n* is too small. Consider the following sentences:

- (6.3) **Gorillas** always like to groom **their** friends.
- (6.4) The **computer** that's on the 3rd floor of our office building **crashed**.

In each example, the words written in bold depend on each other: the likelihood of *their* depends on knowing that *gorillas* is plural, and the likelihood of *crashed* depends on knowing that the subject is a *computer*. If the *n*-grams are not big enough to capture this context, then the resulting language model would offer probabilities that are too low for these sentences, and too high for sentences that fail basic linguistic tests like number agreement.

When *n* is too big. In this case, it is hard good estimates of the n-gram parameters from our dataset, because of data sparsity. To handle the *gorilla* example, it is necessary to model 6-grams, which means accounting for  $V^6$  events. Under a very small vocabulary of  $V = 10^4$ , this means estimating the probability of  $10^{24}$  distinct events.

<span id="page-146-2"></span>These two problems point to another **bias-variance tradeoff** (see § 2.2.4). A small n-gram size introduces high bias, and a large n-gram size introduces high variance. We can even have both problems at the same time! Language is full of long-range dependencies that we cannot capture because n is too small; at the same time, language datasets are full of rare phenomena, whose probabilities we fail to estimate accurately because n is too large. One solution is to try to keep n large, while still making low-variance estimates of the underlying parameters. To do this, we will introduce a different sort of bias: **smoothing**.

## <span id="page-146-0"></span>6.2 Smoothing and discounting

Limited data is a persistent problem in estimating language models. In § 6.1, we presented *n*-grams as a partial solution. Bit sparse data can be a problem even for low-order *n*-grams; at the same time, many linguistic phenomena, like subject-verb agreement, cannot be incorporated into language models without high-order *n*-grams. It is therefore necessary to add additional inductive biases to *n*-gram language models. This section covers some of the most intuitive and common approaches, but there are many more (see Chen and Goodman, 1999).

#### <span id="page-146-1"></span>6.2.1 Smoothing

A major concern in language modeling is to avoid the situation p(w) = 0, which could arise as a result of a single unseen n-gram. A similar problem arose in Naïve Bayes, and the solution was **smoothing**: adding imaginary "pseudo" counts. The same idea can be applied to *n*-gram language models, as shown here in the bigram case,

$$p_{\text{smooth}}(w_m \mid w_{m-1}) = \frac{\text{count}(w_{m-1}, w_m) + \alpha}{\sum_{w' \in \mathcal{V}} \text{count}(w_{m-1}, w') + V\alpha}.$$
 [6.13]

This basic framework is called **Lidstone smoothing**, but special cases have other names:

- Laplace smoothing corresponds to the case  $\alpha = 1$ .
- Jeffreys-Perks law corresponds to the case  $\alpha = 0.5$ , which works well in practice and benefits from some theoretical justification (Manning and Schütze, 1999).

To ensure that the probabilities are properly normalized, anything that we add to the numerator ( $\alpha$ ) must also appear in the denominator ( $V\alpha$ ). This idea is reflected in the concept of **effective counts**:

$$c_i^* = (c_i + \alpha) \frac{M}{M + V\alpha},$$
[6.14]

<span id="page-147-2"></span><span id="page-147-1"></span>

|              |        |                           | Lidstone sm         | Lidstone smoothing, $\alpha=0.1$ |                     | Discounting, $d = 0.1$  |  |  |
|--------------|--------|---------------------------|---------------------|----------------------------------|---------------------|-------------------------|--|--|
|              | counts | unsmoothed<br>probability | effective<br>counts | smoothed<br>probability          | effective<br>counts | smoothed<br>probability |  |  |
| impropriety  | 8      | 0.4                       | 7.826               | 0.391                            | 7.9                 | 0.395                   |  |  |
| offense      | 5      | 0.25                      | 4.928               | 0.246                            | 4.9                 | 0.245                   |  |  |
| damage       | 4      | 0.2                       | 3.961               | 0.198                            | 3.9                 | 0.195                   |  |  |
| deficiencies | 2      | 0.1                       | 2.029               | 0.101                            | 1.9                 | 0.095                   |  |  |
| outbreak     | 1      | 0.05                      | 1.063               | 0.053                            | 0.9                 | 0.045                   |  |  |
| infirmity    | 0      | 0                         | 0.097               | 0.005                            | 0.25                | 0.013                   |  |  |
| cephalopods  | 0      | 0                         | 0.097               | 0.005                            | 0.25                | 0.013                   |  |  |

Table 6.1: Example of Lidstone smoothing and absolute discounting in a bigram language model, for the context (*alleged*, \_), for a toy corpus with a total of twenty counts over the seven words shown. Note that discounting decreases the probability for all but the unseen words, while Lidstone smoothing increases the effective counts and probabilities for *deficiencies* and *outbreak*.

where  $c_i$  is the count of event  $i, c_i^*$  is the effective count, and  $M = \sum_{i=1}^{V} c_i$  is the total number of tokens in the dataset  $(w_1, w_2, \dots, w_M)$ . This term ensures that  $\sum_{i=1}^{V} c_i^* = \sum_{i=1}^{V} c_i = M$ . The **discount** for each n-gram is then computed as,

$$d_i = \frac{c_i^*}{c_i} = \frac{(c_i + \alpha)}{c_i} \frac{M}{(M + V\alpha)}.$$

#### <span id="page-147-0"></span>6.2.2 Discounting and backoff

Discounting "borrows" probability mass from observed *n*-grams and redistributes it. In Lidstone smoothing, the borrowing is done by increasing the denominator of the relative frequency estimates. The borrowed probability mass is then redistributed by increasing the numerator for all *n*-grams. Another approach would be to borrow the same amount of probability mass from all observed *n*-grams, and redistribute it among only the unobserved *n*-grams. This is called **absolute discounting**. For example, suppose we set an absolute discount d = 0.1 in a bigram model, and then redistribute this probability mass equally over the unseen words. The resulting probabilities are shown in Table 6.1.

Discounting reserves some probability mass from the observed data, and we need not redistribute this probability mass equally. Instead, we can **backoff** to a lower-order language model: if you have trigrams, use trigrams; if you don't have trigrams, use bigrams; if you don't even have bigrams, use unigrams. This is called **Katz backoff**. In the simple

<span id="page-148-1"></span>case of backing off from bigrams to unigrams, the bigram probabilities are,

$$c^{*}(i,j) = c(i,j) - d$$
[6.15]

$$\mathsf{p}_{\mathsf{Katz}}(i \mid j) = \begin{cases} \frac{c \ (i,j)}{c(j)} & \text{if } c(i,j) > 0\\ \alpha(j) \times \frac{\mathsf{P}_{\mathsf{unigram}}(i)}{\sum_{i':c(i',j)=0} \mathsf{P}_{\mathsf{unigram}}(i')} & \text{if } c(i,j) = 0. \end{cases}$$

$$[6.16]$$

The term  $\alpha(j)$  indicates the amount of probability mass that has been discounted for context *j*. This probability mass is then divided across all the unseen events,  $\{i' : c(i', j) = 0\}$ , proportional to the unigram probability of each word *i'*. The discount parameter *d* can be optimized to maximize performance (typically held-out log-likelihood) on a development set.

#### <span id="page-148-0"></span>6.2.3 \*Interpolation

Backoff is one way to combine different order *n*-gram models. An alternative approach is **interpolation**: setting the probability of a word in context to a weighted sum of its probabilities across progressively shorter contexts.

Instead of choosing a single n for the size of the n-gram, we can take the weighted average across several n-gram probabilities. For example, for an interpolated trigram model,

$$p_{\text{Interpolation}}(w_m \mid w_{m-1}, w_{m-2}) = \lambda_3 p_3^*(w_m \mid w_{m-1}, w_{m-2}) + \lambda_2 p_2^*(w_m \mid w_{m-1}) + \lambda_1 p_1^*(w_m).$$

In this equation,  $p_n^*$  is the unsmoothed empirical probability given by an *n*-gram language model, and  $\lambda_n$  is the weight assigned to this model. To ensure that the interpolated  $p(\boldsymbol{w})$  is still a valid probability distribution, the values of  $\lambda$  must obey the constraint,  $\sum_{n=1}^{n_{\text{max}}} \lambda_n = 1$ . But how to find the specific values?

An elegant solution is **expectation-maximization**. Recall from chapter 5 that we can think about EM as learning with *missing data*: we just need to choose missing data such that learning would be easy if it weren't missing. What's missing in this case? Think of each word  $w_m$  as drawn from an *n*-gram of unknown size,  $z_m \in \{1 \dots n_{max}\}$ . This  $z_m$  is the missing data that we are looking for. Therefore, the application of EM to this problem involves the following **generative model**:

for Each token  $w_m, m = 1, 2, ..., M$  do: draw the *n*-gram size  $z_m \sim \text{Categorical}(\lambda)$ ; draw  $w_m \sim p_{z_m}^*(w_m \mid w_{m-1}, ..., w_{m-z_m})$ .

If the missing data  $\{Z_m\}$  were known, then  $\lambda$  could be estimated as the relative frequency,

$$\lambda_z = \frac{\operatorname{count}(Z_m = z)}{M}$$
[6.17]

$$\propto \sum_{m=1}^{M} \delta(Z_m = z).$$
[6.18]

But since we do not know the values of the latent variables  $Z_m$ , we impute a distribution  $q_m$  in the E-step, which represents the degree of belief that word token  $w_m$  was generated from a *n*-gram of order  $z_m$ ,

$$q_m(z) \triangleq \Pr(Z_m = z \mid \boldsymbol{w}_{1:m}; \lambda)$$
[6.19]

$$= \frac{p(w_m \mid \boldsymbol{w}_{1:m-1}, Z_m = z) \times p(z)}{\sum_{z'} p(w_m \mid \boldsymbol{w}_{1:m-1}, Z_m = z') \times p(z')}$$
[6.20]

$$\propto \mathbf{p}_z^*(w_m \mid \boldsymbol{w}_{1:m-1}) \times \lambda_z.$$
[6.21]

In the M-step,  $\lambda$  is computed by summing the expected counts under q,

$$\lambda_z \propto \sum_{m=1}^M q_m(z).$$
[6.22]

A solution is obtained by iterating between updates to q and  $\lambda$ . The complete algorithm is shown in Algorithm 10.

#### <span id="page-149-0"></span>Algorithm 10 Expectation-maximization for interpolated language modeling

1: procedure ESTIMATE INTERPOLATED *n*-GRAM ( $w_{1:M}, \{p_n^*\}_{n \in 1:n_{max}}$ ) for  $z \in \{1, 2, \dots, n_{\max}\}$  do  $\lambda_z \leftarrow \frac{1}{n_{\max}}$ ▷ Initialization 2: 3: repeat 4: for  $m \in \{1, 2, ..., M\}$  do 5: ⊳ E-step for  $z \in \{1, 2, ..., n_{\max}\}$  do 6:  $q_m(z) \leftarrow \mathbf{p}_z^*(w_m \mid \boldsymbol{w}_{1:m-}) \times \lambda_z$ 7:  $q_m \leftarrow \text{Normalize}(q_m)$ 8: for  $z \in \{1, 2, ..., n_{\max}\}$  do 9: ⊳ M-step  $\lambda_z \leftarrow \frac{1}{M} \sum_{m=1}^M q_m(z)$ 10: until tired 11: return  $\lambda$ 12:

#### <span id="page-150-2"></span><span id="page-150-0"></span>6.2.4 \*Kneser-Ney smoothing

**Kneser-Ney smoothing** is based on absolute discounting, but it redistributes the resulting probability mass in a different way from Katz backoff. Empirical evidence points to Kneser-Ney smoothing as the state-of-art for *n*-gram language modeling (Goodman, 2001). To motivate Kneser-Ney smoothing, consider the example: *I recently visited* \_. Which of the following is more likely: *Francisco* or *Duluth*?

Now suppose that both bigrams *visited Duluth* and *visited Francisco* are unobserved in the training data, and furthermore, that the unigram probability  $p_1^*(Francisco)$  is greater than  $p_1^*(Duluth)$ . Nonetheless we would still guess that p(visited Duluth) > p(visited Francisco), because *Duluth* is a more "versatile" word: it can occur in many contexts, while *Francisco* usually occurs in a single context, following the word *San*. This notion of versatility is the key to Kneser-Ney smoothing.

Writing u for a context of undefined length, and count(w, u) as the count of word w in context u, we define the Kneser-Ney bigram probability as

$$\mathbf{p}_{KN}(w \mid u) = \begin{cases} \frac{\max(\operatorname{count}(w, u) - d, 0)}{\operatorname{count}(u)}, & \operatorname{count}(w, u) > 0\\ \alpha(u) \times \mathbf{p}_{\operatorname{continuation}}(w), & \operatorname{otherwise} \end{cases}$$
[6.23]

$$\mathbf{p}_{\text{continuation}}(w) = \frac{|u: \operatorname{count}(w, u) > 0|}{\sum_{w' \in \mathcal{V}} |u': \operatorname{count}(w', u') > 0|}.$$
[6.24]

Probability mass using absolute discounting d, which is taken from all unobserved n-grams. The total amount of discounting in context u is  $d \times |w| = \operatorname{count}(w, u) > 0|$ , and we divide this probability mass among the unseen n-grams. To account for versatility, we define the *continuation probability*  $p_{\operatorname{continuation}}(w)$  as proportional to the number of observed contexts in which w appears. The numerator of the continuation probability is the number of contexts u in which w appears; the denominator normalizes the probability by summing the same quantity over all words w'. The coefficient  $\alpha(u)$  is set to ensure that the probability distribution  $p_{KN}(w \mid u)$  sums to one over the vocabulary w.

The idea of modeling versatility by counting contexts may seem heuristic, but there is an elegant theoretical justification from Bayesian nonparametrics (Teh, 2006). Kneser-Ney smoothing on *n*-grams was the dominant language modeling technique before the arrival of neural language models.

# <span id="page-150-1"></span>6.3 Recurrent neural network language models

*N*-gram language models have been largely supplanted by neural networks. These models do not make the *n*-gram assumption of restricted context; indeed, they can incorporate arbitrarily distant contextual information, while remaining computationally and statistically tractable.

<span id="page-151-2"></span><span id="page-151-1"></span>![](_page_151_Figure_1.jpeg)

Figure 6.1: The recurrent neural network language model, viewed as an "unrolled" computation graph. Solid lines indicate direct computation, dotted blue lines indicate probabilistic dependencies, circles indicate random variables, and squares indicate computation nodes.

The first insight behind neural language models is to treat word prediction as a *discriminative* learning task.<sup>2</sup> The goal is to compute the probability p(w | u), where  $w \in V$  is a word, and u is the context, which depends on the previous words. Rather than directly estimating the word probabilities from (smoothed) relative frequencies, we can treat treat language modeling as a machine learning problem, and estimate parameters that maximize the log conditional probability of a corpus.

The second insight is to reparametrize the probability distribution p(w | u) as a function of two dense *K*-dimensional numerical vectors,  $\beta_w \in \mathbb{R}^K$ , and  $v_u \in \mathbb{R}^K$ ,

$$\mathbf{p}(w \mid u) = \frac{\exp(\boldsymbol{\beta}_w \cdot \boldsymbol{v}_u)}{\sum_{w' \in \mathcal{V}} \exp(\boldsymbol{\beta}_{w'} \cdot \boldsymbol{v}_u)},$$
[6.25]

where  $\beta_w \cdot v_u$  represents a dot product. As usual, the denominator ensures that the probability distribution is properly normalized. This vector of probabilities is equivalent to applying the **softmax** transformation (see § 3.1) to the vector of dot-products,

$$\mathbf{p}(\cdot \mid u) = \operatorname{SoftMax}([\boldsymbol{\beta}_1 \cdot \boldsymbol{v}_u, \boldsymbol{\beta}_2 \cdot \boldsymbol{v}_u, \dots, \boldsymbol{\beta}_V \cdot \boldsymbol{v}_u]).$$
[6.26]

The word vectors  $\beta_w$  are parameters of the model, and are estimated directly. The context vectors  $v_u$  can be computed in various ways, depending on the model. A simple but effective neural language model can be built from a **recurrent neural network** (RNN; Mikolov et al., 2010). The basic idea is to recurrently update the context vectors while moving through the sequence. Let  $h_m$  represent the contextual information at position m

<span id="page-151-0"></span><sup>&</sup>lt;sup>2</sup>This idea predates neural language models (e.g., Rosenfeld, 1996; Roark et al., 2007).

<span id="page-152-2"></span>in the sequence. RNN language models are defined,

<span id="page-152-1"></span>
$$\boldsymbol{x}_m \triangleq \boldsymbol{\phi}_{w_m}$$
 [6.27]

$$\boldsymbol{h}_m = \operatorname{RNN}(\boldsymbol{x}_m, \boldsymbol{h}_{m-1})$$
 [6.28]

$$\mathbf{p}(w_{m+1} \mid w_1, w_2, \dots, w_m) = \frac{\exp(\boldsymbol{\beta}_{w_{m+1}} \cdot \boldsymbol{h}_m)}{\sum_{w' \in \mathcal{V}} \exp(\boldsymbol{\beta}_{w'} \cdot \boldsymbol{h}_m)},$$
[6.29]

where  $\phi$  is a matrix of **word embeddings**, and  $x_m$  denotes the embedding for word  $w_m$ . The conversion of  $w_m$  to  $x_m$  is sometimes known as a **lookup layer**, because we simply lookup the embeddings for each word in a table; see § 3.2.4.

The **Elman unit** defines a simple recurrent operation (Elman, 1990),

$$\operatorname{RNN}(\boldsymbol{x}_m, \boldsymbol{h}_{m-1}) \triangleq g(\boldsymbol{\Theta}\boldsymbol{h}_{m-1} + \boldsymbol{x}_m),$$
[6.30]

where  $\Theta \in \mathbb{R}^{K \times K}$  is the recurrence matrix and g is a non-linear transformation function, often defined as the elementwise hyperbolic tangent tanh (see § 3.1).<sup>3</sup> The tanh acts as a **squashing function**, ensuring that each element of  $h_m$  is constrained to the range [-1, 1].

Although each  $w_m$  depends on only the context vector  $h_{m-1}$ , this vector is in turn influenced by *all* previous tokens,  $w_1, w_2, \ldots w_{m-1}$ , through the recurrence operation:  $w_1$ affects  $h_1$ , which affects  $h_2$ , and so on, until the information is propagated all the way to  $h_{m-1}$ , and then on to  $w_m$  (see Figure 6.1). This is an important distinction from *n*-gram language models, where any information outside the *n*-word window is ignored. In principle, the RNN language model can handle long-range dependencies, such as number agreement over long spans of text — although it would be difficult to know where exactly in the vector  $h_m$  this information is represented. The main limitation is that information is attenuated by repeated application of the squashing function *g*. **Long short-term memories** (LSTMs), described below, are a variant of RNNs that address this issue, using memory cells to propagate information through the sequence without applying nonlinearities (Hochreiter and Schmidhuber, 1997).

The denominator in Equation 6.29 is a computational bottleneck, because it involves a sum over the entire vocabulary. One solution is to use a **hierarchical softmax** function, which computes the sum more efficiently by organizing the vocabulary into a tree (Mikolov et al., 2011). Another strategy is to optimize an alternative metric, such as **noise-contrastive estimation** (Gutmann and Hyvärinen, 2012), which learns by distinguishing observed instances from artificial instances generated from a noise distribution (Mnih and Teh, 2012). Both of these strategies are described in § 14.5.3.

<span id="page-152-0"></span><sup>&</sup>lt;sup>3</sup>In the original Elman network, the sigmoid function was used in place of tanh. For an illuminating mathematical discussion of the advantages and disadvantages of various nonlinearities in recurrent neural networks, see the lecture notes from Cho (2015).

#### <span id="page-153-1"></span><span id="page-153-0"></span>**6.3.1 Backpropagation through time**

The recurrent neural network language model has the following parameters:

- φ<sup>i</sup> ∈ R <sup>K</sup>, the "input" word vectors (these are sometimes called **word embeddings**, since each word is embedded in a K-dimensional space; see [chapter 14\)](#page-342-0);
- β<sup>i</sup> ∈ R <sup>K</sup>, the "output" word vectors;
- Θ ∈ R <sup>K</sup>×K, the recurrence operator;
- h0, the initial state.

Each of these parameters can be estimated by formulating an objective function over the training corpus, L(w), and then applying backpropagation to obtain gradients on the parameters from a minibatch of training examples (see § [3.3.1\)](#page-72-0). Gradient-based updates can be computed from an online learning algorithm such as stochastic gradient descent (see § [2.6.2\)](#page-56-0).

The application of backpropagation to recurrent neural networks is known as **backpropagation through time**, because the gradients on units at time m depend in turn on the gradients of units at earlier times n < m. Let `m+1 represent the negative log-likelihood of word m + 1,

$$\ell_{m+1} = -\log \mathbf{p}(w_{m+1} \mid w_1, w_2, \dots, w_m).$$
[6.31]

We require the gradient of this loss with respect to each parameter, such as θk,k0, an individual element in the recurrence matrix Θ. Since the loss depends on the parameters only through hm, we can apply the chain rule of differentiation,

$$\frac{\partial \ell_{m+1}}{\partial \theta_{k,k'}} = \frac{\partial \ell_{m+1}}{\partial h_m} \frac{\partial h_m}{\partial \theta_{k,k'}}.$$
[6.32]

The vector h<sup>m</sup> depends on Θ in several ways. First, h<sup>m</sup> is computed by multiplying Θ by the previous state hm−1. But the previous state hm−<sup>1</sup> also depends on Θ:

$$\boldsymbol{h}_m = g(\boldsymbol{x}_m, \boldsymbol{h}_{m-1}) \tag{6.33}$$

$$\frac{\partial h_{m,k}}{\partial \theta_{k,k'}} = g'(x_{m,k} + \boldsymbol{\theta}_k \cdot \boldsymbol{h}_{m-1})(h_{m-1,k'} + \boldsymbol{\theta}_k \cdot \frac{\partial \boldsymbol{h}_{m-1}}{\partial \theta_{k,k'}}),$$
[6.34]

where g 0 is the local derivative of the nonlinear function g. The key point in this equation is that the derivative <sup>∂</sup>h<sup>m</sup> ∂θk,k<sup>0</sup> depends on <sup>∂</sup>hm−<sup>1</sup> ∂θk,k<sup>0</sup> , which will depend in turn on <sup>∂</sup>hm−<sup>2</sup> ∂θk,k<sup>0</sup> , and so on, until reaching the initial state h0.

Each derivative <sup>∂</sup>h<sup>m</sup> ∂θk,k<sup>0</sup> will be reused many times: it appears in backpropagation from the loss `m, but also in all subsequent losses `n>m. Neural network toolkits such as Torch [\(Collobert et al., 2011\)](#page-522-3) and DyNet [\(Neubig et al., 2017\)](#page-551-2) compute the necessary

<span id="page-154-3"></span>derivatives automatically, and cache them for future use. An important distinction from the feedforward neural networks considered in [chapter 3](#page-64-0) is that the size of the computation graph is not fixed, but varies with the length of the input. This poses difficulties for toolkits that are designed around static computation graphs, such as TensorFlow [\(Abadi](#page-509-1) [et al., 2016\)](#page-509-1).[4](#page-154-2)

#### <span id="page-154-0"></span>**6.3.2 Hyperparameters**

The RNN language model has several hyperparameters that must be tuned to ensure good performance. The model capacity is controlled by the size of the word and context vectors K, which play a role that is somewhat analogous to the size of the n-gram context. For datasets that are large with respect to the vocabulary (i.e., there is a large token-to-type ratio), we can afford to estimate a model with a large K, which enables more subtle distinctions between words and contexts. When the dataset is relatively small, then K must be smaller too, or else the model may "memorize" the training data, and fail to generalize. Unfortunately, this general advice has not yet been formalized into any concrete formula for choosing K, and trial-and-error is still necessary. Overfitting can also be prevented by **dropout**, which involves randomly setting some elements of the computation to zero [\(Sri](#page-563-2)[vastava et al., 2014\)](#page-563-2), forcing the learner not to rely too much on any particular dimension of the word or context vectors. The dropout rate must also be tuned on development data.

#### <span id="page-154-1"></span>**6.3.3 Gated recurrent neural networks**

In principle, recurrent neural networks can propagate information across infinitely long sequences. But in practice, repeated applications of the nonlinear recurrence function causes this information to be quickly attenuated. The same problem affects learning: backpropagation can lead to **vanishing gradients** that decay to zero, or **exploding gradients** that increase towards infinity [\(Bengio et al., 1994\)](#page-513-3). The exploding gradient problem can be addressed by clipping gradients at some maximum value [\(Pascanu et al., 2013\)](#page-554-1). The other issues must be addressed by altering the model itself.

The **long short-term memory** (LSTM; [Hochreiter and Schmidhuber, 1997\)](#page-535-4) is a popular variant of RNNs that is more robust to these problems. This model augments the hidden state h<sup>m</sup> with a **memory cell** cm. The value of the memory cell at each time m is a gated sum of two quantities: its previous value cm−1, and an "update" c˜m, which is computed from the current input x<sup>m</sup> and the previous hidden state hm−1. The next state h<sup>m</sup> is then computed from the memory cell. Because the memory cell is not passed through a nonlinear squashing function during the update, it is possible for information to propagate through the network over long distances.

<span id="page-154-2"></span><sup>4</sup> See <https://www.tensorflow.org/tutorials/recurrent> (retrieved Feb 8, 2018).

<span id="page-155-0"></span>![](_page_155_Figure_1.jpeg)

Figure 6.2: The long short-term memory (LSTM) architecture. Gates are shown in boxes with dotted edges. In an LSTM language model, each  $h_m$  would be used to predict the next word  $w_{m+1}$ .

The gates are functions of the input and previous hidden state. They are computed from elementwise sigmoid activations,  $\sigma(x) = (1 + \exp(-x))^{-1}$ , ensuring that their values will be in the range [0, 1]. They can therefore be viewed as soft, differentiable logic gates. The LSTM architecture is shown in Figure 6.2, and the complete update equations are:

| $oldsymbol{f}_{m+1}=\!\sigma(oldsymbol{\Theta}^{(h ightarrow f)}oldsymbol{h}_m+oldsymbol{\Theta}^{(x ightarrow f)}oldsymbol{x}_{m+1}+oldsymbol{b}_f)$ | forget gate | [6.35] |
|-------------------------------------------------------------------------------------------------------------------------------------------------------|-------------|--------|
| $i_{m+1} = \sigma(\mathbf{\Theta}^{(h \rightarrow i)} \mathbf{h}_{m} + \mathbf{\Theta}^{(x \rightarrow i)} \mathbf{r}_{m+1} + \mathbf{h}_{m})$        | input gate  | [6 36] |

| $\boldsymbol{u}_{m+1} = \sigma(\boldsymbol{\Theta}^{(n+1)}\boldsymbol{n}_m + \boldsymbol{\Theta}^{(n+1)}\boldsymbol{x}_{m+1} + \boldsymbol{o}_i)$     | input gate         | [6.36] |
|-------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------|--------|
| $	ilde{m{c}}_{m+1} = 	anh(m{\Theta}^{(h  ightarrow c)}m{h}_m + m{\Theta}^{(w  ightarrow c)}m{x}_{m+1})$                                               | update candidate   | [6.37] |
| $oldsymbol{c}_{m+1}=oldsymbol{f}_{m+1}\odotoldsymbol{c}_m+oldsymbol{i}_{m+1}\odotoldsymbol{	ilde{c}}_{m+1}$                                           | memory cell update | [6.38] |
| $oldsymbol{o}_{m+1}=\!\sigma(oldsymbol{\Theta}^{(h ightarrow o)}oldsymbol{h}_m+oldsymbol{\Theta}^{(x ightarrow o)}oldsymbol{x}_{m+1}+oldsymbol{b}_o)$ | output gate        | [6.39] |
| $\boldsymbol{h}_{m+1} = \boldsymbol{o}_{m+1} \odot \tanh(\boldsymbol{c}_{m+1})$                                                                       | output.            | [6.40] |

The operator  $\odot$  is an elementwise (Hadamard) product. Each gate is controlled by a vector of weights, which parametrize the previous hidden state (e.g.,  $\Theta^{(h \to f)}$ ) and the current input (e.g.,  $\Theta^{(x \to f)}$ ), plus a vector offset (e.g.,  $b_f$ ). The overall operation can be informally summarized as  $(h_m, c_m) = \text{LSTM}(x_m, (h_{m-1}, c_{m-1}))$ , with  $(h_m, c_m)$  representing the LSTM state after reading token m.

The LSTM outperforms standard recurrent neural networks across a wide range of problems. It was first used for language modeling by Sundermeyer et al. (2012), but can be applied more generally: the vector  $h_m$  can be treated as a complete representation of

<span id="page-156-2"></span>the input sequence up to position *m*, and can be used for any labeling task on a sequence of tokens, as we will see in the next chapter.

There are several LSTM variants, of which the Gated Recurrent Unit (Cho et al., 2014) is one of the more well known. Many software packages implement a variety of RNN architectures, so choosing between them is simple from a user's perspective. Jozefowicz et al. (2015) provide an empirical comparison of various modeling choices circa 2015.

# <span id="page-156-0"></span>6.4 Evaluating language models

Language modeling is not usually an application in itself: language models are typically components of larger systems, and they would ideally be evaluated **extrinisically**. This means evaluating whether the language model improves performance on the application task, such as machine translation or speech recognition. But this is often hard to do, and depends on details of the overall system which may be irrelevant to language modeling. In contrast, **intrinsic evaluation** is task-neutral. Better performance on intrinsic metrics may be expected to improve extrinsic metrics across a variety of tasks, but there is always the risk of over-optimizing the intrinsic metric. This section discusses some intrinsic metrics, but keep in mind the importance of performing extrinsic evaluations to ensure that intrinsic performance gains carry over to real applications.

#### <span id="page-156-1"></span>6.4.1 Held-out likelihood

The goal of probabilistic language models is to accurately measure the probability of sequences of word tokens. Therefore, an intrinsic evaluation metric is the likelihood that the language model assigns to **held-out data**, which is not used during training. Specifically, we compute,

$$\ell(\boldsymbol{w}) = \sum_{m=1}^{M} \log p(w_m \mid w_{m-1}, \dots, w_1),$$
[6.41]

treating the entire held-out corpus as a single stream of tokens.

Typically, unknown words are mapped to the  $\langle \text{UNK} \rangle$  token. This means that we have to estimate some probability for  $\langle \text{UNK} \rangle$  on the training data. One way to do this is to fix the vocabulary  $\mathcal{V}$  to the V - 1 words with the highest counts in the training data, and then convert all other tokens to  $\langle \text{UNK} \rangle$ . Other strategies for dealing with out-of-vocabulary terms are discussed in § 6.5.

#### <span id="page-157-1"></span><span id="page-157-0"></span>6.4.2 Perplexity

Held-out likelihood is usually presented as **perplexity**, which is a deterministic transformation of the log-likelihood into an information-theoretic quantity,

$$\operatorname{Perplex}(\boldsymbol{w}) = 2^{-\frac{\ell(\boldsymbol{w})}{M}}, \qquad [6.42]$$

where *M* is the total number of tokens in the held-out corpus.

Lower perplexities correspond to higher likelihoods, so lower scores are better on this metric — it is better to be less perplexed. Here are some special cases:

- In the limit of a perfect language model, probability 1 is assigned to the held-out corpus, with Perplex( $\boldsymbol{w}$ ) =  $2^{-\frac{1}{M}\log_2 1} = 2^0 = 1$ .
- In the opposite limit, probability zero is assigned to the held-out corpus, which corresponds to an infinite perplexity,  $\text{Perplex}(\boldsymbol{w}) = 2^{-\frac{1}{M}\log_2 0} = 2^{\infty} = \infty$ .
- Assume a uniform, unigram model in which  $p(w_i) = \frac{1}{V}$  for all words in the vocabulary. Then,

$$\log_2(\boldsymbol{w}) = \sum_{m=1}^M \log_2 \frac{1}{V} = -\sum_{m=1}^M \log_2 V = -M \log_2 V$$
  
Perplex(\overline{w}) = 2<sup>\frac{1}{M}M \log\_2 V</sup>  
= 2^{\log\_2 V}  
= V.

This is the "worst reasonable case" scenario, since you could build such a language model without even looking at the data.

In practice, language models tend to give perplexities in the range between 1 and *V*. A small benchmark dataset is the **Penn Treebank**, which contains roughly a million tokens; its vocabulary is limited to 10,000 words, with all other tokens mapped a special  $\langle UNK \rangle$  symbol. On this dataset, a well-smoothed 5-gram model achieves a perplexity of 141 (Mikolov and Zweig, Mikolov and Zweig), and an LSTM language model achieves perplexity of roughly 80 (Zaremba, Sutskever, and Vinyals, Zaremba et al.). Various enhancements to the LSTM architecture can bring the perplexity below 60 (Merity et al., 2018). A larger-scale language modeling dataset is the 1B Word Benchmark (Chelba et al., 2013), which contains text from Wikipedia. On this dataset, perplexities of around 25 can be obtained by averaging together multiple LSTM language models (Jozefowicz et al., 2016).

# <span id="page-158-3"></span><span id="page-158-0"></span>**6.5 Out-of-vocabulary words**

So far, we have assumed a **closed-vocabulary** setting — the vocabulary V is assumed to be a finite set. In realistic application scenarios, this assumption may not hold. Consider, for example, the problem of translating newspaper articles. The following sentence appeared in a Reuters article on January 6, 2017:[5](#page-158-1)

The report said U.S. intelligence agencies believe Russian military intelligence, the **GRU**, used intermediaries such as **WikiLeaks**, **DCLeaks.com** and the **Guccifer** 2.0 "persona" to release emails...

Suppose that you trained a language model on the Gigaword corpus,[6](#page-158-2) which was released in 2003. The bolded terms either did not exist at this date, or were not widely known; they are unlikely to be in the vocabulary. The same problem can occur for a variety of other terms: new technologies, previously unknown individuals, new words (e.g., *hashtag*), and numbers.

One solution is to simply mark all such terms with a special token, hUNKi. While training the language model, we decide in advance on the vocabulary (often the K most common terms), and mark all other terms in the training data as hUNKi. If we do not want to determine the vocabulary size in advance, an alternative approach is to simply mark the first occurrence of each word type as hUNKi.

But is often better to make distinctions about the likelihood of various unknown words. This is particularly important in languages that have rich morphological systems, with many inflections for each word. For example, Portuguese is only moderately complex from a morphological perspective, yet each verb has dozens of inflected forms (see [Fig](#page-96-1)[ure 4.3b\)](#page-96-1). In such languages, there will be many word types that we do not encounter in a corpus, which are nonetheless predictable from the morphological rules of the language. To use a somewhat contrived English example, if *transfenestrate* is in the vocabulary, our language model should assign a non-zero probability to the past tense *transfenestrated*, even if it does not appear in the training data.

One way to accomplish this is to supplement word-level language models with **characterlevel language models**. Such models can use n-grams or RNNs, but with a fixed vocabulary equal to the set of ASCII or Unicode characters. For example, [Ling et al.](#page-545-3) [\(2015\)](#page-545-3) propose an LSTM model over characters, and [Kim](#page-539-6) [\(2014\)](#page-539-6) employ a convolutional neural network. A more linguistically motivated approach is to segment words into meaningful subword units, known as **morphemes** (see [chapter 9\)](#page-208-0). For example, [Botha and Blunsom](#page-515-4)

<span id="page-158-1"></span><sup>5</sup>Bayoumy, Y. and Strobel, W. (2017, January 6). U.S. intel report: Putin directed cyber campaign to help Trump. *Reuters*. Retrieved from [http://www.reuters.com/article/](http://www.reuters.com/article/us-usa-russia-cyber-idUSKBN14Q1T8) [us-usa-russia-cyber-idUSKBN14Q1T8](http://www.reuters.com/article/us-usa-russia-cyber-idUSKBN14Q1T8) on January 7, 2017.

<span id="page-158-2"></span><sup>6</sup><https://catalog.ldc.upenn.edu/LDC2003T05>

(2014) induce vector representations for morphemes, which they build into a log-bilinear language model; Bhatia et al. (2016) incorporate morpheme vectors into an LSTM.

# Additional resources

A variety of neural network architectures have been applied to language modeling. Notable earlier non-recurrent architectures include the neural probabilistic language model (Bengio et al., 2003) and the log-bilinear language model (Mnih and Hinton, 2007). Much more detail on these models can be found in the text by Goodfellow et al. (2016).

### Exercises

1. Prove that *n*-gram language models give valid probabilities if the *n*-gram probabilities are valid. Specifically, assume that,

$$\sum_{w_m}^{\mathcal{V}} \mathbf{p}(w_m \mid w_{m-1}, w_{m-2}, \dots, w_{m-n+1}) = 1$$
 [6.43]

for all contexts  $(w_{m-1}, w_{m-2}, ..., w_{m-n+1})$ . Prove that  $\sum_{\boldsymbol{w}} p_n(\boldsymbol{w}) = 1$  for all  $\boldsymbol{w} \in \mathcal{V}^*$ , where  $p_n$  is the probability under an *n*-gram language model. Your proof should proceed by induction. You should handle the start-of-string case  $p(w_1 \mid \bigsqcup, ..., \bigsqcup)$ ,

but you need not handle the end-of-string token.

2. First, show that RNN language models are valid using a similar proof technique to the one in the previous problem.

Next, let  $p_r(w)$  indicate the probability of w under RNN r. An ensemble of RNN language models computes the probability,

$$\mathbf{p}(\boldsymbol{w}) = \frac{1}{R} \sum_{r=1}^{R} \mathbf{p}_r(\boldsymbol{w}).$$
 [6.44]

Does an ensemble of RNN language models compute a valid probability?

- 3. Consider a unigram language model over a vocabulary of size V. Suppose that a word appears m times in a corpus with M tokens in total. With Lidstone smoothing of  $\alpha$ , for what values of m is the smoothed probability greater than the unsmoothed probability?
- 4. Consider a simple language in which each token is drawn from the vocabulary  $\mathcal{V}$  with probability  $\frac{1}{V}$ , independent of all other tokens.

<span id="page-160-2"></span>Given a corpus of size M, what is the expectation of the fraction of all possible bigrams that have zero count? You may assume V is large enough that  $\frac{1}{V} \approx \frac{1}{V-1}$ .

- 5. Continuing the previous problem, determine the value of M such that the fraction of bigrams with zero count is at most  $\epsilon \in (0, 1)$ . As a hint, you may use the approximation  $\ln(1 + \alpha) \approx \alpha$  for  $\alpha \approx 0$ .
- 6. In real languages, words probabilities are neither uniform nor independent. Assume that word probabilities are independent but not uniform, so that in general  $p(w) \neq \frac{1}{V}$ . Prove that the expected fraction of unseen bigrams will be higher than in the IID case.
- 7. Consider a recurrent neural network with a single hidden unit and a sigmoid activation,  $h_m = \sigma(\theta h_{m-1} + x_m)$ . Prove that if  $|\theta| < 1$ , then the gradient  $\frac{\partial h_m}{\partial h_{m-k}}$  goes to zero as  $k \to \infty$ .<sup>7</sup>
- 8. **Zipf's law** states that if the word types in a corpus are sorted by frequency, then the frequency of the word at rank r is proportional to  $r^{-s}$ , where s is a free parameter, usually around 1. (Another way to view Zipf's law is that a plot of log frequency against log rank will be linear.) Solve for s using the counts of the first and second most frequent words,  $c_1$  and  $c_2$ .
- 9. Download the wikitext-2 dataset.<sup>8</sup> Read in the training data and compute word counts. Estimate the Zipf's law coefficient by,

$$\hat{s} = \exp\left(\frac{(\log \boldsymbol{r}) \cdot (\log \boldsymbol{c})}{||\log \boldsymbol{r}||_2^2}\right),$$
[6.45]

where r = [1, 2, 3, ...] is the vector of ranks of all words in the corpus, and  $c = [c_1, c_2, c_3, ...]$  is the vector of counts of all words in the corpus, sorted in descending order.

Make a log-log plot of the observed counts, and the expected counts according to Zipf's law. The sum  $\sum_{r=1}^{\infty} r^s = \zeta(s)$  is the Riemann zeta function, available in python's scipy library as scipy.special.zeta.

10. Using the Pytorch library, train an LSTM language model from the Wikitext training corpus. After each epoch of training, compute its perplexity on the Wikitext validation corpus. Stop training when the perplexity stops improving.

<span id="page-160-0"></span><sup>&</sup>lt;sup>7</sup>This proof generalizes to vector hidden units by considering the largest eigenvector of the matrix  $\Theta$  (Pascanu et al., 2013).

<span id="page-160-1"></span><sup>&</sup>lt;sup>8</sup>Available at https://github.com/pytorch/examples/tree/master/word\_language\_ model/data/wikitext-2 in September 2018. The dataset is already tokenized, and already replaces rare words with (UNK), so no preprocessing is necessary.

# <span id="page-162-3"></span><span id="page-162-0"></span>**Chapter 7 Sequence labeling**

The goal of sequence labeling is to assign tags to words, or more generally, to assign discrete labels to discrete elements in a sequence. There are many applications of sequence labeling in natural language processing, and [chapter 8](#page-192-0) presents an overview. For now, we'll focus on the classic problem of **part-of-speech tagging**, which requires tagging each word by its grammatical category. Coarse-grained grammatical categories include **N**OUNs, which describe things, properties, or ideas, and **V**ERBs, which describe actions and events. Consider a simple input:

<span id="page-162-2"></span>(7.1) They can fish.

A dictionary of coarse-grained part-of-speech tags might include NOUN as the only valid tag for *they*, but both NOUN and VERB as potential tags for *can* and *fish*. A accurate sequence labeling algorithm should select the verb tag for both *can* and *fish* in [\(7.1\)](#page-162-2), but it should select noun for the same two words in the phrase *can of fish*.

# <span id="page-162-1"></span>**7.1 Sequence labeling as classification**

One way to solve a tagging problem is to turn it into a classification problem. Let f((w, m), y) indicate the feature function for tag y at position m in the sequence w = (w1, w2, . . . , wM). A simple tagging model would have a single base feature, the word itself:

f((w = *they can fish*, m = 1), N) =(*they*, N) [7.1]

$$f((w = they \ can \ fish, m = 2), V) = (can, V)$$
[7.2]

$$f((w = they \ can \ fish, m = 3), V) = (fish, V).$$
[7.3]

Here the feature function takes three arguments as input: the sentence to be tagged (e.g., *they can fish*), the proposed tag (e.g., N or V), and the index of the token to which this tag <span id="page-163-1"></span>is applied. This simple feature function then returns a single feature: a tuple including the word to be tagged and the tag that has been proposed. If the vocabulary size is V and the number of tags is K, then there are V × K features. Each of these features must be assigned a weight. These weights can be learned from a labeled dataset using a classification algorithm such as perceptron, but this isn't necessary in this case: it would be equivalent to define the classification weights directly, with θw,y = 1 for the tag y most frequently associated with word w, and θw,y = 0 for all other tags.

However, it is easy to see that this simple classification approach cannot correctly tag both *they can fish* and *can of fish*, because *can* and *fish* are grammatically ambiguous. To handle both of these cases, the tagger must rely on context, such as the surrounding words. We can build context into the feature set by incorporating the surrounding words as additional features:

$$f((w = they \ can \ fish, 1), \mathbf{N}) = \{(w_m = they, y_m = \mathbf{N}), \\ (w_{m-1} = \Box, y_m = \mathbf{N}), \\ (w_{m+1} = can, y_m = \mathbf{N})\}$$
[7.4]

$$f((w = they \ can \ fish, 2), \mathbf{V}) = \{(w_m = can, y_m = \mathbf{V}), \\ (w_{m-1} = they, y_m = \mathbf{V}), \\ (w_{m+1} = fish, y_m = \mathbf{V})\}$$
[7.5]

$$f((\boldsymbol{w} = \text{they can fish}, 3), \mathbf{V}) = \{(w_m = \text{fish}, y_m = \mathbf{V}), \\ (w_{m-1} = \text{can}, y_m = \mathbf{V}), \\ (w_{m+1} = \blacksquare, y_m = \mathbf{V})\}.$$
[7.6]

These features contain enough information that a tagger should be able to choose the right tag for the word *fish*: words that come after *can* are likely to be verbs, so the feature (wm−<sup>1</sup> = *can*, y<sup>m</sup> = V) should have a large positive weight.

However, even with this enhanced feature set, it may be difficult to tag some sequences correctly. One reason is that there are often relationships between the tags themselves. For example, in English it is relatively rare for a verb to follow another verb particularly if we differentiate **M**ODAL verbs like *can* and *should* from more typical verbs, like *give*, *transcend*, and *befuddle*. We would like to incorporate preferences against tag sequences like VERB-VERB, and in favor of tag sequences like NOUN-VERB. The need for such preferences is best illustrated by a **garden path sentence**:

#### <span id="page-163-0"></span>(7.2) The old man the boat.

Grammatically, the word *the* is a **D**ETERMINER. When you read the sentence, what part of speech did you first assign to *old*? Typically, this word is an ADJECTIVE — abbreviated as J — which is a class of words that modify nouns. Similarly, *man* is usually a noun. The resulting sequence of tags is D J N D N. But this is a mistaken "garden path" interpretation, which ends up leading nowhere. It is unlikely that a determiner would directly

<span id="page-164-3"></span>follow a noun,<sup>1</sup> and it is particularly unlikely that the entire sentence would lack a verb. The only possible verb in (7.2) is the word *man*, which can refer to the act of maintaining and piloting something — often boats. But if *man* is tagged as a verb, then *old* is seated between a determiner and a verb, and must be a noun. And indeed, adjectives often have a second interpretation as nouns when used in this way (e.g., *the young, the restless*). This reasoning, in which the labeling decisions are intertwined, cannot be applied in a setting where each tag is produced by an independent classification decision.

# <span id="page-164-0"></span>7.2 Sequence labeling as structure prediction

As an alternative, think of the entire sequence of tags as a label itself. For a given sequence of words  $w = (w_1, w_2, ..., w_M)$ , there is a set of possible taggings  $\mathcal{Y}(w) = \mathcal{Y}^M$ , where  $\mathcal{Y} = \{N, V, D, ...\}$  refers to the set of individual tags, and  $\mathcal{Y}^M$  refers to the set of tag sequences of length M. We can then treat the sequence labeling problem as a classification problem in the label space  $\mathcal{Y}(w)$ ,

$$\hat{\boldsymbol{y}} = \operatorname*{argmax}_{\boldsymbol{y} \in \mathcal{Y}(\boldsymbol{w})} \Psi(\boldsymbol{w}, \boldsymbol{y}),$$
[7.7]

where  $\boldsymbol{y} = (y_1, y_2, \dots, y_M)$  is a sequence of M tags, and  $\Psi$  is a scoring function on pairs of sequences,  $V^M \times \mathcal{Y}^M \to \mathbb{R}$ . Such a function can include features that capture the relationships between tagging decisions, such as the preference that determiners not follow nouns, or that all sentences have verbs.

Given that the label space is exponentially large in the length of the sequence M, can it ever be practical to perform tagging in this way? The problem of making a series of interconnected labeling decisions is known as **inference**. Because natural language is full of interrelated grammatical structures, inference is a crucial aspect of natural language processing. In English, it is not unusual to have sentences of length M = 20; part-of-speech tag sets vary in size from 10 to several hundred. Taking the low end of this range, we have  $|\mathcal{Y}(\boldsymbol{w}_{1:M})| \approx 10^{20}$ , one hundred billion billion possible tag sequences. Enumerating and scoring each of these sequences would require an amount of work that is exponential in the sequence length, so inference is intractable.

However, the situation changes when we restrict the scoring function. Suppose we choose a function that decomposes into a sum of local parts,

<span id="page-164-2"></span>
$$\Psi(\boldsymbol{w}, \boldsymbol{y}) = \sum_{m=1}^{M+1} \psi(\boldsymbol{w}, y_m, y_{m-1}, m),$$
[7.8]

where each  $\psi(\cdot)$  scores a local part of the tag sequence. Note that the sum goes up to M+1, so that we can include a score for a special end-of-sequence tag,  $\psi(\boldsymbol{w}_{1:M}, \boldsymbol{\diamond}, y_M, M+1)$ . We also define a special tag to begin the sequence,  $y_0 \triangleq \Diamond$ .

<span id="page-164-1"></span><sup>&</sup>lt;sup>1</sup>The main exception occurs with ditransitive verbs, such as *They gave the winner a trophy*.

<span id="page-165-0"></span>In a linear model, local scoring function can be defined as a dot product of weights and features,

$$\psi(\boldsymbol{w}_{1:M}, y_m, y_{m-1}, m) = \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}, y_m, y_{m-1}, m).$$
[7.9]

The feature vector f can consider the entire input w, and can look at pairs of adjacent tags. This is a step up from per-token classification: the weights can assign low scores to infelicitous tag pairs, such as noun-determiner, and high scores for frequent tag pairs, such as determiner-noun and noun-verb.

In the example *they can fish,* a minimal feature function would include features for word-tag pairs (sometimes called **emission features**) and tag-tag pairs (sometimes called **transition features**):

$$f(w = they \ can \ fish, y = N \ V \ V) = \sum_{m=1}^{M+1} f(w, y_m, y_{m-1}, m)$$

$$= f(w, N, \Diamond, 1)$$

$$+ f(w, V, N, 2)$$

$$+ f(w, V, V, 3)$$

$$+ f(w, \blacklozenge, V, 4)$$

$$= (w_m = they, y_m = N) + (y_m = N, y_{m-1} = \diamondsuit)$$

$$+ (w_m = can, y_m = V) + (y_m = V, y_{m-1} = N)$$

$$+ (w_m = fish, y_m = V) + (y_m = V, y_{m-1} = N)$$

$$+ (y_m = \blacklozenge, y_{m-1} = V).$$

$$= (7.10]$$

There are seven active features for this example: one for each word-tag pair, and one for each tag-tag pair, including a final tag  $y_{M+1} = \blacklozenge$ . These features capture the two main sources of information for part-of-speech tagging in English: which tags are appropriate for each word, and which tags tend to follow each other in sequence. Given appropriate weights for these features, taggers can achieve high accuracy, even for difficult cases like *the old man the boat*. We will now discuss how this restricted scoring function enables efficient inference, through the **Viterbi algorithm** (Viterbi, 1967).

# <span id="page-166-2"></span><span id="page-166-0"></span>7.3 The Viterbi algorithm

By decomposing the scoring function into a sum of local parts, it is possible to rewrite the tagging problem as follows:

$$\hat{\boldsymbol{y}} = \underset{\boldsymbol{y} \in \mathcal{Y}(\boldsymbol{w})}{\operatorname{argmax}} \Psi(\boldsymbol{w}, \boldsymbol{y})$$
[7.13]

$$= \underset{\boldsymbol{y}_{1:M}}{\operatorname{argmax}} \sum_{m=1}^{M+1} \psi(\boldsymbol{w}, y_m, y_{m-1}, m)$$
[7.14]

$$= \underset{\boldsymbol{y}_{1:M}}{\operatorname{argmax}} \sum_{m=1}^{M+1} s_m(y_m, y_{m-1}),$$
[7.15]

where the final line simplifies the notation with the shorthand,

$$s_m(y_m, y_{m-1}) \triangleq \psi(\boldsymbol{w}_{1:M}, y_m, y_{m-1}, m).$$
 [7.16]

This inference problem can be solved efficiently using **dynamic programming**, an algorithmic technique for reusing work in recurrent computations. We begin by solving an auxiliary problem: rather than finding the best tag sequence, we compute the *score* of the best tag sequence,

$$\max_{\boldsymbol{y}_{1:M}} \Psi(\boldsymbol{w}, \boldsymbol{y}_{1:M}) = \max_{\boldsymbol{y}_{1:M}} \sum_{m=1}^{M+1} s_m(y_m, y_{m-1}).$$
[7.17]

This score involves a maximization over all tag sequences of length M, written  $\max_{y_{1:M}}$ . This maximization can be broken into two pieces,

<span id="page-166-1"></span>
$$\max_{\boldsymbol{y}_{1:M}} \Psi(\boldsymbol{w}, \boldsymbol{y}_{1:M}) = \max_{y_M} \max_{\boldsymbol{y}_{1:M-1}} \sum_{m=1}^{M+1} s_m(y_m, y_{m-1}).$$
[7.18]

Within the sum, only the final term  $s_{M+1}(\blacklozenge, y_M)$  depends on  $y_M$ , so we can pull this term out of the second maximization,

$$\max_{\boldsymbol{y}_{1:M}} \Psi(\boldsymbol{w}, \boldsymbol{y}_{1:M}) = \left(\max_{y_M} s_{M+1}(\boldsymbol{\diamond}, y_M)\right) + \left(\max_{\boldsymbol{y}_{1:M-1}} \sum_{m=1}^M s_m(y_m, y_{m-1})\right).$$
[7.19]

The second term in Equation 7.19 has the same form as our original problem, with M replaced by M-1. This indicates that the problem can be reformulated as a recurrence. We do this by defining an auxiliary variable called the **Viterbi variable**  $v_m(k)$ , representing

<span id="page-167-3"></span><span id="page-167-2"></span>**Algorithm 11** The Viterbi algorithm. Each  $s_m(k, k')$  is a local score for tag  $y_m = k$  and  $y_{m-1} = k'$ .

```
for k \in \{0, ..., K\} do

v_1(k) = s_1(k, \Diamond)

for m \in \{2, ..., M\} do

for k \in \{0, ..., K\} do

v_m(k) = \max_{k'} s_m(k, k') + v_{m-1}(k')

b_m(k) = \operatorname{argmax}_{k'} s_m(k, k') + v_{m-1}(k')

y_M = \operatorname{argmax}_k s_{M+1}(\blacklozenge, k) + v_M(k)

for m \in \{M - 1, ..., 1\} do

y_m = b_m(y_{m+1})

return y_{1:M}
```

the score of the best sequence terminating in the tag *k*:

$$v_m(y_m) \triangleq \max_{y_{1:m-1}} \sum_{n=1}^m s_n(y_n, y_{n-1})$$
 [7.20]

$$= \max_{y_{m-1}} s_m(y_m, y_{m-1}) + \max_{y_{1:m-2}} \sum_{n=1}^{m-1} s_n(y_n, y_{n-1})$$
[7.21]

$$= \max_{y_{m-1}} s_m(y_m, y_{m-1}) + v_{m-1}(y_{m-1}).$$
[7.22]

Each set of Viterbi variables is computed from the local score  $s_m(y_m, y_{m-1})$ , and from the previous set of Viterbi variables. The initial condition of the recurrence is simply the score for the first tag,

<span id="page-167-1"></span><span id="page-167-0"></span>
$$v_1(y_1) \triangleq s_1(y_1, \Diamond). \tag{7.23}$$

The maximum overall score for the sequence is then the final Viterbi variable,

$$\max_{\boldsymbol{y}_{1:M}} \Psi(\boldsymbol{w}_{1:M}, \boldsymbol{y}_{1:M}) = v_{M+1}(\boldsymbol{\diamond}).$$
[7.24]

Thus, the score of the best labeling for the sequence can be computed in a single forward sweep: first compute all variables  $v_1(\cdot)$  from Equation 7.23, and then compute all variables  $v_2(\cdot)$  from the recurrence in Equation 7.22, continuing until the final variable  $v_{M+1}(\blacklozenge)$ .

The Viterbi variables can be arranged in a structure known as a **trellis**, shown in Figure 7.1. Each column indexes a token m in the sequence, and each row indexes a tag in  $\mathcal{Y}$ ; every  $v_{m-1}(k)$  is connected to every  $v_m(k')$ , indicating that  $v_m(k')$  is computed from  $v_{m-1}(k)$ . Special nodes are set aside for the start and end states.

<span id="page-168-0"></span>![](_page_168_Figure_1.jpeg)

Figure 7.1: The trellis representation of the Viterbi variables, for the example *they can fish*, using the weights shown in Table 7.1.

The original goal was to find the best scoring sequence, not simply to compute its score. But by solving the auxiliary problem, we are almost there. Recall that each  $v_m(k)$  represents the score of the best tag sequence ending in that tag k in position m. To compute this, we maximize over possible values of  $y_{m-1}$ . By keeping track of the "argmax" tag that maximizes this choice at each step, we can walk backwards from the final tag, and recover the optimal tag sequence. This is indicated in Figure 7.1 by the thick lines, which we trace back from the final position. These backward pointers are written  $b_m(k)$ , indicating the optimal tag  $y_{m-1}$  on the path to  $Y_m = k$ .

The complete Viterbi algorithm is shown in Algorithm 11. When computing the initial Viterbi variables  $v_1(\cdot)$ , the special tag  $\Diamond$  indicates the start of the sequence. When computing the final tag  $Y_M$ , another special tag,  $\blacklozenge$  indicates the end of the sequence. These special tags enable the use of transition features for the tags that begin and end the sequence: for example, conjunctions are unlikely to end sentences in English, so we would like a low score for  $s_{M+1}(\blacklozenge, CC)$ ; nouns are relatively likely to appear at the beginning of sentences, so we would like a high score for  $s_1(N, \Diamond)$ , assuming the noun tag is compatible with the first word token  $w_1$ .

**Complexity** If there are *K* tags and *M* positions in the sequence, then there are  $M \times K$  Viterbi variables to compute. Computing each variable requires finding a maximum over *K* possible predecessor tags. The total time complexity of populating the trellis is therefore  $\mathcal{O}(MK^2)$ , with an additional factor for the number of active features at each position. After completing the trellis, we simply trace the backwards pointers to the beginning of the sequence, which takes  $\mathcal{O}(M)$  operations.

<span id="page-169-1"></span>

|   | .1   |     | <i>C</i> 1 |
|---|------|-----|------------|
|   | they | can | fish       |
| Ν | -2   | -3  | -3         |
| V | -10  | -1  | -3         |

|            | Ν  | V  | •         |
|------------|----|----|-----------|
| $\Diamond$ | -1 | -2 | $-\infty$ |
| Ν          | -3 | -1 | -1        |
| V          | -1 | -3 | -1        |

(a) Weights for emission features.

(b) Weights for transition features. The "from" tags are on the columns, and the "to" tags are on the rows.

Table 7.1: Feature weights for the example trellis shown in Figure 7.1. Emission weights from  $\Diamond$  and  $\blacklozenge$  are implicitly set to  $-\infty$ .

#### <span id="page-169-0"></span>7.3.1 Example

Consider the minimal tagset {N, V}, corresponding to nouns and verbs. Even in this tagset, there is considerable ambiguity: for example, the words *can* and *fish* can each take both tags. Of the  $2 \times 2 \times 2 = 8$  possible taggings for the sentence *they can fish*, four are possible given these possible tags, and two are grammatical.<sup>2</sup>

The values in the trellis in Figure 7.1 are computed from the feature weights defined in Table 7.1. We begin with  $v_1(N)$ , which has only one possible predecessor, the start tag  $\Diamond$ . This score is therefore equal to  $s_1(N, \Diamond) = -2 - 1 = -3$ , which is the sum of the scores for the emission and transition features respectively; the backpointer is  $b_1(N) = \Diamond$ . The score for  $v_1(V)$  is computed in the same way:  $s_1(V, \Diamond) = -10 - 2 = -12$ , and again  $b_1(V) = \Diamond$ . The backpointers are represented in the figure by thick lines.

Things get more interesting at m = 2. The score  $v_2(N)$  is computed by maximizing over the two possible predecessors,

$$v_2(\mathbf{N}) = \max(v_1(\mathbf{N}) + s_2(\mathbf{N}, \mathbf{N}), v_1(\mathbf{V}) + s_2(\mathbf{N}, \mathbf{V}))$$
[7.25]

$$= \max(-3 - 3 - 3, \quad -12 - 3 - 1) = -9$$
 [7.26]

$$b_2(N) = N.$$
 [7.27]

This continues until reaching  $v_4(\blacklozenge)$ , which is computed as,

$$v_4(\blacklozenge) = \max(v_3(N) + s_4(\diamondsuit, N), v_3(V) + s_4(\diamondsuit, V))$$
 [7.28]

$$= \max(-9 + 0 - 1, \quad -11 + 0 - 1)$$
 [7.29]

$$= -10,$$
 [7.30]

so  $b_4(\blacklozenge) = N$ . As there is no emission  $w_4$ , the emission features have scores of zero.

<span id="page-169-2"></span><sup>&</sup>lt;sup>2</sup>The tagging *they*/N *can*/V *fish*/N corresponds to the scenario of putting fish into cans, or perhaps of firing them.

To compute the optimal tag sequence, we walk backwards from here, next checking  $b_3(N) = V$ , and then  $b_2(V) = N$ , and finally  $b_1(N) = \Diamond$ . This yields  $\boldsymbol{y} = (N, V, N)$ , which corresponds to the linguistic interpretation of the fishes being put into cans.

#### <span id="page-170-0"></span>7.3.2 Higher-order features

The Viterbi algorithm was made possible by a restriction of the scoring function to local parts that consider only pairs of adjacent tags. We can think of this as a bigram language model over tags. A natural question is how to generalize Viterbi to tag trigrams, which would involve the following decomposition:

$$\Psi(\boldsymbol{w}, \boldsymbol{y}) = \sum_{m=1}^{M+2} \boldsymbol{f}(\boldsymbol{w}, y_m, y_{m-1}, y_{m-2}, m),$$
[7.31]

where  $y_{-1} = \Diamond$  and  $y_{M+2} = \blacklozenge$ .

One solution is to create a new tagset  $\mathcal{Y}^{(2)}$  from the Cartesian product of the original tagset with itself,  $\mathcal{Y}^{(2)} = \mathcal{Y} \times \mathcal{Y}$ . The tags in this product space are ordered pairs, representing adjacent tags at the token level: for example, the tag (N, V) would represent a noun followed by a verb. Transitions between such tags must be consistent: we can have a transition from (N, V) to (V, N) (corresponding to the tag sequence N V N), but not from (N, V) to (N, N), which would not correspond to any coherent tag sequence. This constraint can be enforced in feature weights, with  $\theta_{((a,b),(c,d))} = -\infty$  if  $b \neq c$ . The remaining feature weights can encode preferences for and against various tag trigrams.

In the Cartesian product tag space, there are  $K^2$  tags, suggesting that the time complexity will increase to  $O(MK^4)$ . However, it is unnecessary to max over predecessor tag bigrams that are incompatible with the current tag bigram. By exploiting this constraint, it is possible to limit the time complexity to  $O(MK^3)$ . The space complexity grows to  $O(MK^2)$ , since the trellis must store all possible predecessors of each tag. In general, the time and space complexity of higher-order Viterbi grows exponentially with the order of the tag *n*-grams that are considered in the feature decomposition.

# <span id="page-170-1"></span>7.4 Hidden Markov Models

The Viterbi sequence labeling algorithm is built on the scores  $s_m(y, y')$ . We will now discuss how these scores can be estimated probabilistically. Recall from § 2.2 that the probabilistic Naïve Bayes classifier selects the label y to maximize  $p(y | x) \propto p(y, x)$ . In probabilistic sequence labeling, our goal is similar: select the tag sequence that maximizes  $p(y | w) \propto p(y, w)$ . The locality restriction in Equation 7.8 can be viewed as a conditional independence assumption on the random variables y.

<span id="page-171-2"></span><span id="page-171-0"></span>Algorithm 12 Generative process for the hidden Markov model

| $y_0 \leftarrow \diamondsuit,  m \leftarrow 1$                |                                             |
|---------------------------------------------------------------|---------------------------------------------|
| repeat                                                        |                                             |
| $y_m \sim \text{Categorical}(\boldsymbol{\lambda}_{y_{m-1}})$ | ▷ sample the current tag                    |
| $w_m \sim \text{Categorical}(\phi_{y_m})$                     | ▷ sample the current word                   |
| until $y_m = \blacklozenge$                                   | terminate when the stop symbol is generated |

Naïve Bayes was introduced as a **generative model** — a probabilistic story that explains the observed data as well as the hidden label. A similar story can be constructed for probabilistic sequence labeling: first, the tags are drawn from a prior distribution; next, the tokens are drawn from a conditional likelihood. However, for inference to be tractable, additional independence assumptions are required. First, the probability of each token depends only on its tag, and not on any other element in the sequence:

$$\mathbf{p}(\boldsymbol{w} \mid \boldsymbol{y}) = \prod_{m=1}^{M} \mathbf{p}(w_m \mid y_m).$$
[7.32]

Second, each tag  $y_m$  depends only on its predecessor,

$$\mathbf{p}(\boldsymbol{y}) = \prod_{m=1}^{M} \mathbf{p}(y_m \mid y_{m-1}),$$
[7.33]

where  $y_0 = \Diamond$  in all cases. Due to this **Markov assumption**, probabilistic sequence labeling models are known as **hidden Markov models** (HMMs).

The generative process for the hidden Markov model is shown in Algorithm 12. Given the parameters  $\lambda$  and  $\phi$ , we can compute p(w, y) for any token sequence w and tag sequence y. The HMM is often represented as a **graphical model** (Wainwright and Jordan, 2008), as shown in Figure 7.2. This representation makes the independence assumptions explicit: if a variable  $v_1$  is probabilistically conditioned on another variable  $v_2$ , then there is an arrow  $v_2 \rightarrow v_1$  in the diagram. If there are no arrows between  $v_1$  and  $v_2$ , they are **conditionally independent**, given each variable's **Markov blanket**. In the hidden Markov model, the Markov blanket for each tag  $y_m$  includes the "parent"  $y_{m-1}$ , and the "children"  $y_{m+1}$  and  $w_m$ .<sup>3</sup>

It is important to reflect on the implications of the HMM independence assumptions. A non-adjacent pair of tags  $y_m$  and  $y_n$  are conditionally independent; if m < n and we are given  $y_{n-1}$ , then  $y_m$  offers no additional information about  $y_n$ . However, if we are not given any information about the tags in a sequence, then all tags are probabilistically coupled.

<span id="page-171-1"></span><sup>&</sup>lt;sup>3</sup>In general graphical models, a variable's Markov blanket includes its parents, children, and its children's other parents (Murphy, 2012).

![](_page_172_Figure_1.jpeg)

<span id="page-172-2"></span>Figure 7.2: Graphical representation of the hidden Markov model. Arrows indicate probabilistic dependencies.

#### <span id="page-172-0"></span>**7.4.1 Estimation**

The hidden Markov model has two groups of parameters:

- **Emission probabilities.** The probability p<sup>e</sup> (w<sup>m</sup> | ym; φ) is the emission probability, since the words are treated as probabilistically "emitted", conditioned on the tags.
- **Transition probabilities.** The probability p<sup>t</sup> (y<sup>m</sup> | ym−1; λ) is the transition probability, since it assigns probability to each possible tag-to-tag transition.

Both of these groups of parameters are typically computed from smoothed relative frequency estimation on a labeled corpus (see § [6.2](#page-146-0) for a review of smoothing). The unsmoothed probabilities are,

$$\phi_{k,i} \triangleq \Pr(W_m = i \mid Y_m = k) = \frac{\operatorname{count}(W_m = i, Y_m = k)}{\operatorname{count}(Y_m = k)}$$
$$\lambda_{k,k'} \triangleq \Pr(Y_m = k' \mid Y_{m-1} = k) = \frac{\operatorname{count}(Y_m = k', Y_{m-1} = k)}{\operatorname{count}(Y_{m-1} = k)}.$$

Smoothing is more important for the emission probability than the transition probability, because the vocabulary is much larger than the number of tags.

#### <span id="page-172-1"></span>**7.4.2 Inference**

The goal of inference in the hidden Markov model is to find the highest probability tag sequence,

$$\hat{\boldsymbol{y}} = \operatorname*{argmax}_{\boldsymbol{y}} p(\boldsymbol{y} \mid \boldsymbol{w}).$$
[7.34]

As in Na¨ıve Bayes, it is equivalent to find the tag sequence with the highest *log*-probability, since the logarithm is a monotonically increasing function. It is furthermore equivalent to maximize the joint probability p(y, w) = p(y | w) × p(w) ∝ p(y | w), which is proportional to the conditional probability. Putting these observations together, the inference

problem can be reformulated as,

$$\hat{\boldsymbol{y}} = \operatorname*{argmax}_{\boldsymbol{y}} \log p(\boldsymbol{y}, \boldsymbol{w}).$$
[7.35]

We can now apply the HMM independence assumptions:

$$\log p(\boldsymbol{y}, \boldsymbol{w}) = \log p(\boldsymbol{y}) + \log p(\boldsymbol{w} \mid \boldsymbol{y})$$

$$M+1$$
[7.36]

$$= \sum_{m=1}^{M+1} \log \mathbf{p}_{Y}(y_{m} \mid y_{m-1}) + \log \mathbf{p}_{W|Y}(w_{m} \mid y_{m})$$
[7.37]

$$=\sum_{m=1}^{M+1} \log \lambda_{y_m, y_{m-1}} + \log \phi_{y_m, w_m}$$
[7.38]

$$=\sum_{m=1}^{M+1} s_m(y_m, y_{m-1}),$$
[7.39]

where,

$$s_m(y_m, y_{m-1}) \triangleq \log \lambda_{y_m, y_{m-1}} + \log \phi_{y_m, w_m}, \qquad [7.40]$$

and,

$$\phi_{\blacklozenge,w} = \begin{cases} 1, & w = \blacksquare \\ 0, & \text{otherwise,} \end{cases}$$
[7.41]

which ensures that the stop tag  $\blacklozenge$  can only be applied to the final token  $\blacksquare$ .

This derivation shows that HMM inference can be viewed as an application of the Viterbi decoding algorithm, given an appropriately defined scoring function. The local score  $s_m(y_m, y_{m-1})$  can be interpreted probabilistically,

$$s_m(y_m, y_{m-1}) = \log p_y(y_m \mid y_{m-1}) + \log p_{w|y}(w_m \mid y_m)$$
[7.42]

$$= \log p(y_m, w_m \mid y_{m-1}).$$
 [7.43]

Now recall the definition of the Viterbi variables,

$$v_m(y_m) = \max_{y_{m-1}} s_m(y_m, y_{m-1}) + v_{m-1}(y_{m-1})$$
[7.44]

$$= \max_{y_{m-1}} \log p(y_m, w_m \mid y_{m-1}) + v_{m-1}(y_{m-1}).$$
[7.45]

By setting  $v_{m-1}(y_{m-1}) = \max_{\boldsymbol{y}_{1:m-2}} \log p(\boldsymbol{y}_{1:m-1}, \boldsymbol{w}_{1:m-1})$ , we obtain the recurrence,

$$v_m(y_m) = \max_{y_{m-1}} \log p(y_m, w_m \mid y_{m-1}) + \max_{\boldsymbol{y}_{1:m-2}} \log p(\boldsymbol{y}_{1:m-1}, \boldsymbol{w}_{1:m-1})$$
[7.46]

$$= \max_{\boldsymbol{y}_{1:m-1}} \log p(y_m, w_m \mid y_{m-1}) + \log p(\boldsymbol{y}_{1:m-1}, \boldsymbol{w}_{1:m-1})$$
[7.47]

$$= \max_{y_{1:m-1}} \log p(y_{1:m}, w_{1:m}).$$
 [7.48]

<span id="page-174-2"></span>In words, the Viterbi variable vm(ym) is the log probability of the best tag sequence ending in ym, joint with the word sequence w1:m. The log probability of the best complete tag sequence is therefore,

$$\max_{\boldsymbol{y}_{1:M}} \log p(\boldsymbol{y}_{1:M+1}, \boldsymbol{w}_{1:M+1}) = v_{M+1}(\boldsymbol{\diamond})$$
[7.49]

**\*Viterbi as an example of the max-product algorithm** The Viterbi algorithm can also be implemented using probabilities, rather than log-probabilities. In this case, each vm(ym) is equal to,

$$v_m(y_m) = \max_{y_{1:m-1}} p(y_{1:m-1}, y_m, w_{1:m})$$
[7.50]

$$= \max_{y_{m-1}} p(y_m, w_m \mid y_{m-1}) \times \max_{y_{1:m-2}} p(y_{1:m-2}, y_{m-1}, w_{1:m-1})$$
[7.51]

$$= \max_{y_{m-1}} \mathbf{p}(y_m, w_m \mid y_{m-1}) \times v_{m-1}(y_{m-1})$$
[7.52]

<span id="page-174-1"></span>
$$= \mathbf{p}_{w|y}(w_m \mid y_m) \times \max_{y_{m-1}} \mathbf{p}_y(y_m \mid y_{m-1}) \times v_{m-1}(y_{m-1}).$$
 [7.53]

Each Viterbi variable is computed by *maximizing* over a set of *products*. Thus, the Viterbi algorithm is a special case of the **max-product algorithm** for inference in graphical models [\(Wainwright and Jordan, 2008\)](#page-568-4). However, the product of probabilities tends towards zero over long sequences, so the log-probability version of Viterbi is recommended in practical implementations.

# <span id="page-174-0"></span>**7.5 Discriminative sequence labeling with features**

Today, hidden Markov models are rarely used for supervised sequence labeling. This is because HMMs are limited to only two phenomena:

- word-tag compatibility, via the emission probability pW|<sup>Y</sup> (w<sup>m</sup> | ym);
- local context, via the transition probability p<sup>Y</sup> (y<sup>m</sup> | ym−1).

The Viterbi algorithm permits the inclusion of richer information in the local scoring function ψ(w1:M, ym, ym−1, m), which can be defined as a weighted sum of arbitrary local *features*,

$$\psi(\boldsymbol{w}, y_m, y_{m-1}, m) = \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}, y_m, y_{m-1}, m),$$
[7.54]

where f is a locally-defined feature function, and θ is a vector of weights.

<span id="page-175-1"></span>The local decomposition of the scoring function  $\Psi$  is reflected in a corresponding decomposition of the feature function:

$$\Psi(\boldsymbol{w}, \boldsymbol{y}) = \sum_{m=1}^{M+1} \psi(\boldsymbol{w}, y_m, y_{m-1}, m)$$
[7.55]

$$=\sum_{m=1}^{M+1} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}, y_m, y_{m-1}, m)$$
[7.56]

$$=\boldsymbol{\theta} \cdot \sum_{m=1}^{M+1} \boldsymbol{f}(\boldsymbol{w}, y_m, y_{m-1}, m)$$
[7.57]

$$=\boldsymbol{\theta} \cdot \boldsymbol{f}^{(\text{global})}(\boldsymbol{w}, \boldsymbol{y}_{1:M}), \qquad [7.58]$$

where  $f^{(\text{global})}(w, y)$  is a global feature vector, which is a sum of local feature vectors,

$$\boldsymbol{f}^{(\text{global})}(\boldsymbol{w}, \boldsymbol{y}) = \sum_{m=1}^{M+1} \boldsymbol{f}(\boldsymbol{w}_{1:M}, y_m, y_{m-1}, m), \quad [7.59]$$

with  $y_{M+1} = \blacklozenge$  and  $y_0 = \diamondsuit$  by construction.

Let's now consider what additional information these features might encode.

**Word affix features.** Consider the problem of part-of-speech tagging on the first four lines of the poem *Jabberwocky* (Carroll, 1917):

(7.3) 'Twas brillig, and the slithy toves Did gyre and gimble in the wabe: All mimsy were the borogoves, And the mome raths outgrabe.

Many of these words were made up by the author of the poem, so a corpus would offer no information about their probabilities of being associated with any particular part of speech. Yet it is not so hard to see what their grammatical roles might be in this passage. Context helps: for example, the word *slithy* follows the determiner *the*, so it is probably a noun or adjective. Which do you think is more likely? The suffix *-thy* is found in a number of adjectives, like *frothy*, *healthy*, *pithy*, *worthy*. It is also found in a handful of nouns — e.g., *apathy*, *sympathy* — but nearly all of these have the longer coda *-pathy*, unlike *slithy*. So the suffix gives some evidence that *slithy* is an adjective, and indeed it is: later in the text we find that it is a combination of the adjectives *lithe* and *slimy*.<sup>4</sup>

<span id="page-175-0"></span><sup>&</sup>lt;sup>4</sup>**Morphology** is the study of how words are formed from smaller linguistic units. chapter 9 touches on computational approaches to morphological analysis. See Bender (2013) for an overview of the underlying linguistic principles, and Haspelmath and Sims (2013) or Lieber (2015) for a full treatment.

<span id="page-176-1"></span>**Fine-grained context.** The hidden Markov model captures contextual information in the form of part-of-speech tag bigrams. But sometimes, the necessary contextual information is more specific. Consider the noun phrases *this fish* and *these fish*. Many part-of-speech tagsets distinguish between singular and plural nouns, but do not distinguish between singular and plural determiners; for example, the well known **Penn Treebank** tagset follows these conventions. A hidden Markov model would be unable to correctly label *fish* as singular or plural in both of these cases, because it only has access to two features: the preceding tag (determiner in both cases) and the word (*fish* in both cases). The classificationbased tagger discussed in § [7.1](#page-162-1) had the ability to use preceding and succeeding words as features, and it can also be incorporated into a Viterbi-based sequence labeler as a local feature.

**Example.** Consider the tagging D J N (determiner, adjective, noun) for the sequence *the slithy toves*, so that

$$w =$$
the slithy toves  
 $y =$ D J N.

Let's create the feature vector for this example, assuming that we have word-tag features (indicated by W), tag-tag features (indicated by T), and suffix features (indicated by M). You can assume that you have access to a method for extracting the suffix *-thy* from *slithy*, *-es* from *toves*, and ∅ from *the*, indicating that this word has no suffix.[5](#page-176-0) The resulting feature vector is,

$$\begin{aligned} \boldsymbol{f}(\text{the slithy toves, D J N}) = & \boldsymbol{f}(\text{the slithy toves, D}, \Diamond, 1) \\ &+ \boldsymbol{f}(\text{the slithy toves, J, D, 2}) \\ &+ \boldsymbol{f}(\text{the slithy toves, N, J, 3}) \\ &+ \boldsymbol{f}(\text{the slithy toves, } \blacklozenge, N, 4) \\ = & \{(T : \Diamond, D), (W : \text{the, D}), (M : \varnothing, D), \\ &(T : D, J), (W : \text{slithy, J}), (M : \text{-thy, J}), \\ &(T : J, N), (W : \text{toves, N}), (M : \text{-es, N}) \\ &(T : N, \blacklozenge) \}. \end{aligned}$$

These examples show that local features can incorporate information that lies beyond the scope of a hidden Markov model. Because the features are local, it is possible to apply the Viterbi algorithm to identify the optimal sequence of tags. The remaining question

<span id="page-176-0"></span><sup>5</sup> Such a system is called a **morphological segmenter**. The task of morphological segmentation is briefly described in § [9.1.4;](#page-221-0) a well known segmenter is MORFESSOR [\(Creutz and Lagus, 2007\)](#page-522-6). In real applications, a typical approach is to include features for all orthographic suffixes up to some maximum number of characters: for *slithy*, we would have suffix features for *-y*, *-hy*, and *-thy*.

<span id="page-177-2"></span>is how to estimate the weights on these features. § 2.3 presented three main types of discriminative classifiers: perceptron, support vector machine, and logistic regression. Each of these classifiers has a structured equivalent, enabling it to be trained from labeled sequences rather than individual tokens.

#### <span id="page-177-0"></span>7.5.1 Structured perceptron

The perceptron classifier is trained by increasing the weights for features that are associated with the correct label, and decreasing the weights for features that are associated with incorrectly predicted labels:

$$\hat{y} = \operatorname*{argmax}_{y \in \mathcal{Y}} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}, y)$$
[7.60]

$$\boldsymbol{\theta}^{(t+1)} \leftarrow \boldsymbol{\theta}^{(t)} + \boldsymbol{f}(\boldsymbol{x}, y) - \boldsymbol{f}(\boldsymbol{x}, \hat{y}).$$
 [7.61]

We can apply exactly the same update in the case of structure prediction,

$$\hat{\boldsymbol{y}} = \operatorname*{argmax}_{\boldsymbol{y} \in \mathcal{Y}(\boldsymbol{w})} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}, \boldsymbol{y})$$
[7.62]

$$\boldsymbol{\theta}^{(t+1)} \leftarrow \boldsymbol{\theta}^{(t)} + \boldsymbol{f}(\boldsymbol{w}, \boldsymbol{y}) - \boldsymbol{f}(\boldsymbol{w}, \hat{\boldsymbol{y}}).$$
 [7.63]

This learning algorithm is called **structured perceptron**, because it learns to predict the structured output y. The only difference is that instead of computing  $\hat{y}$  by enumerating the entire set  $\mathcal{Y}$ , the Viterbi algorithm is used to efficiently search the set of possible taggings,  $\mathcal{Y}^M$ . Structured perceptron can be applied to other structured outputs as long as efficient inference is possible. As in perceptron classification, weight averaging is crucial to get good performance (see § 2.3.2).

**Example** For the example *they can fish*, suppose that the reference tag sequence is  $y^{(i)} =$  N V V, but the tagger incorrectly returns the tag sequence  $\hat{y} =$  N V N. Assuming a model with features for emissions  $(w_m, y_m)$  and transitions  $(y_{m-1}, y_m)$ , the corresponding structured perceptron update is:

$$\theta_{(fish,V)} \leftarrow \theta_{(fish,V)} + 1, \qquad \theta_{(fish,N)} \leftarrow \theta_{(fish,N)} - 1$$
[7.64]

$$\theta_{(V,V)} \leftarrow \theta_{(V,V)} + 1, \qquad \theta_{(V,N)} \leftarrow \theta_{(V,N)} - 1$$
[7.65]

$$\theta_{(\mathbf{V}, \blacklozenge)} \leftarrow \theta_{(\mathbf{V}, \blacklozenge)} + 1, \qquad \theta_{(\mathbf{N}, \blacklozenge)} \leftarrow \theta_{(\mathbf{N}, \blacklozenge)} - 1.$$
 [7.66]

#### <span id="page-177-1"></span>7.5.2 Structured support vector machines

Large-margin classifiers such as the support vector machine improve on the perceptron by pushing the classification boundary away from the training instances. The same idea can

<span id="page-178-2"></span>be applied to sequence labeling. A support vector machine in which the output is a structured object, such as a sequence, is called a **structured support vector machine** [\(Tsochan](#page-566-1)[taridis et al., 2004\)](#page-566-1).[6](#page-178-0)

In classification, we formalized the large-margin constraint as,

<span id="page-178-1"></span>
$$\forall y \neq y^{(i)}, \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}, y^{(i)}) - \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}, y) \ge 1,$$
[7.67]

requiring a margin of at least 1 between the scores for all labels y that are not equal to the correct label y (i) . The weights θ are then learned by constrained optimization (see § [2.4.2\)](#page-49-0).

This idea can be applied to sequence labeling by formulating an equivalent set of constraints for all possible labelings Y(w) for an input w. However, there are two problems. First, in sequence labeling, some predictions are more wrong than others: we may miss only one tag out of fifty, or we may get all fifty wrong. We would like our learning algorithm to be sensitive to this difference. Second, the number of constraints is equal to the number of possible labelings, which is exponentially large in the length of the sequence.

The first problem can be addressed by adjusting the constraint to require larger margins for more serious errors. Let c(y (i) , yˆ) ≥ 0 represent the *cost* of predicting label yˆ when the true label is y (i) . We can then generalize the margin constraint,

$$\forall \boldsymbol{y}, \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}^{(i)}, \boldsymbol{y}^{(i)}) - \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}^{(i)}, \boldsymbol{y}) \ge c(\boldsymbol{y}^{(i)}, \boldsymbol{y}).$$
[7.68]

This cost-augmented margin constraint specializes to the constraint in [Equation 7.67](#page-178-1) if we choose the delta function c(y (i) , y) = δ (() y (i) <sup>6</sup><sup>=</sup> <sup>y</sup>). A more expressive cost function is the **Hamming cost**,

$$c(\boldsymbol{y}^{(i)}, \boldsymbol{y}) = \sum_{m=1}^{M} \delta(y_m^{(i)} \neq y_m),$$
[7.69]

which computes the number of errors in y. By incorporating the cost function as the margin constraint, we require that the true labeling be seperated from the alternatives by a margin that is proportional to the number of incorrect tags in each alternative labeling.

The second problem is that the number of constraints is exponential in the length of the sequence. This can be addressed by focusing on the prediction yˆ that *maximally* violates the margin constraint. This prediction can be identified by solving the following **cost-augmented decoding** problem:

$$\hat{\boldsymbol{y}} = \operatorname*{argmax}_{\boldsymbol{y}\neq\boldsymbol{y}^{(i)}} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}^{(i)}, \boldsymbol{y}) - \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}^{(i)}, \boldsymbol{y}^{(i)}) + c(\boldsymbol{y}^{(i)}, \boldsymbol{y})$$
[7.70]

$$= \underset{\boldsymbol{y}\neq\boldsymbol{y}^{(i)}}{\operatorname{argmax}} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}^{(i)}, \boldsymbol{y}) + c(\boldsymbol{y}^{(i)}, \boldsymbol{y}),$$
[7.71]

<span id="page-178-0"></span><sup>6</sup>This model is also known as a **max-margin Markov network** [\(Taskar et al., 2003\)](#page-565-4), emphasizing that the scoring function is constructed from a sum of components, which are Markov independent.

<span id="page-179-3"></span>where in the second line we drop the term <sup>θ</sup> · <sup>f</sup>(w(i) , y (i) ), which is constant in y.

We can now reformulate the margin constraint for sequence labeling,

$$\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}^{(i)}, \boldsymbol{y}^{(i)}) - \max_{\boldsymbol{y} \in \mathcal{Y}(\boldsymbol{w})} \left( \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}^{(i)}, \boldsymbol{y}) + c(\boldsymbol{y}^{(i)}, \boldsymbol{y}) \right) \ge 0.$$
[7.72]

If the score for <sup>θ</sup> · <sup>f</sup>(w(i) , y (i) ) is greater than the cost-augmented score for all alternatives, then the constraint will be met. The name "cost-augmented decoding" is due to the fact that the objective includes the standard decoding problem, maxyˆ∈Y(w) θ · f(w, yˆ), plus an additional term for the cost. Essentially, we want to train against predictions that are strong and wrong: they should score highly according to the model, yet incur a large loss with respect to the ground truth. Training adjusts the weights to reduce the score of these predictions.

For cost-augmented decoding to be tractable, the cost function must decompose into local parts, just as the feature function f(·) does. The Hamming cost, defined above, obeys this property. To perform cost-augmented decoding using the Hamming cost, we need only to add features fm(ym) = δ(y<sup>m</sup> 6= y (i) <sup>m</sup> ), and assign a constant weight of 1 to these features. Decoding can then be performed using the Viterbi algorithm.[7](#page-179-1)

As with large-margin classifiers, it is possible to formulate the learning problem in an unconstrained form, by combining a regularization term on the weights and a Lagrangian for the constraints:

$$\min_{\boldsymbol{\theta}} \quad \frac{1}{2} ||\boldsymbol{\theta}||_2^2 - C\left(\sum_i \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}^{(i)}, \boldsymbol{y}^{(i)}) - \max_{\boldsymbol{y} \in \mathcal{Y}(\boldsymbol{w}^{(i)})} \left[\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}^{(i)}, \boldsymbol{y}) + c(\boldsymbol{y}^{(i)}, \boldsymbol{y})\right]\right), \quad [7.73]$$

In this formulation, C is a parameter that controls the tradeoff between the regularization term and the margin constraints. A number of optimization algorithms have been proposed for structured support vector machines, some of which are discussed in § [2.4.2.](#page-49-0) An empirical comparison by [Kummerfeld et al.](#page-541-2) [\(2015\)](#page-541-2) shows that stochastic subgradient descent — which is essentially a cost-augmented version of the structured perceptron is highly competitive.

### <span id="page-179-0"></span>**7.5.3 Conditional random fields**

The **conditional random field** (CRF; [Lafferty et al., 2001\)](#page-541-3) is a conditional probabilistic model for sequence labeling; just as structured perceptron is built on the perceptron classifier, conditional random fields are built on the logistic regression classifier.[8](#page-179-2) The basic

<span id="page-179-1"></span><sup>7</sup>Are there cost functions that do not decompose into local parts? Suppose we want to assign a constant loss c to any prediction yˆ in which k or more predicted tags are incorrect, and zero loss otherwise. This loss function is combinatorial over the predictions, and thus we cannot decompose it into parts.

<span id="page-179-2"></span><sup>8</sup>The name "conditional random field" is derived from **Markov random fields**, a general class of models in which the probability of a configuration of variables is proportional to a product of scores across pairs (or

<span id="page-180-2"></span>probability model is,

<span id="page-180-1"></span>
$$p(\boldsymbol{y} \mid \boldsymbol{w}) = \frac{\exp(\Psi(\boldsymbol{w}, \boldsymbol{y}))}{\sum_{\boldsymbol{y}' \in \mathcal{Y}(\boldsymbol{w})} \exp(\Psi(\boldsymbol{w}, \boldsymbol{y}'))}.$$
[7.74]

This is almost identical to logistic regression (§ 2.5), but because the label space is now sequences of tags, we require efficient algorithms for both **decoding** (searching for the best tag sequence given a sequence of words w and a model  $\theta$ ) and for **normalization** (summing over all tag sequences). These algorithms will be based on the usual locality assumption on the scoring function,  $\Psi(w, y) = \sum_{m=1}^{M+1} \psi(w, y_m, y_{m-1}, m)$ .

#### **Decoding in CRFs**

Decoding — finding the tag sequence  $\hat{y}$  that maximizes  $p(y \mid w)$  — is a direct application of the Viterbi algorithm. The key observation is that the decoding problem does not depend on the denominator of  $p(y \mid w)$ ,

$$\hat{\boldsymbol{y}} = \underset{\boldsymbol{y}}{\operatorname{argmax}} \log p(\boldsymbol{y} \mid \boldsymbol{w})$$

$$= \underset{\boldsymbol{y}}{\operatorname{argmax}} \Psi(\boldsymbol{y}, \boldsymbol{w}) - \log \sum_{\boldsymbol{y}' \in \mathcal{Y}(\boldsymbol{w})} \exp \Psi(\boldsymbol{y}', \boldsymbol{w})$$

$$= \underset{\boldsymbol{y}}{\operatorname{argmax}} \Psi(\boldsymbol{y}, \boldsymbol{w}) = \underset{\boldsymbol{y}}{\operatorname{argmax}} \sum_{m=1}^{M+1} s_m(y_m, y_{m-1}).$$

This is identical to the decoding problem for structured perceptron, so the same Viterbi recurrence as defined in Equation 7.22 can be used.

#### Learning in CRFs

As with logistic regression, the weights  $\theta$  are learned by minimizing the regularized negative log-probability,

$$\ell = \frac{\lambda}{2} ||\boldsymbol{\theta}||^2 - \sum_{i=1}^{N} \log p(\boldsymbol{y}^{(i)} \mid \boldsymbol{w}^{(i)}; \boldsymbol{\theta})$$
[7.75]

<span id="page-180-0"></span>
$$= \frac{\lambda}{2} ||\boldsymbol{\theta}||^2 - \sum_{i=1}^{N} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}^{(i)}, \boldsymbol{y}^{(i)}) + \log \sum_{\boldsymbol{y}' \in \mathcal{Y}(\boldsymbol{w}^{(i)})} \exp\left(\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}^{(i)}, \boldsymbol{y}')\right), \quad [7.76]$$

more generally, cliques) of variables in a **factor graph**. In sequence labeling, the pairs of variables include all adjacent tags  $(y_m, y_{m-1})$ . The probability is *conditioned* on the words w, which are always observed, motivating the term "conditional" in the name.

<span id="page-181-3"></span>where λ controls the amount of regularization. The final term in [Equation 7.76](#page-180-0) is a sum over all possible labelings. This term is the log of the denominator in [Equation 7.74,](#page-180-1) sometimes known as the **partition function**. [9](#page-181-0) There are |Y|<sup>M</sup> possible labelings of an input of size M, so we must again exploit the decomposition of the scoring function to compute this sum efficiently.

The sum P <sup>y</sup>∈Yw(i) exp Ψ(y, w) can be computed efficiently using the **forward recurrence**, which is closely related to the Viterbi recurrence. We first define a set of **forward variables**, αm(ym), which is equal to the sum of the scores of all paths leading to tag y<sup>m</sup> at position m:

$$\alpha_m(y_m) \triangleq \sum_{\boldsymbol{y}_{1:m-1}} \exp \sum_{n=1}^m s_n(y_n, y_{n-1})$$
[7.77]

<span id="page-181-1"></span>
$$= \sum_{\boldsymbol{y}_{1:m-1}} \prod_{n=1}^{m} \exp s_n(y_n, y_{n-1}).$$
 [7.78]

Note the similarity to the definition of the Viterbi variable, vm(ym) = maxy1:m−<sup>1</sup> P<sup>m</sup> <sup>n</sup>=1 sn(yn, yn−1). In the hidden Markov model, the Viterbi recurrence had an alternative interpretation as the max-product algorithm (see [Equation 7.53\)](#page-174-1); analogously, the forward recurrence is known as the **sum-product algorithm**, because of the form of [\[7.78\]](#page-181-1). The forward variable can also be computed through a recurrence:

$$\alpha_m(y_m) = \sum_{y_{1:m-1}} \prod_{n=1}^m \exp s_n(y_n, y_{n-1})$$
[7.79]

$$= \sum_{y_{m-1}} (\exp s_m(y_m, y_{m-1})) \sum_{y_{1:m-2}} \prod_{n=1}^{m-1} \exp s_n(y_n, y_{n-1})$$
 [7.80]

<span id="page-181-2"></span>
$$= \sum_{y_{m-1}} (\exp s_m(y_m, y_{m-1})) \times \alpha_{m-1}(y_{m-1}).$$
 [7.81]

Using the forward recurrence, it is possible to compute the denominator of the conditional probability,

$$\sum_{\boldsymbol{y}\in\mathcal{Y}(\boldsymbol{w})}\Psi(\boldsymbol{w},\boldsymbol{y}) = \sum_{\boldsymbol{y}_{1:M}}(\exp s_{M+1}(\boldsymbol{\diamond},y_M))\prod_{m=1}^M\exp s_m(y_m,y_{m-1})$$
[7.82]

$$=\alpha_{M+1}(\blacklozenge).$$
[7.83]

<span id="page-181-0"></span><sup>9</sup>The terminology of "potentials" and "partition functions" comes from statistical mechanics [\(Bishop,](#page-514-9) [2006\)](#page-514-9).

<span id="page-182-3"></span>The conditional log-likelihood can be rewritten,

<span id="page-182-0"></span>
$$\ell = \frac{\lambda}{2} ||\boldsymbol{\theta}||^2 - \sum_{i=1}^{N} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}^{(i)}, \boldsymbol{y}^{(i)}) + \log \alpha_{M+1}(\boldsymbol{\diamond}).$$
 [7.84]

Probabilistic programming environments, such as TORCH (Collobert et al., 2011) and DYNET (Neubig et al., 2017), can compute the gradient of this objective using automatic differentiation. The programmer need only implement the forward algorithm as a computation graph.

As in logistic regression, the gradient of the likelihood with respect to the parameters is a difference between observed and expected feature counts:

$$\frac{d\ell}{d\theta_j} = \lambda \theta_j + \sum_{i=1}^N E[f_j(\boldsymbol{w}^{(i)}, \boldsymbol{y})] - f_j(\boldsymbol{w}^{(i)}, \boldsymbol{y}^{(i)}), \qquad [7.85]$$

where  $f_j(\boldsymbol{w}^{(i)}, \boldsymbol{y}^{(i)})$  refers to the count of feature *j* for token sequence  $\boldsymbol{w}^{(i)}$  and tag sequence  $\boldsymbol{y}^{(i)}$ . The expected feature counts are computed "under the hood" when automatic differentiation is applied to Equation 7.84 (Eisner, 2016).

Before the widespread use of automatic differentiation, it was common to compute the feature expectations from marginal tag probabilities  $p(y_m | w)$ . These marginal probabilities are sometimes useful on their own, and can be computed using the **forwardbackward algorithm**. This algorithm combines the forward recurrence with an equivalent **backward recurrence**, which traverses the input from  $w_M$  back to  $w_1$ .

#### <span id="page-182-2"></span>\*Forward-backward algorithm

Marginal probabilities over tag bigrams can be written as,<sup>10</sup>

$$\Pr(Y_{m-1} = k', Y_m = k \mid \boldsymbol{w}) = \frac{\sum_{\boldsymbol{y}: Y_m = k, Y_{m-1} = k'} \prod_{n=1}^M \exp s_n(y_n, y_{n-1})}{\sum_{\boldsymbol{y}'} \prod_{n=1}^M \exp s_n(y'_n, y'_{n-1})}.$$
[7.86]

The numerator sums over all tag sequences that include the transition  $(Y_{m-1} = k') \rightarrow (Y_m = k)$ . Because we are only interested in sequences that include the tag bigram, this sum can be decomposed into three parts: the *prefixes*  $y_{1:m-1}$ , terminating in  $Y_{m-1} = k'$ ; the

<span id="page-182-1"></span><sup>&</sup>lt;sup>10</sup>Recall the notational convention of upper-case letters for random variables, e.g.  $Y_m$ , and lower case letters for specific values, e.g.,  $y_m$ , so that  $Y_m = k$  is interpreted as the event of random variable  $Y_m$  taking the value k.

<span id="page-183-2"></span><span id="page-183-1"></span>![](_page_183_Figure_1.jpeg)

Figure 7.3: A schematic illustration of the computation of the marginal probability  $Pr(Y_{m-1} = k', Y_m = k)$ , using the forward score  $\alpha_{m-1}(k')$  and the backward score  $\beta_m(k)$ .

transition  $(Y_{m-1} = k') \rightarrow (Y_m = k)$ ; and the *suffixes*  $y_{m:M}$ , beginning with the tag  $Y_m = k$ :

$$\sum_{\boldsymbol{y}:Y_m=k,Y_{m-1}=k'} \prod_{n=1}^{M} \exp s_n(y_n, y_{n-1}) = \sum_{\boldsymbol{y}_{1:m-1}:Y_{m-1}=k'} \prod_{n=1}^{m-1} \exp s_n(y_n, y_{n-1}) \times \exp s_m(k, k') \times \sum_{\boldsymbol{y}_{m:M}:Y_m=k} \prod_{n=m+1}^{M+1} \exp s_n(y_n, y_{n-1}). \quad [7.87]$$

The result is product of three terms: a score that sums over all the ways to get to the position  $(Y_{m-1} = k')$ , a score for the transition from k' to k, and a score that sums over all the ways of finishing the sequence from  $(Y_m = k)$ . The first term of Equation 7.87 is equal to the **forward variable**,  $\alpha_{m-1}(k')$ . The third term — the sum over ways to finish the sequence — can also be defined recursively, this time moving over the trellis from right to left, which is known as the **backward recurrence**:

$$\beta_m(k) \triangleq \sum_{\boldsymbol{y}_{m:M}:Y_m=k} \prod_{n=m}^{M+1} \exp s_n(y_n, y_{n-1})$$
[7.88]

<span id="page-183-0"></span>
$$= \sum_{k' \in \mathcal{Y}} \exp s_{m+1}(k',k) \sum_{\boldsymbol{y}_{m+1:M}: Y_m = k'} \prod_{n=m+1}^{M+1} \exp s_n(y_n, y_{n-1})$$
[7.89]

$$= \sum_{k' \in \mathcal{V}} \exp s_{m+1}(k', k) \times \beta_{m+1}(k').$$
[7.90]

To understand this computation, compare with the forward recurrence in Equation 7.81.

<span id="page-184-3"></span>In practice, numerical stability demands that we work in the log domain,

$$\log \alpha_m(k) = \log \sum_{k' \in \mathcal{Y}} \exp\left(\log s_m(k, k') + \log \alpha_{m-1}(k')\right)$$
[7.91]

$$\log \beta_{m-1}(k) = \log \sum_{k' \in \mathcal{Y}} \exp\left(\log s_m(k',k) + \log \beta_m(k')\right).$$
[7.92]

The application of the forward and backward probabilities is shown in Figure 7.3. Both the forward and backward recurrences operate on the trellis, which implies a space complexity  $\mathcal{O}(MK)$ . Because both recurrences require computing a sum over K terms at each node in the trellis, their time complexity is  $\mathcal{O}(MK^2)$ .

# <span id="page-184-0"></span>7.6 Neural sequence labeling

In neural network approaches to sequence labeling, we construct a vector representation for each tagging decision, based on the word and its context. Neural networks can perform tagging as a per-token classification decision, or they can be combined with the Viterbi algorithm to tag the entire sequence globally.

#### <span id="page-184-1"></span>7.6.1 Recurrent neural networks

Recurrent neural networks (RNNs) were introduced in chapter 6 as a language modeling technique, in which the context at token m is summarized by a recurrently-updated vector,

$$\boldsymbol{h}_m = g(\boldsymbol{x}_m, \boldsymbol{h}_{m-1}), \quad m = 1, 2, \dots M,$$

where  $x_m$  is the vector **embedding** of the token  $w_m$  and the function g defines the recurrence. The starting condition  $h_0$  is an additional parameter of the model. The long short-term memory (LSTM) is a more complex recurrence, in which a memory cell is through a series of gates, avoiding repeated application of the non-linearity. Despite these bells and whistles, both models share the basic architecture of recurrent updates across a sequence, and both will be referred to as RNNs here.

A straightforward application of RNNs to sequence labeling is to score each tag  $y_m$  as a linear function of  $h_m$ :

$$\psi_m(y) = \beta_y \cdot \boldsymbol{h}_m \tag{7.93}$$

<span id="page-184-2"></span>
$$\hat{y}_m = \operatorname*{argmax} \psi_m(y).$$
[7.94]

The score  $\psi_m(y)$  can also be converted into a probability distribution using the usual softmax operation,

$$\mathbf{p}(y \mid \boldsymbol{w}_{1:m}) = \frac{\exp \psi_m(y)}{\sum_{y' \in \mathcal{Y}} \exp \psi_m(y')}.$$
[7.95]

<span id="page-185-0"></span>Using this transformation, it is possible to train the tagger from the negative log-likelihood of the tags, as in a conditional random field. Alternatively, a hinge loss or margin loss objective can be constructed from the raw scores  $\psi_m(y)$ .

The hidden state  $h_m$  accounts for information in the input leading up to position m, but it ignores the subsequent tokens, which may also be relevant to the tag  $y_m$ . This can be addressed by adding a second RNN, in which the input is reversed, running the recurrence from  $w_M$  to  $w_1$ . This is known as a **bidirectional recurrent neural network** (Graves and Schmidhuber, 2005), and is specified as:

$$\overleftarrow{\boldsymbol{h}}_m = g(\boldsymbol{x}_m, \overleftarrow{\boldsymbol{h}}_{m+1}), \quad m = 1, 2, \dots, M.$$
 [7.96]

The hidden states of the left-to-right RNN are denoted  $\overrightarrow{h}_m$ . The left-to-right and right-toleft vectors are concatenated,  $h_m = [\overleftarrow{h}_m; \overrightarrow{h}_m]$ . The scoring function in Equation 7.93 is applied to this concatenated vector.

Bidirectional RNN tagging has several attractive properties. Ideally, the representation  $h_m$  summarizes the useful information from the surrounding context, so that it is not necessary to design explicit features to capture this information. If the vector  $h_m$  is an adequate summary of this context, then it may not even be necessary to perform the tagging jointly: in general, the gains offered by joint tagging of the entire sequence are diminished as the individual tagging model becomes more powerful. Using backpropagation, the word vectors x can be trained "end-to-end", so that they capture word properties that are useful for the tagging task. Alternatively, if limited labeled data is available, we can use word embeddings that are "pre-trained" from unlabeled data, using a language modeling objective (as in § 6.3) or a related word embedding technique (see chapter 14). It is even possible to combine both fine-tuned and pre-trained embeddings in a single model.

**Neural structure prediction** The bidirectional recurrent neural network incorporates information from throughout the input, but each tagging decision is made independently. In some sequence labeling applications, there are very strong dependencies between tags: it may even be impossible for one tag to follow another. In such scenarios, the tagging decision must be made jointly across the entire sequence.

Neural sequence labeling can be combined with the Viterbi algorithm by defining the local scores as:

$$s_m(y_m, y_{m-1}) = \beta_{y_m} \cdot h_m + \eta_{y_{m-1}, y_m},$$
[7.97]

where  $h_m$  is the RNN hidden state,  $\beta_{y_m}$  is a vector associated with tag  $y_m$ , and  $\eta_{y_{m-1},y_m}$  is a scalar parameter for the tag transition  $(y_{m-1}, y_m)$ . These local scores can then be incorporated into the Viterbi algorithm for inference, and into the forward algorithm for training. This model is shown in Figure 7.4. It can be trained from the conditional log-likelihood objective defined in Equation 7.76, backpropagating to the tagging parameters

<span id="page-186-2"></span><span id="page-186-1"></span>![](_page_186_Figure_1.jpeg)

Figure 7.4: **Bidirectional LSTM** for sequence labeling. The solid lines indicate computation, the dashed lines indicate probabilistic dependency, and the dotted lines indicate the optional additional probabilistic dependencies between labels in the biLSTM-CRF.

β and η, as well as the parameters of the RNN. This model is called the **LSTM-CRF**, due to its combination of aspects of the long short-term memory and conditional random field models [\(Huang et al., 2015\)](#page-536-3).

The LSTM-CRF is especially effective on the task of **named entity recognition** [\(Lample](#page-542-3) [et al., 2016\)](#page-542-3), a sequence labeling task that is described in detail in § [8.3.](#page-200-0) This task has strong dependencies between adjacent tags, so structure prediction is especially important.

#### <span id="page-186-0"></span>**7.6.2 Character-level models**

As in language modeling, rare and unseen words are a challenge: if we encounter a word that was not in the training data, then there is no obvious choice for the word embedding xm. One solution is to use a generic **unseen word** embedding for all such words. However, in many cases, properties of unseen words can be guessed from their spellings. For example, *whimsical* does not appear in the Universal Dependencies (UD) English Treebank, yet the suffix *-al* makes it likely to be adjective; by the same logic, *unflinchingly* is likely to be an adverb, and *barnacle* is likely to be a noun.

In feature-based models, these morphological properties were handled by suffix features; in a neural network, they can be incorporated by constructing the embeddings of unseen words from their spellings or morphology. One way to do this is to incorporate an additional layer of bidirectional RNNs, one for each word in the vocabulary [\(Ling](#page-545-3) [et al., 2015\)](#page-545-3). For each such character-RNN, the inputs are the characters, and the output is the concatenation of the final states of the left-facing and right-facing passes, φ<sup>w</sup> =

<span id="page-187-2"></span> $[\vec{h}_{N_w}^{(w)}; \overleftarrow{h}_0^{(w)}]$ , where  $\vec{h}_{N_w}^{(w)}$  is the final state of the right-facing pass for word w, and  $N_w$  is the number of characters in the word. The character RNN model is trained by backpropagation from the tagging objective. On the test data, the trained RNN is applied to out-of-vocabulary words (or all words), yielding inputs to the word-level tagging RNN. Other approaches to compositional word embeddings are described in § 14.7.1.

#### <span id="page-187-0"></span>7.6.3 Convolutional Neural Networks for Sequence Labeling

One disadvantage of recurrent neural networks is that the architecture requires iterating through the sequence of inputs and predictions: each hidden vector  $h_m$  must be computed from the previous hidden vector  $h_{m-1}$ , before predicting the tag  $y_m$ . These iterative computations are difficult to parallelize, and fail to exploit the speedups offered by **graphics processing units (GPUs)** on operations such as matrix multiplication. **Convolutional neural networks** achieve better computational performance by predicting each label  $y_m$  from a set of matrix operations on the neighboring word embeddings,  $x_{m-k:m+k}$  (Collobert et al., 2011). Because there is no hidden state to update, the predictions for each  $y_m$  can be computed in parallel. For more on convolutional neural networks, see § 3.4. Character-based word embeddings can also be computed using convolutional neural networks (Santos and Zadrozny, 2014).

## <span id="page-187-1"></span>7.7 \*Unsupervised sequence labeling

In unsupervised sequence labeling, the goal is to induce a hidden Markov model from a corpus of *unannotated* text ( $w^{(1)}, w^{(2)}, \ldots, w^{(N)}$ ), where each  $w^{(i)}$  is a sequence of length  $M^{(i)}$ . This is an example of the general problem of **structure induction**, which is the unsupervised version of structure prediction. The tags that result from unsupervised sequence labeling might be useful for some downstream task, or they might help us to better understand the language's inherent structure. For part-of-speech tagging, it is common to use a tag dictionary that lists the allowed tags for each word, simplifying the problem (Christodoulopoulos et al., 2010).

Unsupervised learning in hidden Markov models can be performed using the **Baum-Welch algorithm**, which combines the forward-backward algorithm (§ 7.5.3) with expectation-maximization (EM; § 5.1.2). In the M-step, the HMM parameters from expected counts:

$$\Pr(W = i \mid Y = k) = \phi_{k,i} = \frac{E[\operatorname{count}(W = i, Y = k)]}{E[\operatorname{count}(Y = k)]}$$
$$\Pr(Y_m = k \mid Y_{m-1} = k') = \lambda_{k',k} = \frac{E[\operatorname{count}(Y_m = k, Y_{m-1} = k')]}{E[\operatorname{count}(Y_{m-1} = k')]}$$

#### 7.7. \*UNSUPERVISED SEQUENCE LABELING 171

The expected counts are computed in the E-step, using the forward and backward recurrences. The local scores follow the usual definition for hidden Markov models,

$$s_m(k,k') = \log p_E(w_m \mid Y_m = k; \phi) + \log p_T(Y_m = k \mid Y_{m-1} = k'; \lambda).$$
[7.98]

The expected transition counts for a single instance are,

$$E[\text{count}(Y_m = k, Y_{m-1} = k') \mid \boldsymbol{w}] = \sum_{m=1}^{M} \Pr(Y_{m-1} = k', Y_m = k \mid \boldsymbol{w})$$
[7.99]

$$=\frac{\sum_{\boldsymbol{y}:Y_m=k,Y_{m-1}=k'}\prod_{n=1}^{M}\exp s_n(y_n,y_{n-1})}{\sum_{\boldsymbol{y}'}\prod_{n=1}^{M}\exp s_n(y'_n,y'_{n-1})}.$$
 [7.100]

As described in § [7.5.3,](#page-182-2) these marginal probabilities can be computed from the forwardbackward recurrence,

$$\Pr(Y_{m-1} = k', Y_m = k \mid \boldsymbol{w}) = \frac{\alpha_{m-1}(k') \times \exp s_m(k, k') \times \beta_m(k)}{\alpha_{M+1}(\boldsymbol{\diamond})}.$$
[7.101]

In a hidden Markov model, each element of the forward-backward computation has a special interpretation:

$$\alpha_{m-1}(k') = \mathbf{p}(Y_{m-1} = k', \boldsymbol{w}_{1:m-1})$$
[7.102]

$$\exp s_m(k,k') = \mathbf{p}(Y_m = k, w_m \mid Y_{m-1} = k')$$
[7.103]

$$\beta_m(k) = \mathbf{p}(\boldsymbol{w}_{m+1:M} \mid Y_m = k).$$
 [7.104]

Applying the conditional independence assumptions of the hidden Markov model (defined in [Algorithm 12\)](#page-171-0), the product is equal to the joint probability of the tag bigram and the entire input,

$$\alpha_{m-1}(k') \times \exp s_m(k,k') \times \beta_m(k) = p(Y_{m-1} = k', \boldsymbol{w}_{1:m-1}) \\ \times p(Y_m = k, w_m \mid Y_{m-1} = k') \\ \times p(\boldsymbol{w}_{m+1:M} \mid Y_m = k) \\ = p(Y_{m-1} = k', Y_m = k, \boldsymbol{w}_{1:M}).$$
[7.105]

Dividing by αM+1() = p(w1:M) gives the desired probability,

$$\frac{\alpha_{m-1}(k') \times s_m(k,k') \times \beta_m(k)}{\alpha_{M+1}(\blacklozenge)} = \frac{p(Y_{m-1} = k', Y_m = k, \boldsymbol{w}_{1:M})}{p(\boldsymbol{w}_{1:M})}$$
[7.106]

$$= \Pr(Y_{m-1} = k', Y_m = k \mid \boldsymbol{w}_{1:M}).$$
 [7.107]

The expected emission counts can be computed in a similar manner, using the product αm(k) × βm(k).

#### <span id="page-189-4"></span><span id="page-189-0"></span>7.7.1 Linear dynamical systems

The forward-backward algorithm can be viewed as Bayesian state estimation in a discrete state space. In a continuous state space,  $y_m \in \mathbb{R}^K$ , the equivalent algorithm is the **Kalman smoother**. It also computes marginals  $p(y_m | x_{1:M})$ , using a similar two-step algorithm of forward and backward passes. Instead of computing a trellis of values at each step, the Kalman smoother computes a probability density function  $q_{y_m}(y_m; \mu_m, \Sigma_m)$ , characterized by a mean  $\mu_m$  and a covariance  $\Sigma_m$  around the latent state. Connections between the Kalman smoother and the forward-backward algorithm are elucidated by Minka (1999) and Murphy (2012).

#### <span id="page-189-1"></span>7.7.2 Alternative unsupervised learning methods

As noted in § 5.5, expectation-maximization is just one of many techniques for structure induction. One alternative is to use **Markov Chain Monte Carlo (MCMC)** sampling algorithms, which are briefly described in § 5.5.1. For the specific case of sequence labeling, Gibbs sampling can be applied by iteratively sampling each tag  $y_m$  conditioned on all the others (Finkel et al., 2005):

$$p(y_m \mid y_{-m}, w_{1:M}) \propto p(w_m \mid y_m) p(y_m \mid y_{-m}).$$
 [7.108]

Gibbs Sampling has been applied to unsupervised part-of-speech tagging by Goldwater and Griffiths (2007). **Beam sampling** is a more sophisticated sampling algorithm, which randomly draws entire sequences  $y_{1:M}$ , rather than individual tags  $y_m$ ; this algorithm was applied to unsupervised part-of-speech tagging by Van Gael et al. (2009). Spectral learning (see § 5.5.2) can also be applied to sequence labeling. By factoring matrices of co-occurrence counts of word bigrams and trigrams (Song et al., 2010; Hsu et al., 2012), it is possible to obtain globally optimal estimates of the transition and emission parameters, under mild assumptions.

#### <span id="page-189-2"></span>7.7.3 Semiring notation and the generalized viterbi algorithm

The Viterbi and Forward recurrences can each be performed over probabilities or log probabilities, yielding a total of four closely related recurrences. These four recurrence scan in fact be expressed as a single recurrence in a more general notation, known as **semiring algebra**. Let the symbols  $\oplus$  and  $\otimes$  represent generalized addition and multiplication respectively.<sup>11</sup> Given these operators, a generalized Viterbi recurrence is denoted,

$$v_m(k) = \bigoplus_{k' \in \mathcal{Y}} s_m(k, k') \otimes v_{m-1}(k').$$
[7.109]

<span id="page-189-3"></span><sup>&</sup>lt;sup>11</sup>In a semiring, the addition and multiplication operators must both obey associativity, and multiplication must distribute across addition; the addition operator must be commutative; there must be additive and multiplicative identities  $\overline{0}$  and  $\overline{1}$ , such that  $a \oplus \overline{0} = a$  and  $a \otimes \overline{1} = a$ ; and there must be a multiplicative annihilator  $\overline{0}$ , such that  $a \otimes \overline{0} = \overline{0}$ .

<span id="page-190-1"></span>Each recurrence that we have seen so far is a special case of this generalized Viterbi recurrence:

- In the max-product Viterbi recurrence over probabilities, the ⊕ operation corresponds to maximization, and the ⊗ operation corresponds to multiplication.
- In the forward recurrence over probabilities, the ⊕ operation corresponds to addition, and the ⊗ operation corresponds to multiplication.
- In the max-product Viterbi recurrence over log-probabilities, the ⊕ operation corresponds to maximization, and the ⊗ operation corresponds to addition.<sup>12</sup>
- In the forward recurrence over log-probabilities, the ⊕ operation corresponds to logaddition, a ⊕ b = log(e<sup>a</sup> + e<sup>b</sup>). The ⊗ operation corresponds to addition.

The mathematical abstraction offered by semiring notation can be applied to the software implementations of these algorithms, yielding concise and modular implementations. For example, in the OPENFST library, generic operations are parametrized by the choice of semiring (Allauzen et al., 2007).

# Exercises

- 1. Extend the example in § 7.3.1 to the sentence *they can can fish*, meaning that "they can put fish into cans." Build the trellis for this example using the weights in Table 7.1, and identify the best-scoring tag sequence. If the scores for noun and verb are tied, then you may assume that the backpointer always goes to noun.
- 2. Using the tagset  $\mathcal{Y} = \{N, V\}$ , and the feature set  $f(w, y_m, y_{m-1}, m) = \{(w_m, y_m), (y_m, y_{m-1})\}$ , show that there is no set of weights that give the correct tagging for both *they can fish* (N V V) and *they can can fish* (N V V N).
- 3. Work out what happens if you train a structured perceptron on the two examples mentioned in the previous problem, using the transition and emission features  $(y_m, y_{m-1})$  and  $(y_m, w_m)$ . Initialize all weights at 0, and assume that the Viterbi algorithm always chooses N when the scores for the two tags are tied, so that the initial prediction for *they can fish* is N N N.
- 4. Consider the garden path sentence, *The old man the boat*. Given word-tag and tag-tag features, what inequality in the weights must hold for the correct tag sequence to outscore the garden path tag sequence for this example?

<span id="page-190-0"></span><sup>&</sup>lt;sup>12</sup>This is sometimes called the **tropical semiring**, in honor of the Brazilian mathematician Imre Simon.

- 5. Using the weights in Table 7.1, explicitly compute the log-probabilities for all possible taggings of the input *fish can*. Verify that the forward algorithm recovers the aggregate log probability.
- 6. Sketch out an algorithm for a variant of Viterbi that returns the top-*n* label sequences. What is the time and space complexity of this algorithm?
- 7. Show how to compute the marginal probability  $Pr(y_{m-2} = k, y_m = k' | \boldsymbol{w}_{1:M})$ , in terms of the forward and backward variables, and the potentials  $s_n(y_n, y_{n-1})$ .
- 8. Suppose you receive a stream of text, where some of tokens have been replaced at random with *NOISE*. For example:
  - Source: I try all things, I achieve what I can
  - Message received: I try NOISE NOISE, I NOISE what I NOISE

Assume you have access to a pre-trained bigram language model, which gives probabilities  $p(w_m \mid w_{m-1})$ . These probabilities can be assumed to be non-zero for all bigrams.

Show how to use the Viterbi algorithm to recover the source by maximizing the bigram language model log-probability. Specifically, set the scores  $s_m(y_m, y_{m-1})$  so that the Viterbi algorithm selects a sequence of words that maximizes the bigram language model log-probability, while leaving the non-noise tokens intact. Your solution should not modify the logic of the Viterbi algorithm, it should only set the scores  $s_m(y_m, y_{m-1})$ .

- 9. Let  $\alpha(\cdot)$  and  $\beta(\cdot)$  indicate the forward and backward variables as defined in § 7.5.3. Prove that  $\alpha_{M+1}(\blacklozenge) = \beta_0(\diamondsuit) = \sum_y \alpha_m(y)\beta_m(y), \forall m \in \{1, 2, ..., M\}.$
- 10. Consider an RNN tagging model with a tanh activation function on the hidden layer, and a hinge loss on the output. (The problem also works for the margin loss and negative log-likelihood.) Suppose you initialize all parameters to zero: this includes the word embeddings that make up x, the transition matrix  $\Theta$ , the output weights  $\beta$ , and the initial hidden state  $h_0$ .
  - a) Prove that for any data and for any gradient-based learning algorithm, all parameters will be stuck at zero.
  - b) Would a sigmoid activation function avoid this problem?

# <span id="page-192-2"></span><span id="page-192-0"></span>**Chapter 8 Applications of sequence labeling**

Sequence labeling has applications throughout natural language processing. This chapter focuses on part-of-speech tagging, morpho-syntactic attribute tagging, named entity recognition, and tokenization. It also touches briefly on two applications to interactive settings: dialogue act recognition and the detection of code-switching points between languages.

# <span id="page-192-1"></span>**8.1 Part-of-speech tagging**

The **syntax** of a language is the set of principles under which sequences of words are judged to be grammatically acceptable by fluent speakers. One of the most basic syntactic concepts is the **part-of-speech** (POS), which refers to the syntactic role of each word in a sentence. This concept was used informally in the previous chapter, and you may have some intuitions from your own study of English. For example, in the sentence *We like vegetarian sandwiches*, you may already know that *we* and *sandwiches* are nouns, *like* is a verb, and *vegetarian* is an adjective. These labels depend on the context in which the word appears: in *she eats like a vegetarian*, the word *like* is a preposition, and the word *vegetarian* is a noun.

Parts-of-speech can help to disentangle or explain various linguistic problems. Recall Chomsky's proposed distinction in [chapter 6:](#page-142-0)

- (8.1) a. Colorless green ideas sleep furiously.
  - b. \* Ideas colorless furiously green sleep.

One difference between these two examples is that the first contains part-of-speech transitions that are typical in English: adjective to adjective, adjective to noun, noun to verb, and verb to adverb. The second example contains transitions that are unusual: noun to adjective and adjective to verb. The ambiguity in a headline like,

#### <span id="page-193-3"></span>(8.2) Teacher Strikes Idle Children

can also be explained in terms of parts of speech: in the interpretation that was likely intended, *strikes* is a noun and *idle* is a verb; in the alternative explanation, *strikes* is a verb and *idle* is an adjective.

Part-of-speech tagging is often taken as a early step in a natural language processing pipeline. Indeed, parts-of-speech provide features that can be useful for many of the tasks that we will encounter later, such as parsing [\(chapter 10\)](#page-242-0), coreference resolution [\(chapter 15\)](#page-368-0), and relation extraction [\(chapter 17\)](#page-420-0).

#### <span id="page-193-0"></span>**8.1.1 Parts-of-Speech**

The **Universal Dependencies** project (UD) is an effort to create syntactically-annotated corpora across many languages, using a single annotation standard [\(Nivre et al., 2016\)](#page-552-3). As part of this effort, they have designed a part-of-speech **tagset**, which is meant to capture word classes across as many languages as possible.[1](#page-193-1) This section describes that inventory, giving rough definitions for each of tags, along with supporting examples.

Part-of-speech tags are **morphosyntactic**, rather than semantic, categories. This means that they describe words in terms of how they pattern together and how they are internally constructed (e.g., what suffixes and prefixes they include). For example, you may think of a noun as referring to objects or concepts, and verbs as referring to actions or events. But events can also be nouns:

#### (8.3) . . . the **howling** of the **shrieking** storm.

Here *howling* and *shrieking* are events, but grammatically they act as a noun and adjective respectively.

#### **The Universal Dependency part-of-speech tagset**

The UD tagset is broken up into three groups: open class tags, closed class tags, and "others."

**Open class tags** Nearly all languages contain nouns, verbs, adjectives, and adverbs.[2](#page-193-2) These are all **open word classes**, because new words can easily be added to them. The UD tagset includes two other tags that are open classes: proper nouns and interjections.

<span id="page-193-1"></span>• **Nouns** (UD tag: NOUN) tend to describe entities and concepts, e.g.,

<sup>1</sup>The UD tagset builds on earlier work from [Petrov et al.](#page-555-3) [\(2012\)](#page-555-3), in which a set of twelve universal tags was identified by creating mappings from tagsets for individual languages.

<span id="page-193-2"></span><sup>2</sup>One prominent exception is Korean, which some linguists argue does not have adjectives [Kim](#page-539-8) [\(2002\)](#page-539-8).

<span id="page-194-1"></span>(8.4) **Toes** are scarce among veteran **blubber men**.

In English, nouns tend to follow determiners and adjectives, and can play the subject role in the sentence. They can be marked for the plural number by an *-s* suffix.

- **Proper nouns** (PROPN) are tokens in names, which uniquely specify a given entity,
  - (8.5) "**Moby Dick**?" shouted **Ahab**.
- **Verbs** (VERB), according to the UD guidelines, "typically signal events and actions." But they are also defined grammatically: they "can constitute a minimal predicate in a clause, and govern the number and types of other constituents which may occur in a clause."[3](#page-194-0)
  - (8.6) "Moby Dick?" **shouted** Ahab.
  - (8.7) Shall we **keep chasing** this murderous fish?

English verbs tend to come in between the subject and some number of direct objects, depending on the verb. They can be marked for**tense** and **aspect** using suffixes such as *-ed* and *-ing*. (These suffixes are an example of **inflectional morphology**, which is discussed in more detail in § [9.1.4.](#page-218-0))

- **Adjectives** (ADJ) describe properties of entities,
  - (8.8) a. Shall we keep chasing this **murderous** fish?
    - b. Toes are **scarce** among **veteran** blubber men.

In the second example, *scarce* is a predicative adjective, linked to the subject by the **copula verb** *are*. In contrast, *murderous* and *veteran* are attributive adjectives, modifying the noun phrase in which they are embedded.

- **Adverbs** (ADV) describe properties of events, and may also modify adjectives or other adverbs:
  - (8.9) a. It is not down on any map; true places **never** are.
    - b. . . .**treacherously** hidden beneath the loveliest tints of azure
    - c. Not drowned **entirely**, though.
- **Interjections** (INTJ) are used in exclamations, e.g.,
  - (8.10) **Aye aye**! it was that accursed white whale that razed me.

<span id="page-194-0"></span><sup>3</sup>http://universaldependencies.org/u/pos/VERB.html

<span id="page-195-0"></span>**Closed class tags** Closed word classes rarely receive new members. They are sometimes referred to as **function words** — as opposed to **content words** — as they have little lexical meaning of their own, but rather, help to organize the components of the sentence.

- **Adpositions** (ADP) describe the relationship between a complement (usually a noun phrase) and another unit in the sentence, typically a noun or verb phrase.
  - (8.11) a. Toes are scarce **among** veteran blubber men.
    - b. It is not **down on** any map.
    - c. Give not thyself **up** then.

As the examples show, English generally uses prepositions, which are adpositions that appear before their complement. (An exception is *ago*, as in, *we met three days ago*). Postpositions are used in other languages, such as Japanese and Turkish.

- **Auxiliary verbs** (AUX) are a closed class of verbs that add information such as tense, aspect, person, and number.
  - (8.12) a. **Shall** we keep chasing this murderous fish?
    - b. What the white whale was to Ahab, **has been** hinted.
    - c. Ahab **must** use tools.
    - d. Meditation and water **are** wedded forever.
    - e. Toes **are** scarce among veteran blubber men.

The final example is a copula verb, which is also tagged as an auxiliary in the UD corpus.

• **Coordinating conjunctions** (CCONJ) express relationships between two words or phrases, which play a parallel role:

(8.13) Meditation **and** water are wedded forever.

- **Subordinating conjunctions** (SCONJ) link two clauses, making one syntactically subordinate to the other:
  - (8.14) It is the easiest thing in the world for a man to look as **if** he had a great secret in him.

Note that

- **Pronouns** (PRON) are words that substitute for nouns or noun phrases.
  - (8.15) a. Be **it what it** will, **I**'ll go to **it** laughing.

#### <span id="page-196-1"></span>8.1. PART-OF-SPEECH TAGGING

b. I try all things, I achieve what I can.

The example includes the personal pronouns *I* and *it*, as well as the relative pronoun *what*. Other pronouns include *myself*, *somebody*, and *nothing*.

- **Determiners** (DET) provide additional information about the nouns or noun phrases that they modify:
  - (8.16) a. What **the** white whale was to Ahab, has been hinted.
    - b. It is not down on **any** map.
    - c. I try all things ...
    - d. Shall we keep chasing this murderous fish?

Determiners include articles (*the*), possessive determiners (*their*), demonstratives (*this murderous fish*), and quantifiers (*any map*).

- Numerals (NUM) are an infinite but closed class, which includes integers, fractions, and decimals, regardless of whether spelled out or written in numerical form.
  - (8.17) a. How then can this **one** small heart beat.
    - b. I am going to put him down for the **three hundredth**.
- **Particles** (PART) are a catch-all of function words that combine with other words or phrases, but do not meet the conditions of the other tags. In English, this includes the infinitival *to*, the possessive marker, and negation.
  - (8.18) a. Better **to** sleep with a sober cannibal than a drunk Christian.
    - b. So man's insanity is heaven's sense
    - c. It is **not** down on any map

As the second example shows, the possessive marker is not considered part of the same token as the word that it modifies, so that *man's* is split into two tokens. (Tokenization is described in more detail in § 8.4.) A non-English example of a particle is the Japanese question marker ka:<sup>4</sup>

(8.19) *Sensei desu ka* Teacher is ? Is she a teacher?

<span id="page-196-0"></span><sup>&</sup>lt;sup>4</sup>In this notation, the first line is the transliterated Japanese text, the second line is a token-to-token **gloss**, and the third line is the translation.

<span id="page-197-1"></span>**Other** The remaining UD tags include punctuation (PUN) and symbols (SYM). Punctuation is purely structural — e.g., commas, periods, colons — while symbols can carry content of their own. Examples of symbols include dollar and percentage symbols, mathematical operators, emoticons, emojis, and internet addresses. A final catch-all tag is X, which is used for words that cannot be assigned another part-of-speech category. The X tag is also used in cases of **code switching** (between languages), described in § [8.5.](#page-203-0)

#### **Other tagsets**

Prior to the Universal Dependency treebank, part-of-speech tagging was performed using language-specific tagsets. The dominant tagset for English was designed as part of the **Penn Treebank** (PTB), and it includes 45 tags — more than three times as many as the UD tagset. This granularity is reflected in distinctions between singular and plural nouns, verb tenses and aspects, possessive and non-possessive pronouns, comparative and superlative adjectives and adverbs (e.g., *faster*, *fastest*), and so on. The Brown corpus includes a tagset that is even more detailed, with 87 tags [\(Francis, 1964\)](#page-529-5), including special tags for individual auxiliary verbs such as *be*, *do*, and *have*.

Different languages make different distinctions, and so the PTB and Brown tagsets are not appropriate for a language such as Chinese, which does not mark the verb tense [\(Xia,](#page-570-4) [2000\)](#page-570-4); nor for Spanish, which marks every combination of person and number in the verb ending; nor for German, which marks the case of each noun phrase. Each of these languages requires more detail than English in some areas of the tagset, and less in other areas. The strategy of the Universal Dependencies corpus is to design a coarse-grained tagset to be used across all languages, and then to additionally annotate language-specific **morphosyntactic attributes**, such as number, tense, and case. The attribute tagging task is described in more detail in § [8.2.](#page-199-0)

Social media such as Twitter have been shown to require tagsets of their own [\(Gimpel](#page-530-4) [et al., 2011\)](#page-530-4). Such corpora contain some tokens that are not equivalent to anything encountered in a typical written corpus: e.g., emoticons, URLs, and hashtags. Social media also includes dialectal words like *gonna* ('going to', e.g. *We gonna be fine*) and *Ima* ('I'm going to', e.g., *Ima tell you one more time*), which can be analyzed either as non-standard orthography (making tokenization impossible), or as lexical items in their own right. In either case, it is clear that existing tags like NOUN and VERB cannot handle cases like *Ima*, which combine aspects of the noun and verb. [Gimpel et al.](#page-530-4) [\(2011\)](#page-530-4) therefore propose a new set of tags to deal with these cases.

#### <span id="page-197-0"></span>**8.1.2 Accurate part-of-speech tagging**

Part-of-speech tagging is the problem of selecting the correct tag for each word in a sentence. Success is typically measured by accuracy on an annotated test set, which is simply the fraction of tokens that were tagged correctly.

#### <span id="page-198-0"></span>8.1. PART-OF-SPEECH TAGGING

#### **Baselines**

A simple baseline for part-of-speech tagging is to choose the most common tag for each word. For example, in the Universal Dependencies treebank, the word *talk* appears 96 times, and 85 of those times it is labeled as a VERB: therefore, this baseline will always predict VERB for this word. For words that do not appear in the training corpus, the baseline simply guesses the most common tag overall, which is NOUN. In the Penn Treebank, this simple baseline obtains accuracy above 92%. A more rigorous evaluation is the accuracy on **out-of-vocabulary words**, which are not seen in the training data. Tagging these words correctly requires attention to the context and the word's internal structure.

#### **Contemporary approaches**

Conditional random fields and structured perceptron perform at or near the state-of-theart for part-of-speech tagging in English. For example, (Collins, 2002) achieved 97.1% accuracy on the Penn Treebank, using a structured perceptron with the following base features (originally introduced by Ratnaparkhi (1996)):

- current word,  $w_m$
- previous words,  $w_{m-1}, w_{m-2}$
- next words,  $w_{m+1}, w_{m+2}$
- previous tag,  $y_{m-1}$
- previous two tags,  $(y_{m-1}, y_{m-2})$
- for rare words:
  - first *k* characters, up to k = 4
  - last *k* characters, up to k = 4
  - whether  $w_m$  contains a number, uppercase character, or hyphen.

Similar results for the PTB data have been achieved using conditional random fields (CRFs; Toutanova et al., 2003).

More recent work has demonstrated the power of neural sequence models, such as the **long short-term memory (LSTM)** (§ 7.6). Plank et al. (2016) apply a CRF and a bidirectional LSTM to twenty-two languages in the UD corpus, achieving an average accuracy of 94.3% for the CRF, and 96.5% with the bi-LSTM. Their neural model employs three types of embeddings: fine-tuned word embeddings, which are updated during training; pre-trained word embeddings, which are never updated, but which help to tag out-of-vocabulary words; and character-based embeddings. The character-based embeddings are computed by running an LSTM on the individual characters in each word, thereby capturing common orthographic patterns such as prefixes, suffixes, and capitalization. Extensive evaluations show that these additional embeddings are crucial to their model's success.

<span id="page-199-4"></span><span id="page-199-3"></span>

| word          | PTB tag | UD tag | UD attributes                                            |
|---------------|---------|--------|----------------------------------------------------------|
| The           | DT      | DET    | DEFINITE=DEF PRONTYPE=ART                                |
| German        | JJ      | ADJ    | DEGREE=POS                                               |
| Expressionist | NN      | NOUN   | NUMBER=SING                                              |
| movement      | NN      | NOUN   | NUMBER=SING                                              |
| was           | VBD     | AUX    | MOOD=IND NUMBER=SING PERSON=3<br>TENSE=PAST VERBFORM=FIN |
| destroyed     | VBN     | VERB   | TENSE=PAST VERBFORM=PART<br>VOICE=PASS                   |
| as            | IN      | ADP    |                                                          |
| a             | DT      | DET    | DEFINITE=IND PRONTYPE=ART                                |
| result        | NN      | NOUN   | NUMBER=SING                                              |
|               |         | PUNCT  |                                                          |

Figure 8.1: UD and PTB part-of-speech tags, and UD morphosyntactic attributes. Example selected from the UD 1.4 English corpus.

# <span id="page-199-0"></span>**8.2 Morphosyntactic Attributes**

There is considerably more to say about a word than whether it is a noun or a verb: in English, verbs are distinguish by features such tense and aspect, nouns by number, adjectives by degree, and so on. These features are language-specific: other languages distinguish other features, such as **case** (the role of the noun with respect to the action of the sentence, which is marked in languages such as Latin and German[5](#page-199-1) ) and **evidentiality** (the source of information for the speaker's statement, which is marked in languages such as Turkish). In the UD corpora, these attributes are annotated as feature-value pairs for each token.[6](#page-199-2)

An example is shown in [Figure 8.1.](#page-199-3) The determiner *the* is marked with two attributes: PRONTYPE=ART, which indicates that it is an **article** (as opposed to another type of deter-

<span id="page-199-2"></span><span id="page-199-1"></span><sup>5</sup>Case is marked in English for some personal pronouns, e.g., *She saw her*, *They* saw *them*.

<sup>6</sup>The annotation and tagging of morphosyntactic attributes can be traced back to earlier work on Turkish [\(Oflazer and Kuruoz, 1994\)](#page-552-4) and Czech (Haji ¨ [c and Hladk](#page-533-6) ˇ a, [1998\)](#page-533-6). MULTEXT-East was an early multilin- ´ gual corpus to include morphosyntactic attributes [\(Dimitrova et al., 1998\)](#page-525-5).

#### <span id="page-200-1"></span>8.3. NAMED ENTITY RECOGNITION

miner or pronominal modifier), and DEFINITE=DEF, which indicates that it is a **definite article** (referring to a specific, known entity). The verbs are each marked with several attributes. The auxiliary verb *was* is third-person, singular, past tense, finite (conjugated), and indicative (describing an event that has happened or is currently happenings); the main verb *destroyed* is in participle form (so there is no additional person and number information), past tense, and passive voice. Some, but not all, of these distinctions are reflected in the PTB tags VBD (past-tense verb) and VBN (past participle).

While there are thousands of papers on part-of-speech tagging, there is comparatively little work on automatically labeling morphosyntactic attributes. Faruqui et al. (2016) train a support vector machine classification model, using a minimal feature set that includes the word itself, its prefixes and suffixes, and type-level information listing all possible morphosyntactic attributes for each word and its neighbors. Mueller et al. (2013) use a conditional random field (CRF), in which the tag space consists of all observed combinations of morphosyntactic attributes (e.g., the tag would be DEF+ART for the word *the* in Figure 8.1). This massive tag space is managed by decomposing the feature space over individual attributes, and pruning paths through the trellis. More recent work has employed bidirectional LSTM sequence models. For example, Pinter et al. (2017) train a bidirectional LSTM sequence model. The input layer and hidden vectors in the LSTM are shared across attributes, but each attribute has its own output layer, culminating in a softmax over all attribute values, e.g.  $y_t^{\text{NUMBER}} \in {\text{SING}, \text{PLURAL}, \ldots}$ . They find that character-level information is crucial, especially when the amount of labeled data is limited.

Evaluation is performed by first computing recall and precision for each attribute. These scores can then be averaged at either the type or token level to obtain micro- or macro-F-MEASURE. Pinter et al. (2017) evaluate on 23 languages in the UD treebank, reporting a median micro-F-MEASURE of 0.95. Performance is strongly correlated with the size of the labeled dataset for each language, with a few outliers: for example, Chinese is particularly difficult, because although the dataset is relatively large (10<sup>5</sup> tokens in the UD 1.4 corpus), only 6% of tokens have any attributes, offering few useful labeled instances.

### <span id="page-200-0"></span>8.3 Named Entity Recognition

A classical problem in information extraction is to recognize and extract mentions of **named entities** in text. In news documents, the core entity types are people, locations, and organizations; more recently, the task has been extended to include amounts of money, percentages, dates, and times. In item 8.20a (Figure 8.2), the named entities include: *The U.S. Army*, an organization; *Atlanta*, a location; and *May* 14, 1864, a date. Named entity recognition is also a key task in **biomedical natural language processing**, with entity types including proteins, DNA, RNA, and cell lines (e.g., Collier et al., 2000; Ohta et al., 2002). Figure 8.2 shows an example from the GENIA corpus of biomedical research ab-

<span id="page-201-2"></span><span id="page-201-1"></span><span id="page-201-0"></span>(8.20) a. *The* B-ORG *U.S.* I-ORG *Army* I-ORG *captured* O *Atlanta* B-LOC *on* O *May* B-DATE *14* I-DATE *,* I-DATE *1864* I-DATE

b. *Number* O *of* O *glucocorticoid* B-PROTEIN *receptors* I-PROTEIN *in* O *lymphocytes* B-CELLTYPE *and* O *. . .* . . .

Figure 8.2: BIO notation for named entity recognition. Example [\(8.20b\)](#page-201-2) is drawn from the GENIA corpus of biomedical documents [\(Ohta et al., 2002\)](#page-553-7).

stracts.

A standard approach to tagging named entity spans is to use discriminative sequence labeling methods such as conditional random fields. However, the named entity recognition (NER) task would seem to be fundamentally different from sequence labeling tasks like part-of-speech tagging: rather than tagging each token, the goal in is to recover *spans* of tokens, such as *The United States Army*.

This is accomplished by the **BIO notation**, shown in [Figure 8.2.](#page-201-1) Each token at the beginning of a name span is labeled with a B- prefix; each token within a name span is labeled with an I- prefix. These prefixes are followed by a tag for the entity type, e.g. B-LOC for the beginning of a location, and I-PROTEIN for the inside of a protein name. Tokens that are not parts of name spans are labeled as O. From this representation, the entity name spans can be recovered unambiguously. This tagging scheme is also advantageous for learning: tokens at the beginning of name spans may have different properties than tokens within the name, and the learner can exploit this. This insight can be taken even further, with special labels for the **l**ast tokens of a name span, and for **u**nique tokens in name spans, such as *Atlanta* in the example in [Figure 8.2.](#page-201-1) This is called BILOU notation, and it can yield improvements in supervised named entity recognition [\(Ratinov and Roth,](#page-557-6) [2009\)](#page-557-6).

**Feature-based sequence labeling** Named entity recognition was one of the first applications of conditional random fields [\(McCallum and Li, 2003\)](#page-547-2). The use of Viterbi decoding restricts the feature function f(w, y) to be a sum of local features, P <sup>m</sup> f(w, ym, ym−1, m), so that each feature can consider only local adjacent tags. Typical features include tag transitions, word features for w<sup>m</sup> and its neighbors, character-level features for prefixes and suffixes, and "word shape" features for capitalization and other orthographic properties. As an example, base features for the word *Army* in the example in [\(8.20a\)](#page-201-0) include:

(CURR-WORD:*Army*, PREV-WORD:*U.S.*, NEXT-WORD:*captured*, PREFIX-1:*A-*, PREFIX-2:*Ar-*, SUFFIX-1:*-y*, SUFFIX-2:*-my*, SHAPE:*Xxxx*)

Features can also be obtained from a **gazetteer**, which is a list of known entity names. For example, the U.S. Social Security Administration provides a list of tens of thousands of

<span id="page-201-3"></span>

(1) 日文 Japanese 章魚 octopus <sup>怎</sup>麼 how 說? say How to say octopus in Japanese?

(2) 日 Japan 文章 essay 魚 fish 怎麼 how 說? say

<span id="page-202-2"></span><span id="page-202-1"></span>Figure 8.3: An example of tokenization ambiguity in Chinese [\(Sproat et al., 1996\)](#page-563-7)

given names — more than could be observed in any annotated corpus. Tokens or spans that match an entry in a gazetteer can receive special features; this provides a way to incorporate hand-crafted resources such as name lists in a learning-driven framework.

**Neural sequence labeling for NER** Current research has emphasized neural sequence labeling, using similar LSTM models to those employed in part-of-speech tagging [\(Ham](#page-533-7)[merton, 2003;](#page-533-7) [Huang et al., 2015;](#page-536-3) [Lample et al., 2016\)](#page-542-3). The bidirectional LSTM-CRF [\(Fig](#page-186-1)[ure 7.4](#page-186-1) in § [7.6\)](#page-184-0) does particularly well on this task, due to its ability to model tag-to-tag dependencies. However, [Strubell et al.](#page-564-1) [\(2017\)](#page-564-1) show that **convolutional neural networks** can be equally accurate, with significant improvement in speed due to the efficiency of implementing ConvNets on **graphics processing units (GPUs)**. The key innovation in this work was the use of **dilated convolution**, which is described in more detail in § [3.4.](#page-79-0)

# <span id="page-202-0"></span>**8.4 Tokenization**

A basic problem for text analysis, first discussed in § [4.3.1,](#page-93-1) is to break the text into a sequence of discrete tokens. For alphabetic languages such as English, deterministic scripts usually suffice to achieve accurate tokenization. However, in logographic writing systems such as Chinese script, words are typically composed of a small number of characters, without intervening whitespace. The tokenization must be determined by the reader, with the potential for occasional ambiguity, as shown in [Figure 8.3.](#page-202-1) One approach is to match character sequences against a known dictionary (e.g., [Sproat et al., 1996\)](#page-563-7), using additional statistical information about word frequency. However, no dictionary is completely comprehensive, and dictionary-based approaches can struggle with such out-of-vocabulary words.

Chinese word segmentation has therefore been approached as a supervised sequence labeling problem. [Xue et al.](#page-570-2) [\(2003\)](#page-570-2) train a logistic regression classifier to make independent segmentation decisions while moving a sliding window across the document. A set of rules is then used to convert these individual classification decisions into an overall tokenization of the input. However, these individual decisions may be globally suboptimal, motivating a structure prediction approach. [Peng et al.](#page-554-3) [\(2004\)](#page-554-3) train a conditional random

<span id="page-203-2"></span>field to predict labels of START or NONSTART on each character. More recent work has employed neural network architectures. For example, [Chen et al.](#page-519-5) [\(2015\)](#page-519-5) use an LSTM-CRF architecture, as described in § [7.6:](#page-184-0) they construct a trellis, in which each tag is scored according to the hidden state of an LSTM, and tag-tag transitions are scored according to learned transition weights. The best-scoring segmentation is then computed by the Viterbi algorithm.

# <span id="page-203-0"></span>**8.5 Code switching**

Multilingual speakers and writers do not restrict themselves to a single language. **Code switching** is the phenomenon of switching between languages in speech and text [\(Auer,](#page-511-6) [2013;](#page-511-6) [Poplack, 1980\)](#page-556-2). Written code switching has become more common in online social media, as in the following extract from the website of Canadian President Justin Trudeau:[7](#page-203-1)

(8.21) *Although everything written on this site est* is *disponible* available *en* in *anglais* English *and in French, my personal videos seront* will be *bilingues* bilingual

Accurately analyzing such texts requires first determining which languages are being used. Furthermore, quantitative analysis of code switching can provide insights on the languages themselves and their relative social positions.

Code switching can be viewed as a sequence labeling problem, where the goal is to label each token as a candidate switch point. In the example above, the words *est*, *and*, and *seront* would be labeled as switch points. [Solorio and Liu](#page-562-6) [\(2008\)](#page-562-6) detect English-Spanish switch points using a supervised classifier, with features that include the word, its part-ofspeech in each language (according to a supervised part-of-speech tagger), and the probabilities of the word and part-of-speech in each language. [Nguyen and Dogruoz](#page-552-5) [\(2013\)](#page-552-5) ¨ apply a conditional random field to the problem of detecting code switching between Turkish and Dutch.

Code switching is a special case of the more general problem of word level language identification, which [Barman et al.](#page-512-4) [\(2014\)](#page-512-4) address in the context of trilingual code switching between Bengali, English, and Hindi. They further observe an even more challenging phenomenon: intra-word code switching, such as the use of English suffixes with Bengali roots. They therefore mark each token as either (1) belonging to one of the three languages; (2) a mix of multiple languages; (3) "universal" (e.g., symbols, numbers, emoticons); or (4) undefined.

<span id="page-203-1"></span><sup>7</sup>As quoted in [http://blogues.lapresse.ca/lagace/2008/09/08/](http://blogues.lapresse.ca/lagace/2008/09/08/justin-trudeau-really-parfait-bilingue/) [justin-trudeau-really-parfait-bilingue/](http://blogues.lapresse.ca/lagace/2008/09/08/justin-trudeau-really-parfait-bilingue/), accessed August 21, 2017.

<span id="page-204-3"></span><span id="page-204-2"></span>

| Speaker | Dialogue Act         | Utterance                                                |
|---------|----------------------|----------------------------------------------------------|
| А       | Yes-No-Question      | So do you go college right now?                          |
| А       | Abandoned            | Are yo-                                                  |
| В       | Yes-Answer           | Yeah,                                                    |
| В       | Statement            | It's my last year [laughter].                            |
| А       | DECLARATIVE-QUESTION | You're a, so you're a senior now.                        |
| В       | Yes-Answer           | Yeah,                                                    |
| В       | Statement            | I'm working on my projects trying to graduate [laughter] |
| А       | APPRECIATION         | Oh, good for you.                                        |
| В       | BACKCHANNEL          | Yeah.                                                    |

Figure 8.4: An example of dialogue act labeling (Stolcke et al., 2000)

### <span id="page-204-0"></span>8.6 Dialogue acts

The sequence labeling problems that we have discussed so far have been over sequences of word tokens or characters (in the case of tokenization). However, sequence labeling can also be performed over higher-level units, such as **utterances**. **Dialogue acts** are labels over utterances in a dialogue, corresponding roughly to the speaker's intention — the utterance's **illocutionary force** (Austin, 1962). For example, an utterance may state a proposition (*it is not down on any map*), pose a question (*shall we keep chasing this murderous fish?*), or provide a response (*aye aye!*). Stolcke et al. (2000) describe how a set of 42 dialogue acts were annotated for the 1,155 conversations in the Switchboard corpus (Godfrey et al., 1992).<sup>8</sup>

An example is shown in Figure 8.4. The annotation is performed over UTTERANCES, with the possibility of multiple utterances per **conversational turn** (in cases such as interruptions, an utterance may split over multiple turns). Some utterances are clauses (e.g., *So do you go to college right now?*), while others are single words (e.g., *yeah*). Stolcke et al. (2000) report that hidden Markov models (HMMs) achieve 96% accuracy on supervised utterance segmentation. The labels themselves reflect the conversational goals of the speaker: the utterance *yeah* functions as an answer in response to the question *you're a senior now*, but in the final line of the excerpt, it is a **backchannel** (demonstrating comprehension).

For task of dialogue act labeling, Stolcke et al. (2000) apply a hidden Markov model. The probability  $p(w_m | y_m)$  must generate the entire sequence of words in the utterance, and it is modeled as a trigram language model (§ 6.1). Stolcke et al. (2000) also account for acoustic features, which capture the **prosody** of each utterance — for example, tonal and rhythmic properties of speech, which can be used to distinguish dialogue acts such

<span id="page-204-1"></span><sup>&</sup>lt;sup>8</sup>Dialogue act modeling is not restricted to speech; it is relevant in any interactive conversation. For example, Jeong et al. (2009) annotate a more limited set of **speech acts** in a corpus of emails and online forums.

as questions and answers. These features are handled with an additional emission distribution, p(a<sup>m</sup> | ym), which is modeled with a probabilistic decision tree [\(Murphy, 2012\)](#page-551-1). While acoustic features yield small improvements overall, they play an important role in distinguish questions from statements, and agreements from backchannels.

Recurrent neural architectures for dialogue act labeling have been proposed by [Kalch](#page-538-4)[brenner and Blunsom](#page-538-4) [\(2013\)](#page-538-4) and [Ji et al.](#page-537-3) [\(2016\)](#page-537-3), with strong empirical results. Both models are recurrent at the utterance level, so that each complete utterance updates a hidden state. The recurrent-convolutional network of [Kalchbrenner and Blunsom](#page-538-4) [\(2013\)](#page-538-4) uses convolution to obtain a representation of each individual utterance, while [Ji et al.](#page-537-3) [\(2016\)](#page-537-3) use a second level of recurrence, over individual words. This enables their method to also function as a language model, giving probabilities over sequences of words in a document.

# **Exercises**

- 1. Using the Universal Dependencies part-of-speech tags, annotate the following sentences. You may examine the UD tagging guidelines. Tokenization is shown with whitespace. Don't forget about punctuation.
  - (8.22) a. I try all things , I achieve what I can .
    - b. It was that accursed white whale that razed me .
    - c. Better to sleep with a sober cannibal , than a drunk Christian .
    - d. Be it what it will , I 'll go to it laughing .
- 2. Select three short sentences from a recent news article, and annotate them for UD part-of-speech tags. Ask a friend to annotate the same three sentences without looking at your annotations. Compute the rate of agreement, using the Kappa metric defined in § [4.5.2.](#page-107-1) Then work together to resolve any disagreements.
- 3. Choose one of the following morphosyntactic attributes: MOOD, TENSE, VOICE. Research the definition of this attribute on the universal dependencies website, [http:](http://universaldependencies.org/u/feat/index.html) [//universaldependencies.org/u/feat/index.html](http://universaldependencies.org/u/feat/index.html). Returning to the examples in the first exercise, annotate all verbs for your chosen attribute. It may be helpful to consult examples from an English-language universal dependencies corpus, available at [https://github.com/UniversalDependencies/UD\\_Englis](https://github.com/UniversalDependencies/UD_English-EWT/tree/master)h-EWT/ [tree/master](https://github.com/UniversalDependencies/UD_English-EWT/tree/master).
- 4. Download a dataset annotated for universal dependencies, such as the English Treebank at [https://github.com/UniversalDependencies/UD\\_English-EWT/](https://github.com/UniversalDependencies/UD_English-EWT/tree/master) [tree/master](https://github.com/UniversalDependencies/UD_English-EWT/tree/master). This corpus is already segmented into training, development, and test data.

- a) First, train a logistic regression or SVM classifier using character suffixes: character n-grams up to length 4. Compute the recall, precision, and *F*-MEASURE on the development data.
- b) Next, augment your classifier using the same character suffixes of the preceding and succeeding tokens. Again, evaluate your classifier on heldout data.
- c) Optionally, train a Viterbi-based sequence labeling model, using a toolkit such as CRFSuite (http://www.chokkan.org/software/crfsuite/) or your own Viterbi implementation. This is more likely to be helpful for attributes in which agreement is required between adjacent words. For example, many Romance languages require gender and number agreement for determiners, nouns, and adjectives.
- 5. Provide BIO-style annotation of the named entities (person, place, organization, date, or product) in the following expressions:
  - (8.23) a. The third mate was Flask, a native of Tisbury, in Martha's Vineyard.
    - b. Its official Nintendo announced today that they Will release the Nintendo 3DS in north America march 27 (Ritter et al., 2011).
    - c. Jessica Reif, a media analyst at Merrill Lynch & Co., said, "If they can get up and running with exclusive programming within six months, it doesn't set the venture back that far."<sup>9</sup>
- 6. Run the examples above through the online version of a named entity recognition tagger, such as the Allen NLP system here: http://demo.allennlp.org/named-entity-recognition. Do the predicted tags match your annotations?
- 7. Build a whitespace tokenizer for English:
  - a) Using the NLTK library, download the complete text to the novel *Alice in Wonderland* (Carroll, 1865). Hold out the final 1000 words as a test set.
  - b) Label each alphanumeric character as a segmentation point,  $y_m = 1$  if m is the final character of a token. Label every other character as  $y_m = 0$ . Then concatenate all the tokens in the training and test sets.Make sure that the number of labels  $\{y_m\}_{m=1}^M$  is identical to the number of characters  $\{c_m\}_{m=1}^M$  in your concatenated datasets.
  - c) Train a logistic regression classifier to predict  $y_m$ , using the surrounding characters  $c_{m-5:m+5}$  as features. After training the classifier, run it on the test set, using the predicted segmentation points to re-tokenize the text.

<span id="page-206-0"></span><sup>&</sup>lt;sup>9</sup>From the Message Understanding Conference (MUC-7) dataset (Chinchor and Robinson, 1997).

- d) Compute the per-character segmentation accuracy on the test set. You should be able to get at least 88% accuracy.
- e) Print out a sample of segmented text from the test set, e.g.

```
Thereareno mice in the air , I ' m afraid , but y oumight cat
   chabat , and that ' svery like a mouse , youknow . But
   docatseat bats , I wonder ?'
```

- 8. Perform the following extensions to your tokenizer in the previous problem.
  - a) Train a conditional random field sequence labeler, by incorporating the tag bigrams (ym−1, ym) as additional features. You may use a structured prediction library such as CRFSuite, or you may want to implement Viterbi yourself. Compare the accuracy with your classification-based approach.
  - b) Compute the token-level performance: treating the original tokenization as ground truth, compute the number of true positives (tokens that are in both the ground truth and predicted tokenization), false positives (tokens that are in the predicted tokenization but not the ground truth), and false negatives (tokens that are in the ground truth but not the predicted tokenization). Compute the F-measure.

Hint: to match predicted and ground truth tokens, add "anchors" for the start character of each token. The number of true positives is then the size of the intersection of the sets of predicted and ground truth tokens.

c) Apply the same methodology in a more practical setting: tokenization of Chinese, which is written without whitespace. You can find annotated datasets at [http://alias-i.com/lingpipe/demos/tutorial/chineseTokens/r](http://alias-i.com/lingpipe/demos/tutorial/chineseTokens/read-me.html)ead-me. [html](http://alias-i.com/lingpipe/demos/tutorial/chineseTokens/read-me.html).

# <span id="page-208-1"></span><span id="page-208-0"></span>**Chapter 9 Formal language theory**

We have now seen methods for learning to label individual words, vectors of word counts, and sequences of words; we will soon proceed to more complex structural transformations. Most of these techniques could apply to counts or sequences from any discrete vocabulary; there is nothing fundamentally linguistic about, say, a hidden Markov model. This raises a basic question that this text has not yet considered: what is a language?

This chapter will take the perspective of **formal language theory**, in which a language is defined as a set of **strings**, each of which is a sequence of elements from a finite alphabet. For interesting languages, there are an infinite number of strings that are in the language, and an infinite number of strings that are not. For example:

- the set of all even-length sequences from the alphabet {a, b}, e.g., {∅, aa, ab, ba, bb, aaaa, aaab, . . .};
- the set of all sequences from the alphabet {a, b} that contain aaa as a substring, e.g., {aaa, aaaa, baaa, aaab, . . .};
- the set of all sequences of English words (drawn from a finite dictionary) that contain at least one verb (a finite subset of the dictionary);
- the PYTHON programming language.

Formal language theory defines classes of languages and their computational properties. Of particular interest is the computational complexity of solving the **membership problem** — determining whether a string is in a language. The chapter will focus on three classes of formal languages: regular, context-free, and "mildly" context-sensitive languages.

A key insight of 20th century linguistics is that formal language theory can be usefully applied to natural languages such as English, by designing formal languages that capture as many properties of the natural language as possible. For many such formalisms, a useful linguistic analysis comes as a byproduct of solving the membership problem. The <span id="page-209-1"></span>membership problem can be generalized to the problems of *scoring* strings for their acceptability (as in language modeling), and of **transducing** one string into another (as in translation).

# <span id="page-209-0"></span>9.1 Regular languages

If you have written a **regular expression**, then you have defined a **regular language**: a regular language is any language that can be defined by a regular expression. Formally, a regular expression can include the following elements:

- A literal character drawn from some finite alphabet  $\Sigma$ .
- The empty string  $\epsilon$ .
- The concatenation of two regular expressions RS, where R and S are both regular expressions. The resulting expression accepts any string that can be decomposed x = yz, where y is accepted by R and z is accepted by S.
- The alternation  $R \mid S$ , where R and S are both regular expressions. The resulting expression accepts a string x if it is accepted by R or it is accepted by S.
- The **Kleene star** *R*<sup>\*</sup>, which accepts any string *x* that can be decomposed into a sequence of strings which are all accepted by *R*.
- Parenthesization (*R*), which is used to limit the scope of the concatenation, alternation, and Kleene star operators.

Here are some example regular expressions:

- The set of all even length strings on the alphabet  $\{a, b\}$ :  $((aa)|(ab)|(ba)|(bb))^*$
- The set of all sequences of the alphabet  $\{a, b\}$  that contain *aaa* as a substring:  $(a|b)^*aaa(a|b)^*$
- The set of all sequences of English words that contain at least one verb:  $W^*VW^*$ , where W is an alternation between all words in the dictionary, and V is an alternation between all verbs ( $V \subseteq W$ ).

This list does not include a regular expression for the Python programming language, because this language is not regular — there is no regular expression that can capture its syntax. We will discuss why towards the end of this section.

Regular languages are **closed** under union, intersection, and concatenation. This means that if two languages  $L_1$  and  $L_2$  are regular, then so are the languages  $L_1 \cup L_2$ ,  $L_1 \cap L_2$ , and the language of strings that can be decomposed as s = tu, with  $s \in L_1$  and  $t \in L_2$ . Regular languages are also closed under negation: if L is regular, then so is the language  $\overline{L} = \{s \notin L\}$ .

![](_page_210_Figure_1.jpeg)

<span id="page-210-2"></span>Figure 9.1: State diagram for the finite state acceptor M1.

#### <span id="page-210-1"></span><span id="page-210-0"></span>**9.1.1 Finite state acceptors**

A regular expression defines a regular language, but does not give an algorithm for determining whether a string is in the language that it defines. **Finite state automata** are theoretical models of computation on regular languages, which involve transitions between a finite number of states. The most basic type of finite state automaton is the **finite state acceptor (FSA)**, which describes the computation involved in testing if a string is a member of a language. Formally, a finite state acceptor is a tuple M = (Q, Σ, q0, F, δ), consisting of:

- a finite alphabet Σ of input symbols;
- a finite set of states Q = {q0, q1, . . . , qn};
- a start state q<sup>0</sup> ∈ Q;
- a set of final states F ⊆ Q;
- a transition function δ : Q × (Σ ∪ {}) → 2 <sup>Q</sup>. The transition function maps from a state and an input symbol (or empty string ) to a *set* of possible resulting states.

A **path** in M is a sequence of transitions, π = t1, t2, . . . , t<sup>N</sup> , where each t<sup>i</sup> traverses an arc in the transition function δ. The finite state acceptor M accepts a string ω if there is an accepting path, in which the initial transition t<sup>1</sup> begins at the start state q0, the final transition t<sup>N</sup> terminates in a final state in Q, and the entire input ω is consumed.

#### **Example**

Consider the following FSA, M1.

$$\Sigma = \{a, b\}$$
[9.1]

$$Q = \{q_0, q_1\}$$
[9.2]

$$F = \{q_1\}$$

$$[9.3]$$

$$\delta = \{ (q_0, a) \to q_0, (q_0, b) \to q_1, (q_1, b) \to q_1 \}.$$
[9.4]

This FSA defines a language over an alphabet of two symbols, *a* and *b*. The transition function δ is written as a set of arcs: (q0, a) → q<sup>0</sup> says that if the machine is in state

<span id="page-211-2"></span>q<sup>0</sup> and reads symbol a, it stays in q0. [Figure 9.1](#page-210-1) provides a graphical representation of M1. Because each pair of initial state and symbol has at most one resulting state, M<sup>1</sup> is **deterministic**: each string ω induces at most one accepting path. Note that there are no transitions for the symbol a in state q1; if a is encountered in q1, then the acceptor is stuck, and the input string is rejected.

What strings does M<sup>1</sup> accept? The start state is q0, and we have to get to q1, since this is the only final state. Any number of a symbols can be consumed in q0, but a b symbol is required to transition to q1. Once there, any number of b symbols can be consumed, but an a symbol cannot. So the regular expression corresponding to the language defined by M<sup>1</sup> is a ∗ bb<sup>∗</sup> .

#### **Computational properties of finite state acceptors**

The key computational question for finite state acceptors is: how fast can we determine whether a string is accepted? For determistic FSAs, this computation can be performed by Dijkstra's algorithm, with time complexity O(V log V + E), where V is the number of vertices in the FSA, and E is the number of edges [\(Cormen et al., 2009\)](#page-522-0). Non-deterministic FSAs (NFSAs) can include multiple transitions from a given symbol and state. Any NSFA can be converted into a deterministic FSA, but the resulting automaton may have a number of states that is exponential in the number of size of the original NFSA [\(Mohri et al.,](#page-550-5) [2002\)](#page-550-5).

#### <span id="page-211-0"></span>**9.1.2 Morphology as a regular language**

Many words have internal structure, such as prefixes and suffixes that shape their meaning. The study of word-internal structure is the domain of **morphology**, of which there are two main types:

- **Derivational morphology** describes the use of affixes to convert a word from one grammatical category to another (e.g., from the noun *grace* to the adjective *graceful*), or to change the meaning of the word (e.g., from *grace* to *disgrace*).
- **Inflectional morphology** describes the addition of details such as gender, number, person, and tense (e.g., the *-ed* suffix for past tense in English).

Morphology is a rich topic in linguistics, deserving of a course in its own right.[1](#page-211-1) The focus here will be on the use of finite state automata for morphological analysis. The

<span id="page-211-1"></span><sup>1</sup>A good starting point would be a chapter from a linguistics textbook (e.g., [Akmajian et al., 2010;](#page-510-0) [Bender,](#page-513-0) [2013\)](#page-513-0). A key simplification in this chapter is the focus on affixes at the sole method of derivation and inflection. English makes use of affixes, but also incorporates **apophony**, such as the inflection of *foot* to *feet*. Semitic languages like Arabic and Hebrew feature a template-based system of morphology, in which roots are triples of consonants (e.g., *ktb*), and words are created by adding vowels: *kataba* (Arabic: he wrote), *kutub* (books), *maktab* (desk). For more detail on morphology, see texts from [Haspelmath and Sims](#page-533-5) [\(2013\)](#page-533-5) and [Lieber](#page-544-2) [\(2015\)](#page-544-2).

#### <span id="page-212-0"></span>9.1. REGULAR LANGUAGES 195

current section deals with derivational morphology; inflectional morphology is discussed in § [9.1.4.](#page-219-0)

Suppose that we want to write a program that accepts only those words that are constructed in accordance with the rules of English derivational morphology:

- (9.1) a. grace, graceful, gracefully, \*gracelyful
  - b. disgrace, \*ungrace, disgraceful, disgracefully
  - c. allure, \*allureful, alluring, alluringly
  - d. fairness, unfair, \*disfair, fairly

(Recall that the asterisk indicates that a linguistic example is judged unacceptable by fluent speakers of a language.) These examples cover only a tiny corner of English derivational morphology, but a number of things stand out. The suffix *-ful* converts the nouns *grace* and *disgrace* into adjectives, and the suffix *-ly* converts adjectives into adverbs. These suffixes must be applied in the correct order, as shown by the unacceptability of *\*gracelyful*. The *-ful* suffix works for only some words, as shown by the use of *alluring* as the adjectival form of *allure*. Other changes are made with prefixes, such as the derivation of *disgrace* from *grace*, which roughly corresponds to a negation; however, *fair* is negated with the *un-* prefix instead. Finally, while the first three examples suggest that the direction of derivation is noun → adjective → adverb, the example of *fair* suggests that the adjective can also be the base form, with the *-ness* suffix performing the conversion to a noun.

Can we build a computer program that accepts only well-formed English words, and rejects all others? This might at first seem trivial to solve with a brute-force attack: simply make a dictionary of all valid English words. But such an approach fails to account for morphological **productivity** — the applicability of existing morphological rules to new words and names, such as *Trump* to *Trumpy* and *Trumpkin*, and *Clinton* to *Clintonian* and *Clintonite*. We need an approach that represents morphological rules explicitly, and for this we will try a finite state acceptor.

The dictionary approach can be implemented as a finite state acceptor, with the vocabulary Σ equal to the vocabulary of English, and a transition from the start state to the accepting state for each word. But this would of course fail to generalize beyond the original vocabulary, and would not capture anything about the **morphotactic** rules that govern derivations from new words. The first step towards a more general approach is shown in [Figure 9.2,](#page-213-1) which is the state diagram for a finite state acceptor in which the vocabulary consists of **morphemes**, which include **stems** (e.g., *grace, allure*) and **affixes** (e.g., *dis-, -ing, -ly*). This finite state acceptor consists of a set of paths leading away from the start state, with derivational affixes added along the path. Except for qneg, the states on these paths are all final, so the FSA will accept *disgrace*, *disgraceful*, and *disgracefully*, but not *dis-*.

<span id="page-213-2"></span><span id="page-213-1"></span>![](_page_213_Figure_1.jpeg)

Figure 9.2: A finite state acceptor for a fragment of English derivational morphology. Each path represents possible derivations from a single root form.

This FSA can be **minimized** to the form shown in [Figure 9.3,](#page-214-0) which makes the generality of the finite state approach more apparent. For example, the transition from q<sup>0</sup> to qJ<sup>2</sup> can be made to accept not only *fair* but any single-morpheme (**monomorphemic**) adjective that takes *-ness* and *-ly* as suffixes. In this way, the finite state acceptor can easily be extended: as new word stems are added to the vocabulary, their derived forms will be accepted automatically. Of course, this FSA would still need to be extended considerably to cover even this small fragment of English morphology. As shown by cases like *music* → *musical*, *athlete* → *athletic*, English includes several classes of nouns, each with its own rules for derivation.

The FSAs shown in [Figure 9.2](#page-213-1) and [9.3](#page-214-0) accept *allureing*, not *alluring*. This reflects a distinction between morphology — the question of which morphemes to use, and in what order — and **orthography** — the question of how the morphemes are rendered in written language. Just as orthography requires dropping the *e* preceding the *-ing* suffix, **phonology** imposes a related set of constraints on how words are rendered in speech. As we will see soon, these issues can be handled by **finite state!transducers**, which are finite state automata that take inputs and produce outputs.

#### <span id="page-213-0"></span>**9.1.3 Weighted finite state acceptors**

According to the FSA treatment of morphology, every word is either in or out of the language, with no wiggle room. Perhaps you agree that *musicky* and *fishful* are not valid English words; but if forced to choose, you probably find *a fishful stew* or *a musicky tribute* preferable to *behaving disgracelyful*. Rather than asking whether a word is acceptable, we might like to ask how acceptable it is. [Aronoff](#page-511-7) [\(1976,](#page-511-7) page 36) puts it another way:

#### <span id="page-214-3"></span>9.1. REGULAR LANGUAGES

<span id="page-214-0"></span>![](_page_214_Figure_1.jpeg)

Figure 9.3: Minimization of the finite state acceptor shown in Figure 9.2.

"Though many things are possible in morphology, some are more possible than others." But finite state acceptors give no way to express preferences among technically valid choices.

Weighted finite state acceptors (WFSAs) are generalizations of FSAs, in which each accepting path is assigned a score, computed from the transitions, the initial state, and the final state. Formally, a weighted finite state acceptor  $M = (Q, \Sigma, \lambda, \rho, \delta)$  consists of:

- a finite set of states  $Q = \{q_0, q_1, \dots, q_n\};$
- a finite alphabet  $\Sigma$  of input symbols;
- an initial weight function,  $\lambda : Q \to \mathbb{R}$ ;
- a final weight function  $\rho : Q \to \mathbb{R}$ ;
- a transition function  $\delta : Q \times \Sigma \times Q \to \mathbb{R}$ .

WFSAs depart from the FSA formalism in three ways: every state can be an initial state, with score  $\lambda(q)$ ; every state can be an accepting state, with score  $\rho(q)$ ; transitions are possible between any pair of states on any input, with a score  $\delta(q_i, \omega, q_j)$ . Nonetheless, FSAs can be viewed as a special case: for any FSA M we can build an equivalent WFSA by setting  $\lambda(q) = \infty$  for all  $q \neq q_0$ ,  $\rho(q) = \infty$  for all  $q \notin F$ , and  $\delta(q_i, \omega, q_j) = \infty$  for all transitions  $\{(q_1, \omega) \rightarrow q_2\}$  that are not permitted by the transition function of M.

The total score for any path  $\pi = t_1, t_2, \dots, t_N$  is equal to the sum of these scores,

<span id="page-214-2"></span>
$$d(\pi) = \lambda(\text{from-state}(t_1)) + \sum_{n=1}^{N} \delta(t_n) + \rho(\text{to-state}(t_N)).$$
[9.5]

A **shortest-path algorithm** is used to find the minimum-cost path through a WFSA for string  $\omega$ , with time complexity  $\mathcal{O}(E + V \log V)$ , where *E* is the number of edges and *V* is the number of vertices (Cormen et al., 2009).<sup>2</sup>

<span id="page-214-1"></span><sup>&</sup>lt;sup>2</sup>Shortest-path algorithms find the path with the minimum cost. In many cases, the path weights are log

#### <span id="page-215-1"></span><span id="page-215-0"></span>N-gram language models as WFSAs

In *n*-gram language models (see § 6.1), the probability of a sequence of tokens  $w_1, w_2, \ldots, w_M$  is modeled as,

$$\mathbf{p}(w_1, \dots, w_M) \approx \prod_{m=1}^M \mathbf{p}_n(w_m \mid w_{m-1}, \dots, w_{m-n+1}).$$
 [9.6]

The log probability under an *n*-gram language model can be modeled in a WFSA. First consider a unigram language model. We need only a single state  $q_0$ , with transition scores  $\delta(q_0, \omega, q_0) = \log p_1(\omega)$ . The initial and final scores can be set to zero. Then the path score for  $w_1, w_2, \ldots, w_M$  is equal to,

$$0 + \sum_{m}^{M} \delta(q_0, w_m, q_0) + 0 = \sum_{m}^{M} \log \mathbf{p}_1(w_m).$$
[9.7]

For an *n*-gram language model with n > 1, we need probabilities that condition on the past history. For example, in a bigram language model, the transition weights must represent  $\log p_2(w_m | w_{m-1})$ . The transition scoring function must somehow "remember" the previous word or words. This can be done by adding more states: to model the bigram probability  $p_2(w_m | w_{m-1})$ , we need a state for every possible  $w_{m-1}$  — a total of V states. The construction indexes each state  $q_i$  by a context event  $w_{m-1} = i$ . The weights are then assigned as follows:

$$\delta(q_i, \omega, q_j) = \begin{cases} \log \Pr(w_m = j \mid w_{m-1} = i), & \omega = j \\ -\infty, & \omega \neq j \end{cases}$$
$$\lambda(q_i) = \log \Pr(w_1 = i \mid w_0 = \Box)$$
$$\rho(q_i) = \log \Pr(w_{M+1} = \blacksquare \mid w_M = i). \end{cases}$$

The transition function is designed to ensure that the context is recorded accurately: we can move to state j on input  $\omega$  only if  $\omega = j$ ; otherwise, transitioning to state j is forbidden by the weight of  $-\infty$ . The initial weight function  $\lambda(q_i)$  is the log probability of receiving i as the first token, and the final weight function  $\rho(q_i)$  is the log probability of receiving an "end-of-string" token after observing  $w_M = i$ .

#### \*Semiring weighted finite state acceptors

The *n*-gram language model WFSA is deterministic: each input has exactly one accepting path, for which the WFSA computes a score. In non-deterministic WFSAs, a given input

probabilities, so we want the path with the maximum score, which can be accomplished by making each local score into a *negative* log-probability.

#### <span id="page-216-0"></span>9.1. REGULAR LANGUAGES

may have multiple accepting paths. In some applications, the score for the input is aggregated across all such paths. Such aggregate scores can be computed by generalizing WFSAs with **semiring notation**, first introduced in  $\S$  7.7.3.

Let  $d(\pi)$  represent the total score for path  $\pi = t_1, t_2, \ldots, t_N$ , which is computed as,

$$d(\pi) = \lambda(\text{from-state}(t_1)) \otimes \delta(t_1) \otimes \delta(t_2) \otimes \ldots \otimes \delta(t_N) \otimes \rho(\text{to-state}(t_N)).$$
[9.8]

This is a generalization of Equation 9.5 to semiring notation, using the semiring multiplication operator  $\otimes$  in place of addition.

Now let  $s(\omega)$  represent the total score for all paths  $\Pi(\omega)$  that consume input  $\omega$ ,

$$s(\omega) = \bigoplus_{\pi \in \Pi(\omega)} d(\pi).$$
[9.9]

Here, semiring addition  $(\oplus)$  is used to combine the scores of multiple paths.

The generalization to semirings covers a number of useful special cases. In the logprobability semiring, multiplication is defined as  $\log p(x) \otimes \log p(y) = \log p(x) + \log p(y)$ , and addition is defined as  $\log p(x) \oplus \log p(y) = \log(p(x) + p(y))$ . Thus,  $s(\omega)$  represents the log-probability of accepting input  $\omega$ , marginalizing over all paths  $\pi \in \Pi(\omega)$ . In the **boolean semiring**, the  $\otimes$  operator is logical conjunction, and the  $\oplus$  operator is logical disjunction. This reduces to the special case of unweighted finite state acceptors, where the score  $s(\omega)$  is a boolean indicating whether there exists any accepting path for  $\omega$ . In the **tropical semiring**, the  $\oplus$  operator is a maximum, so the resulting score is the score of the best-scoring path through the WFSA. The OPENFST toolkit uses semirings and polymorphism to implement general algorithms for weighted finite state automata (Allauzen et al., 2007).

#### \*Interpolated *n*-gram language models

Recall from § 6.2.3 that an **interpolated** *n***-gram language model** combines the probabilities from multiple *n*-gram models. For example, an interpolated bigram language model computes the probability,

$$\hat{\mathbf{p}}(w_m \mid w_{m-1}) = \lambda_1 \mathbf{p}_1(w_m) + \lambda_2 \mathbf{p}_2(w_m \mid w_{m-1}),$$
[9.10]

with  $\hat{p}$  indicating the interpolated probability,  $p_2$  indicating the bigram probability, and  $p_1$  indicating the unigram probability. Setting  $\lambda_2 = (1 - \lambda_1)$  ensures that the probabilities sum to one.

Interpolated bigram language models can be implemented using a non-deterministic WFSA (Knight and May, 2009). The basic idea is shown in Figure 9.4. In an interpolated bigram language model, there is one state for each element in the vocabulary — in this

<span id="page-217-0"></span>![](_page_217_Figure_1.jpeg)

Figure 9.4: WFSA implementing an interpolated bigram/unigram language model, on the alphabet  $\Sigma = \{a, b\}$ . For simplicity, the WFSA is contrained to force the first token to be generated from the unigram model, and does not model the emission of the end-of-sequence token.

case, the states  $q_A$  and  $q_B$  — which are capture the contextual conditioning in the bigram probabilities. To model unigram probabilities, there is an additional state  $q_U$ , which "forgets" the context. Transitions out of  $q_U$  involve unigram probabilities,  $p_1(a)$  and  $p_2(b)$ ; transitions into  $q_U$  emit the empty symbol  $\epsilon$ , and have probability  $\lambda_1$ , reflecting the interpolation weight for the unigram model. The interpolation weight for the bigram model is included in the weight of the transition  $q_A \rightarrow q_B$ .

The epsilon transitions into  $q_U$  make this WFSA non-deterministic. Consider the score for the sequence (a, b, b). The initial state is  $q_U$ , so the symbol a is generated with score  $p_1(a)^3$  Next, we can generate b from the unigram model by taking the transition  $q_A \rightarrow q_B$ , with score  $\lambda_2 p_2(b \mid a)$ . Alternatively, we can take a transition back to  $q_U$  with score  $\lambda_1$ , and then emit b from the unigram model with score  $p_1(b)$ . To generate the final b token, we face the same choice: emit it directly from the self-transition to  $q_B$ , or transition to  $q_U$ first.

The total score for the sequence (a, b, b) is the semiring sum over all accepting paths,

<span id="page-217-2"></span>
$$s(a, b, b) = (p_1(a) \otimes \lambda_2 p_2(b \mid a) \otimes \lambda_2 p(b \mid b))$$
  

$$\oplus (p_1(a) \otimes \lambda_1 \otimes p_1(b) \otimes \lambda_2 p(b \mid b))$$
  

$$\oplus (p_1(a) \otimes \lambda_2 p_2(b \mid a) \otimes p_1(b) \otimes p_1(b))$$
  

$$\oplus (p_1(a) \otimes \lambda_1 \otimes p_1(b) \otimes p_1(b) \otimes p_1(b)).$$
[9.11]

Each line in Equation 9.11 represents the probability of a specific path through the WFSA. In the probability semiring,  $\otimes$  is multiplication, so that each path is the product of each

<span id="page-217-1"></span><sup>&</sup>lt;sup>3</sup>We could model the sequence-initial bigram probability  $p_2(a \mid \Box)$ , but for simplicity the WFSA does not admit this possibility, which would require another state.

<span id="page-218-2"></span>transition weight, which are themselves probabilities. The ⊕ operator is addition, so that the total score is the sum of the scores (probabilities) for each path. This corresponds to the probability under the interpolated bigram language model.

#### <span id="page-218-0"></span>**9.1.4 Finite state transducers**

Finite state acceptors can determine whether a string is in a regular language, and weighted finite state acceptors can compute a score for every string over a given alphabet. **Finite state transducers** (FSTs) extend the formalism further, by adding an output symbol to each transition. Formally, a finite state transducer is a tuple T = (Q, Σ, Ω, λ, ρ, δ), with Ω representing an output vocabulary and the transition function δ : Q×(Σ ∪)×(Ω ∪)×Q → R mapping from states, input symbols, and output symbols to states. The remaining elements (Q, Σ, λ, ρ) are identical to their definition in weighted finite state acceptors (§ [9.1.3\)](#page-213-0). Thus, each path through the FST T transduces the input string into an output.

#### <span id="page-218-1"></span>**String edit distance**

The **edit distance** between two strings s and t is a measure of how many operations are required to transform one string into another. There are several ways to compute edit distance, but one of the most popular is the Levenshtein edit distance, which counts the minimum number of insertions, deletions, and substitutions. This can be computed by a one-state weighted finite state transducer, in which the input and output alphabets are identical. For simplicity, consider the alphabet Σ = Ω = {a, b}. The edit distance can be computed by a one-state transducer with the following transitions,

$$\delta(q, a, a, q) = \delta(q, b, b, q) = 0$$
[9.12]

$$\delta(q, a, b, q) = \delta(q, b, a, q) = 1$$
[9.13]

$$\delta(q, a, \epsilon, q) = \delta(q, b, \epsilon, q) = 1$$
[9.14]

$$\delta(q,\epsilon,a,q) = \delta(q,\epsilon,b,q) = 1.$$
[9.15]

The state diagram is shown in [Figure 9.5.](#page-219-1)

For a given string pair, there are multiple paths through the transducer: the bestscoring path from *dessert* to *desert* involves a single deletion, for a total score of 1; the worst-scoring path involves seven deletions and six additions, for a score of 13.

#### **The Porter stemmer**

The [Porter](#page-556-1) [\(1980\)](#page-556-1) stemming algorithm is a "lexicon-free" algorithm for stripping suffixes from English words, using a sequence of character-level rules. Each rule can be described

<span id="page-219-2"></span><span id="page-219-1"></span>![](_page_219_Figure_1.jpeg)

Figure 9.5: State diagram for the Levenshtein edit distance finite state transducer. The label x/y : c indicates a cost of c for a transition with input x and output y.

by an unweighted finite state transducer. The first rule is:

$$-sses \rightarrow -ss$$
 e.g., dresses  $\rightarrow dress$  [9.16]

$$-ies \rightarrow -i$$
 e.g., parties  $\rightarrow$  parti [9.17]

$$-ss \rightarrow -ss$$
 e.g., dress  $\rightarrow$  dress [9.18]

$$-s \to \epsilon \quad \text{e.g., } cats \to cat$$
 [9.19]

The final two lines appear to conflict; they are meant to be interpreted as an instruction to remove a terminal *-s* unless it is part of an *-ss* ending. A state diagram to handle just these final two lines is shown in [Figure 9.6.](#page-220-0) Make sure you understand how this finite state transducer handles *cats*, *steps*, *bass*, and *basses*.

#### <span id="page-219-0"></span>**Inflectional morphology**

In **inflectional morphology**, word **lemmas** are modified to add grammatical information such as tense, number, and case. For example, many English nouns are pluralized by the suffix *-s*, and many verbs are converted to past tense by the suffix *-ed*. English's inflectional morphology is considerably simpler than many of the world's languages. For example, Romance languages (derived from Latin) feature complex systems of verb suffixes which must agree with the person and number of the verb, as shown in [Table 9.1.](#page-220-1)

The task of morphological analysis is to read a form like *canto*, and output an analysis like CANTAR+VERB+PRESIND+1P+SING, where +PRESIND describes the tense as present indicative, +1P indicates the first-person, and +SING indicates the singular number. The task of morphological generation is the reverse, going from CANTAR+VERB+PRESIND+1P+SING to *canto*. Finite state transducers are an attractive solution, because they can solve both problems with a single model [\(Beesley and Karttunen, 2003\)](#page-513-5). As an example, [Figure 9.7](#page-221-1) shows a fragment of a finite state transducer for Spanish inflectional morphology. The

<span id="page-220-2"></span><span id="page-220-0"></span>start q<sup>1</sup> q<sup>2</sup> ¬s/¬s s/ q3 q4 . . . a/s b/s /a /b

Figure 9.6: State diagram for final two lines of step 1a of the Porter stemming diagram. States q<sup>3</sup> and q<sup>4</sup> "remember" the observations a and b respectively; the ellipsis . . . represents additional states for each symbol in the input alphabet. The notation ¬s/¬s is not part of the FST formalism; it is a shorthand to indicate a set of self-transition arcs for every input/output symbol except s.

<span id="page-220-1"></span>

| infinitive                                         | cantar (to sing) | comer (to eat)  | vivir (to live) |
|----------------------------------------------------|------------------|-----------------|-----------------|
| yo (1st singular)                                  | canto            | como            | vivo            |
| tu (2nd singular)                                  | cantas           | comes           | vives           |
| el, ella, usted (3rd singular)<br>´                | canta            | come            | vive            |
| nosotros (1st plural)                              | cantamos         | comemos         | vivimos         |
| vosotros (2nd plural, informal)                    | cantais<br>´     | com<br>eis<br>´ | viv<br>´ıs      |
| ellos, ellas (3rd plural);<br>ustedes (2nd plural) | cantan           | comen           | viven           |

Table 9.1: Spanish verb inflections for the present indicative tense. Each row represents a person and number, and each column is a regular example from a class of verbs, as indicated by the ending of the infinitive form.

input vocabulary Σ corresponds to the set of letters used in Spanish spelling, and the output vocabulary Ω corresponds to these same letters, plus the vocabulary of morphological features (e.g., +SING, +VERB). In [Figure 9.7,](#page-221-1) there are two paths that take *canto* as input, corresponding to the verb and noun meanings; the choice between these paths could be guided by a part-of-speech tagger. By **inversion**, the inputs and outputs for each transition are switched, resulting in a finite state generator, capable of producing the correct **surface form** for any morphological analysis.

Finite state morphological analyzers and other unweighted transducers can be designed by hand. The designer's goal is to avoid **overgeneration** — accepting strings or making transductions that are not valid in the language — as well as **undergeneration**

<span id="page-221-2"></span><span id="page-221-1"></span>![](_page_221_Figure_1.jpeg)

Figure 9.7: Fragment of a finite state transducer for Spanish morphology. There are two accepting paths for the input *canto*: *canto*+NOUN+MASC+SING (masculine singular noun, meaning a song), and *cantar*+VERB+PRESIND+1P+SING (I sing). There is also an accepting path for *canta*, with output *cantar*+VERB+PRESIND+3P+SING (he/she sings).

— failing to accept strings or transductions that are valid. For example, a pluralization transducer that does not accept *foot/feet* would undergenerate. Suppose we "fix" the transducer to accept this example, but as a side effect, it now accepts *boot/beet*; the transducer would then be said to overgenerate. If a transducer accepts *foot/foots* but not *foot/feet*, then it simultaneously overgenerates and undergenerates.

#### <span id="page-221-0"></span>Finite state composition

Designing finite state transducers to capture the full range of morphological phenomena in any real language is a huge task. Modularization is a classic computer science approach for this situation: decompose a large and unwieldly problem into a set of subproblems, each of which will hopefully have a concise solution. Finite state automata can be modularized through **composition**: feeding the output of one transducer  $T_1$  as the input to another transducer  $T_2$ , written  $T_2 \circ T_1$ . Formally, if there exists some y such that  $(x, y) \in T_1$ (meaning that  $T_1$  produces output y on input x), and  $(y, z) \in T_2$ , then  $(x, z) \in (T_2 \circ T_1)$ . Because finite state transducers are closed under composition, there is guaranteed to be a single finite state transducer that  $T_3 = T_2 \circ T_1$ , which can be constructed as a machine with one state for each pair of states in  $T_1$  and  $T_2$  (Mohri et al., 2002).

**Example: Morphology and orthography** In English morphology, the suffix *-ed* is added to signal the past tense for many verbs:  $cook \rightarrow cooked$ ,  $want \rightarrow wanted$ , etc. However, English **orthography** dictates that this process cannot produce a spelling with consecutive *e*'s, so that  $bake \rightarrow baked$ , not *bakeed*. A modular solution is to build separate transducers for morphology and orthography. The morphological transducer  $T_M$  transduces from bake+PAST to bake+ed, with the + symbol indicating a segment boundary. The input alphabet of  $T_M$  includes the lexicon of words and the set of morphological features; the output alphabet includes the characters *a*-*z* and the + boundary marker. Next, an orthographic transducer  $T_O$  is responsible for the transductions  $cook+ed \rightarrow cooked$ , and  $bake+ed \rightarrow baked$ . The input alphabet of  $T_O$  must be the same as the output alphabet for  $T_M$ , and the output alphabet

<span id="page-222-0"></span>is simply the characters *a*-*z*. The composed transducer  $(T_O \circ T_M)$  then transduces from *bake*+PAST to the spelling *baked*. The design of  $T_O$  is left as an exercise.

**Example: Hidden Markov models** Hidden Markov models (chapter 7) can be viewed as weighted finite state transducers, and they can be constructed by transduction. Recall that a hidden Markov model defines a joint probability over words and tags, p(w, y), which can be computed as a path through a **trellis** structure. This trellis is itself a weighted finite state acceptor, with edges between all adjacent nodes  $q_{m-1,i} \rightarrow q_{m,j}$  on input  $Y_m = j$ . The edge weights are log-probabilities,

$$\delta(q_{m-1,i}, Y_m = j, q_{m,j}) = \log p(w_m, Y_m = j \mid Y_{m-i} = j)$$
[9.20]

$$= \log p(w_m \mid Y_m = j) + \log \Pr(Y_m = j \mid Y_{m-1} = i).$$
 [9.21]

Because there is only one possible transition for each tag  $Y_m$ , this WFSA is deterministic. The score for any tag sequence  $\{y_m\}_{m=1}^M$  is the sum of these log-probabilities, corresponding to the total log probability  $\log p(w, y)$ . Furthermore, the trellis can be constructed by the composition of simpler FSTs.

- First, construct a "transition" transducer to represent a bigram probability model over tag sequences,  $T_T$ . This transducer is almost identical to the *n*-gram language model acceptor in § 9.1.3: there is one state for each tag, and the edge weights equal to the transition log-probabilities,  $\delta(q_i, j, j, q_j) = \log \Pr(Y_m = j \mid Y_{m-1} = i)$ . Note that  $T_T$  is a transducer, with identical input and output at each arc; this makes it possible to compose  $T_T$  with other transducers.
- Next, construct an "emission" transducer to represent the probability of words given tags,  $T_E$ . This transducer has only a single state, with arcs for each word/tag pair,  $\delta(q_0, i, j, q_0) = \log \Pr(W_m = j | Y_m = i)$ . The input vocabulary is the set of all tags, and the output vocabulary is the set of all words.
- The composition  $T_E \circ T_T$  is a finite state transducer with one state per tag, as shown in Figure 9.8. Each state has  $V \times K$  outgoing edges, representing transitions to each of the *K* other states, with outputs for each of the *V* words in the vocabulary. The weights for these edges are equal to,

$$\delta(q_i, Y_m = j, w_m, q_j) = \log \mathbf{p}(w_m, Y_m = j \mid Y_{m-1} = i).$$
[9.22]

• The trellis is a structure with  $M \times K$  nodes, for each of the M words to be tagged and each of the K tags in the tagset. It can be built by composition of  $(T_E \circ T_T)$  against an unweighted **chain FSA**  $M_A(w)$  that is specially constructed to accept only a given input  $w_1, w_2, \ldots, w_M$ , shown in Figure 9.9. The trellis for input w is built from the composition  $M_A(w) \circ (T_E \circ T_T)$ . Composing with the unweighted  $M_A(w)$  does not affect the edge weights from  $(T_E \circ T_T)$ , but it selects the subset of paths that generate the word sequence w.

<span id="page-223-3"></span><span id="page-223-1"></span>![](_page_223_Figure_1.jpeg)

Figure 9.8: Finite state transducer for hidden Markov models, with a small tagset of **n**ouns and **v**erbs. For each pair of tags (including self-loops), there is an edge for every word in the vocabulary. For simplicity, input and output are only shown for the edges from the start state. Weights are also omitted from the diagram; for each edge from q<sup>i</sup> to q<sup>j</sup> , the weight is equal to log p(wm, Y<sup>m</sup> = j | Ym−<sup>1</sup> = i), except for edges to the end state, which are equal to log Pr(Y<sup>m</sup> <sup>=</sup> <sup>|</sup> <sup>Y</sup>m−<sup>1</sup> <sup>=</sup> <sup>i</sup>).

![](_page_223_Figure_3.jpeg)

<span id="page-223-2"></span>Figure 9.9: Chain finite state acceptor for the input *They can fish*.

#### <span id="page-223-0"></span>**9.1.5 \*Learning weighted finite state automata**

In generative models such as n-gram language models and hidden Markov models, the edge weights correspond to log probabilities, which can be obtained from relative frequency estimation. However, in other cases, we wish to learn the edge weights from input/output pairs. This is difficult in non-deterministic finite state automata, because we do not observe the specific arcs that are traversed in accepting the input, or in transducing from input to output. The path through the automaton is a **latent variable**.

Chapter [5](#page-112-0) presented one method for learning with latent variables: expectation maximization (EM). This involves computing a distribution q(·) over the latent variable, and iterating between updates to this distribution and updates to the parameters — in this case, the arc weights. The **forward-backward algorithm** (§ [7.5.3\)](#page-182-2) describes a dynamic program for computing a distribution over arcs in the trellis structure of a hidden Markov

<span id="page-224-2"></span>model, but this is a special case of the more general problem for finite state automata. Eisner (2002) describes an **expectation semiring**, which enables the expected number of transitions across each arc to be computed through a semiring shortest-path algorithm. Alternative approaches for generative models include Markov Chain Monte Carlo (Chiang et al., 2010) and spectral learning (Balle et al., 2011).

Further afield, we can take a perceptron-style approach, with each arc corresponding to a feature. The classic perceptron update would update the weights by subtracting the difference between the feature vector corresponding to the predicted path and the feature vector corresponding to the correct path. Since the path is not observed, we resort to a **latent variable perceptron**. The model is described formally in § 12.4, but the basic idea is to compute an update from the difference between the features from the predicted path and the feature and the features for the best-scoring path that generates the correct output.

# <span id="page-224-0"></span>9.2 Context-free languages

Beyond the class of regular languages lie the context-free languages. An example of a language that is context-free but not finite state is the set of arithmetic expressions with balanced parentheses. Intuitively, to accept only strings in this language, an FSA would have to "count" the number of left parentheses, and make sure that they are balanced against the number of right parentheses. An arithmetic expression can be arbitrarily long, yet by definition an FSA has a finite number of states. Thus, for any FSA, there will be a string with too many parentheses to count. More formally, the **pumping lemma** is a proof technique for showing that languages are not regular. It is typically demonstrated for the simpler case  $a^n b^n$ , the language of strings containing a sequence of a's, and then an equal-length sequence of b's.<sup>4</sup>

There are at least two arguments for the relevance of non-regular formal languages to linguistics. First, there are natural language phenomena that are argued to be isomorphic to  $a^n b^n$ . For English, the classic example is **center embedding**, shown in Figure 9.10. The initial expression *the dog* specifies a single dog. Embedding this expression into *the cat* \_\_\_\_\_ *chased* specifies a particular cat — the one chased by the dog. This cat can then be embedded again to specify a goat, in the less felicitous but arguably grammatical expression, *the goat the cat the dog chased kissed*, which refers to the goat who was kissed by the cat which was chased by the dog. Chomsky (1957) argues that to be grammatical, a center-embedded construction must be balanced: if it contains *n* noun phrases (e.g., *the cat*), they must be followed by exactly n - 1 verbs. An FSA that could recognize such expressions would also be capable of recognizing the language  $a^n b^n$ . Because we can prove that no FSA exists for  $a^n b^n$ , no FSA can exist for center embedded constructions either. En-

<span id="page-224-1"></span><sup>&</sup>lt;sup>4</sup>Details of the proof can be found in an introductory computer science theory textbook (e.g., Sipser, 2012).

<span id="page-225-3"></span><span id="page-225-1"></span>

|          |         | the dog |        |        |
|----------|---------|---------|--------|--------|
|          | the cat | the dog | chased |        |
| the goat | the cat | the dog | chased | kissed |
|          |         |         |        |        |

Figure 9.10: Three levels of center embedding

glish includes center embedding, and so the argument goes, English grammar as a whole cannot be regular.[5](#page-225-2)

A more practical argument for moving beyond regular languages is modularity. Many linguistic phenomena — especially in syntax — involve constraints that apply at long distance. Consider the problem of determiner-noun number agreement in English: we can say *the coffee* and *these coffees*, but not *\*these coffee*. By itself, this is easy enough to model in an FSA. However, fairly complex modifying expressions can be inserted between the determiner and the noun:

- (9.2) a. the burnt coffee
  - b. the badly-ground coffee
  - c. the burnt and badly-ground Italian coffee
  - d. these burnt and badly-ground Italian coffees
  - e. \* these burnt and badly-ground Italian coffee

Again, an FSA can be designed to accept modifying expressions such as *burnt and badlyground Italian*. Let's call this FSA FM. To reject the final example, a finite state acceptor must somehow "remember" that the determiner was plural when it reaches the noun *coffee* at the end of the expression. The only way to do this is to make two identical copies of FM: one for singular determiners, and one for plurals. While this is possible in the finite state framework, it is inconvenient — especially in languages where more than one attribute of the noun is marked by the determiner. **Context-free languages** facilitate modularity across such long-range dependencies.

#### <span id="page-225-0"></span>**9.2.1 Context-free grammars**

Context-free languages are specified by **context-free grammars** (CFGs), which are tuples (N, Σ, R, S) consisting of:

<span id="page-225-2"></span><sup>5</sup>The claim that arbitrarily deep center-embedded expressions are grammatical has drawn skepticism. Corpus evidence shows that embeddings of depth greater than two are exceedingly rare [\(Karlsson, 2007\)](#page-539-9), and that embeddings of depth greater than three are completely unattested. If center-embedding is capped at some finite depth, then it is regular.

$$S \rightarrow S \text{ Op } S \mid \text{Num}$$
  
 $\text{Op} \rightarrow + \mid - \mid \times \mid \div$   
 $\text{Num} \rightarrow \text{Num Digit} \mid \text{Digit}$   
 $\text{Digit} \rightarrow 0 \mid 1 \mid 2 \mid \dots \mid 9$ 

<span id="page-226-1"></span><span id="page-226-0"></span>Figure 9.11: A context-free grammar for arithmetic expressions

- a finite set of **non-terminals** *N*;
- a finite alphabet Σ of **terminal symbols**;
- a set of **production rules** *R*, each of the form  $A \rightarrow \beta$ , where  $A \in N$  and  $\beta \in (\Sigma \cup N)^*$ ;
- a designated start symbol *S*.

In the production rule  $A \rightarrow \beta$ , the left-hand side (LHS) A must be a non-terminal; the right-hand side (RHS) can be a sequence of terminals or non-terminals,  $\{n, \sigma\}^*, n \in N, \sigma \in \Sigma$ . A non-terminal can appear on the left-hand side of many production rules. A non-terminal can appear on both the left-hand side and the right-hand side; this is a **recursive production**, and is analogous to self-loops in finite state automata. The name "context-free" is based on the property that the production rule depends only on the LHS, and not on its ancestors or neighbors; this is analogous to Markov property of finite state automata, in which the behavior at each step depends only on the current state, and not on the path by which that state was reached.

A **derivation**  $\tau$  is a sequence of steps from the start symbol *S* to a surface string  $w \in \Sigma^*$ , which is the **yield** of the derivation. A string w is in a context-free language if there is some derivation from *S* yielding w. **Parsing** is the problem of finding a derivation for a string in a grammar. Algorithms for parsing are described in chapter 10.

Like regular expressions, context-free grammars define the language but not the computation necessary to recognize it. The context-free analogues to finite state acceptors are **pushdown automata**, a theoretical model of computation in which input symbols can be pushed onto a stack with potentially infinite depth. For more details, see Sipser (2012).

#### Example

Figure 9.11 shows a context-free grammar for arithmetic expressions such as  $1 + 2 \div 3 - 4$ . In this grammar, the terminal symbols include the digits  $\{1, 2, \ldots, 9\}$  and the operators  $\{+, -, \times, \div\}$ . The rules include the | symbol, a notational convenience that makes it possible to specify multiple right-hand sides on a single line: the statement  $A \rightarrow x \mid y$ 

<span id="page-227-3"></span><span id="page-227-0"></span>![](_page_227_Figure_1.jpeg)

Figure 9.12: Some example derivations from the arithmetic grammar in [Figure 9.11](#page-226-0)

defines *two* productions, A → x and A → y. This grammar is recursive: the non-termals S and NUM can produce themselves.

Derivations are typically shown as trees, with production rules applied from the top to the bottom. The tree on the left in [Figure 9.12](#page-227-0) describes the derivation of a single digit, through the sequence of productions S → NUM → DIGIT → 4 (these are all **unary productions**, because the right-hand side contains a single element). The other two trees in [Figure 9.12](#page-227-0) show alternative derivations of the string 1 + 2 − 3. The existence of multiple derivations for a string indicates that the grammar is **ambiguous**.

Context-free derivations can also be written out according to the pre-order tree traversal.[6](#page-227-1) For the two derivations of 1 + 2 - 3 in [Figure 9.12,](#page-227-0) the notation is:

$$(S (S (Num (Digit 1))) (Op +) (S (Num (Digit 2)))) (Op -) (S (Num (Digit 3)))) [9.23] (S (S (Num (Digit 1))) (Op +) (S (Num (Digit 2)) (Op -) (S (Num (Digit 3))))). [9.24]$$

#### <span id="page-227-2"></span>**Grammar equivalence and Chomsky Normal Form**

A single context-free language can be expressed by more than one context-free grammar. For example, the following two grammars both define the language a n b n for n > 0.

$$S \to aSb \mid ab$$
$$S \to aSb \mid aabb \mid ab$$

Two grammars are **weakly equivalent** if they generate the same strings. Two grammars are **strongly equivalent** if they generate the same strings via the same derivations. The grammars above are only weakly equivalent.

<span id="page-227-1"></span><sup>6</sup>This is a depth-first left-to-right search that prints each node the first time it is encountered [\(Cormen](#page-522-0) [et al., 2009,](#page-522-0) chapter 12).

<span id="page-228-2"></span>In **Chomsky Normal Form (CNF)**, the right-hand side of every production includes either two non-terminals, or a single terminal symbol:

$$\begin{array}{c} A \to BC \\ A \to a \end{array}$$

All CFGs can be converted into a CNF grammar that is weakly equivalent. To convert a grammar into CNF, we first address productions that have more than two non-terminals on the RHS by creating new "dummy" non-terminals. For example, if we have the production,

$$W \rightarrow X Y Z,$$
 [9.25]

it is replaced with two productions,

$$W \rightarrow X W \setminus X$$
 [9.26]

$$W \setminus X \to Y Z.$$
 [9.27]

In these productions,  $W \setminus X$  is a new dummy non-terminal. This transformation **binarizes** the grammar, which is critical for efficient bottom-up parsing, as we will see in chapter 10. Productions whose right-hand side contains a mix of terminal and non-terminal symbols can be replaced in a similar fashion.

Unary non-terminal productions  $A \rightarrow B$  are replaced as follows: for each production  $B \rightarrow \alpha$  in the grammar, add a new production  $A \rightarrow \alpha$ . For example, in the grammar described in Figure 9.11, we would replace NUM  $\rightarrow$  DIGIT with NUM  $\rightarrow$  1 | 2 | ... | 9. However, we keep the production NUM  $\rightarrow$  NUM DIGIT, which is a valid binary production.

#### <span id="page-228-0"></span>9.2.2 Natural language syntax as a context-free language

Context-free grammars can be used to represent **syntax**, which is the set of rules that determine whether an utterance is judged to be grammatical. If this representation were perfectly faithful, then a natural language such as English could be transformed into a formal language, consisting of exactly the (infinite) set of strings that would be judged to be grammatical by a fluent English speaker. We could then build parsing software that would automatically determine if a given utterance were grammatical.<sup>7</sup>

Contemporary theories generally do *not* consider natural languages to be context-free (see § 9.3), yet context-free grammars are widely used in natural language parsing. The reason is that context-free representations strike a good balance: they cover a broad range of syntactic phenomena, and they can be parsed efficiently. This section therefore describes how to handle a core fragment of English syntax in context-free form, following

<span id="page-228-1"></span><sup>&</sup>lt;sup>7</sup>To move beyond this cursory treatment of syntax, consult the short introductory manuscript by Bender (2013), or the longer text by Akmajian et al. (2010).

<span id="page-229-0"></span>the conventions of the **Penn Treebank** (PTB; [Marcus et al., 1993\)](#page-547-0), a large-scale annotation of English language syntax. The generalization to "mildly" context-sensitive languages is discussed in § [9.3.](#page-235-1)

The Penn Treebank annotation is a **phrase-structure grammar** of English. This means that sentences are broken down into **constituents**, which are contiguous sequences of words that function as coherent units for the purpose of linguistic analysis. Constituents generally have a few key properties:

**Movement.** Constituents can often be moved around sentences as units.

- (9.3) a. Abigail gave (her brother) (a fish).
  - b. Abigail gave (a fish) to (her brother).

In contrast, *gave her* and *brother a* cannot easily be moved while preserving grammaticality.

**Substitution.** Constituents can be substituted by other phrases of the same type.

- (9.4) a. Max thanked (his older sister).
  - b. Max thanked (her).

In contrast, substitution is not possible for other contiguous units like *Max thanked* and *thanked his*.

**Coordination.** Coordinators like *and* and *or* can conjoin constituents.

- (9.5) a. (Abigail) and (her younger brother) bought a fish.
  - b. Abigail (bought a fish) and (gave it to Max).
  - c. Abigail (bought) and (greedily ate) a fish.

Units like *brother bought* and *bought a* cannot easily be coordinated.

These examples argue for units such as *her brother* and *bought a fish* to be treated as constituents. Other sequences of words in these examples, such as *Abigail gave* and *brother a fish*, cannot be moved, substituted, and coordinated in these ways. In phrase-structure grammar, constituents are nested, so that *the senator from New Jersey* contains the constituent *from New Jersey*, which in turn contains *New Jersey*. The sentence itself is the maximal constituent; each word is a minimal constituent, derived from a unary production from a part-of-speech tag. Between part-of-speech tags and sentences are **phrases**. In phrase-structure grammar, phrases have a type that is usually determined by their **head word**: for example, a **noun phrase** corresponds to a noun and the group of words that

<span id="page-230-1"></span>modify it, such as *her younger brother*; a **verb phrase** includes the verb and its modifiers, such as *bought a fish* and *greedily ate it*.

In context-free grammars, each phrase type is a non-terminal, and each constituent is the substring that the non-terminal yields. Grammar design involves choosing the right set of non-terminals. Fine-grained non-terminals make it possible to represent more finegrained linguistic phenomena. For example, by distinguishing singular and plural noun phrases, it is possible to have a grammar of English that generates only sentences that obey subject-verb agreement. However, enforcing subject-verb agreement is considerably more complicated in languages like Spanish, where the verb must agree in both person and number with subject. In general, grammar designers must trade off between **overgeneration** — a grammar that permits ungrammatical sentences — and **undergeneration** — a grammar that fails to generate grammatical sentences. Furthermore, if the grammar is to support manual annotation of syntactic structure, it must be simple enough to annotate efficiently.

#### <span id="page-230-0"></span>**9.2.3 A phrase-structure grammar for English**

To better understand how phrase-structure grammar works, let's consider the specific case of the Penn Treebank grammar of English. The main phrase categories in the Penn Treebank (PTB) are based on the main part-of-speech classes: noun phrase (NP), verb phrase (VP), prepositional phrase (PP), adjectival phrase (ADJP), and adverbial phrase (ADVP). The top-level category is S, which conveniently stands in for both "sentence" and the "start" symbol. **Complement clauses** (e.g., *I take the good old fashioned ground that the whale is a fish*) are represented by the non-terminal SBAR. The terminal symbols in the grammar are individual words, which are generated from unary productions from part-of-speech tags (the PTB tagset is described in § [8.1\)](#page-192-1).

This section describes some of the most common productions from the major phraselevel categories, explaining how to generate individual tag sequences. The production rules are approached in a "theory-driven" manner: first the syntactic properties of each phrase type are described, and then some of the necessary production rules are listed. But it is important to keep in mind that the Penn Treebank was produced in a "data-driven" manner. After the set of non-terminals was specified, annotators were free to analyze each sentence in whatever way seemed most linguistically accurate, subject to some high-level guidelines. The grammar of the Penn Treebank is simply the set of productions that were required to analyze the several million words of the corpus. By design, the grammar overgenerates — it does not exclude ungrammatical sentences. Furthermore, while the productions shown here cover some of the most common cases, they are only a small fraction of the several thousand different types of productions in the Penn Treebank.

#### **Sentences**

The most common production rule for sentences is,

$$S \rightarrow NP VP$$
 [9.28]

which accounts for simple sentences like *Abigail ate the kimchi* — as we will see, the direct object *the kimchi* is part of the verb phrase. But there are more complex forms of sentences as well:

| S →ADVP NP VP | Unfortunately Abigail ate the kimchi.        | [9.29] |
|---------------|----------------------------------------------|--------|
| S →S CC S     | Abigail ate the kimchi and Max had a burger. | [9.30] |
| S →VP         | Eat the kimchi.                              | [9.31] |

where ADVP is an adverbial phrase (e.g., *unfortunately*, *very unfortunately*) and CC is a coordinating conjunction (e.g., *and*, *but*).[8](#page-231-0)

#### **Noun phrases**

Noun phrases refer to entities, real or imaginary, physical or abstract: *Asha*, *the steamed dumpling*, *parts and labor*, *nobody*, *the whiteness of the whale*, and *the rise of revolutionary syndicalism in the early twentieth century.* Noun phrase productions include "bare" nouns, which may optionally follow determiners, as well as pronouns:

NP →N<sup>N</sup> | NNS | NNP | PRP [9.32]

$$NP \rightarrow DET NN \mid DET NNS \mid DET NNP$$
 [9.33]

The tags NN, NNS, and NNP refer to singular, plural, and proper nouns; PRP refers to personal pronouns, and DET refers to determiners. The grammar also contains terminal productions from each of these tags, e.g., PRP → *I* | *you* | *we* | . . . .

Noun phrases may be modified by adjectival phrases (ADJP; e.g., *the small Russian dog*) and numbers (CD; e.g., *the five pastries*), each of which may optionally follow a determiner:

> NP →ADJP N<sup>N</sup> | ADJP NNS | DET ADJP N<sup>N</sup> | DET ADJP NNS [9.34] NP →C<sup>D</sup> NNS | DET C<sup>D</sup> NNS | . . . [9.35]

Some noun phrases include multiple nouns, such as *the liberation movement* and *an antelope horn*, necessitating additional productions:

$$NP \rightarrow NN NN | NN NNS | DET NN NN | \dots$$
[9.36]

<span id="page-231-0"></span><sup>8</sup>Notice that the grammar does not include the recursive production S → ADVP S. It may be helpful to think about why this production would cause the grammar to overgenerate.

<span id="page-232-1"></span>These multiple noun constructions can be combined with adjectival phrases and cardinal numbers, leading to a large number of additional productions.

Recursive noun phrase productions include coordination, prepositional phrase attachment, subordinate clauses, and verb phrase adjuncts:

| NP →NP CC NP | e.g., the red and the black                                | [9.37] |
|--------------|------------------------------------------------------------|--------|
| NP →NP PP    | e.g., the President of the Georgia Institute of Technology | [9.38] |
| NP →NP SBAR  | e.g., a whale which he had wounded                         | [9.39] |
| NP →NP VP    | e.g., a whale taken near Shetland                          | [9.40] |

These recursive productions are a major source of ambiguity, because the VP and PP nonterminals can also generate NP children. Thus, the *the President of the Georgia Institute of Technology* can be derived in two ways, as can *a whale taken near Shetland in October*.

But aside from these few recursive productions, the noun phrase fragment of the Penn Treebank grammar is relatively flat, containing a large of number of productions that go from NP directly to a sequence of parts-of-speech. If noun phrases had more internal structure, the grammar would need fewer rules, which, as we will see, would make parsing faster and machine learning easier. [Vadas and Curran](#page-567-2) [\(2011\)](#page-567-2) propose to add additional structure in the form of a new non-terminal called a **nominal modifier** (NML), e.g.,

(9.6) a. (NP (NN crude) (NN oil) (NNS prices)) (PTB analysis) b. (NP (NML (NN crude) (NN oil)) (NNS prices)) (NML-style analysis).

Another proposal is to treat the determiner as the head of a **determiner phrase** (DP; [Abney, 1987\)](#page-509-2). There are linguistic arguments for and against determiner phrases (e.g., [Van Eynde, 2006\)](#page-567-3). From the perspective of context-free grammar, DPs enable more structured analyses of some constituents, e.g.,

```
(9.7) a. (NP (DT the) (JJ white) (NN whale)) (PTB analysis)
      b. (DP (DT the) (NP (JJ white) (NN whale))) (DP-style analysis).
```

#### **Verb phrases**

Verb phrases describe actions, events, and states of being. The PTB tagset distinguishes several classes of verb inflections: base form (VB; *she likes to snack*), present-tense thirdperson singular (VBZ; *she snacks*), present tense but not third-person singular (VBP; *they snack*), past tense (VBD; *they snacked*), present participle (VBG; *they are snacking*), and past participle (VBN; *they had snacked*).[9](#page-232-0) Each of these forms can constitute a verb phrase on its

<span id="page-232-0"></span><sup>9</sup>This tagset is specific to English: for example, VBP is a meaningful category only because English morphology distinguishes third-person singular from all person-number combinations.

own:

$$VP \rightarrow VB \mid VBZ \mid VBD \mid VBN \mid VBG \mid VBP$$
[9.41]

More complex verb phrases can be formed by a number of recursive productions, including the use of coordination, modal verbs (MD; *she should snack*), and the infinitival *to* (TO):

| VP → MD VP    | She<br>will snack                   | [9.42] |
|---------------|-------------------------------------|--------|
| VP → VBD VP   | She<br>had snacked                  | [9.43] |
| VP → VBZ VP   | She<br>has been snacking            | [9.44] |
| VP → VBN VP   | She has<br>been snacking            | [9.45] |
| VP → TO VP    | She wants<br>to snack               | [9.46] |
| VP → VP CC VP | She<br>buys and eats<br>many snacks | [9.47] |

Each of these productions uses recursion, with the VP non-terminal appearing in both the LHS and RHS. This enables the creation of complex verb phrases, such as *She will have wanted to have been snacking*.

Transitive verbs take noun phrases as direct objects, and ditransitive verbs take two direct objects:

| VP → VBZ NP    | She<br>teaches algebra               | [9.48] |
|----------------|--------------------------------------|--------|
| VP → VBG NP    | She has been<br>teaching algebra     | [9.49] |
| VP → VBD NP NP | She<br>taught<br>her brother algebra | [9.50] |

These productions are *not* recursive, so a unique production is required for each verb part-of-speech. They also do not distinguish transitive from intransitive verbs, so the resulting grammar overgenerates examples like *\*She sleeps sushi* and *\*She learns Boyang algebra*. Sentences can also be direct objects:

| VP → VBZ S    | Hunter<br>wants to eat the kimchi           | [9.51] |
|---------------|---------------------------------------------|--------|
| VP → VBZ SBAR | Hunter<br>knows that Tristan ate the kimchi | [9.52] |

The first production overgenerates, licensing sentences like *\*Hunter sees Tristan eats the kimchi*. This problem could be addressed by designing a more specific set of sentence non-terminals, indicating whether the main verb can be conjugated.

Verbs can also be modified by prepositional phrases and adverbial phrases:

| VP → VBZ PP   | She<br>studies at night    | [9.53] |
|---------------|----------------------------|--------|
| VP → VBZ ADVP | She<br>studies intensively | [9.54] |
| VP → ADVP VBG | She is<br>not studying     | [9.55] |

<span id="page-234-0"></span>Again, because these productions are not recursive, the grammar must include productions for every verb part-of-speech.

A special set of verbs, known as **copula**, can take **predicative adjectives** as direct objects:

| VP → VBZ ADJP | is hungry<br>She                       | [9.56] |
|---------------|----------------------------------------|--------|
| VP → VBP ADJP | Success<br>seems increasingly unlikely | [9.57] |

The PTB does not have a special non-terminal for copular verbs, so this production generates non-grammatical examples such as *\*She eats tall*.

**Particles** (PRT as a phrase; RP as a part-of-speech) work to create phrasal verbs:

| VP → VB PRT     | She told them to<br>fuck off           | [9.58] |
|-----------------|----------------------------------------|--------|
| VP → VBD PRT NP | They<br>gave up their ill-gotten gains | [9.59] |

As the second production shows, particle productions are required for all configurations of verb parts-of-speech and direct objects.

#### **Other contituents**

The remaining constituents require far fewer productions. **Prepositional phrases** almost always consist of a preposition and a noun phrase,

| PP → IN NP | the whiteness<br>of the whale                        | [9.60] |
|------------|------------------------------------------------------|--------|
| PP → TO NP | What the white whale was<br>to Ahab, has been hinted | [9.61] |

Similarly, complement clauses consist of a complementizer (usually a preposition, possibly null) and a sentence,

| SBAR → IN S | She said<br>that it was spicy | [9.62] |
|-------------|-------------------------------|--------|
| SBAR → S    | She said<br>it was spicy      | [9.63] |

Adverbial phrases are usually bare adverbs (ADVP → RB), with a few exceptions:

| ADVP → RB RBR  | They went<br>considerably further             | [9.64] |
|----------------|-----------------------------------------------|--------|
| ADVP → ADVP PP | considerably further than before<br>They went | [9.65] |

The tag RBR is a comparative adverb.

| ADJP → RB JJ    | very hungry         | [9.66] |
|-----------------|---------------------|--------|
| ADJP → RBR JJ   | more hungry         | [9.67] |
| ADJP → JJS JJ   | best possible       | [9.68] |
| ADJP → RB JJR   | even bigger         | [9.69] |
| ADJP → JJ CC JJ | high and mighty     | [9.70] |
| ADJP → JJ JJ    | West German         | [9.71] |
| ADJP → RB VBN   | previously reported | [9.72] |

<span id="page-235-2"></span>Adjectival phrases extend beyond bare adjectives (ADJP → JJ) in a number of ways:

The tags JJR and JJS refer to comparative and superlative adjectives respectively.

All of these phrase types can be coordinated:

| PP →PP CC PP       | on time and under budget     | [9.73] |
|--------------------|------------------------------|--------|
| ADVP →ADVP CC ADVP | now and two years ago        | [9.74] |
| ADJP →ADJP CC ADJP | quaint and rather deceptive  | [9.75] |
| SBAR →SBAR CC SBAR | whether they want control    | [9.76] |
|                    | or whether they want exports |        |

### <span id="page-235-0"></span>**9.2.4 Grammatical ambiguity**

Context-free parsing is useful not only because it determines whether a sentence is grammatical, but mainly because the constituents and their relations can be applied to tasks such as information extraction [\(chapter 17\)](#page-420-0) and sentence compression [\(Jing, 2000;](#page-537-4) [Clarke](#page-521-4) [and Lapata, 2008\)](#page-521-4). However, the **ambiguity** of wide-coverage natural language grammars poses a serious problem for such potential applications. As an example, [Figure 9.13](#page-236-1) shows two possible analyses for the simple sentence *We eat sushi with chopsticks*, depending on whether the *chopsticks* modify *eat* or *sushi*. Realistic grammars can license thousands or even millions of parses for individual sentences. **Weighted context-free grammars** solve this problem by attaching weights to each production, and selecting the derivation with the highest score. This is the focus of [chapter 10.](#page-242-0)

# <span id="page-235-1"></span>**9.3 \*Mildly context-sensitive languages**

Beyond context-free languages lie **context-sensitive languages**, in which the expansion of a non-terminal depends on its neighbors. In the general class of context-sensitive languages, computation becomes much more challenging: the membership problem for context-sensitive languages is PSPACE-complete. Since PSPACE contains the complexity class NP (problems that can be solved in polynomial time on a non-deterministic Turing

<span id="page-236-4"></span><span id="page-236-1"></span>![](_page_236_Figure_1.jpeg)

Figure 9.13: Two derivations of the same sentence

machine), PSPACE-complete problems cannot be solved efficiently if  $P \neq NP$ . Thus, designing an efficient parsing algorithm for the full class of context-sensitive languages is probably hopeless.<sup>10</sup>

However, Joshi (1985) identifies a set of properties that define **mildly context-sensitive languages**, which are a strict subset of context-sensitive languages. Like context-free languages, mildly context-sensitive languages are parseable in polynomial time. However, the mildly context-sensitive languages include non-context-free languages, such as the "copy language" { $ww \mid w \in \Sigma^*$ } and the language  $a^m b^n c^m d^n$ . Both are characterized by **cross-serial dependencies**, linking symbols at long distance across the string.<sup>11</sup> For example, in the language  $a^n b^m c^n d^m$ , each *a* symbol is linked to exactly one *c* symbol, regardless of the number of intervening *b* symbols.

#### <span id="page-236-0"></span>9.3.1 Context-sensitive phenomena in natural language

Such phenomena are occasionally relevant to natural language. A classic example is found in Swiss-German (Shieber, 1985), in which sentences such as *we let the children help Hans paint the house* are realized by listing all nouns before all verbs, i.e., *we the children Hans the house let help paint*. Furthermore, each noun's determiner is dictated by the noun's **case marking** (the role it plays with respect to the verb). Using an argument that is analogous to the earlier discussion of center-embedding (§ 9.2), Shieber describes these case marking constraints as a set of cross-serial dependencies, homomorphic to  $a^m b^n c^m d^n$ , and therefore not context-free.

<span id="page-236-2"></span><sup>&</sup>lt;sup>10</sup>If PSPACE  $\neq$  NP, then it contains problems that cannot be solved in polynomial time on a nondeterministic Turing machine; equivalently, solutions to these problems cannot even be checked in polynomial time (Arora and Barak, 2009).

<span id="page-236-3"></span><sup>&</sup>lt;sup>11</sup>A further condition of the set of mildly-context-sensitive languages is *constant growth*: if the strings in the language are arranged by length, the gap in length between any pair of adjacent strings is bounded by some language specific constant. This condition excludes languages such as  $\{a^{2^n} \mid n \ge 0\}$ .

<span id="page-237-2"></span>![](_page_237_Figure_1.jpeg)

<span id="page-237-1"></span>Figure 9.14: A syntactic analysis in CCG involving forward and backward function application

As with the move from regular to context-free languages, mildly context-sensitive languages can also be motivated by expedience. While finite sequences of cross-serial dependencies can in principle be handled in a context-free grammar, it is often more convenient to use a mildly context-sensitive formalism like **tree-adjoining grammar** (TAG) and **combinatory categorial grammar** (CCG). TAG-inspired parsers have been shown to be particularly effective in parsing the Penn Treebank [\(Collins, 1997;](#page-521-5) [Carreras et al., 2008\)](#page-518-4), and CCG plays a leading role in current research on semantic parsing [\(Zettlemoyer and](#page-572-3) [Collins, 2005\)](#page-572-3). These two formalisms are weakly equivalent: any language that can be specified in TAG can also be specified in CCG, and vice versa [\(Joshi et al., 1991\)](#page-538-6). The remainder of the chapter gives a brief overview of CCG, but you are encouraged to consult [Joshi and Schabes](#page-538-7) [\(1997\)](#page-538-7) and [Steedman and Baldridge](#page-563-8) [\(2011\)](#page-563-8) for more detail on TAG and CCG respectively.

#### <span id="page-237-0"></span>**9.3.2 Combinatory categorial grammar**

In combinatory categorial grammar, structural analyses are built up through a small set of generic combinatorial operations, which apply to immediately adjacent sub-structures. These operations act on the categories of the sub-structures, producing a new structure with a new category. The basic categories include S (sentence), NP (noun phrase), VP (verb phrase) and N (noun). The goal is to label the entire span of text as a sentence, S.

Complex categories, or types, are constructed from the basic categories, parentheses, and forward and backward slashes: for example, S/NP is a complex type, indicating a sentence that is lacking a noun phrase to its right; S\NP is a sentence lacking a noun phrase to its left. Complex types act as functions, and the most basic combinatory operations are function application to either the right or left neighbor. For example, the type of a verb phrase, such as *eats*, would be S\NP. Applying this function to a subject noun phrase to its left results in an analysis of *Abigail eats* as category S, indicating a successful parse.

Transitive verbs must first be applied to the direct object, which in English appears to the right of the verb, before the subject, which appears on the left. They therefore have the more complex type (S\NP)/NP. Similarly, the application of a determiner to the noun at

| Abigail | might                            | learn            | Swahili |
|---------|----------------------------------|------------------|---------|
| NP      | $\overline{(S \setminus NP)/VP}$ |                  | NP      |
|         | $(S \setminus NP)$               | <u>/NP</u> >B    |         |
|         |                                  | $S \setminus NP$ | >       |
|         | S                                |                  | <       |

<span id="page-238-2"></span><span id="page-238-1"></span>Figure 9.15: A syntactic analysis in CCG involving function composition (example modified from Steedman and Baldridge, 2011)

its right results in a noun phrase, so determiners have the type NP/N. Figure 9.14 provides an example involving a transitive verb and a determiner. A key point from this example is that it can be trivially transformed into phrase-structure tree, by treating each function application as a constituent phrase. Indeed, when CCG's only combinatory operators are forward and backward function application, it is equivalent to context-free grammar. However, the location of the "effort" has changed. Rather than designing good productions, the grammar designer must focus on the **lexicon** — choosing the right categories for each word. This makes it possible to parse a wide range of sentences using only a few generic combinatory operators.

Things become more interesting with the introduction of two additional operators: **composition** and **type-raising**. Function composition enables the combination of complex types:  $X/Y \circ Y/Z \Rightarrow_{\mathbf{B}} X/Z$  (forward composition) and  $Y \setminus Z \circ X \setminus Y \Rightarrow_{\mathbf{B}} X \setminus Z$  (backward composition).<sup>12</sup> Composition makes it possible to "look inside" complex types, and combine two adjacent units if the "input" for one is the "output" for the other. Figure 9.15 shows how function composition can be used to handle modal verbs. While this sentence can be parsed using only function application, the composition-based analysis is preferable because the unit *might learn* functions just like a transitive verb, as in the example *Abigail studies Swahili*. This in turn makes it possible to analyze conjunctions such as *Abigail studies and might learn Swahili*, attaching the direct object *Swahili* to the entire conjoined verb phrase *studies and might learn*. The Penn Treebank grammar fragment from § 9.2.3 would be unable to handle this case correctly: the direct object *Swahili* could attach only to the second verb *learn*.

Type raising converts an element of type X to a more complex type:  $X \Rightarrow_T T/(T \setminus X)$ (forward type-raising to type T), and  $X \Rightarrow_T T \setminus (T/X)$  (backward type-raising to type T). Type-raising makes it possible to reverse the relationship between a function and its argument — by transforming the argument into a function over functions over arguments! An example may help. Figure 9.15 shows how to analyze an object relative clause, *a story that Abigail tells*. The problem is that *tells* is a transitive verb, expecting a direct object to its right. As a result, *Abigail tells* is not a valid constituent. The issue is resolved by raising

<span id="page-238-0"></span><sup>&</sup>lt;sup>12</sup>The subscript **B** follows notation from Curry and Feys (1958).

<span id="page-239-1"></span>![](_page_239_Figure_1.jpeg)

Figure 9.16: A syntactic analysis in CCG involving an object relative clause

*Abigail* from NP to the complex type  $(S/NP)\setminus NP$ . This function can then be combined with the transitive verb *tells* by forward composition, resulting in the type (S/NP), which is a sentence lacking a direct object to its right.<sup>13</sup> From here, we need only design the lexical entry for the complementizer *that* to expect a right neighbor of type (S/NP), and the remainder of the derivation can proceed by function application.

Composition and type-raising give CCG considerable power and flexibility, but at a price. The simple sentence *Abigail tells Max* can be parsed in two different ways: by function application (first forming the verb phrase *tells Max*), and by type-raising and composition (first forming the non-constituent *Abigail tells*). This **derivational ambiguity** does not affect the resulting linguistic analysis, so it is sometimes known as **spurious ambiguity**. Hockenmaier and Steedman (2007) present a translation algorithm for converting the Penn Treebank into CCG derivations, using composition and type-raising only when necessary.

#### **Exercises**

- 1. Sketch out the state diagram for finite-state acceptors for the following languages on the alphabet  $\{a, b\}$ .
  - a) Even-length strings. (Be sure to include 0 as an even number.)
  - b) Strings that contain *aaa* as a substring.
  - c) Strings containing an even number of *a* and an odd number of *b* symbols.
  - d) Strings in which the substring *bbb* must be terminal if it appears the string need not contain *bbb*, but if it does, nothing can come after it.
- 2. Levenshtein edit distance is the number of insertions, substitutions, or deletions required to convert one string to another.

<span id="page-239-0"></span><sup>&</sup>lt;sup>13</sup>The missing direct object would be analyzed as a **trace** in CFG-like approaches to syntax, including the Penn Treebank.

- a) Define a finite-state acceptor that accepts all strings with edit distance 1 from the target string, *target*.
- b) Now think about how to generalize your design to accept all strings with edit distance from the target string equal to d. If the target string has length  $\ell$ , what is the minimal number of states required?
- 3. Construct an FSA in the style of Figure 9.3, which handles the following examples:
  - nation/N, national/ADJ, nationalize/V, nationalizer/N
  - America/N, American/ADJ, Americanize/V, Americanizer/N

Be sure that your FSA does not accept any further derivations, such as *\*nationalizeral* and *\*Americanizern*.

- 4. Show how to construct a trigram language model in a weighted finite-state acceptor. Make sure that you handle the edge cases at the beginning and end of the input.
- 5. Extend the FST in Figure 9.6 to handle the other two parts of rule 1a of the Porter stemmer:  $-sses \rightarrow ss$ , and  $-ies \rightarrow -i$ .
- 6. § 9.1.4 describes  $T_O$ , a transducer that captures English orthography by transducing  $cook + ed \rightarrow cooked$  and  $bake + ed \rightarrow baked$ . Design an unweighted finite-state transducer that captures this property of English orthography.

Next, augment the transducer to appropriately model the suffix -*s* when applied to words ending in *s*, e.g.  $kiss+s \rightarrow kisses$ .

- 7. Add parenthesization to the grammar in Figure 9.11 so that it is no longer ambiguous.
- 8. Construct three examples a noun phrase, a verb phrase, and a sentence which can be derived from the Penn Treebank grammar fragment in § 9.2.3, yet are not grammatical. Avoid reusing examples from the text. Optionally, propose corrections to the grammar to avoid generating these cases.
- 9. Produce parses for the following sentences, using the Penn Treebank grammar fragment from  $\S$  9.2.3.
  - (9.8) This aggression will not stand.
  - (9.9) I can get you a toe.
  - (9.10) Sometimes you eat the bar and sometimes the bar eats you.

Then produce parses for three short sentences from a news article from this week.

10. \* One advantage of CCG is its flexibility in handling coordination:

(9.11) a. *Hunter and Tristan speak Hawaiian* b. *Hunter speaks and Tristan understands Hawaiian*

Define the lexical entry for *and* as

$$and := (X/X) \backslash X,$$
[9.77]

where X can refer to any type. Using this lexical entry, show how to parse the two examples above. In the second example, *Swahili* should be combined with the coordination *Abigail speaks and Max understands*, and not just with the verb *understands*.

# <span id="page-242-1"></span><span id="page-242-0"></span>**Chapter 10 Context-free parsing**

Parsing is the task of determining whether a string can be derived from a given contextfree grammar, and if so, how. A parser's output is a tree, like the ones shown in [Fig](#page-236-1)[ure 9.13.](#page-236-1) Such trees can answer basic questions of who-did-what-to-whom, and have applications in downstream tasks like semantic analysis [\(chapter 12](#page-302-0) and [13\)](#page-322-0) and information extraction [\(chapter 17\)](#page-420-0).

For a given input and grammar, how many parse trees are there? Consider a minimal context-free grammar with only one non-terminal, X, and the following productions:

> X →X X X →*aardvark* | *abacus* | . . . | *zyther*

The second line indicates unary productions to every nonterminal in Σ. In this grammar, the number of possible derivations for a string w is equal to the number of binary bracketings, e.g.,

((((w<sup>1</sup> w2) w3) w4) w5), (((w<sup>1</sup> (w<sup>2</sup> w3)) w4) w5), ((w<sup>1</sup> (w2(w<sup>3</sup> w4))) w5), . . . .

The number of such bracketings is a **Catalan number**, which grows super-exponentially in the length of the sentence, C<sup>n</sup> = (2n)! (n+1)!n! . As with sequence labeling, it is only possible to exhaustively search the space of parses by resorting to locality assumptions, which make it possible to search efficiently by reusing shared substructures with dynamic programming. This chapter focuses on a bottom-up dynamic programming algorithm, which enables exhaustive search of the space of possible parses, but imposes strict limitations on the form of scoring function. These limitations can be relaxed by abandoning exhaustive search. Non-exact search methods will be briefly discussed at the end of this chapter, and one of them — **transition-based parsing** — will be the focus of [chapter 11.](#page-274-0)

<span id="page-243-3"></span><span id="page-243-2"></span> $\begin{array}{lll} S & \rightarrow NP \ VP \\ NP & \rightarrow NP \ PP \ | \ we \ | \ sushi \ | \ chopsticks \\ PP & \rightarrow IN \ NP \\ IN & \rightarrow \ with \\ VP & \rightarrow V \ NP \ | \ VP \ PP \\ V & \rightarrow \ eat \end{array}$ 

Table 10.1: A toy example context-free grammar

# <span id="page-243-0"></span>10.1 Deterministic bottom-up parsing

The **CKY algorithm**<sup>1</sup> is a bottom-up approach to parsing in a context-free grammar. It efficiently tests whether a string is in a language, without enumerating all possible parses. The algorithm first forms small constituents, and then tries to merge them into larger constituents.

To understand the algorithm, consider the input, *We eat sushi with chopsticks*. According to the toy grammar in Table 10.1, each terminal symbol can be generated by exactly one unary production, resulting in the sequence NP V NP IN NP. In real examples, there may be many unary productions for each individual token. In any case, the next step is to try to apply binary productions to merge adjacent symbols into larger constituents: for example, V NP can be merged into a verb phrase (VP), and IN NP can be merged into a prepositional phrase (PP). Bottom-up parsing searches for a series of mergers that ultimately results in the start symbol S covering the entire input.

The CKY algorithm systematizes this search by incrementally constructing a table t in which each cell t[i, j] contains the set of nonterminals that can derive the span  $w_{i+1:j}$ . The algorithm fills in the upper right triangle of the table; it begins with the diagonal, which corresponds to substrings of length 1, and then computes derivations for progressively larger substrings, until reaching the upper right corner t[0, M], which corresponds to the entire input,  $w_{1:M}$ . If the start symbol S is in t[0, M], then the string w is in the language defined by the grammar. This process is detailed in Algorithm 13, and the resulting data structure is shown in Figure 10.1. Informally, here's how it works:

- Begin by filling in the diagonal: the cells t[m-1,m] for all  $m \in \{1, 2, ..., M\}$ . These cells are filled with terminal productions that yield the individual tokens; for the word  $w_2 = sushi$ , we fill in  $t[1, 2] = \{NP\}$ , and so on.
- Then fill in the next diagonal, in which each cell corresponds to a subsequence of length two:  $t[0,2], t[1,3], \ldots, t[M-2,M]$ . These cells are filled in by looking for

<span id="page-243-1"></span><sup>&</sup>lt;sup>1</sup>The name is for Cocke-Kasami-Younger, the inventors of the algorithm. It is a special case of **chart parsing**, because its stores reusable computations in a chart-like data structure.

<span id="page-244-2"></span>binary productions capable of producing at least one entry in each of the cells corresponding to left and right children. For example, VP can be placed in the cell t[1,3] because the grammar includes the production VP  $\rightarrow$  V NP, and because the chart contains V  $\in t[1,2]$  and NP  $\in t[2,3]$ .

- At the next diagonal, the entries correspond to spans of length three. At this level, there is an additional decision at each cell: where to split the left and right children. The cell t[i, j] corresponds to the subsequence  $w_{i+1:j}$ , and we must choose some *split point* i < k < j, so that the span  $w_{i+1:k}$  is the left child, and the span  $w_{k+1:j}$  is the right child. We consider all possible k, looking for productions that generate elements in t[i, k] and t[k, j]; the left-hand side of all such productions can be added to t[i, j]. When it is time to compute t[i, j], the cells t[i, k] and t[k, j] are guaranteed to be complete, since these cells correspond to shorter sub-strings of the input.
- The process continues until we reach t[0, M].

Figure 10.1 shows the chart that arises from parsing the sentence *We eat sushi with chopsticks* using the grammar defined above.

#### <span id="page-244-0"></span>10.1.1 Recovering the parse tree

As with the Viterbi algorithm, it is possible to identify a successful parse by storing and traversing an additional table of back-pointers. If we add an entry X to cell t[i, j] by using the production  $X \to YZ$  and the split point k, then we store the back-pointer b[i, j, X] = (Y, Z, k). Once the table is complete, we can recover a parse by tracing this pointers, starting at b[0, M, S], and stopping when they ground out at terminal productions.

For ambiguous sentences, there will be multiple paths to reach  $S \in t[0, M]$ . For example, in Figure 10.1, the goal state  $S \in t[0, M]$  is reached through the state  $VP \in t[1, 5]$ , and there are two different ways to generate this constituent: one with (*eat sushi*) and (*with chopsticks*) as children, and another with (*eat*) and (*sushi with chopsticks*) as children. The presence of multiple paths indicates that the input can be generated by the grammar in more than one way. In Algorithm 13, one of these derivations is selected arbitrarily. As discussed in § 10.3, **weighted context-free grammars** compute a score for all permissible derivations, and a minor modification of CKY allows it to identify the single derivation with the maximum score.

#### <span id="page-244-1"></span>10.1.2 Non-binary productions

As presented above, the CKY algorithm assumes that all productions with non-terminals on the right-hand side (RHS) are binary. In real grammars, such as the one considered in chapter 9, there are other types of productions: some have more than two elements on the right-hand side, and others produce a single non-terminal.

<span id="page-245-1"></span><span id="page-245-0"></span>**Algorithm 13** The CKY algorithm for parsing a sequence  $w \in \Sigma^*$  in a context-free grammar  $G = (N, \Sigma, R, S)$ , with non-terminals N, production rules R, and start symbol S. The grammar is assumed to be in Chomsky normal form (§ 9.2.1). The function PICKFROM(b[i, j, X]) selects an element of the set b[i, j, X] arbitrarily. All values of t and b are initialized to  $\emptyset$ .

```
1: procedure CKY(\boldsymbol{w}, G = (N, \Sigma, R, S))
        for m \in \{1 \dots M\} do
 2:
             t[m-1,m] \leftarrow \{X : (X \to w_m) \in R\}
 3:
 4:
        for \ell \in \{2, 3, ..., M\} do
                                                                              ▷ Iterate over constituent lengths
             for m \in \{0, 1, \dots, M - \ell\} do
                                                                                     ▷ Iterate over left endpoints
 5.
                 for k \in \{m + 1, m + 2, \dots, m + \ell - 1\} do
                                                                                        ▷ Iterate over split points
 6:
                     for (X \to Y Z) \in R do
                                                                                               ▷ Iterate over rules
 7 \cdot
                         if Y \in t[m,k] \land Z \in t[k,m+\ell] then
 8:
                             t[m, m+\ell] \leftarrow t[m, m+\ell] \cup X
                                                                                    ▷ Add non-terminal to table
 9.
                             b[m, m+\ell, X] \leftarrow b[m, m+\ell, X] \cup (Y, Z, k)
                                                                                             ▷ Add back-pointers
10:
11:
        if S \in t[0, M] then
12:
             return TRACEBACK(S, 0, M, b)
13:
        else
             return Ø
14:
15: procedure TRACEBACK(X, i, j, b)
16:
        if j = i + 1 then
17:
             return X
18:
        else
             (Y, Z, k) \leftarrow \operatorname{PICKFROM}(b[i, j, X])
19.
            return X \rightarrow (\text{TRACEBACK}(Y, i, k, b), \text{TRACEBACK}(Z, k, j, b))
20:
```

- Productions with more than two elements on the right-hand side can be binarized by creating additional non-terminals, as described in § 9.2.1. For example, the production VP → V NP NP (for ditransitive verbs) can be converted to VP → VP<sub>ditrans</sub>/NP NP, by adding the non-terminal VP<sub>ditrans</sub>/NP and the production VP<sub>ditrans</sub>/NP → V NP.
- What about unary productions like VP → V? While such productions are not a part of Chomsky Normal Form and can therefore be eliminated in preprocessing the grammar in practice, a more typical solution is to modify the CKY algorithm. The algorithm makes a second pass on each diagonal in the table, augmenting each cell *t*[*i*, *j*] with all possible unary productions capable of generating each item already in the cell: formally, *t*[*i*, *j*] is extended to its **unary closure**. Suppose the example grammar in Table 10.1 were extended to include the production VP → V, enabling sentences with intransitive verb phrases, like *we eat*. Then the cell *t*[1, 2] corresponding to the word *eat* would first include the set {V}, and would be augmented to the set {V, VP} during this second pass.

<span id="page-246-3"></span><span id="page-246-2"></span>![](_page_246_Figure_1.jpeg)

Figure 10.1: An example completed CKY chart. The solid and dashed lines show the back pointers resulting from the two different derivations of VP in position t[1, 5].

#### <span id="page-246-0"></span>10.1.3 Complexity

For an input of length M and a grammar with R productions and N non-terminals, the space complexity of the CKY algorithm is  $\mathcal{O}(M^2N)$ : the number of cells in the chart is  $\mathcal{O}(M^2)$ , and each cell must hold  $\mathcal{O}(N)$  elements. The time complexity is  $\mathcal{O}(M^3R)$ : each cell is computed by searching over  $\mathcal{O}(M)$  split points, with R possible productions for each split point. Both the time and space complexity are considerably worse than the Viterbi algorithm, which is linear in the length of the input.

# <span id="page-246-1"></span>10.2 Ambiguity

In natural language, there is rarely a single parse for a given sentence. The main culprit is ambiguity, which is endemic to natural language syntax. Here are a few broad categories:

- Attachment ambiguity: e.g., *We eat sushi with chopsticks*, *I shot an elephant in my pajamas*. In these examples, the prepositions (*with*, *in*) can attach to either the verb or the direct object.
- **Modifier scope**: e.g., *southern food store, plastic cup holder*. In these examples, the first word could be modifying the subsequent adjective, or the final noun.
- **Particle versus preposition**: e.g., *The puppy tore up the staircase*. Phrasal verbs like *tore up* often include particles which could also act as prepositions. This has structural implications: if *up* is a preposition, then *up the staircase* is a prepositional phrase; if *up* is a particle, then *the staircase* is the direct object to the verb.
- **Complement structure**: e.g., *The students complained to the professor that they didn't understand*. This is another form of attachment ambiguity, where the complement

<span id="page-247-2"></span>*that they didn't understand* could attach to the main verb (*complained*), or to the indirect object (*the professor*).

• **Coordination scope**: e.g., *"I see," said the blind man, as he picked up the hammer and saw.* In this example, the lexical ambiguity for *saw* enables it to be coordinated either with the noun *hammer* or the verb *picked up*.

These forms of ambiguity can combine, so that seemingly simple headlines like *Fed raises interest rates* have dozens of possible analyses even in a minimal grammar. In a broad coverage grammar, typical sentences can have millions of parses. While careful grammar design can chip away at this ambiguity, a better strategy is combine broad coverage parsers with data-driven strategies for identifying the correct analysis.

### <span id="page-247-0"></span>**10.2.1 Parser evaluation**

Before continuing to parsing algorithms that are able to handle ambiguity, let us stop to consider how to measure parsing performance. Suppose we have a set of *reference parses* — the ground truth — and a set of *system parses* that we would like to score. A simple solution would be per-sentence accuracy: the parser is scored by the proportion of sentences on which the system and reference parses exactly match.[2](#page-247-1) But as any student knows, it always nice to get *partial credit*, which we can assign to analyses that correctly match parts of the reference parse. The PARSEval metrics [\(Grishman et al., 1992\)](#page-532-4) score each system parse via:

- **Precision:** the fraction of constituents in the system parse that match a constituent in the reference parse.
- **Recall:** the fraction of constituents in the reference parse that match a constituent in the system parse.

In **labeled precision** and **recall**, the system must also match the phrase type for each constituent; in **unlabeled precision** and **recall**, it is only required to match the constituent structure. As described in [chapter 4,](#page-86-0) the precision and recall can be combined into an F -MEASURE by their harmonic mean.

Suppose that the left tree of [Figure 10.2](#page-248-1) is the system parse, and that the right tree is the reference parse. Then:

<span id="page-247-1"></span>• S → w1:5 is *true positive*, because it appears in both trees.

<sup>2</sup>Most parsing papers do not report results on this metric, but [Suzuki et al.](#page-564-4) [\(2018\)](#page-564-4) find that a strong parser recovers the exact parse in roughly 50% of all sentences. Performance on short sentences is generally much higher.

<span id="page-248-1"></span>![](_page_248_Figure_1.jpeg)

Figure 10.2: Two possible analyses from the grammar in Table 10.1

- VP  $\rightarrow w_{2:5}$  is true positive as well.
- NP  $\rightarrow$   $w_{3:5}$  is *false positive*, because it appears only in the system output.
- PP  $\rightarrow$   $w_{4:5}$  is *true positive*, because it appears in both trees.
- VP  $\rightarrow$   $w_{2:3}$  is *false negative*, because it appears only in the reference.

The labeled and unlabeled precision of this parse is  $\frac{3}{4} = 0.75$ , and the recall is  $\frac{3}{4} = 0.75$ , for an F-measure of 0.75. For an example in which precision and recall are not equal, suppose the reference parse instead included the production VP  $\rightarrow$  V NP PP. In this parse, the reference does not contain the constituent  $w_{2:3}$ , so the recall would be 1.<sup>3</sup>

#### <span id="page-248-0"></span>**10.2.2** Local solutions

Some ambiguity can be resolved locally. Consider the following examples,

- (10.1) a. We met the President on Monday.
  - b. We met the President of Mexico.

Each case ends with a prepositional phrase, which can be attached to the verb *met* or the noun phrase *the president*. If given a labeled corpus, we can compare the likelihood of the observing the preposition alongside each candidate attachment point,

$$p(on \mid met) \ge p(on \mid President)$$
 [10.1]

$$p(of \mid met) \ge p(of \mid President).$$
 [10.2]

<span id="page-248-2"></span><sup>&</sup>lt;sup>3</sup>While the grammar must be binarized before applying the CKY algorithm, evaluation is performed on the original parses. It is therefore necessary to "unbinarize" the output of a CKY-based parser, converting it back to the original grammar.

<span id="page-249-2"></span>A comparison of these probabilities would successfully resolve this case [\(Hindle and](#page-534-3) [Rooth, 1993\)](#page-534-3). Other cases, such as the example *we eat sushi with chopsticks*, require considering the object of the preposition: consider the alternative *we eat sushi with soy sauce*. With sufficient labeled data, some instances of attachment ambiguity can be solved by supervised classification [\(Ratnaparkhi et al., 1994\)](#page-557-7).

However, there are inherent limitations to local solutions. While toy examples may have just a few ambiguities to resolve, realistic sentences have thousands or millions of possible parses. Furthermore, attachment decisions are interdependent, as shown in the garden path example:

(10.2) Cats scratch people with claws with knives.

We may want to attach *with claws* to *scratch*, as would be correct in the shorter sentence in *cats scratch people with claws*. But this leaves nowhere to attach *with knives*. The correct interpretation can be identified only be considering the attachment decisions jointly. The huge number of potential parses may seem to make exhaustive search impossible. But as with sequence labeling, locality assumptions make it possible to search this space efficiently.

# <span id="page-249-0"></span>**10.3 Weighted Context-Free Grammars**

Let us define a derivation τ as a set of **anchored productions**,

$$\tau = \{ X \to \alpha, (i, j, k) \},$$
[10.3]

with X corresponding to the left-hand side non-terminal and α corresponding to the righthand side. For grammars in Chomsky normal formal, α is either a pair of non-terminals or a terminal symbol. The indices i, j, k anchor the production in the input, with X deriving the span wi+1:<sup>j</sup> . For binary productions, wi+1:<sup>k</sup> indicates the span of the left child, and wk+1:<sup>j</sup> indicates the span of the right child; for unary productions, k is ignored. For an input w, the optimal parse is,

<span id="page-249-1"></span>
$$\hat{\tau} = \operatorname*{argmax}_{\tau \in \mathcal{T}(\boldsymbol{w})} \Psi(\tau), \qquad [10.4]$$

where T (w) is the set of derivations that yield the input w.

Define a scoring function Ψ that decomposes across anchored productions,

$$\Psi(\tau) = \sum_{(X \to \alpha, (i, j, k)) \in \tau} \psi(X \to \alpha, (i, j, k)).$$
[10.5]

This is a locality assumption, akin to the assumption in Viterbi sequence labeling. In this case, the assumption states that the overall score is a sum over scores of productions,

<span id="page-250-1"></span><span id="page-250-0"></span>

|    |                                         | $\psi(\cdot)$ | $\exp\psi(\cdot)$                    |
|----|-----------------------------------------|---------------|--------------------------------------|
| S  | $\rightarrow$ NP VP                     | 0             | 1                                    |
| NP | $\rightarrow$ NP PP                     | -1            | $\frac{1}{2}$                        |
|    | $\rightarrow we$                        | -2            | $\frac{\overline{1}}{4}$             |
|    | $\rightarrow$ sushi                     | -3            | 1<br>2<br>1<br>4<br>1<br>8<br>1<br>8 |
|    | ightarrow chopsticks                    | -3            | $\frac{1}{8}$                        |
| PP | $\rightarrow$ IN NP                     | 0             | 1                                    |
| In | ightarrow with                          | 0             | 1                                    |
| VP | $\rightarrow V NP$                      | -1            | $\frac{1}{2}$                        |
|    | $\rightarrow \mathrm{VP} \ \mathrm{PP}$ | -2            | $\frac{1}{4}$                        |
|    | $\rightarrow \text{Md} V$               | -2            | $\frac{\frac{1}{2}}{\frac{1}{4}}$    |
| V  | $\rightarrow eat$                       | 0             | 1                                    |

Table 10.2: An example weighted context-free grammar (WCFG). The weights are chosen so that  $\exp \psi(\cdot)$  sums to one over right-hand sides for each non-terminal; this is required by probabilistic context-free grammars, but not by WCFGs in general.

which are computed independently. In a **weighted context-free grammar** (WCFG), the score of each anchored production  $X \rightarrow (\alpha, (i, j, k))$  is simply  $\psi(X \rightarrow \alpha)$ , ignoring the anchor (i, j, k). In other parsing models, the anchors can be used to access features of the input, while still permitting efficient bottom-up parsing.

**Example** Consider the weighted grammar shown in Table 10.2, and the analysis in Figure 10.2b.

$$\begin{split} \Psi(\tau) = &\psi(\mathsf{S} \to \mathsf{NP} \, \mathsf{VP}) + \psi(\mathsf{VP} \to \mathsf{VP} \, \mathsf{PP}) + \psi(\mathsf{VP} \to \mathsf{V} \, \mathsf{NP}) + \psi(\mathsf{PP} \to \mathsf{IN} \, \mathsf{NP}) \\ &+ \psi(\mathsf{NP} \to \mathsf{We}) + \psi(\mathsf{V} \to \mathit{eat}) + \psi(\mathsf{NP} \to \mathit{sushi}) + \psi(\mathsf{IN} \to \mathit{with}) + \psi(\mathsf{NP} \to \mathit{chopsticks}) \\ & \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad$$

In the alternative parse in Figure 10.2a, the production VP  $\rightarrow$  VP PP (with score -2) is replaced with the production NP  $\rightarrow$  NP PP (with score -1); all other productions are the same. As a result, the score for this parse is -10. This example hints at a problem with WCFG parsing on non-terminals such as NP, VP, and PP: a WCFG will *always* prefer either VP or NP attachment, regardless of what is being attached! Solutions to this issue are discussed in § 10.5.

<span id="page-251-1"></span>**Algorithm 14** CKY algorithm for parsing a string  $w \in \Sigma^*$  in a weighted context-free grammar  $(N, \Sigma, R, S)$ , where N is the set of non-terminals and R is the set of weighted productions. The grammar is assumed to be in Chomsky normal form (§ 9.2.1). The function TRACEBACK is defined in Algorithm 13.

 $\begin{array}{l} \textbf{procedure WCKY}(w,G=(N,\Sigma,R,S)) \\ \textbf{for all } i,j,X \ \textbf{do} & \triangleright \text{Initialization} \\ t[i,j,X] \leftarrow 0 \\ b[i,j,X] \leftarrow \varnothing \\ \textbf{for } m \in \{1,2,\ldots,M\} \ \textbf{do} \\ \textbf{for all } X \in N \ \textbf{do} \\ t[m,m+1,X] \leftarrow \psi(X \rightarrow w_m,(m,m+1,m)) \\ \textbf{for } \ell \in \{2,3,\ldots,M\} \ \textbf{do} \\ \textbf{for } m \in \{0,1,\ldots,M-\ell\} \ \textbf{do} \\ \textbf{for } m \in \{0,1,\ldots,M-\ell\} \ \textbf{do} \\ \textbf{for } k \in \{m+1,m+2,\ldots,m+\ell-1\} \ \textbf{do} \\ t[m,m+\ell,X] \leftarrow \max_{k,Y,Z} \psi(X \rightarrow Y Z,(m,m+\ell,k)) + t[m,k,Y] + t[k,m+\ell,Z] \\ b[m,m+\ell,X] \leftarrow \operatorname*{argmax}_{k,Y,Z} \psi(X \rightarrow Y Z,(m+\ell,k)) + t[m,k,Y] + t[k,m+\ell,Z] \\ \textbf{return TRACEBACK}(S,0,M,b) \end{array}$ 

#### <span id="page-251-0"></span>**10.3.1** Parsing with weighted context-free grammars

The optimization problem in Equation 10.4 can be solved by modifying the CKY algorithm. In the deterministic CKY algorithm, each cell t[i, j] stored a set of non-terminals capable of deriving the span  $w_{i+1:j}$ . We now augment the table so that the cell t[i, j, X] is the *score of the best derivation* of  $w_{i+1:j}$  from non-terminal X. This score is computed recursively: for the anchored binary production  $(X \to Y Z, (i, j, k))$ , we compute:

- the score of the anchored production,  $\psi(X \to Y Z, (i, j, k))$ ;
- the score of the best derivation of the left child, *t*[*i*, *k*, *Y*];
- the score of the best derivation of the right child, t[k, j, Z].

These scores are combined by addition. As in the unweighted CKY algorithm, the table is constructed by considering spans of increasing length, so the scores for spans t[i, k, Y] and t[k, j, Z] are guaranteed to be available at the time we compute the score t[i, j, X]. The value t[0, M, S] is the score of the best derivation of w from the grammar. Algorithm 14 formalizes this procedure.

As in unweighted CKY, the parse is recovered from the table of back pointers b, where each b[i, j, X] stores the argmax split point k and production  $X \rightarrow Y Z$  in the derivation of  $w_{i+1:j}$  from X. The top scoring parse can be obtained by tracing these pointers backwards from b[0, M, S], all the way to the terminal symbols. This is analogous to the computation

<span id="page-252-2"></span>of the best sequence of labels in the Viterbi algorithm by tracing pointers backwards from the end of the trellis. Note that we need only store back-pointers for the *best* path to t[i, j, X]; this follows from the locality assumption that the global score for a parse is a combination of the local scores of each production in the parse.

**Example** Let's revisit the parsing table in Figure 10.1. In a weighted CFG, each cell would include a score for each non-terminal; non-terminals that cannot be generated are assumed to have a score of  $-\infty$ . The first diagonal contains the scores of unary productions: t[0, 1, NP] = -2, t[1, 2, V] = 0, and so on. The next diagonal contains the scores for spans of length 2: t[1, 3, VP] = -1 + 0 - 3 = -4, t[3, 5, PP] = 0 + 0 - 3 = -3, and so on. Things get interesting when we reach the cell t[1, 5, VP], which contains the score for the derivation of the span  $w_{2:5}$  from the non-terminal VP. This score is computed as a max over two alternatives,

$$t[1,5, VP] = \max(\psi(VP \to VP PP, (1,3,5)) + t[1,3, VP] + t[3,5, PP], \psi(VP \to V NP, (1,2,5)) + t[1,2, V] + t[2,5, NP])$$
[10.8]

$$= \max(-2 - 4 - 3, -1 + 0 - 7) = -8.$$
 [10.9]

Since the second case is the argmax, we set the back-pointer b[1, 5, VP] = (V, NP, 2), enabling the optimal derivation to be recovered.

#### <span id="page-252-0"></span>**10.3.2** Probabilistic context-free grammars

**Probabilistic context-free grammars (PCFGs)** are a special case of weighted contextfree grammars that arises when the weights correspond to probabilities. Specifically, the weight  $\psi(X \to \alpha, (i, j, k)) = \log p(\alpha \mid X)$ , where the probability of the right-hand side  $\alpha$  is conditioned on the non-terminal X, and the anchor (i, j, k) is ignored. These probabilities must be normalized over all possible right-hand sides, so that  $\sum_{\alpha} p(\alpha \mid X) = 1$ , for all X. For a given parse  $\tau$ , the product of the probabilities of the productions is equal to  $p(\tau)$ , under the **generative model**  $\tau \sim \text{DRAWSUBTREE}(S)$ , where the function DRAWSUBTREE is defined in Algorithm 15.

The conditional probability of a parse given a string is,

<span id="page-252-1"></span>
$$\mathbf{p}(\tau \mid \boldsymbol{w}) = \frac{\mathbf{p}(\tau)}{\sum_{\tau' \in \mathcal{T}(\boldsymbol{w})} \mathbf{p}(\tau')} = \frac{\exp \Psi(\tau)}{\sum_{\tau' \in \mathcal{T}(\boldsymbol{w})} \exp \Psi(\tau')},$$
[10.10]

where  $\Psi(\tau) = \sum_{X \to \alpha, (i,j,k) \in \tau} \psi(X \to \alpha, (i, j, k))$ . Because the probability is monotonic in the score  $\Psi(\tau)$ , the maximum likelihood parse can be identified by the CKY algorithm without modification. If a normalized probability  $p(\tau \mid w)$  is required, the denominator of Equation 10.10 can be computed by the **inside recurrence**, described below.

<span id="page-253-0"></span>**Algorithm 15** Generative model for derivations from probabilistic context-free grammars in Chomsky Normal Form (CNF).

procedure DRAWSUBTREE(X) sample  $(X \to \alpha) \sim p(\alpha \mid X)$ if  $\alpha = (Y Z)$  then return DRAWSUBTREE(Y)  $\cup$  DRAWSUBTREE(Z) else return  $(X \to \alpha)$   $\triangleright$  In CNF, all unary productions yield terminal symbols

**Example** The WCFG in Table 10.2 is designed so that the weights are log-probabilities, satisfying the constraint  $\sum_{\alpha} \exp \psi(X \to \alpha) = 1$ . As noted earlier, there are two parses in  $\mathcal{T}(we \ eat \ sushi \ with \ chopsticks)$ , with scores  $\Psi(\tau_1) = \log p(\tau_1) = -10$  and  $\Psi(\tau_2) = \log p(\tau_2) = -11$ . Therefore, the conditional probability  $p(\tau_1 \mid w)$  is equal to,

$$p(\tau_1 \mid \boldsymbol{w}) = \frac{p(\tau_1)}{p(\tau_1) + p(\tau_2)} = \frac{\exp \Psi(\tau_1)}{\exp \Psi(\tau_1) + \exp \Psi(\tau_2)} = \frac{2^{-10}}{2^{-10} + 2^{-11}} = \frac{2}{3}.$$
 [10.11]

**The inside recurrence** The denominator of Equation 10.10 can be viewed as a language model, summing over all valid derivations of the string w,

$$\mathbf{p}(\boldsymbol{w}) = \sum_{\tau': \text{yield}(\tau') = \boldsymbol{w}} \mathbf{p}(\tau').$$
[10.12]

Just as the CKY algorithm makes it possible to maximize over all such analyses, with a few modifications it can also compute their sum. Each cell t[i, j, X] must store the log probability of deriving  $w_{i+1:j}$  from non-terminal X. To compute this, we replace the maximization over split points k and productions  $X \rightarrow Y Z$  with a "log-sum-exp" operation, which exponentiates the log probabilities of the production and the children, sums them in probability space, and then converts back to the log domain:

$$t[i, j, X] = \log \sum_{k, Y, Z} \exp(\psi(X \to Y Z) + t[i, k, Y] + t[k, j, Z])$$

$$= \log \sum_{k, Y, Z} \exp(\log p(Y Z \mid X) + \log p(Y \to \boldsymbol{w}_{i+1:k}) + \log p(Z \to \boldsymbol{w}_{k+1:j}))$$
[10.13]

[10.14]

$$= \log \sum_{k,Y,Z} \mathbf{p}(Y \ Z \mid X) \times \mathbf{p}(Y \rightarrow \boldsymbol{w}_{i+1:k}) \times \mathbf{p}(Z \rightarrow \boldsymbol{w}_{k+1:j})$$
[10.15]

$$= \log \sum_{k,Y,Z} p(Y Z, \boldsymbol{w}_{i+1:k}, \boldsymbol{w}_{k+1:j} \mid X)$$
[10.16]

$$= \log p(X \rightsquigarrow \boldsymbol{w}_{i+1:j}), \qquad [10.17]$$

<span id="page-254-1"></span>with  $X \rightsquigarrow w_{i+1:j}$  indicating the event that non-terminal X yields the span  $w_{i+1}, w_{i+2}, \ldots, w_j$ . The recursive computation of t[i, j, X] is called the **inside recurrence**, because it computes the probability of each subtree as a combination of the probabilities of the smaller subtrees that are inside of it. The name implies a corresponding **outside recurrence**, which computes the probability of a non-terminal X spanning  $w_{i+1:j}$ , joint with the outside context  $(w_{1:i}, w_{j+1:M})$ . This recurrence is described in § 10.4.3. The inside and outside recurrences are analogous to the forward and backward recurrences in probabilities of individual anchored productions,  $p(X \to \alpha, (i, j, k) \mid w)$ , summing over all possible derivations of w.

#### <span id="page-254-0"></span>10.3.3 \*Semiring weighted context-free grammars

The weighted and unweighted CKY algorithms can be unified with the inside recurrence using the same semiring notation described in  $\S$  7.7.3. The generalized recurrence is:

$$t[i,j,X] = \bigoplus_{k,Y,Z} \psi(X \to Y|Z, (i,j,k)) \otimes t[i,k,Y] \otimes t[k,j,Z].$$
[10.18]

This recurrence subsumes all of the algorithms that have been discussed in this chapter to this point.

- **Unweighted CKY.** When  $\psi(X \to \alpha, (i, j, k))$  is a *Boolean truth value*  $\{\top, \bot\}$ ,  $\otimes$  is logical conjunction, and  $\bigoplus$  is logical disjunction, then we derive CKY recurrence for unweighted context-free grammars, discussed in § 10.1 and Algorithm 13.
- Weighted CKY. When  $\psi(X \to \alpha, (i, j, k))$  is a scalar score,  $\otimes$  is addition, and  $\bigoplus$  is maximization, then we derive the CKY recurrence for weighted context-free grammars, discussed in § 10.3 and Algorithm 14. When  $\psi(X \to \alpha, (i, j, k)) = \log p(\alpha \mid X)$ , this same setting derives the CKY recurrence for finding the maximum likelihood derivation in a probabilistic context-free grammar.
- **Inside recurrence.** When  $\psi(X \to \alpha, (i, j, k))$  is a log probability,  $\otimes$  is addition, and  $\bigoplus = \log \sum \exp$ , then we derive the inside recurrence for probabilistic context-free grammars, discussed in § 10.3.2. It is also possible to set  $\psi(X \to \alpha, (i, j, k))$  directly equal to the probability  $p(\alpha \mid X)$ . In this case,  $\otimes$  is multiplication, and  $\bigoplus$  is addition. While this may seem more intuitive than working with log probabilities, there is the risk of underflow on long inputs.

Regardless of how the scores are combined, the key point is the locality assumption: the score for a derivation is the combination of the independent scores for each anchored

<span id="page-255-2"></span>production, and these scores do not depend on any other part of the derivation. For example, if two non-terminals are siblings, the scores of productions from these non-terminals are computed independently. This locality assumption is analogous to the first-order Markov assumption in sequence labeling, where the score for transitions between tags depends only on the previous tag and current tag, and not on the history. As with sequence labeling, this assumption makes it possible to find the optimal parse efficiently; its linguistic limitations are discussed in § [10.5.](#page-259-1)

# <span id="page-255-0"></span>**10.4 Learning weighted context-free grammars**

Like sequence labeling, context-free parsing is a form of structure prediction. As a result, WCFGs can be learned using the same set of algorithms: generative probabilistic models, structured perceptron, maximum conditional likelihood, and maximum margin learning. In all cases, learning requires a **treebank**, which is a dataset of sentences labeled with context-free parses. Parsing research was catalyzed by the **Penn Treebank** [\(Marcus et al.,](#page-547-0) [1993\)](#page-547-0), the first large-scale dataset of this type (see § [9.2.2\)](#page-228-0). Phrase structure treebanks exist for roughly two dozen other languages, with coverage mainly restricted to European and East Asian languages, plus Arabic and Urdu.

#### <span id="page-255-1"></span>**10.4.1 Probabilistic context-free grammars**

Probabilistic context-free grammars are similar to hidden Markov models, in that they are generative models of text. In this case, the parameters of interest correspond to probabilities of productions, conditional on the left-hand side. As with hidden Markov models, these parameters can be estimated by relative frequency:

$$\psi(X \to \alpha) = \log p(X \to \alpha)$$
 [10.19]

$$\hat{\mathbf{p}}(X \to \alpha) = \frac{\operatorname{count}(X \to \alpha)}{\operatorname{count}(X)}.$$
[10.20]

For example, the probability of the production NP → DET N<sup>N</sup> is the corpus count of this production, divided by the count of the non-terminal NP. This estimator applies to terminal productions as well: the probability of N<sup>N</sup> → *whale* is the count of how often *whale* appears in the corpus as generated from an NN tag, divided by the total count of the NN tag. Even with the largest treebanks — currently on the order of one million tokens — it is difficult to accurately compute probabilities of even moderately rare events, such as N<sup>N</sup> → *whale*. Therefore, smoothing is critical for making PCFGs effective.

#### <span id="page-256-0"></span>**10.4.2 Feature-based parsing**

The scores for each production can be computed as an inner product of weights and features,

$$\psi(X \to \alpha, (i, j, k)) = \boldsymbol{\theta} \cdot \boldsymbol{f}(X, \alpha, (i, j, k), \boldsymbol{w}),$$
[10.21]

where the feature vector f is a function of the left-hand side X, the right-hand side α, the anchor indices (i, j, k), and the input w.

The basic feature f(X, α,(i, j, k)) = {(X, α)} encodes only the identity of the production itself. This gives rise to a discriminatively-trained model with the same expressiveness as a PCFG. Features on anchored productions can include the words that border the span w<sup>i</sup> , wj+1, the word at the split point wk+1, the presence of a verb or noun in the left child span wi+1:k, and so on [\(Durrett and Klein, 2015\)](#page-526-7). Scores on anchored productions can be incorporated into CKY parsing without any modification to the algorithm, because it is still possible to compute each element of the table t[i, j, X] recursively from its immediate children.

Other features can be obtained by grouping elements on either the left-hand or righthand side: for example it can be particularly beneficial to compute additional features by clustering terminal symbols, with features corresponding to groups of words with similar syntactic properties. The clustering can be obtained from unlabeled datasets that are much larger than any treebank, improving coverage. Such methods are described in [chapter 14.](#page-342-0)

Feature-based parsing models can be estimated using the usual array of discriminative learning techniques. For example, a structure perceptron update can be computed as [\(Carreras et al., 2008\)](#page-518-4),

$$\boldsymbol{f}(\tau, \boldsymbol{w}^{(i)}) = \sum_{(X \to \alpha, (i, j, k)) \in \tau} \boldsymbol{f}(X, \alpha, (i, j, k), \boldsymbol{w}^{(i)})$$
[10.22]

$$\hat{\tau} = \underset{\tau \in \mathcal{T}(\boldsymbol{w})}{\operatorname{argmax}} \boldsymbol{\theta} \cdot \boldsymbol{f}(\tau, \boldsymbol{w}^{(i)})$$
[10.23]

$$\boldsymbol{\theta} \leftarrow \boldsymbol{f}(\tau^{(i)}, \boldsymbol{w}^{(i)}) - \boldsymbol{f}(\hat{\tau}, \boldsymbol{w}^{(i)}).$$
[10.24]

A margin-based objective can be optimized by selecting τˆ through cost-augmented decoding (§ [2.4.2\)](#page-49-0), enforcing a margin of ∆(ˆτ, τ ) between the hypothesis and the reference parse, where ∆ is a non-negative cost function, such as the Hamming loss [\(Stern et al., 2017\)](#page-563-9). It is also possible to train feature-based parsing models by conditional log-likelihood, as described in the next section.

<span id="page-257-3"></span><span id="page-257-1"></span>![](_page_257_Figure_1.jpeg)

Figure 10.3: The two cases faced by the outside recurrence in the computation of  $\beta(i, j, X)$ 

#### <span id="page-257-0"></span>10.4.3 \*Conditional random field parsing

The score of a derivation  $\Psi(\tau)$  can be converted into a probability by normalizing over all possible derivations,

$$p(\tau \mid \boldsymbol{w}) = \frac{\exp \Psi(\tau)}{\sum_{\tau' \in \mathcal{T}(\boldsymbol{w})} \exp \Psi(\tau')}.$$
[10.25]

Using this probability, a WCFG can be trained by maximizing the conditional log-likelihood of a labeled corpus.

Just as in logistic regression and the conditional random field over sequences, the gradient of the conditional log-likelihood is the difference between the observed and expected counts of each feature. The expectation  $E_{\tau|w}[f(\tau, w^{(i)}); \theta]$  requires summing over all possible parses, and computing the marginal probabilities of anchored productions,  $p(X \to \alpha, (i, j, k) | w)$ . In CRF sequence labeling, marginal probabilities over tag bigrams are computed by the two-pass **forward-backward algorithm** (§ 7.5.3). The analogue for context-free grammars is the **inside-outside algorithm**, in which marginal probabilities are computed from terms generated by an upward and downward pass over the parsing chart:

• The upward pass is performed by the inside recurrence, which is described in § 10.3.2. Each inside variable  $\alpha(i, j, X)$  is the score of deriving  $w_{i+1:j}$  from the non-terminal X. In a PCFG, this corresponds to the log-probability  $\log p(w_{i+1:j} | X)$ . This is computed by the recurrence,

<span id="page-257-2"></span>
$$\alpha(i,j,X) \triangleq \log \sum_{(X \to Y|Z)} \sum_{k=i+1}^{j} \exp\left(\psi(X \to Y|Z,(i,j,k)) + \alpha(i,k,Y) + \alpha(k,j,Z)\right).$$
[10.26]

The initial condition of this recurrence is  $\alpha(m-1,m,X) = \psi(X \to w_m)$ . The denominator  $\sum_{\tau \in \mathcal{T}(w)} \exp \Psi(\tau)$  is equal to  $\exp \alpha(0, M, S)$ .

• The downward pass is performed by the **outside recurrence**, which recursively populates the same table structure, starting at the root of the tree. Each outside variable

#### <span id="page-258-1"></span>10.4. LEARNING WEIGHTED CONTEXT-FREE GRAMMARS 241

β(i, j, X) is the score of having a phrase of type X covering the span (i + 1 : j), joint with the exterior context w1:<sup>i</sup> and wj+1:M. In a PCFG, this corresponds to the log probability log p((X, i + 1, j), w1:<sup>i</sup> , wj+1:M). Each outside variable is computed by the recurrence,

$$\exp\beta(i,j,X) \triangleq \sum_{(Y \to X} \sum_{Z)} \sum_{k=j+1}^{M} \exp\left[\psi(Y \to X Z, (i,k,j)) + \alpha(j,k,Z) + \beta(i,k,Y)\right]$$
[10.27]

<span id="page-258-0"></span>+ 
$$\sum_{(Y \to Z \ X)} \sum_{k=0}^{i-1} \exp\left[\psi(Y \to Z \ X, (k, i, j)) + \alpha(k, i, Z) + \beta(k, j, Y)\right].$$
[10.28]

The first line of [Equation 10.28](#page-258-0) is the score under the condition that X is a left child of its parent, which spans wi+1:k, with k > j; the second line is the score under the condition that X is a right child of its parent Y , which spans wk+1:<sup>j</sup> , with k < i. The two cases are shown in [Figure 10.3.](#page-257-1) In each case, we sum over all possible productions with X on the right-hand side. The parent Y is bounded on one side by either i or j, depending on whether X is a left or right child of Y ; we must sum over all possible values for the other boundary. The initial conditions for the outside recurrence are β(0, M, S) = 0 and β(0, M, X 6= S) = −∞.

The marginal probability of a non-terminal <sup>X</sup> over span <sup>w</sup>i+1:<sup>j</sup> is written p(<sup>X</sup> <sup>w</sup>i+1:<sup>j</sup> <sup>|</sup> <sup>w</sup>). This probability can be computed from the inside and outside scores,

$$p(X \rightsquigarrow \boldsymbol{w}_{i+1:j} \mid \boldsymbol{w}) = \frac{p(X \rightsquigarrow \boldsymbol{w}_{i+1:j}, \boldsymbol{w})}{p(\boldsymbol{w})}$$
[10.29]

$$=\frac{\mathbf{p}(\boldsymbol{w}_{i+1:j} \mid X) \times \mathbf{p}(X, \boldsymbol{w}_{1:i}, \boldsymbol{x}_{j+1:M})}{\mathbf{p}(\boldsymbol{w})}$$
[10.30]

$$=\frac{\exp\left(\alpha(i,j,X)+\beta(i,j,X)\right)}{\exp\alpha(0,M,S)}.$$
[10.31]

Marginal probabilities of individual productions can be computed similarly (see exercise 2). These marginal probabilities can be used for training a conditional random field parser, and also for the task of unsupervised **grammar induction**, in which a PCFG is estimated from a dataset of unlabeled text [\(Lari and Young, 1990;](#page-542-4) [Pereira and Schabes, 1992\)](#page-554-4).

#### <span id="page-259-3"></span><span id="page-259-0"></span>**10.4.4** Neural context-free grammars

Neural networks and can be applied to parsing by representing each span with a dense numerical vector (Socher et al., 2013; Durrett and Klein, 2015; Cross and Huang, 2016).<sup>4</sup> For example, the anchor (i, j, k) and sentence w can be associated with a fixed-length column vector,

$$\boldsymbol{v}_{(i,j,k)} = [\boldsymbol{u}_{w_{i-1}}; \boldsymbol{u}_{w_i}; \boldsymbol{u}_{w_{j-1}}; \boldsymbol{u}_{w_j}; \boldsymbol{u}_{w_{k-1}}; \boldsymbol{u}_{w_k}],$$
[10.32]

where  $u_{w_i}$  is a word embedding associated with the word  $w_i$ . The vector  $v_{i,j,k}$  can then be passed through a feedforward neural network, and used to compute the score of the anchored production. For example, this score can be computed as a bilinear product (Durrett and Klein, 2015),

$$\tilde{v}_{(i,j,k)} = \text{FeedForward}(v_{(i,j,k)})$$
 [10.33]

$$\psi(X \to \alpha, (i, j, k)) = \tilde{\boldsymbol{v}}_{(i, j, k)}^{\top} \boldsymbol{\Theta} \boldsymbol{f}(X \to \alpha), \qquad [10.34]$$

where  $f(X \to \alpha)$  is a vector of features of the production, and  $\Theta$  is a parameter matrix. The matrix  $\Theta$  and the parameters of the feedforward network can be learned by backpropagating from an objective such as the margin loss or the negative conditional log-likelihood.

## <span id="page-259-1"></span>**10.5** Grammar refinement

The locality assumptions underlying CFG parsing depend on the granularity of the nonterminals. For the Penn Treebank non-terminals, there are several reasons to believe that these assumptions are too strong (Johnson, 1998):

- The context-free assumption is too strict: for example, the probability of the production NP → NP PP is much higher (in the PTB) if the parent of the noun phrase is a verb phrase (indicating that the NP is a direct object) than if the parent is a sentence (indicating that the NP is the subject of the sentence).
- The Penn Treebank non-terminals are too coarse: there are many kinds of noun phrases and verb phrases, and accurate parsing sometimes requires knowing the difference. As we have already seen, when faced with prepositional phrase attachment ambiguity, a weighted CFG will either always choose NP attachment (if ψ(NP → NP PP) > ψ(VP → VP PP)), or it will always choose VP attachment. To get more nuanced behavior, more fine-grained non-terminals are needed.
- More generally, accurate parsing requires some amount of **semantics** understanding the meaning of the text to be parsed. Consider the example *cats scratch people*

<span id="page-259-2"></span><sup>&</sup>lt;sup>4</sup>Earlier work on neural constituent parsing used transition-based parsing algorithms (§ 10.6.2) rather than CKY-style chart parsing (Henderson, 2004; Titov and Henderson, 2007).

<span id="page-260-1"></span>![](_page_260_Figure_1.jpeg)

Figure 10.4: The left parse is preferable because of the conjunction of phrases headed by *France* and *Italy*, but these parses cannot be distinguished by a WCFG.

*with claws*: knowledge of about cats, claws, and scratching is necessary to correctly resolve the attachment ambiguity.

An extreme example is shown in [Figure 10.4.](#page-260-1) The analysis on the left is preferred because of the conjunction of similar entities *France* and *Italy*. But given the non-terminals shown in the analyses, there is no way to differentiate these two parses, since they include exactly the same productions. What is needed seems to be more precise non-terminals. One possibility would be to rethink the linguistics behind the Penn Treebank, and ask the annotators to try again. But the original annotation effort took five years, and there is a little appetite for another annotation effort of this scope. Researchers have therefore turned to automated techniques.

#### <span id="page-260-0"></span>**10.5.1 Parent annotations and other tree transformations**

The key assumption underlying context-free parsing is that productions depend only on the identity of the non-terminal on the left-hand side, and not on its ancestors or neighbors. The validity of this assumption is an empirical question, and it depends on the non-terminals themselves: ideally, every noun phrase (and verb phrase, etc) would be distributionally identical, so the assumption would hold. But in the Penn Treebank, the observed probability of productions often depends on the parent of the left-hand side. For example, noun phrases are more likely to be modified by prepositional phrases when they are in the object position (e.g., *they amused the students from Georgia*) than in the subject position (e.g., *the students from Georgia amused them*). This means that the NP → NP PP production is more likely if the entire constituent is the child of a VP than if it is the child

<span id="page-261-2"></span><span id="page-261-1"></span>![](_page_261_Figure_1.jpeg)

Figure 10.5: Parent annotation in a CFG derivation

of S. The observed statistics are [\(Johnson, 1998\)](#page-538-8):

Pr(NP → NP PP) =11% [10.35]

Pr(NP under S → NP PP) =9% [10.36]

Pr(NP under VP → NP PP) =23%. [10.37]

This phenomenon can be captured by **parent annotation** [\(Johnson, 1998\)](#page-538-8), in which each non-terminal is augmented with the identity of its parent, as shown in [Figure 10.5\)](#page-261-1). This is sometimes called **vertical Markovization**, since a Markov dependency is introduced between each node and its parent [\(Klein and Manning, 2003\)](#page-540-1). It is analogous to moving from a bigram to a trigram context in a hidden Markov model. In principle, parent annotation squares the size of the set of non-terminals, which could make parsing considerably less efficient. But in practice, the increase in the number of non-terminals that actually appear in the data is relatively modest [\(Johnson, 1998\)](#page-538-8).

Parent annotation weakens the WCFG locality assumptions. This improves accuracy by enabling the parser to make more fine-grained distinctions, which better capture real linguistic phenomena. However, each production is more rare, and so careful smoothing or regularization is required to control the variance over production scores.

#### <span id="page-261-0"></span>**10.5.2 Lexicalized context-free grammars**

The examples in § [10.2.2](#page-248-0) demonstrate the importance of individual words in resolving parsing ambiguity: the preposition *on* is more likely to attach to *met*, while the preposition *of* is more likely to attachment to *President*. But of all word pairs, which are relevant to attachment decisions? Consider the following variants on the original examples:

- (10.3) a. We met the President of Mexico.
  - b. We met the first female President of Mexico.
  - c. They had supposedly met the President on Monday.

The underlined words are the **head words** of their respective phrases: *met* heads the verb phrase, and *President* heads the direct object noun phrase. These heads provide useful

<span id="page-262-1"></span><span id="page-262-0"></span>![](_page_262_Figure_1.jpeg)

(a) Lexicalization and attachment ambiguity

![](_page_262_Figure_3.jpeg)

(b) Lexicalization and coordination scope ambiguity

Figure 10.6: Examples of lexicalization

semantic information. But they break the context-free assumption, which states that the score for a production depends only on the parent and its immediate children, and not the substructure under each child.

The incorporation of head words into context-free parsing is known as **lexicalization**, and is implemented in rules of the form,

$$NP(President) \rightarrow NP(President) PP(of)$$
[10.38]

$$NP(President) \rightarrow NP(President) PP(on).$$
[10.39]

Lexicalization was a major step towards accurate PCFG parsing in the 1990s and early 2000s. It requires solving three problems: identifying the heads of all constituents in a treebank; parsing efficiently while keeping track of the heads; and estimating the scores for lexicalized productions.

<span id="page-263-3"></span><span id="page-263-1"></span>

| Non-terminal | Direction | Priority                                |
|--------------|-----------|-----------------------------------------|
| S            | right     | VP SBAR ADJP UCP NP                     |
| VP           | left      | VBD VBN MD VBZ TO VB VP VBG VBP ADJP NP |
| NP           | right     | N* EX \$ CD QP PRP                      |
| PP           | left      | IN TO FW                                |

Table 10.3: A fragment of head percolation rules for English (Magerman, 1995; Collins, 1997)

#### **Identifying head words**

The head of a constituent is the word that is the most useful for determining how that constituent is integrated into the rest of the sentence.<sup>5</sup> The head word of a constituent is determined recursively: for any non-terminal production, the head of the left-hand side must be the head of one of the children. The head is typically selected according to a set of deterministic rules, sometimes called **head percolation rules**. In many cases, these rules are straightforward: the head of a noun phrase in a NP  $\rightarrow$  DET NN production is the head of the verb phrase.

Table 10.3 shows a fragment of the head percolation rules used in many English parsing systems. The meaning of the first rule is that to find the head of an S constituent, first look for the rightmost VP child; if you don't find one, then look for the rightmost SBAR child, and so on down the list. Verb phrases are headed by left verbs (the head of *can plan on walking* is *planned*, since the modal verb *can* is tagged MD); noun phrases are headed by the rightmost noun-like non-terminal (so the head of *the red cat* is *cat*),<sup>6</sup> and prepositional phrases are headed by the preposition (the head of *at Georgia Tech* is *at*). Some of these rules are somewhat arbitrary — there's no particular reason why the head of *cats and dogs* should be *dogs* — but the point here is just to get some lexical information that can support parsing, not to make deep claims about syntax. Figure 10.6 shows the application of these rules to two of the running examples.

#### Parsing lexicalized context-free grammars

A naïve application of lexicalization would simply increase the set of non-terminals by taking the cross-product with the set of terminal symbols, so that the non-terminals now

<span id="page-263-0"></span><sup>&</sup>lt;sup>5</sup>This is a pragmatic definition, befitting our goal of using head words to improve parsing; for a more formal definition, see (Bender, 2013, chapter 7).

<span id="page-263-2"></span><sup>&</sup>lt;sup>6</sup>The noun phrase non-terminal is sometimes treated as a special case. Collins (1997) uses a heuristic that looks for the rightmost child which is a noun-like part-of-speech (e.g., NN, NNP), a possessive marker, or a superlative adjective (e.g., *the greatest*). If no such child is found, the heuristic then looks for the *leftmost* NP. If there is no child with tag NP, the heuristic then applies another priority list, this time from right to left.

include symbols like NP(*President*) and VP(*meet*). Under this approach, the CKY parsing algorithm could be applied directly to the lexicalized production rules. However, the complexity would be cubic in the size of the vocabulary of terminal symbols, which would clearly be intractable.

Another approach is to augment the CKY table with an additional index, keeping track of the head of each constituent. The cell t[i, j, h, X] stores the score of the best derivation in which non-terminal X spans  $w_{i+1:j}$  with head word h, where  $i < h \leq j$ . To compute such a table recursively, we must consider the possibility that each phrase gets its head from either its left or right child. The scores of the best derivations in which the head comes from the left and right child are denoted  $t_{\ell}$  and  $t_r$  respectively, leading to the following recurrence:

$$t_{\ell}[i, j, h, X] = \max_{(X \to YZ)} \max_{k > h} \max_{k < h' \le j} t[i, k, h, Y] + t[k, j, h', Z] + \psi(X(h) \to Y(h)Z(h'))$$
[10.40]  

$$t_{r}[i, j, h, X] = \max_{(X \to YZ)} \max_{k < h} \max_{i < h' \le k} t[i, k, h', Y] + t[k, j, h, Z] + (\psi(X(h) \to Y(h')Z(h)))$$

$$(X \to YZ) \quad \kappa < n \quad i < n \leq \kappa$$
[10.41]

$$t[i, j, h, X] = \max\left(t_{\ell}[i, j, h, X], t_{r}[i, j, h, X]\right).$$
[10.42]

To compute  $t_{\ell}$ , we maximize over all split points k > h, since the head word must be in the left child. We then maximize again over possible head words h' for the right child. An analogous computation is performed for  $t_r$ . The size of the table is now  $\mathcal{O}(M^3N)$ , where M is the length of the input and N is the number of non-terminals. Furthermore, each cell is computed by performing  $\mathcal{O}(M^2)$  operations, since we maximize over both the split point k and the head h'. The time complexity of the algorithm is therefore  $\mathcal{O}(RM^5N)$ , where R is the number of rules in the grammar. Fortunately, more efficient solutions are possible. In general, the complexity of parsing can be reduced to  $\mathcal{O}(M^4)$  in the length of the input; for a broad class of lexicalized CFGs, the complexity can be made cubic in the length of the input, just as in unlexicalized CFGs (Eisner, 2000).

#### Estimating lexicalized context-free grammars

The final problem for lexicalized parsing is how to estimate weights for lexicalized productions  $X(i) \rightarrow Y(j) Z(k)$ . These productions are said to be bilexical, because they involve scores over pairs of words: in the example *meet the President of Mexico*, we hope to choose the correct attachment point by modeling the bilexical affinities of (*meet*, of) and (*President*, of). The number of such word pairs is quadratic in the size of the vocabulary, making it difficult to estimate the weights of lexicalized production rules directly from data. This is especially true for probabilistic context-free grammars, in which the weights are obtained from smoothed relative frequency. In a treebank with a million tokens, a

<span id="page-265-2"></span>vanishingly small fraction of the possible lexicalized productions will be observed more than once.[7](#page-265-1) The [Charniak](#page-518-5) [\(1997\)](#page-518-5) and [Collins](#page-521-5) [\(1997\)](#page-521-5) parsers therefore focus on approximating the probabilities of lexicalized productions, using various smoothing techniques and independence assumptions.

In discriminatively-trained weighted context-free grammars, the scores for each production can be computed from a set of features, which can be made progressively more fine-grained [\(Finkel et al., 2008\)](#page-528-3). For example, the score of the lexicalized production NP(*President*) → NP(*President*) PP(*of*) can be computed from the following features:

$$\begin{split} \boldsymbol{f}(\text{NP}(\textit{President}) \rightarrow \text{NP}(\textit{President}) \ \text{PP}(\textit{of})) &= \{\text{NP}(^*) \rightarrow \text{NP}(^*) \ \text{PP}(^*), \\ & \text{NP}(\textit{President}) \rightarrow \text{NP}(\textit{President}) \ \text{PP}(^*), \\ & \text{NP}(^*) \rightarrow \text{NP}(^*) \ \text{PP}(\textit{of}), \\ & \text{NP}(\textit{President}) \rightarrow \text{NP}(\textit{President}) \ \text{PP}(\textit{of})\} \end{split}$$

The first feature scores the unlexicalized production NP → NP PP; the next two features lexicalize only one element of the production, thereby scoring the appropriateness of NP attachment for the individual words *President* and *of*; the final feature scores the specific bilexical affinity of *President* and *of*. For bilexical pairs that are encountered frequently in the treebank, this bilexical feature can play an important role in parsing; for pairs that are absent or rare, regularization will drive its weight to zero, forcing the parser to rely on the more coarse-grained features.

In [chapter 14,](#page-342-0) we will encounter techniques for clustering words based on their **distributional** properties — the contexts in which they appear. Such a clustering would group rare and common words, such as *whale, shark, beluga, Leviathan*. Word clusters can be used as features in discriminative lexicalized parsing, striking a middle ground between full lexicalization and non-terminals [\(Finkel et al., 2008\)](#page-528-3). In this way, labeled examples containing relatively common words like *whale* can help to improve parsing for rare words like *beluga*, as long as those two words are clustered together.

#### <span id="page-265-0"></span>**10.5.3 \*Refinement grammars**

Lexicalization improves on context-free parsing by adding detailed information in the form of lexical heads. However, estimating the scores of lexicalized productions is difficult. [Klein and Manning](#page-540-1) [\(2003\)](#page-540-1) argue that the right level of linguistic detail is somewhere between treebank categories and individual words. Some parts-of-speech and nonterminals are truly substitutable: for example, *cat*/N and *dog*/N. But others are not: for example, the preposition *of* exclusively attaches to nouns, while the preposition *as* is more

<span id="page-265-1"></span><sup>7</sup>The real situation is even more difficult, because non-binary context-free grammars can involve **trilexical** or higher-order dependencies, between the head of the constituent and multiple of its children [\(Carreras et al.,](#page-518-4) [2008\)](#page-518-4).

<span id="page-266-2"></span>likely to modify verb phrases. [Klein and Manning](#page-540-1) [\(2003\)](#page-540-1) obtained a 2% improvement in F -MEASURE on a parent-annotated PCFG parser by making a single change: splitting the preposition category into six subtypes. They propose a series of linguistically-motivated refinements to the Penn Treebank annotations, which in total yielded a 40% error reduction.

Non-terminal refinement process can be automated by treating the refined categories as **latent variables**. For example, we might split the noun phrase non-terminal into NP1, NP2, NP3, . . . , without defining in advance what each refined non-terminal corresponds to. This can be treated as partially supervised learning, similar to the multicomponent document classification model described in § [5.2.3.](#page-123-0) A latent variable PCFG can be estimated by expectation maximization [\(Matsuzaki et al., 2005\)](#page-547-3):[8](#page-266-1)

- In the E-step, estimate a marginal distribution q over the refinement type of each non-terminal in each derivation. These marginals are constrained by the original annotation: an NP can be reannotated as NP4, but not as VP3. Marginal probabilities over refined productions can be computed from the **inside-outside algorithm**, as described in § [10.4.3,](#page-257-0) where the E-step enforces the constraints imposed by the original annotations.
- In the M-step, recompute the parameters of the grammar, by summing over the probabilities of anchored productions that were computed in the E-step:

$$E[\text{count}(X \to Y Z)] = \sum_{i=0}^{M} \sum_{j=i}^{M} \sum_{k=i}^{j} p(X \to Y Z, (i, j, k) \mid \boldsymbol{w}).$$
[10.43]

As usual, this process can be iterated to convergence. To determine the number of refinement types for each tag, [Petrov et al.](#page-555-6) [\(2006\)](#page-555-6) apply a split-merge heuristic; [Liang et al.](#page-544-3) [\(2007\)](#page-544-3) and [Finkel et al.](#page-528-4) [\(2007\)](#page-528-4) apply **Bayesian nonparametrics** [\(Cohen, 2016\)](#page-521-2).

Some examples of refined non-terminals are shown in [Table 10.4.](#page-267-1) The proper nouns differentiate months, first names, middle initials, last names, first names of places, and second names of places; each of these will tend to appear in different parts of grammatical productions. The personal pronouns differentiate grammatical role, with PRP-0 appearing in subject position at the beginning of the sentence (note the capitalization), PRP-1 appearing in subject position but not at the beginning of the sentence, and PRP-2 appearing in object position.

# <span id="page-266-0"></span>**10.6 Beyond context-free parsing**

<span id="page-266-1"></span>8

In the context-free setting, the score for a parse is a combination of the scores of individual productions. As we have seen, these models can be improved by using finer-grained non-

Spectral learning, described in § [5.5.2,](#page-134-0) has also been applied to refinement grammars [\(Cohen et al., 2014\)](#page-521-6).

<span id="page-267-2"></span><span id="page-267-1"></span>

| Proper nouns      |      |           |        |
|-------------------|------|-----------|--------|
| NNP-14            | Oct. | Nov.      | Sept.  |
| NNP-12            | John | Robert    | James  |
| NNP-2             | J.   | E.        | L.     |
| NNP-1             | Bush | Noriega   | Peters |
| NNP-15            | New  | San       | Wall   |
| NNP-3             | York | Francisco | Street |
| Personal Pronouns |      |           |        |
| PRP-0             | It   | He        | I      |
| PRP-1             | it   | he        | they   |
| PRP-2             | it   | them      | him    |

Table 10.4: Examples of automatically refined non-terminals and some of the words that they generate [\(Petrov et al., 2006\)](#page-555-6).

terminals, via parent-annotation, lexicalization, and automated refinement. However, the inherent limitations to the expressiveness of context-free parsing motivate the consideration of other search strategies. These strategies abandon the optimality guaranteed by bottom-up parsing, in exchange for the freedom to consider arbitrary properties of the proposed parses.

#### <span id="page-267-0"></span>**10.6.1 Reranking**

A simple way to relax the restrictions of context-free parsing is to perform a two-stage process, in which a context-free parser generates a k-best list of candidates, and a **reranker** then selects the best parse from this list [\(Charniak and Johnson, 2005;](#page-519-6) [Collins and Koo,](#page-521-7) [2005\)](#page-521-7). The reranker can be trained from an objective that is similar to multi-class classification: the goal is to learn weights that assign a high score to the reference parse, or to the parse on the k-best list that has the lowest error. In either case, the reranker need only evaluate the K best parses, and so no context-free assumptions are necessary. This opens the door to more expressive scoring functions:

- It is possible to incorporate arbitrary non-local features, such as the structural parallelism and right-branching orientation of the parse [\(Charniak and Johnson, 2005\)](#page-519-6).
- Reranking enables the use of **recursive neural networks**, in which each constituent span wi+1:<sup>j</sup> receives a vector ui,j which is computed from the vector representations of its children, using a composition function that is linked to the production

#### <span id="page-268-1"></span>10.6. BEYOND CONTEXT-FREE PARSING 251

rule [\(Socher et al., 2013\)](#page-561-4), e.g.,

$$\boldsymbol{u}_{i,j} = f\left(\Theta_{X \to Y \ Z} \begin{bmatrix} \boldsymbol{u}_{i,k} \\ \boldsymbol{u}_{k,j} \end{bmatrix}\right)$$
[10.44]

The overall score of the parse can then be computed from the final vector, Ψ(τ ) = θu0,M.

Reranking can yield substantial improvements in accuracy. The main limitation is that it can only find the best parse among the K-best offered by the generator, so it is inherently limited by the ability of the bottom-up parser to find high-quality candidates.

#### <span id="page-268-0"></span>**10.6.2 Transition-based parsing**

Structure prediction can be viewed as a form of search. An alternative to bottom-up parsing is to read the input from left-to-right, gradually building up a parse structure through a series of **transitions**. Transition-based parsing is described in more detail in the next chapter, in the context of dependency parsing. However, it can also be applied to CFG parsing, as briefly described here.

For any context-free grammar, there is an equivalent **pushdown automaton**, a model of computation that accepts exactly those strings that can be derived from the grammar. This computational model consumes the input from left to right, while pushing and popping elements on a stack. This architecture provides a natural transition-based parsing framework for context-free grammars, known as **shift-reduce parsing**.

Shift-reduce parsing is a type of transition-based parsing, in which the parser can take the following actions:

- *shift* the next terminal symbol onto the stack;
- *unary-reduce* the top item on the stack, using a unary production rule in the grammar;
- *binary-reduce* the top two items onto the stack, using a binary production rule in the grammar.

The set of available actions is constrained by the situation: the parser can only shift if there are remaining terminal symbols in the input, and it can only reduce if an applicable production rule exists in the grammar. If the parser arrives at a state where the input has been completely consumed, and the stack contains only the element S, then the input is accepted. If the parser arrives at a non-accepting state where there are no possible actions, the input is rejected. A parse error occurs if there is some action sequence that would accept an input, but the parser does not find it.

<span id="page-269-0"></span>**Example** Consider the input *we eat sushi* and the grammar in [Table 10.1.](#page-243-2) The input can be parsed through the following sequence of actions:

- 1. **Shift** the first token *we* onto the stack.
- 2. **Reduce** the top item on the stack to NP, using the production NP → *we*.
- 3. **Shift** the next token *eat* onto the stack, and **reduce** it to V with the production V → *eat*.
- 4. **Shift** the final token *sushi* onto the stack, and **reduce** it to NP. The input has been completely consumed, and the stack contains [NP, V, NP].
- 5. **Reduce** the top two items using the production VP → V NP. The stack now contains [VP, NP].
- 6. **Reduce** the top two items using the production S → NP VP. The stack now contains [S]. Since the input is empty, this is an accepting state.

One thing to notice from this example is that the number of shift actions is equal to the length of the input. The number of reduce actions is equal to the number of non-terminals in the analysis, which grows linearly in the length of the input. Thus, the overall time complexity of shift-reduce parsing is linear in the length of the input (assuming the complexity of each individual classification decision is constant in the length of the input). This is far better than the cubic time complexity required by CKY parsing.

**Transition-based parsing as inference** In general, it is not possible to guarantee that a transition-based parser will find the optimal parse, argmax<sup>τ</sup> Ψ(τ ; w), even under the usual CFG independence assumptions. We could assign a score to each anchored parsing action in each context, with ψ(a, c) indicating the score of performing action a in context c. One might imagine that transition-based parsing could efficiently find the derivation that maximizes the sum of such scores. But this too would require backtracking and searching over an exponentially large number of possible action sequences: if a bad decision is made at the beginning of the derivation, then it may be impossible to recover the optimal action sequence without backtracking to that early mistake. This is known as a **search error**. Transition-based parsers can incorporate arbitrary features, without the restrictive independence assumptions required by chart parsing; search errors are the price that must be paid for this flexibility.

**Learning transition-based parsing** Transition-based parsing can be combined with machine learning by training a classifier to select the correct action in each situation. This classifier is free to choose any feature of the input, the state of the parser, and the parse history. However, there is no optimality guarantee: the parser may choose a suboptimal parse, due to a mistake at the beginning of the analysis. Nonetheless, some of the strongest

#### 10.6. BEYOND CONTEXT-FREE PARSING

CFG parsers are based on the shift-reduce architecture, rather than CKY. A recent generation of models links shift-reduce parsing with recurrent neural networks, updating a hidden state vector while consuming the input (e.g., Cross and Huang, 2016; Dyer et al., 2016). Learning algorithms for transition-based parsing are discussed in more detail in § 11.3.

# Exercises

- 1. Design a grammar that handles English subject-verb agreement. Specifically, your grammar should handle the examples below correctly:
  - (10.4) a. She sings.
    - b. We sing.
  - (10.5) a. \*She sing.
    - b. \*We sings.
- 2. Extend your grammar from the previous problem to include the auxiliary verb *can*, so that the following cases are handled:
  - (10.6) a. She can sing.
    - b. We can sing.
  - (10.7) a. \*She can sings.
    - b. \*We can sings.
- 3. French requires subjects and verbs to agree in person and number, and it requires determiners and nouns to agree in gender and number. Verbs and their objects need not agree. Assuming that French has two genders (feminine and masculine), three persons (first [*me*], second [*you*], third [*her*]), and two numbers (singular and plural), how many productions are required to extend the following simple grammar to handle agreement?

| S  | $\rightarrow$ | NP VP              |
|----|---------------|--------------------|
| VP | $\rightarrow$ | V   V NP   V NP NP |
| NP | $\rightarrow$ | Det Nn             |

4. Consider the grammar:

 $\begin{array}{llllllllllllllllllllllllllllllllllll$ 

Apply the CKY algorithm and identify all possible parses for the sentence *fish fish fish fish*.

- 5. Choose one of the possible parses for the previous problem, and show how it can be derived by a series of shift-reduce actions.
- 6. To handle VP coordination, a grammar includes the production VP → VP CC VP. To handle adverbs, it also includes the production VP → VP ADV. Assume all verbs are generated from a sequence of unary productions, e.g., VP → V → eat.
  - a) Show how to binarize the production  $VP \rightarrow VP CC VP$ .
  - b) Use your binarized grammar to parse the sentence *They eat and drink together*, treating *together* as an adverb.
  - c) Prove that a weighted CFG cannot distinguish the two possible derivations of this sentence. Your explanation should focus on the productions in the original, non-binary grammar.
  - d) Explain what condition must hold for a parent-annotated WCFG to prefer the derivation in which *together* modifies the coordination *eat and drink*.
- 7. Consider the following PCFG:

$$p(X \to X | X) = \frac{1}{2}$$
 [10.45]

$$\mathbf{p}(X \to Y) = \frac{1}{2} \tag{10.46}$$

$$p(Y \to \sigma) = \frac{1}{|\Sigma|}, \forall \sigma \in \Sigma$$
 [10.47]

- a) Compute the probability  $p(\hat{\tau})$  of the maximum probability parse for a string  $\boldsymbol{w} \in \Sigma^M$ .
- b) Compute the conditional probability  $p(\hat{\tau} \mid \boldsymbol{w})$ .
- 8. Context-free grammars can be used to parse the internal structure of words. Using the weighted CKY algorithm and the following weighted context-free grammar, identify the best parse for the sequence of morphological segments *in+flame+able*.

| S       | $\rightarrow$ | V         | 0  |
|---------|---------------|-----------|----|
| S       | $\rightarrow$ | Ν         | 0  |
| S       | $\rightarrow$ | J         | 0  |
| V       | $\rightarrow$ | VPref N   | -1 |
| J       | $\rightarrow$ | N JSuff   | 1  |
| J       | $\rightarrow$ | V JSuff   | 0  |
| J       | $\rightarrow$ | NegPref J | 1  |
| VPref   | $\rightarrow$ | in+       | 2  |
| NegPref | $\rightarrow$ | in+       | 1  |
| Ν       | $\rightarrow$ | flame     | 0  |
| JSuff   | $\rightarrow$ | +able     | 0  |

- 9. Use the inside and outside scores to compute the marginal probability  $p(X_{i+1:j} \rightarrow Y_{i+1:k} Z_{k+1:j} | w)$ , indicating that *Y* spans  $w_{i+1:k}$ , *Z* spans  $w_{k+1:j}$ , and *X* is the parent of *Y* and *Z*, spanning  $w_{i+1:j}$ .
- 10. Suppose that the potentials  $\Psi(X \to \alpha)$  are log-probabilities, so that  $\sum_{\alpha} \exp \Psi(X \to \alpha) = 1$  for all *X*. Verify that the semiring inside recurrence from Equation 10.26 generates the log-probability  $\log p(w) = \log \sum_{\tau: \text{vield}(\tau) = w} p(\tau)$ .

# <span id="page-274-3"></span><span id="page-274-0"></span>**Chapter 11 Dependency parsing**

The previous chapter discussed algorithms for analyzing sentences in terms of nested constituents, such as noun phrases and verb phrases. However, many of the key sources of ambiguity in phrase-structure analysis relate to questions of **attachment**: where to attach a prepositional phrase or complement clause, how to scope a coordinating conjunction, and so on. These attachment decisions can be represented with a more lightweight structure: a directed graph over the words in the sentence, known as a **dependency parse**. Syntactic annotation has shifted its focus to such dependency structures: at the time of this writing, the **Universal Dependencies** project offers more than 100 dependency treebanks for more than 60 languages.[1](#page-274-2) This chapter will describe the linguistic ideas underlying dependency grammar, and then discuss exact and transition-based parsing algorithms. The chapter will also discuss recent research on **learning to search** in transition-based structure prediction.

# <span id="page-274-1"></span>**11.1 Dependency grammar**

While **dependency grammar** has a rich history of its own [\(Tesniere, 1966;](#page-565-7) K ` [ubler et al.,](#page-541-4) ¨ [2009\)](#page-541-4), it can be motivated by extension from the lexicalized context-free grammars that we encountered in previous chapter (§ [10.5.2\)](#page-261-0). Recall that lexicalization augments each non-terminal with a **head word**. The head of a constituent is identified recursively, using a set of **head rules**, as shown in [Table 10.3.](#page-263-1) An example of a lexicalized context-free parse is shown in [Figure 11.1a.](#page-275-1) In this sentence, the head of the S constituent is the main verb, *scratch*; this non-terminal then produces the noun phrase *the cats*, whose head word is *cats*, and from which we finally derive the word *the*. Thus, the word *scratch* occupies the central position for the sentence, with the word *cats* playing a supporting role. In turn, *cats*

<span id="page-274-2"></span><sup>1</sup><universaldependencies.org>

<span id="page-275-2"></span><span id="page-275-1"></span>![](_page_275_Figure_1.jpeg)

Figure 11.1: Dependency grammar is closely linked to lexicalized context free grammars: each lexical head has a dependency path to every other word in the constituent. (This example is based on the lexicalization rules from § [10.5.2,](#page-261-0) which make the preposition the head of a prepositional phrase. In the more contemporary Universal Dependencies annotations, the head of *with claws* would be *claws*, so there would be an edge *scratch* → *claws*.)

occupies the central position for the noun phrase, with the word *the* playing a supporting role.

The relationships between words in a sentence can be formalized in a directed graph, based on the lexicalized phrase-structure parse: create an edge (i, j) iff word i is the head of a phrase whose child is a phrase headed by word j. Thus, in our example, we would have *scratch* → *cats* and *cats* → *the*. We would not have the edge *scratch* → *the*, because although S(*scratch*) dominates DET(*the*) in the phrase-structure parse tree, it is not its immediate parent. These edges describe **syntactic dependencies**, a bilexical relationship between a **head** and a **dependent**, which is at the heart of dependency grammar.

Continuing to build out this **dependency graph**, we will eventually reach every word in the sentence, as shown in [Figure 11.1b.](#page-275-1) In this graph — and in all graphs constructed in this way — every word has exactly one incoming edge, except for the root word, which is indicated by a special incoming arrow from above. Furthermore, the graph is *weakly connected*: if the directed edges were replaced with undirected edges, there would be a path between all pairs of nodes. From these properties, it can be shown that there are no cycles in the graph (or else at least one node would have to have more than one incoming edge), and therefore, the graph is a tree. Because the graph includes all vertices, it is a **spanning tree**.

#### <span id="page-275-0"></span>**11.1.1 Heads and dependents**

A dependency edge implies an asymmetric syntactic relationship between the head and dependent words, sometimes called **modifiers**. For a pair like *the cats* or *cats scratch*, how

#### <span id="page-276-2"></span>11.1. DEPENDENCY GRAMMAR 259

do we decide which is the head? Here are some possible criteria:

- The head sets the syntactic category of the construction: for example, nouns are the heads of noun phrases, and verbs are the heads of verb phrases.
- The modifier may be optional while the head is mandatory: for example, in the sentence *cats scratch people with claws*, the subtrees *cats scratch* and *cats scratch people* are grammatical sentences, but *with claws* is not.
- The head determines the morphological form of the modifier: for example, in languages that require gender agreement, the gender of the noun determines the gender of the adjectives and determiners.
- Edges should first connect content words, and then connect function words.

These guidelines are not universally accepted, and they sometimes conflict. The Universal Dependencies (UD) project has attempted to identify a set of principles that can be applied to dozens of different languages [\(Nivre et al., 2016\)](#page-552-3).[2](#page-276-1) These guidelines are based on the universal part-of-speech tags from [chapter 8.](#page-192-0) They differ somewhat from the head rules described in § [10.5.2:](#page-261-0) for example, on the principle that dependencies should relate content words, the prepositional phrase *with claws* would be headed by *claws*, resulting in an edge *scratch* → *claws*, and another edge *claws* → *with*.

One objection to dependency grammar is that not all syntactic relations are asymmetric. One such relation is coordination [\(Popel et al., 2013\)](#page-555-7): in the sentence, *Abigail and Max like kimchi* [\(Figure 11.2\)](#page-277-1), which word is the head of the coordinated noun phrase *Abigail and Max*? Choosing either *Abigail* or *Max* seems arbitrary; fairness argues for making *and* the head, but this seems like the least important word in the noun phrase, and selecting it would violate the principle of linking content words first. The Universal Dependencies annotation system arbitrarily chooses the left-most item as the head — in this case, *Abigail* — and includes edges from this head to both *Max* and the coordinating conjunction *and*. These edges are distinguished by the labels CONJ (for the thing begin conjoined) and CC (for the coordinating conjunction). The labeling system is discussed next.

#### <span id="page-276-0"></span>**11.1.2 Labeled dependencies**

Edges may be **labeled** to indicate the nature of the syntactic relation that holds between the two elements. For example, in [Figure 11.2,](#page-277-1) the label NSUBJ on the edge from *like* to *Abigail* indicates that the subtree headed by *Abigail* is the noun subject of the verb *like*; similarly, the label OBJ on the edge from *like* to *kimchi* indicates that the subtree headed by

<span id="page-276-1"></span><sup>2</sup>The latest and most specific guidelines are available at [universaldependencies.org/](universaldependencies.org/guidelines.html) [guidelines.html](universaldependencies.org/guidelines.html)

<span id="page-277-4"></span><span id="page-277-1"></span>![](_page_277_Figure_1.jpeg)

Figure 11.2: In the Universal Dependencies annotation system, the left-most item of a coordination is the head.

<span id="page-277-3"></span>![](_page_277_Figure_3.jpeg)

Figure 11.3: A labeled dependency parse from the English UD Treebank (reviews-361348- 0006)

*kimchi* is the object.[3](#page-277-2) The negation *not* is treated as an adverbial modifier (ADVMOD) on the noun *jook*.

A slightly more complex example is shown in [Figure 11.3.](#page-277-3) The multiword expression *New York pizza* is treated as a "flat" unit of text, with the elements linked by the COM-POUND relation. The sentence includes two clauses that are conjoined in the same way that noun phrases are conjoined in [Figure 11.2.](#page-277-1) The second clause contains a **copula** verb (see § [8.1.1\)](#page-193-0). For such clauses, we treat the "object" of the verb as the root — in this case, *it* — and label the verb as a dependent, with the COP relation. This example also shows how punctuations are treated, with label PUNCT.

#### <span id="page-277-0"></span>**11.1.3 Dependency subtrees and constituents**

Dependency trees hide information that would be present in a CFG parse. Often what is hidden is in fact irrelevant: for example, [Figure 11.4](#page-278-0) shows three different ways of

<span id="page-277-2"></span><sup>3</sup>Earlier work distinguished direct and indirect objects [\(De Marneffe and Manning, 2008\)](#page-524-6), but this has been dropped in version 2.0 of the Universal Dependencies annotation system.

<span id="page-278-1"></span><span id="page-278-0"></span>![](_page_278_Figure_1.jpeg)

Figure 11.4: The three different CFG analyses of this verb phrase all correspond to a single dependency structure.

representing prepositional phrase adjuncts to the verb *ate*. Because there is apparently no meaningful difference between these analyses, the Penn Treebank decides by convention to use the two-level representation (see [Johnson, 1998,](#page-538-8) for a discussion). As shown in [Figure 11.4d,](#page-278-0) these three cases all look the same in a dependency parse.

But dependency grammar imposes its own set of annotation decisions, such as the identification of the head of a coordination (§ [11.1.1\)](#page-275-0); without lexicalization, context-free grammar does not require either element in a coordination to be privileged in this way. Dependency parses can be disappointingly flat: for example, in the sentence *Yesterday, Abigail was reluctantly giving Max kimchi*, the root *giving* is the head of every dependency! The constituent parse arguably offers a more useful structural analysis for such cases.

**Projectivity** Thus far, we have defined dependency trees as spanning trees over a graph in which each word is a vertex. As we have seen, one way to construct such trees is by connecting the heads in a lexicalized constituent parse. However, there are spanning trees that cannot be constructed in this way. Syntactic constituents are *contiguous* spans. In a spanning tree constructed from a lexicalized constituent parse, the head h of any constituent that spans the nodes from i to j must have a path to every node in this span. This is property is known as **projectivity**, and projective dependency parses are a restricted class of spanning trees. Informally, projectivity means that "crossing edges" are prohibited. The formal definition follows:

<span id="page-279-2"></span>

|         | % non-projective edges | % non-projective sentences |
|---------|------------------------|----------------------------|
| Czech   | 1.86%                  | 22.42%                     |
| English | 0.39%                  | 7.63%                      |
| German  | 2.33%                  | 28.19%                     |

<span id="page-279-1"></span>Table 11.1: Frequency of non-projective dependencies in three languages (Kuhlmann and Nivre, 2010)

![](_page_279_Figure_3.jpeg)

Lucia ate a pizza yesterday which was vegetarian

Figure 11.5: An example of a non-projective dependency parse. The "crossing edge" arises from the relative clause *which was vegetarian* and the oblique temporal modifier *yesterday*.

**Definition 2** (Projectivity). An edge from *i* to *j* is projective iff all *k* between *i* and *j* are descendants of *i*. A dependency parse is projective iff all its edges are projective.

Figure 11.5 gives an example of a non-projective dependency graph in English. This dependency graph does not correspond to any constituent parse. As shown in Table 11.1, non-projectivity is more common in languages such as Czech and German. Even though relatively few dependencies are non-projective in these languages, many sentences have at least one such dependency. As we will soon see, projectivity has important algorithmic consequences.

# <span id="page-279-0"></span>11.2 Graph-based dependency parsing

Let  $\boldsymbol{y} = \{(i \xrightarrow{r} j)\}$  represent a dependency graph, in which each edge is a relation r from head word  $i \in \{1, 2, ..., M, \text{ROOT}\}$  to modifier  $j \in \{1, 2, ..., M\}$ . The special node ROOT indicates the root of the graph, and M is the length of the input  $|\boldsymbol{w}|$ . Given a scoring function  $\Psi(\boldsymbol{y}, \boldsymbol{w}; \boldsymbol{\theta})$ , the optimal parse is,

$$\hat{\boldsymbol{y}} = \operatorname*{argmax}_{\boldsymbol{y} \in \mathcal{Y}(\boldsymbol{w})} \Psi(\boldsymbol{y}, \boldsymbol{w}; \boldsymbol{\theta}),$$
[11.1]

where  $\mathcal{Y}(\boldsymbol{w})$  is the set of valid dependency parses on the input  $\boldsymbol{w}$ . As usual, the number of possible labels  $|\mathcal{Y}(\boldsymbol{w})|$  is exponential in the length of the input (Wu and Chao, 2004).

<span id="page-280-1"></span>

| First order  | h m                           |
|--------------|-------------------------------|
| Second order | h s m g h m                   |
| Third order  | g $h$ $s$ $m$ $h$ $t$ $s$ $m$ |

<span id="page-280-0"></span>Figure 11.6: Feature templates for higher-order dependency parsing

Algorithms that search over this space of possible graphs are known as **graph-based dependency parsers**.

In sequence labeling and constituent parsing, it was possible to search efficiently over an exponential space by choosing a feature function that decomposes into a sum of local feature vectors. A similar approach is possible for dependency parsing, by requiring the scoring function to decompose across dependency arcs:

$$\Psi(\boldsymbol{y}, \boldsymbol{w}; \boldsymbol{\theta}) = \sum_{i \xrightarrow{r} j \in \boldsymbol{y}} \psi(i \xrightarrow{r} j, \boldsymbol{w}; \boldsymbol{\theta}).$$
[11.2]

Dependency parsers that operate under this assumption are known as **arc-factored**, since the score of a graph is the product of the scores of all arcs.

**Higher-order dependency parsing** The arc-factored decomposition can be relaxed to allow higher-order dependencies. In **second-order dependency parsing**, the scoring function may include grandparents and siblings, as shown by the templates in Figure 11.6. The scoring function is,

$$\Psi(\boldsymbol{y}, \boldsymbol{w}; \boldsymbol{\theta}) = \sum_{\substack{i \xrightarrow{r} j \in \boldsymbol{y} \\ i \xrightarrow{r'} i \in \boldsymbol{y} \\ k \xrightarrow{r'} i \in \boldsymbol{y} \\ + \sum_{\substack{k \xrightarrow{r'} i \in \boldsymbol{y} \\ i \xrightarrow{r'} s \in \boldsymbol{y} \\ s \neq i}} \psi_{\text{sibling}}(i \xrightarrow{r} j, s, r', \boldsymbol{w}; \boldsymbol{\theta})$$

$$(11.3)$$

The top line scores computes a scoring function that includes the grandparent *k*; the bottom line computes a scoring function for each sibling *s*. For projective dependency graphs, there are efficient algorithms for second-order and third-order dependency parsing (Eisner, 1996; McDonald and Pereira, 2006; Koo and Collins, 2010); for non-projective dependency graphs, second-order dependency parsing is NP-hard (McDonald and Pereira, 2006). The specific algorithms are discussed in the next section.

#### <span id="page-281-1"></span><span id="page-281-0"></span>11.2.1 Graph-based parsing algorithms

The distinction between projective and non-projective dependency trees (§ 11.1.3) plays a key role in the choice of algorithms. Because projective dependency trees are closely related to (and can be derived from) lexicalized constituent trees, lexicalized parsing algorithms can be applied directly. For the more general problem of parsing to arbitrary spanning trees, a different class of algorithms is required. In both cases, arc-factored dependency parsing relies on precomputing the scores  $\psi(i \xrightarrow{r} j, \boldsymbol{w}; \boldsymbol{\theta})$  for each potential edge. There are  $\mathcal{O}(M^2R)$  such scores, where M is the length of the input and R is the number of dependency relation types, and this is a lower bound on the time and space complexity of any exact algorithm for arc-factored dependency parsing.

#### Projective dependency parsing

Any lexicalized constituency tree can be converted into a projective dependency tree by creating arcs between the heads of constituents and their parents, so any algorithm for lexicalized constituent parsing can be converted into an algorithm for projective dependency parsing, by converting arc scores into scores for lexicalized productions. As noted in § 10.5.2, there are cubic time algorithms for lexicalized constituent parsing, which are extensions of the CKY algorithm. Therefore, arc-factored projective dependency parsing can be performed in cubic time in the length of the input.

Second-order projective dependency parsing can also be performed in cubic time, with minimal modifications to the lexicalized parsing algorithm (Eisner, 1996). It is possible to go even further, to **third-order dependency parsing**, in which the scoring function may consider great-grandparents, grand-siblings, and "tri-siblings", as shown in Figure 11.6. Third-order dependency parsing can be performed in  $O(M^4)$  time, which can be made practical through the use of pruning to eliminate unlikely edges (Koo and Collins, 2010).

#### Non-projective dependency parsing

In non-projective dependency parsing, the goal is to identify the highest-scoring spanning tree over the words in the sentence. The arc-factored assumption ensures that the score for each spanning tree will be computed as a sum over scores for the edges, which are precomputed. Based on these scores, we build a weighted connected graph. Arc-factored non-projective dependency parsing is then equivalent to finding the spanning tree that achieves the maximum total score,  $\Psi(\boldsymbol{y}, \boldsymbol{w}) = \sum_{i \stackrel{r}{\rightarrow} j \in \boldsymbol{y}} \psi(i \stackrel{r}{\rightarrow} j, \boldsymbol{w})$ . The **Chu-Liu-Edmonds algorithm** (Chu and Liu, 1965; Edmonds, 1967) computes this **maximum directed spanning tree** efficiently. It does this by first identifying the best incoming edge  $i \stackrel{r}{\rightarrow} j$  for each vertex j. If the resulting graph does not contain cycles, it is the maximum spanning tree. If there is a cycle, it is collapsed into a super-vertex, whose incoming and outgoing edges are based on the edges to the vertices in the cycle. The algorithm is

#### 11.2. GRAPH-BASED DEPENDENCY PARSING

then applied recursively to the resulting graph, and process repeats until a graph without cycles is obtained.

The time complexity of identifying the best incoming edge for each vertex is  $O(M^2R)$ , where M is the length of the input and R is the number of relations; in the worst case, the number of cycles is O(M). Therefore, the complexity of the Chu-Liu-Edmonds algorithm is  $O(M^3R)$ . This complexity can be reduced to  $O(M^2N)$  by storing the edge scores in a Fibonnaci heap (Gabow et al., 1986). For more detail on graph-based parsing algorithms, see Eisner (1997) and Kübler et al. (2009).

**Higher-order non-projective dependency parsing** Given the tractability of higher-order projective dependency parsing, you may be surprised to learn that non-projective second-order dependency parsing is NP-Hard. This can be proved by reduction from the vertex cover problem (Neuhaus and Bröker, 1997). A heuristic solution is to do projective parsing first, and then post-process the projective dependency parse to add non-projective edges (Nivre and Nilsson, 2005). More recent work has applied techniques for approximate inference in graphical models, including belief propagation (Smith and Eisner, 2008), integer linear programming (Martins et al., 2009), variational inference (Martins et al., 2010), and Markov Chain Monte Carlo (Zhang et al., 2014).

#### <span id="page-282-0"></span>11.2.2 Computing scores for dependency arcs

The arc-factored scoring function  $\psi(i \xrightarrow{r} j, w; \theta)$  can be defined in several ways:

| Linear $\psi(i \xrightarrow{r} j, \boldsymbol{w}; \boldsymbol{\theta}) = \boldsymbol{\theta} \cdot \boldsymbol{f}(i \xrightarrow{r} j, \boldsymbol{w})$ | [11.4] |
|---------------------------------------------------------------------------------------------------------------------------------------------------------|--------|
|---------------------------------------------------------------------------------------------------------------------------------------------------------|--------|

Neural  $\psi(i \xrightarrow{r} j, \boldsymbol{w}; \boldsymbol{\theta}) = \text{Feedforward}([\boldsymbol{u}_{w_i}; \boldsymbol{u}_{w_j}]; \boldsymbol{\theta})$  [11.5]

Generative  $\psi(i \xrightarrow{r} j, \boldsymbol{w}; \boldsymbol{\theta}) = \log p(w_j, r \mid w_i).$  [11.6]

#### Linear feature-based arc scores

Linear models for dependency parsing incorporate many of the same features used in sequence labeling and discriminative constituent parsing. These include:

- the length and direction of the arc;
- the words  $w_i$  and  $w_j$  linked by the dependency relation;
- the prefixes, suffixes, and parts-of-speech of these words;
- the neighbors of the dependency arc,  $w_{i-1}, w_{i+1}, w_{j-1}, w_{j+1}$ ;
- the prefixes, suffixes, and part-of-speech of these neighbor words.

<span id="page-283-0"></span>Each of these features can be conjoined with the dependency edge label r. Note that features in an arc-factored parser can refer to words other than w<sup>i</sup> and w<sup>j</sup> . The restriction is that the features consider only a single arc.

**Bilexical features** (e.g., *sushi* → *chopsticks*) are powerful but rare, so it is useful to augment them with coarse-grained alternatives, by "backing off" to the part-of-speech or affix. For example, the following features are created by backing off to part-of-speech tags in an unlabeled dependency parser:

$$\begin{split} \boldsymbol{f}(3 \rightarrow 5, \textit{we eat sushi with chopsticks}) &= \langle \textit{sushi} \rightarrow \textit{chopsticks}, \\ & \textit{sushi} \rightarrow \textit{NNS}, \\ & \textit{NN} \rightarrow \textit{chopsticks}, \\ & \textit{NNS} \rightarrow \textit{NN} \rangle. \end{split}$$

Regularized discriminative learning algorithms can then trade off between features at varying levels of detail. [McDonald et al.](#page-547-7) [\(2005\)](#page-547-7) take this approach as far as *tetralexical* features (e.g., (w<sup>i</sup> , wi+1, wj−1, w<sup>j</sup> )). Such features help to avoid choosing arcs that are unlikely due to the intervening words: for example, there is unlikely to be an edge between two nouns if the intervening span contains a verb. A large list of first and second-order features is provided by [Bohnet](#page-515-5) [\(2010\)](#page-515-5), who uses a hashing function to store these features efficiently.

#### **Neural arc scores**

Given vector representations x<sup>i</sup> for each word w<sup>i</sup> in the input, a set of arc scores can be computed from a feedforward neural network:

$$\psi(i \xrightarrow{r} j, \boldsymbol{w}; \boldsymbol{\theta}) = \text{FeedForward}([\boldsymbol{x}_i; \boldsymbol{x}_j]; \boldsymbol{\theta}_r),$$
 [11.7]

where unique weights θ<sup>r</sup> are available for each arc type [\(Pei et al., 2015;](#page-554-5) [Kiperwasser and](#page-540-2) [Goldberg, 2016\)](#page-540-2). [Kiperwasser and Goldberg](#page-540-2) [\(2016\)](#page-540-2) use a feedforward network with a single hidden layer,

$$\boldsymbol{z} = g(\boldsymbol{\Theta}_r[\boldsymbol{x}_i; \boldsymbol{x}_j] + b_r^{(z)})$$
[11.8]

$$\psi(i \xrightarrow{r} j) = \beta_r z + b_r^{(y)}, \qquad [11.9]$$

where Θ<sup>r</sup> is a matrix, β<sup>r</sup> is a vector, each b<sup>r</sup> is a scalar, and the function g is an elementwise tanh activation function.

The vector x<sup>i</sup> can be set equal to the word embedding, which may be pre-trained or learned by backpropagation [\(Pei et al., 2015\)](#page-554-5). Alternatively, contextual information can be incorporated by applying a bidirectional recurrent neural network across the input, as

described in  $\S$  7.6. The RNN hidden states at each word can be used as inputs to the arc scoring function (Kiperwasser and Goldberg, 2016).

Feature-based arc scores are computationally expensive, due to the costs of storing and searching a huge table of weights. Neural arc scores can be viewed as a compact solution to this problem. Rather than working in the space of tuples of lexical features, the hidden layers of a feedforward network can be viewed as implicitly computing feature combinations, with each layer of the network evaluating progressively more words. An early paper on neural dependency parsing showed substantial speed improvements at test time, while also providing higher accuracy than feature-based models (Chen and Manning, 2014).

#### **Probabilistic arc scores**

If each arc score is equal to the log probability  $\log p(w_j, r \mid w_i)$ , then the sum of scores gives the log probability of the sentence and arc labels, by the chain rule. For example, consider the unlabeled parse of *we eat sushi with rice*,

$$\boldsymbol{y} = \{ (\text{ROOT}, 2), (2, 1), (2, 3), (3, 5), (5, 4) \}$$
[11.10]

$$\log p(\boldsymbol{w} \mid \boldsymbol{y}) = \sum_{(i \to j) \in \boldsymbol{y}} \log p(w_j \mid w_i)$$
[11.11]

$$= \log p(eat \mid \text{ROOT}) + \log p(we \mid eat) + \log p(sushi \mid eat) + \log p(rice \mid sushi) + \log p(with \mid rice).$$
[11.12]

Probabilistic generative models are used in combination with expectation-maximization (chapter 5) for unsupervised dependency parsing (Klein and Manning, 2004).

#### <span id="page-284-0"></span>11.2.3 Learning

Having formulated graph-based dependency parsing as a structure prediction problem, we can apply similar learning algorithms to those used in sequence labeling. Given a loss function  $\ell(\theta; w^{(i)}, y^{(i)})$ , we can compute gradient-based updates to the parameters. For a model with feature-based arc scores and a perceptron loss, we obtain the usual structured perceptron update,

$$\hat{\boldsymbol{y}} = \underset{\boldsymbol{y}' \in \mathcal{Y}(\boldsymbol{w})}{\operatorname{argmax}} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}, \boldsymbol{y}')$$
[11.13]

$$\boldsymbol{\theta} = \boldsymbol{\theta} + \boldsymbol{f}(\boldsymbol{w}, \boldsymbol{y}) - \boldsymbol{f}(\boldsymbol{w}, \hat{\boldsymbol{y}})$$
[11.14]

In this case, the argmax requires a maximization over all dependency trees for the sentence, which can be computed using the algorithms described in § 11.2.1. We can apply all the usual tricks from § 2.3: weight averaging, a large margin objective, and regularization. McDonald et al. (2005) were the first to treat dependency parsing as a structure

<span id="page-285-1"></span>prediction problem, using MIRA, an online margin-based learning algorithm. Neural arc scores can be learned in the same way, backpropagating from a margin loss to updates on the feedforward network that computes the score for each edge.

A conditional random field for arc-factored dependency parsing is built on the probability model,

$$p(\boldsymbol{y} \mid \boldsymbol{w}) = \frac{\exp\sum_{i \xrightarrow{r} j \in \boldsymbol{y}} \psi(i \xrightarrow{r} j, \boldsymbol{w}; \boldsymbol{\theta})}{\sum_{\boldsymbol{y}' \in \mathcal{Y}(\boldsymbol{w})} \exp\sum_{i \xrightarrow{r} j \in \boldsymbol{y}'} \psi(i \xrightarrow{r} j, \boldsymbol{w}; \boldsymbol{\theta})}$$
[11.15]

Such a model is trained to minimize the negative log conditional-likelihood. Just as in CRF sequence models (§ 7.5.3) and the logistic regression classifier (§ 2.5), the gradients involve marginal probabilities  $p(i \xrightarrow{r} j | w; \theta)$ , which in this case are probabilities over individual dependencies. In arc-factored models, these probabilities can be computed in polynomial time. For projective dependency trees, the marginal probabilities can be computed in cubic time, using a variant of the inside-outside algorithm (Lari and Young, 1990). For non-projective dependency parsing, marginals can also be computed in cubic time, using the **matrix-tree theorem** (Koo et al., 2007; McDonald et al., 2007; Smith and Smith, 2007). Details of these methods are described by Kübler et al. (2009).

# <span id="page-285-0"></span>11.3 Transition-based dependency parsing

Graph-based dependency parsing offers exact inference, meaning that it is possible to recover the best-scoring parse for any given model. But this comes at a price: the scoring function is required to decompose into local parts — in the case of non-projective parsing, these parts are restricted to individual arcs. These limitations are felt more keenly in dependency parsing than in sequence labeling, because second-order dependency features are critical to correctly identify some types of attachments. For example, prepositional phrase attachment depends on the attachment point, the object of the preposition, and the preposition itself; arc-factored scores cannot account for all three of these features simultaneously. Graph-based dependency parsing may also be criticized on the basis of intuitions about human language processing: people read and listen to sentences *sequentially*, incrementally building mental models of the sentence structure and meaning before getting to the end (Jurafsky, 1996). This seems hard to reconcile with graph-based algorithms, which perform bottom-up operations on the entire sentence, requiring the parser to keep every word in memory. Finally, from a practical perspective, graph-based dependency parsing is relatively slow, running in cubic time in the length of the input.

Transition-based algorithms address all three of these objections. They work by moving through the sentence sequentially, while performing actions that incrementally update a stored representation of what has been read thus far. As with the shift-reduce

<span id="page-286-2"></span>parser from § [10.6.2,](#page-268-0) this representation consists of a stack, onto which parsing substructures can be pushed and popped. In shift-reduce, these substructures were constituents; in the transition systems that follow, they will be projective dependency trees over partial spans of the input.[4](#page-286-1) Parsing is complete when the input is consumed and there is only a single structure on the stack. The sequence of actions that led to the parse is known as the **derivation**. One problem with transition-based systems is that there may be multiple derivations for a single parse structure — a phenomenon known as **spurious ambiguity**.

#### <span id="page-286-0"></span>**11.3.1 Transition systems for dependency parsing**

A **transition system** consists of a representation for describing configurations of the parser, and a set of transition actions, which manipulate the configuration. There are two main transition systems for dependency parsing: **arc-standard**, which is closely related to shiftreduce, and **arc-eager**, which adds an additional action that can simplify derivations [\(Ab](#page-509-3)[ney and Johnson, 1991\)](#page-509-3). In both cases, transitions are between **configurations** that are represented as triples, C = (σ, β, A), where σ is the stack, β is the input buffer, and A is the list of arcs that have been created [\(Nivre, 2008\)](#page-552-7). In the initial configuration,

$$C_{\text{initial}} = ([\text{ROOT}], \boldsymbol{w}, \varnothing), \qquad [11.16]$$

indicating that the stack contains only the special node ROOT, the entire input is on the buffer, and the set of arcs is empty. An accepting configuration is,

$$C_{\text{accept}} = ([\text{ROOT}], \emptyset, A), \qquad [11.17]$$

where the stack contains only ROOT, the buffer is empty, and the arcs A define a spanning tree over the input. The arc-standard and arc-eager systems define a set of transitions between configurations, which are capable of transforming an initial configuration into an accepting configuration. In both of these systems, the number of actions required to parse an input grows linearly in the length of the input, making transition-based parsing considerably more efficient than graph-based methods.

#### **Arc-standard**

The **arc-standard** transition system is closely related to shift-reduce, and to the LR algorithm that is used to parse programming languages [\(Aho et al., 2006\)](#page-509-4). It includes the following classes of actions:

• SHIFT: move the first item from the input buffer on to the top of the stack,

$$(\sigma, i|\beta, A) \Rightarrow (\sigma|i, \beta, A),$$
 [11.18]

<span id="page-286-1"></span><sup>4</sup>Transition systems also exist for non-projective dependency parsing (e.g., [Nivre, 2008\)](#page-552-7).

<span id="page-287-0"></span>where we write  $i|\beta$  to indicate that *i* is the leftmost item in the input buffer, and  $\sigma|i$  to indicate the result of pushing *i* on to stack  $\sigma$ .

ARC-LEFT: create a new left-facing arc of type *r* between the item on the top of the stack and the first item in the input buffer. The head of this arc is *j*, which remains at the front of the input buffer. The arc *j* <sup>*r*</sup>→ *i* is added to *A*. Formally,

$$(\sigma|i, j|\beta, A) \Rightarrow (\sigma, j|\beta, A \oplus j \xrightarrow{r} i),$$
[11.19]

where *r* is the label of the dependency arc, and  $\oplus$  concatenates the new arc  $j \xrightarrow{r} i$  to the list *A*.

ARC-RIGHT: creates a new right-facing arc of type *r* between the item on the top of the stack and the first item in the input buffer. The head of this arc is *i*, which is "popped" from the stack and pushed to the front of the input buffer. The arc *i* → *j* is added to *A*. Formally,

$$(\sigma|i, j|\beta, A) \Rightarrow (\sigma, i|\beta, A \oplus i \xrightarrow{r} j), \qquad [11.20]$$

where again r is the label of the dependency arc.

Each action has preconditions. The SHIFT action can be performed only when the buffer has at least one element. The ARC-LEFT action cannot be performed when the root node ROOT is on top of the stack, since this node must be the root of the entire tree. The ARC-LEFT and ARC-RIGHT remove the modifier words from the stack (in the case of ARC-LEFT) and from the buffer (in the case of ARC-RIGHT), so it is impossible for any word to have more than one parent. Furthermore, the end state can only be reached when every word is removed from the buffer and stack, so the set of arcs is guaranteed to constitute a spanning tree. An example arc-standard derivation is shown in Table 11.2.

#### Arc-eager dependency parsing

In the arc-standard transition system, a word is completely removed from the parse once it has been made the modifier in a dependency arc. At this time, any dependents of this word must have already been identified. Right-branching structures are common in English (and many other languages), with words often modified by units such as prepositional phrases to their right. In the arc-standard system, this means that we must first shift all the units of the input onto the stack, and then work backwards, creating a series of arcs, as occurs in Table 11.2. Note that the decision to shift *bagels* onto the stack guarantees that the prepositional phrase *with lox* will attach to the noun phrase, and that this decision must be made before the prepositional phrase is itself parsed. This has been argued to be cognitively implausible (Abney and Johnson, 1991); from a computational perspective, it means that a parser may need to look several steps ahead to make the correct decision.

<span id="page-288-0"></span>

|     | σ                            | β                         | action    | arc added to $\mathcal{A}$       |
|-----|------------------------------|---------------------------|-----------|----------------------------------|
| 1.  | [Root]                       | they like bagels with lox | Shift     |                                  |
| 2.  | [ROOT, they]                 | like bagels with lox      | Arc-Left  | $(they \leftarrow like)$         |
| 3.  | [ROOT]                       | like bagels with lox      | Shift     |                                  |
| 4.  | [ROOT, <i>like</i> ]         | bagels with lox           | Shift     |                                  |
| 5.  | [ROOT, <i>like, bagels</i> ] | with lox                  | Shift     |                                  |
| 6.  | [ROOT, like, bagels, with]   | lox                       | Arc-Left  | $(with \leftarrow lox)$          |
| 7.  | [ROOT, <i>like, bagels</i> ] | lox                       | Arc-Right | $(bagels \rightarrow lox)$       |
| 8.  | [ROOT, <i>like</i> ]         | bagels                    | Arc-Right | $(like \rightarrow bagels)$      |
| 9.  | [ROOT]                       | like                      | Arc-Right | $(\text{ROOT} \rightarrow like)$ |
| 10. | [Root]                       | Ø                         | Done      |                                  |

Table 11.2: Arc-standard derivation of the unlabeled dependency parse for the input *they like bagels with lox*.

**Arc-eager dependency parsing** changes the ARC-RIGHT action so that right dependents can be attached before all of their dependents have been found. Rather than removing the modifier from both the buffer and stack, the ARC-RIGHT action pushes the modifier on to the stack, on top of the head. Because the stack can now contain elements that already have parents in the partial dependency graph, two additional changes are necessary:

- A precondition is required to ensure that the ARC-LEFT action cannot be applied when the top element on the stack already has a parent in *A*.
- A new REDUCE action is introduced, which can remove elements from the stack if they already have a parent in *A*:

$$(\sigma|i,\beta,A) \Rightarrow (\sigma,\beta,A).$$
 [11.21]

As a result of these changes, it is now possible to create the arc *like*  $\rightarrow$  *bagels* before parsing the prepositional phrase *with lox*. Furthermore, this action does not imply a decision about whether the prepositional phrase will attach to the noun or verb. Noun attachment is chosen in the parse in Table 11.3, but verb attachment could be achieved by applying the REDUCE action at step 5 or 7.

#### Projectivity

The arc-standard and arc-eager transition systems are guaranteed to produce projective dependency trees, because all arcs are between the word at the top of the stack and the

<span id="page-289-3"></span><span id="page-289-1"></span>

|     | σ                          | β                         | action    | arc added to $\mathcal{A}$       |
|-----|----------------------------|---------------------------|-----------|----------------------------------|
| 1.  | [Root]                     | they like bagels with lox | Shift     |                                  |
| 2.  | [ROOT, <i>they</i> ]       | like bagels with lox      | Arc-Left  | $(they \leftarrow like)$         |
| 3.  | [ROOT]                     | like bagels with lox      | Arc-Right | $(\text{ROOT} \rightarrow like)$ |
| 4.  | [ROOT, <i>like</i> ]       | bagels with lox           | Arc-Right | $(like \rightarrow bagels)$      |
| 5.  | [ROOT, like, bagels]       | with lox                  | Shift     |                                  |
| 6.  | [ROOT, like, bagels, with] | lox                       | Arc-Left  | $(with \leftarrow lox)$          |
| 7.  | [ROOT, like, bagels]       | lox                       | Arc-Right | $(bagels \rightarrow lox)$       |
| 8.  | [ROOT, like, bagels, lox]  | Ø                         | Reduce    |                                  |
| 9.  | [ROOT, like, bagels]       | Ø                         | Reduce    |                                  |
| 10. | [ROOT, <i>like</i> ]       | Ø                         | Reduce    |                                  |
| 11. | [ROOT]                     | Ø                         | Done      |                                  |

Table 11.3: Arc-eager derivation of the unlabeled dependency parse for the input *they like bagels with lox*.

left-most edge of the buffer (Nivre, 2008). Non-projective transition systems can be constructed by adding actions that create arcs with words that are second or third in the stack (Attardi, 2006), or by adopting an alternative configuration structure, which maintains a list of all words that do not yet have heads (Covington, 2001). In **pseudo-projective dependency parsing**, a projective dependency parse is generated first, and then a set of graph transformation techniques are applied, producing non-projective edges (Nivre and Nilsson, 2005).

#### <span id="page-289-0"></span>Beam search

In "greedy" transition-based parsing, the parser tries to make the best decision at each configuration. This can lead to search errors, when an early decision locks the parser into a poor derivation. For example, in Table 11.2, if ARC-RIGHT were chosen at step 4, then the parser would later be forced to attach the prepositional phrase *with lox* to the verb *likes*. Note that the *likes*  $\rightarrow$  *bagels* arc is indeed part of the correct dependency parse, but the arc-standard transition system requires it to be created later in the derivation.

Beam search is a general technique for ameliorating search errors in incremental decoding.<sup>5</sup> While searching, the algorithm maintains a set of partially-complete hypotheses, called a beam. At step t of the derivation, there is a set of k hypotheses, each of which

<span id="page-289-2"></span><sup>&</sup>lt;sup>5</sup>Beam search is used throughout natural language processing, and beyond. In this text, it appears again in coreference resolution (§ 15.2.4) and machine translation (§ 18.4).

<span id="page-290-1"></span>![](_page_290_Figure_1.jpeg)

Figure 11.7: Beam search for unlabeled dependency parsing, with beam size K = 2. The arc lists for each configuration are not shown, but can be computed from the transitions.

includes a score  $s_t^{(k)}$  and a set of dependency arcs  $A_t^{(k)}$ :

$$h_t^{(k)} = (s_t^{(k)}, A_t^{(k)})$$
[11.22]

Each hypothesis is then "expanded" by considering the set of all valid actions from the current configuration  $c_t^{(k)}$ , written  $\mathcal{A}(c_t^{(k)})$ . This yields a large set of new hypotheses. For each action  $a \in \mathcal{A}(c_t^{(k)})$ , we score the new hypothesis  $A_t^{(k)} \oplus a$ . The top k hypotheses by this scoring metric are kept, and parsing proceeds to the next step (Zhang and Clark, 2008). Note that beam search requires a scoring function for action *sequences*, rather than individual actions. This issue will be revisited in the next section.

Figure 11.7 shows the application of beam search to dependency parsing, with a beam size of K = 2. For the first transition, the only valid action is SHIFT, so there is only one possible configuration at t = 2. From this configuration, there are three possible actions. The two best scoring actions are ARC-RIGHT and ARC-LEFT, and so the resulting hypotheses from these actions are on the beam at t = 3. From these configurations, there are three possible actions each, but the best two are expansions of the bottom hypothesis at t = 3. Parsing continues until t = 5, at which point both hypotheses reach an accepting state. The best-scoring hypothesis is then selected as the parse.

#### <span id="page-290-0"></span>11.3.2 Scoring functions for transition-based parsers

Transition-based parsing requires selecting a series of actions. In greedy transition-based parsing, this can be done by training a classifier,

$$\hat{a} = \operatorname*{argmax}_{a \in \mathcal{A}(c)} \Psi(a, c, \boldsymbol{w}; \boldsymbol{\theta}),$$
[11.23]

where  $\mathcal{A}(c)$  is the set of admissible actions in the current configuration c, w is the input, and  $\Psi$  is a scoring function with parameters  $\theta$  (Yamada and Matsumoto, 2003).

A feature-based score can be computed,  $\Psi(a, c, w) = \theta \cdot f(a, c, w)$ , using features that may consider any aspect of the current configuration and input sequence. Typical features for transition-based dependency parsing include: the word and part-of-speech of the top

element on the stack; the word and part-of-speech of the first, second, and third elements on the input buffer; pairs and triples of words and parts-of-speech from the top of the stack and the front of the buffer; the distance (in tokens) between the element on the top of the stack and the element in the front of the input buffer; the number of modifiers of each of these elements; and higher-order dependency features as described above in the section on graph-based dependency parsing (see, e.g., Zhang and Nivre, 2011).

Parse actions can also be scored by neural networks. For example, Chen and Manning (2014) build a feedforward network in which the input layer consists of the concatenation of embeddings of several words and tags:

- the top three words on the stack, and the first three words on the buffer;
- the first and second leftmost and rightmost children (dependents) of the top two words on the stack;
- the leftmost and right most grandchildren of the top two words on the stack;
- embeddings of the part-of-speech tags of these words.

Let us call this base layer  $\boldsymbol{x}(c, \boldsymbol{w})$ , defined as,

$$c = (\sigma, \beta, A)$$
  
$$\boldsymbol{x}(c, \boldsymbol{w}) = [\boldsymbol{v}_{w_{\sigma_1}}, \boldsymbol{v}_{t_{\sigma_1}}, \boldsymbol{v}_{w_{\sigma_2}}, \boldsymbol{v}_{t_{\sigma_2}}, \boldsymbol{v}_{w_{\sigma_3}}, \boldsymbol{v}_{t_{\sigma_3}}, \boldsymbol{v}_{w_{\beta_1}}, \boldsymbol{v}_{t_{\beta_1}}, \boldsymbol{v}_{w_{\beta_2}}, \boldsymbol{v}_{t_{\beta_2}}, \ldots]$$

where  $v_{w_{\sigma_1}}$  is the embedding of the first word on the stack,  $v_{t_{\beta_2}}$  is the embedding of the part-of-speech tag of the second word on the buffer, and so on. Given this base encoding of the parser state, the score for the set of possible actions is computed through a feedforward network,

$$\boldsymbol{z} = g(\Theta^{(x \to z)} \boldsymbol{x}(c, \boldsymbol{w}))$$
[11.24]

$$\psi(a, c, \boldsymbol{w}; \boldsymbol{\theta}) = \Theta_a^{(z \to y)} \boldsymbol{z}, \qquad [11.25]$$

where the vector z plays the same role as the features f(a, c, w), but is a learned representation. Chen and Manning (2014) use a cubic elementwise activation function,  $g(x) = x^3$ , so that the hidden layer models products across all triples of input features. The learning algorithm updates the embeddings as well as the parameters of the feedforward network.

#### <span id="page-291-0"></span>11.3.3 Learning to parse

Transition-based dependency parsing suffers from a mismatch between the supervision, which comes in the form of dependency trees, and the classifier's prediction space, which is a set of parsing actions. One solution is to create new training data by converting parse trees into action sequences; another is to derive supervision directly from the parser's performance.

#### <span id="page-292-2"></span>Oracle-based training

A transition system can be viewed as a function from action sequences (derivations) to parse trees. The inverse of this function is a mapping from parse trees to derivations, which is called an **oracle**. For the arc-standard and arc-eager parsing system, an oracle can be computed in linear time in the length of the derivation (Kübler et al., 2009, page 32). Both the arc-standard and arc-eager transition systems suffer from spurious ambiguity: there exist dependency parses for which multiple derivations are possible, such as  $1 \leftarrow 2 \rightarrow 3$ . The oracle must choose between these different derivations. For example, the algorithm described by Kübler et al. (2009) would first create the left arc  $(1 \leftarrow 2)$ , and then create the right arc,  $(1 \leftarrow 2) \rightarrow 3$ ; another oracle might begin by shifting twice, resulting in the derivation  $1 \leftarrow (2 \rightarrow 3)$ .

Given such an oracle, a dependency treebank can be converted into a set of oracle action sequences  $\{A^{(i)}\}_{i=1}^N$ . The parser can be trained by stepping through the oracle action sequences, and optimizing on an classification-based objective that rewards selecting the oracle action. For transition-based dependency parsing, maximum conditional likelihood is a typical choice (Chen and Manning, 2014; Dyer et al., 2015):

$$p(a \mid c, \boldsymbol{w}) = \frac{\exp \Psi(a, c, \boldsymbol{w}; \boldsymbol{\theta})}{\sum_{a' \in \mathcal{A}(c)} \exp \Psi(a', c, \boldsymbol{w}; \boldsymbol{\theta})}$$
[11.26]

<span id="page-292-0"></span>
$$\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta}}{\operatorname{argmax}} \sum_{i=1}^{N} \sum_{t=1}^{|A^{(i)}|} \log p(a_t^{(i)} \mid c_t^{(i)}, \boldsymbol{w}), \qquad [11.27]$$

where  $|A^{(i)}|$  is the length of the action sequence  $A^{(i)}$ .

Recall that beam search requires a scoring function for action sequences. Such a score can be obtained by adding the log-likelihoods (or hinge losses) across all actions in the sequence (Chen and Manning, 2014).

#### <span id="page-292-1"></span>Global objectives

The objective in Equation 11.27 is **locally-normalized**: it is the product of normalized probabilities over individual actions. A similar characterization could be made of non-probabilistic algorithms in which hinge-loss objectives are summed over individual actions. In either case, training on individual actions can be sub-optimal with respect to global performance, due to the **label bias problem** (Lafferty et al., 2001; Andor et al., 2016).

As a stylized example, suppose that a given configuration appears 100 times in the training data, with action  $a_1$  as the oracle action in 51 cases, and  $a_2$  as the oracle action in the other 49 cases. However, in cases where  $a_2$  is correct, choosing  $a_1$  results in a cascade of subsequent errors, while in cases where  $a_1$  is correct, choosing  $a_2$  results in only a single

<span id="page-293-1"></span>error. A classifier that is trained on a local objective function will learn to always choose  $a_1$ , but choosing  $a_2$  would minimize the overall number of errors.

This observation motivates a global objective, such as the globally-normalized conditional likelihood,

$$p(A^{(i)} \mid \boldsymbol{w}; \boldsymbol{\theta}) = \frac{\exp \sum_{t=1}^{|A^{(i)}|} \Psi(a_t^{(i)}, c_t^{(i)}, \boldsymbol{w})}{\sum_{A' \in \mathbb{A}(\boldsymbol{w})} \exp \sum_{t=1}^{|A'|} \Psi(a_t', c_t', \boldsymbol{w})},$$
[11.28]

where the denominator sums over the set of all possible action sequences,  $\mathbb{A}(w)$ .<sup>6</sup> In the conditional random field model for sequence labeling (§ 7.5.3), it was possible to compute this sum explicitly, using dynamic programming. In transition-based parsing, this is not possible. However, the sum can be approximated using beam search,

$$\sum_{A' \in \mathbb{A}(\boldsymbol{w})} \exp \sum_{t=1}^{|A'|} \Psi(a'_t, c'_t, \boldsymbol{w}) \approx \sum_{k=1}^{K} \exp \sum_{t=1}^{|A^{(k)}|} \Psi(a^{(k)}_t, c^{(k)}_t, \boldsymbol{w}),$$
[11.29]

where  $A^{(k)}$  is an action sequence on a beam of size *K*. This gives rise to the following loss function,

$$L(\boldsymbol{\theta}) = -\sum_{t=1}^{|A^{(i)}|} \Psi(a_t^{(i)}, c_t^{(i)}, \boldsymbol{w}) + \log \sum_{k=1}^{K} \exp \sum_{t=1}^{|A^{(k)}|} \Psi(a_t^{(k)}, c_t^{(k)}, \boldsymbol{w}).$$
[11.30]

The derivatives of this loss involve expectations with respect to a probability distribution over action sequences on the beam.

#### \*Early update and the incremental perceptron

When learning in the context of beam search, the goal is to learn a decision function so that the gold dependency parse is always reachable from at least one of the partial derivations on the beam. (The combination of a transition system (such as beam search) and a scoring function for actions is known as a **policy**.) To achieve this, we can make an **early update** as soon as the oracle action sequence "falls off" the beam, even before a complete analysis is available (Collins and Roark, 2004; Daumé III and Marcu, 2005). The loss can be based on the best-scoring hypothesis on the beam, or the sum of all hypotheses (Huang et al., 2012).

For example, consider the beam search in Figure 11.7. In the correct parse, *fish* is the head of dependency arcs to both of the other two words. In the arc-standard system,

<span id="page-293-0"></span><sup>&</sup>lt;sup>6</sup>Andor et al. (2016) prove that the set of globally-normalized conditional distributions is a strict superset of the set of locally-normalized conditional distributions, and that globally-normalized conditional models are therefore strictly more expressive.

<span id="page-294-1"></span>this can be achieved only by using SHIFT for the first two actions. At t = 3, the oracle action sequence has fallen off the beam. The parser should therefore stop, and update the parameters by the gradient <sup>∂</sup> ∂θ L(A (i) 1:3, {A (k) 1:3}; θ), where A (i) 1:3 is the first three actions of the oracle sequence, and {A (k) 1:3} is the beam.

This integration of incremental search and learning was first developed in the **incremental perceptron** [\(Collins and Roark, 2004\)](#page-521-8). This method updates the parameters with respect to a hinge loss, which compares the top-scoring hypothesis and the gold action sequence, up to the current point t. Several improvements to this basic protocol are possible:

- As noted earlier, the gold dependency parse can be derived by multiple action sequences. Rather than checking for the presence of a single oracle action sequence on the beam, we can check if the gold dependency parse is *reachable* from the current beam, using a **dynamic oracle** [\(Goldberg and Nivre, 2012\)](#page-531-10).
- By maximizing the score of the gold action sequence, we are training a decision function to find the correct action given the gold context. But in reality, the parser will make errors, and the parser is not trained to find the best action given a context that may not itself be optimal. This issue is addressed by various generalizations of incremental perceptron, known as **learning to search** [\(Daume III et al., 2009\)](#page-523-5). Some ´ of these methods are discussed in [chapter 15.](#page-368-0)

# <span id="page-294-0"></span>**11.4 Applications**

Dependency parsing is used in many real-world applications: any time you want to know about pairs of words which might not be adjacent, you can use dependency arcs instead of regular expression search patterns. For example, you may want to match strings like *delicious pastries*, *delicious French pastries*, and *the pastries are delicious*.

It is possible to search the Google n-grams corpus by dependency edges, finding the trend in how often a dependency edge appears over time. For example, we might be interested in knowing when people started talking about *writing code*, but we also want *write some code*, *write good code*, *write all the code*, etc. The result of a search on the dependency edge *write* → *code* is shown in [Figure 11.8.](#page-295-0) This capability has been applied to research in digital humanities, such as the analysis of gender in Shakespeare [Muralidharan and](#page-551-7) [Hearst](#page-551-7) [\(2013\)](#page-551-7).

A classic application of dependency parsing is **relation extraction**, which is described

<span id="page-295-0"></span>![](_page_295_Figure_1.jpeg)

Figure 11.8: Google n-grams results for the bigram *write code* and the dependency arc *write* => *code* (and their morphological variants)

in [chapter 17.](#page-420-0) The goal of relation extraction is to identify entity pairs, such as

(MELVILLE, MOBY-DICK) (TOLSTOY, WAR AND PEACE) (MARQUEZ´ , 100 YEARS OF SOLITUDE) (SHAKESPEARE, A MIDSUMMER NIGHT'S DREAM),

which stand in some relation to each other (in this case, the relation is authorship). Such entity pairs are often referenced via consistent chains of dependency relations. Therefore, dependency paths are often a useful feature in supervised systems which learn to detect new instances of a relation, based on labeled examples of other instances of the same relation type [\(Culotta and Sorensen, 2004;](#page-523-6) [Fundel et al., 2007;](#page-529-8) [Mintz et al., 2009\)](#page-549-3).

[Cui et al.](#page-523-7) [\(2005\)](#page-523-7) show how dependency parsing can improve automated question answering. Suppose you receive the following query:

(11.1) What percentage of the nation's cheese does Wisconsin produce?

The corpus contains this sentence:

(11.2) In Wisconsin, where farmers produce 28% of the nation's cheese, . . .

The location of *Wisconsin* in the surface form of this string makes it a poor match for the query. However, in the dependency graph, there is an edge from *produce* to *Wisconsin* in both the question and the potential answer, raising the likelihood that this span of text is relevant to the question.

A final example comes from sentiment analysis. As discussed in [chapter 4,](#page-86-0) the polarity of a sentence can be reversed by negation, e.g.

#### <span id="page-296-0"></span>11.4. APPLICATIONS

(11.3) There is no reason at all to believe the polluters will suddenly become reasonable.

By tracking the sentiment polarity through the dependency parse, we can better identify the overall polarity of the sentence, determining when key sentiment words are reversed (Wilson et al., 2005; Nakagawa et al., 2010).

# Additional resources

More details on dependency grammar and parsing algorithms can be found in the manuscript by Kübler et al. (2009). For a comprehensive but whimsical overview of graph-based dependency parsing algorithms, see Eisner (1997). Jurafsky and Martin (2019) describe an **agenda-based** version of beam search, in which the beam contains hypotheses of varying lengths. New hypotheses are added to the beam only if their score is better than the worst item currently on the beam. Another search algorithm for transition-based parsing is **easy-first**, which abandons the left-to-right traversal order, and adds the highest-scoring edges first, regardless of where they appear (Goldberg and Elhadad, 2010). Goldberg et al. (2013) note that although transition-based methods can be implemented in linear time in the length of the input, naïve implementations of beam search will require quadratic time, due to the cost of copying each hypothesis when it is expanded on the beam. This issue can be addressed by using a more efficient data structure for the stack.

# Exercises

- 1. The dependency structure  $1 \leftarrow 2 \rightarrow 3$ , with 2 as the root, can be obtained from more than one set of actions in arc-standard parsing. List both sets of actions that can obtain this parse. Don't forget about the edge ROOT  $\rightarrow 2$ .
- 2. This problem develops the relationship between dependency parsing and lexicalized context-free parsing. Suppose you have a set of unlabeled arc scores  $\{\psi(i \rightarrow j)\}_{i,j=1}^{M} \cup \{\psi(\text{ROOT} \rightarrow j)\}_{j=1}^{M}$ .
  - a) Assuming each word type occurs no more than once in the input  $((i \neq j) \Rightarrow (w_i \neq w_j))$ , how would you construct a weighted lexicalized context-free grammar so that the score of *any* projective dependency tree is equal to the score of some equivalent derivation in the lexicalized context-free grammar?
  - b) Verify that your method works for the example *They fish*.
  - c) Does your method require the restriction that each word type occur no more than once in the input? If so, why?
  - d) \*If your method required that each word type occur only once in the input, show how to generalize it.

- <span id="page-297-0"></span>3. In arc-factored dependency parsing of an input of length M, the score of a parse is the sum of M scores, one for each arc. In second order dependency parsing, the total score is the sum over many more terms. How many terms are the score of the parse for Figure 11.2, using a second-order dependency parser with grandparent and sibling features? Assume that a child of ROOT has no grandparent score, and that a node with no siblings has no sibling scores.
- a) In the worst case, how many terms can be involved in the score of an input of length *M*, assuming second-order dependency parsing? Describe the structure of the worst-case parse. As in the previous problem, assume that there is only one child of ROOT, and that it does not have any grandparent scores.
  - b) What about third-order dependency parsing?
- 5. Provide the UD-style unlabeled dependency parse for the sentence *Xi-Lan eats shoots and leaves,* assuming *shoots* is a noun and *leaves* is a verb. Provide arc-standard and arc-eager derivations for this dependency parse.
- Compute an upper bound on the number of successful derivations in arc-standard shift-reduce parsing for unlabeled dependencies, as a function of the length of the input, *M*. Hint: a lower bound is the number of projective decision trees, 1/(M+1) (<sup>3M-2</sup>)/<sub>M-1</sub> (Zhang, 2017), where (<sup>a</sup><sub>b</sub>) = a!/(a-b)!b!.
- 7. The **label bias problem** arises when a decision is locally correct, yet leads to a cascade of errors in some situations (§ 11.3.3). Design a scenario in which this occurs. Specifically:
  - Assume an arc-standard dependency parser, whose action classifier considers only the words at the top of the stack and at the front of the input buffer.
  - Design two examples, which both involve a decision with identical features.
    - In one example, shift is the correct decision; in the other example, arc-left or arc-right is the correct decision.
    - In one of the two examples, a mistake should lead to at least two attachment errors.
    - In the other example, a mistake should lead only to a single attachment error.

For the following exercises, run a dependency parser, such as Stanford's CoreNLP parser, on a large corpus of text (at least 10<sup>5</sup> tokens), such as nltk.corpus.webtext.

8. The dependency relation NMOD:POSS indicates possession. Compute the top ten words most frequently possessed by each of the following pronouns: *his, her, our, my, your,* and *their* (inspired by Muralidharan and Hearst, 2013).

#### <span id="page-298-0"></span>11.4. APPLICATIONS 281

9. Count all pairs of words grouped by the CONJ relation. Select all pairs of words (i, j) for which i and j each participate in CONJ relations at least five times. Compute and sort by the **pointwise mutual information**, which is defined in § [14.3](#page-346-1) as,

$$PMI(i,j) = \log \frac{p(i,j)}{p(i)p(j)}.$$
[11.31]

Here, p(i) is the fraction of CONJ relations containing word i (in either position), and p(i, j) is the fraction of such relations linking i and j (in any order).

10. In § [4.2,](#page-90-0) we encountered lexical semantic relationships such as **synonymy** (same meaning), **antonymy** (opposite meaning), and **hypernymy** (i is a special case of j). Another relevant relation is **co-hypernymy**, which means that i and j share a hypernym. Of the top 20 pairs identified by PMI in the previous problem, how many participate in synsets that are linked by one of these four relations? Use WORDNET to check for these relations, and count a pair of words if any of their synsets are linked.

# **Part III**

# <span id="page-300-0"></span>**Meaning**

# <span id="page-302-1"></span><span id="page-302-0"></span>**Chapter 12 Logical semantics**

The previous few chapters have focused on building systems that reconstruct the **syntax** of natural language — its structural organization — through tagging and parsing. But some of the most exciting and promising potential applications of language technology involve going beyond syntax to **semantics** — the underlying meaning of the text:

- Answering questions, such as *where is the nearest coffeeshop?* or *what is the middle name of the mother of the 44th President of the United States?*.
- Building a robot that can follow natural language instructions to execute tasks.
- Translating a sentence from one language into another, while preserving the underlying meaning.
- Fact-checking an article by searching the web for contradictory evidence.
- Logic-checking an argument by identifying contradictions, ambiguity, and unsupported assertions.

Semantic analysis involves converting natural language into a **meaning representation**. To be useful, a meaning representation must meet several criteria:

- **c1**: it should be unambiguous: unlike natural language, there should be exactly one meaning per statement;
- **c2**: it should provide a way to link language to external knowledge, observations, and actions;
- **c3**: it should support computational **inference**, so that meanings can be combined to derive additional knowledge;
- **c4**: it should be expressive enough to cover the full range of things that people talk about in natural language.

<span id="page-303-1"></span>Much more than this can be said about the question of how best to represent knowledge for computation (e.g., Sowa, 2000), but this chapter will focus on these four criteria.

### <span id="page-303-0"></span>12.1 Meaning and denotation

The first criterion for a meaning representation is that statements in the representation should be unambiguous — they should have only one possible interpretation. Natural language does not have this property: as we saw in chapter 10, sentences like *cats scratch people with claws* have multiple interpretations.

But what does it mean for a statement to be unambiguous? Programming languages provide a useful example: the output of a program is completely specified by the rules of the language and the properties of the environment in which the program is run. For example, the python code 5 + 3 will have the output 8, as will the codes (4\*4) - (3\*3) + 1 and ((8)). This output is known as the **denotation** of the program, and can be written as,

$$[5+3] = [(4*4) - (3*3) + 1] = [((8))] = 8.$$
[12.1]

The denotations of these arithmetic expressions are determined by the meaning of the **constants** (e.g., 5, 3) and the **relations** (e.g., +, \*, (, )). Now let's consider another snippet of python code, double (4). The denotation of this code could be, [double (4)] = 8, or it could be [double (4)] = 44 — it depends on the meaning of double. This meaning is defined in a **world model**  $\mathcal{M}$  as an infinite set of pairs. We write the denotation with respect to model  $\mathcal{M}$  as  $[\cdot]_{\mathcal{M}}$ , e.g.,  $[double]_{\mathcal{M}} = \{(0, 0), (1, 2), (2, 4), ...\}$ . The world model would also define the (infinite) list of constants, e.g.,  $\{0, 1, 2, ...\}$ . As long as the denotation of string  $\phi$  in model  $\mathcal{M}$  can be computed unambiguously, the language can be said to be unambiguous.

This approach to meaning is known as **model-theoretic semantics**, and it addresses not only criterion c1 (no ambiguity), but also c2 (connecting language to external knowledge, observations, and actions). For example, we can connect a representation of the meaning of a statement like *the capital of Georgia* with a world model that includes knowledge base of geographical facts, obtaining the denotation Atlanta. We might populate a world model by detecting and analyzing the objects in an image, and then use this world model to evaluate **propositions** like *a man is riding a moose*. Another desirable property of model-theoretic semantics is that when the facts change, the denotations change too: the meaning representation of *President of the USA* would have a different denotation in the model  $\mathcal{M}_{2014}$  as it would in  $\mathcal{M}_{2022}$ .

# <span id="page-304-3"></span><span id="page-304-0"></span>**12.2 Logical representations of meaning**

Criterion c3 requires that the meaning representation support inference — for example, automatically deducing new facts from known premises. While many representations have been proposed that meet these criteria, the most mature is the language of first-order logic.[1](#page-304-2)

#### <span id="page-304-1"></span>**12.2.1 Propositional logic**

The bare bones of logical meaning representation are Boolean operations on propositions:

- **Propositional symbols.** Greek symbols like φ and ψ will be used to represent **propositions**, which are statements that are either true or false. For example, φ may correspond to the proposition, *bagels are delicious*.
- **Boolean operators.** We can build up more complex propositional formulas from Boolean operators. These include:
  - Negation ¬φ, which is true if φ is false.
  - Conjunction, φ ∧ ψ, which is true if both φ and ψ are true.
  - Disjunction, φ ∨ ψ, which is true if at least one of φ and ψ is true
  - Implication, φ ⇒ ψ, which is true unless φ is true and ψ is false. Implication has identical truth conditions to ¬φ ∨ ψ.
  - Equivalence, φ ⇔ ψ, which is true if φ and ψ are both true or both false. Equivalence has identical truth conditions to (φ ⇒ ψ) ∧ (ψ ⇒ φ).

It is not strictly necessary to have all five Boolean operators: readers familiar with Boolean logic will know that it is possible to construct all other operators from either the NAND (not-and) or NOR (not-or) operators. Nonetheless, it is clearest to use all five operators. From the truth conditions for these operators, it is possible to define a number of "laws" for these Boolean operators, such as,

- *Commutativity*: φ ∧ ψ = ψ ∧ φ, φ ∨ ψ = ψ ∨ φ
- *Associativity*: φ ∧ (ψ ∧ χ) = (φ ∧ ψ) ∧ χ, φ ∨ (ψ ∨ χ) = (φ ∨ ψ) ∨ χ
- *Complementation*: φ ∧ ¬φ = ⊥, φ ∨ ¬φ = >, where > indicates a true proposition and ⊥ indicates a false proposition.

<span id="page-304-2"></span><sup>1</sup>Alternatives include the "variable-free" representation used in semantic parsing of geographical queries [\(Zelle and Mooney, 1996\)](#page-572-8) and robotic control [\(Ge and Mooney, 2005\)](#page-530-5), and dependency-based compositional semantics [\(Liang et al., 2013\)](#page-544-4).

<span id="page-305-1"></span>These laws can be combined to derive further equivalences, which can support logical inferences. For example, suppose  $\phi = The \ music \ is \ loud$  and  $\psi = Max \ can't \ sleep$ . Then if we are given,

$$\phi \Rightarrow \psi$$
 If the music is loud, Max can't sleep.  
 $\phi$  The music is loud.

we can derive  $\psi$  (*Max can't sleep*) by application of **modus ponens**, which is one of a set of **inference rules** that can be derived from more basic laws and used to manipulate propositional formulas. **Automated theorem provers** are capable of applying inference rules to a set of premises to derive desired propositions (Loveland, 2016).

#### <span id="page-305-0"></span>12.2.2 First-order logic

Propositional logic is so named because it treats propositions as its base units. However, the criterion c4 states that our meaning representation should be sufficiently expressive. Now consider the sentence pair,

(12.1) If anyone is making noise, then Max can't sleep. Abigail is making noise.

People are capable of making inferences from this sentence pair, but such inferences require formal tools that are beyond propositional logic. To understand the relationship between the statement *anyone is making noise* and the statement *Abigail is making noise*, our meaning representation requires the additional machinery of **first-order logic** (FOL).

In FOL, logical propositions can be constructed from relationships between entities. Specifically, FOL extends propositional logic with the following classes of terms:

- **Constants.** These are elements that name individual entities in the model, such as MAX and ABIGAIL. The denotation of each constant in a model  $\mathcal{M}$  is an element in the model, e.g., [MAX] = m and [ABIGAIL] = a.
- **Relations.** Relations can be thought of as sets of entities, or sets of tuples. For example, the relation CAN-SLEEP is defined as the set of entities who can sleep, and has the denotation [[CAN-SLEEP]] = {a,m,...}. To test the truth value of the proposition CAN-SLEEP(MAX), we ask whether [[MAX]] ∈ [[CAN-SLEEP]]. Logical relations that are defined over sets of entities are sometimes called *properties*.

Relations may also be ordered tuples of entities. For example BROTHER(MAX,ABIGAIL) expresses the proposition that MAX is the brother of ABIGAIL. The denotation of such relations is a set of tuples,  $[BROTHER] = \{(m, a), (x, y), ...\}$ . To test the truth value of the proposition BROTHER(MAX,ABIGAIL), we ask whether the tuple ([MAX], [ABIGAIL]) is in the denotation [BROTHER].

<span id="page-306-1"></span>Using constants and relations, it is possible to express statements like *Max can't sleep* and *Max is Abigail's brother*:

These statements can also be combined using Boolean operators, such as,

 $(BROTHER(MAX, ABIGAIL) \lor BROTHER(MAX, STEVE)) \Rightarrow \neg CAN-SLEEP(MAX).$ 

This fragment of first-order logic permits only statements about specific entities. To support inferences about statements like *If* anyone *is making noise, then Max can't sleep*, two more elements must be added to the meaning representation:

- **Variables.** Variables are mechanisms for referring to entities that are not locally specified. We can then write CAN-SLEEP(x) or BROTHER(x, ABIGAIL). In these cases, x is a **free variable**, meaning that we have not committed to any particular assignment.
- **Quantifiers.** Variables are bound by quantifiers. There are two quantifiers in first-order logic.<sup>2</sup>
  - The **existential quantifier**  $\exists$ , which indicates that there must be at least one entity to which the variable can bind. For example, the statement  $\exists x \text{MAKES-NOISE}(X)$  indicates that there is at least one entity for which MAKES-NOISE is true.
  - The **universal quantifier** ∀, which indicates that the variable must be able to bind to any entity in the model. For example, the statement,

$$MAKES-NOISE(ABIGAIL) \Rightarrow (\forall x \neg CAN-SLEEP(x))$$
[12.3]

asserts that if Abigail makes noise, no one can sleep.

The expressions  $\exists x \text{ and } \forall x \text{ make } x \text{ into a$ **bound variable**. A formula that contains no free variables is a**sentence**.

**Functions.** Functions map from entities to entities, e.g., [[CAPITAL-OF(GEORGIA)]] = [[ATLANTA]]. With functions, it is convenient to add an equality operator, supporting statements like,

$$\forall x \exists y \text{MOTHER-OF}(x) = \text{DAUGHTER-OF}(y).$$
 [12.4]

$$\forall P \forall x ((\text{GOOD-BOXER}(x) \Rightarrow P(x)) \Rightarrow P(\text{BUTCH})).$$
[12.2]

<span id="page-306-0"></span><sup>&</sup>lt;sup>2</sup>In first-order logic, it is possible to quantify only over entities. In **second-order logic**, it is possible to quantify over properties. This makes it possible to represent statements like *Butch has every property that a good boxer has* (example from Blackburn and Bos, 2005),

<span id="page-307-0"></span>Note that MOTHER-OF is a functional analogue of the relation MOTHER, so that MOTHER-OF(x) = y if MOTHER(x, y). Any logical formula that uses functions can be rewritten using only relations and quantification. For example,

can be rewritten as  $\exists x \text{MAKES-NOISE}(x) \land \text{MOTHER}(x, \text{ABIGAIL})$ .

An important property of quantifiers is that the order can matter. Unfortunately, natural language is rarely clear about this! The issue is demonstrated by examples like *everyone speaks a language*, which has the following interpretations:

$$\forall x \exists y \text{ SPEAKS}(x, y)$$
 [12.6]

$$\exists y \forall x \text{ SPEAKS}(x, y).$$
[12.7]

In the first case, *y* may refer to several different languages, while in the second case, there is a single *y* that is spoken by everyone.

#### **Truth-conditional semantics**

One way to look at the meaning of an FOL sentence  $\phi$  is as a set of **truth conditions**, or models under which  $\phi$  is satisfied. But how to determine whether a sentence is true or false in a given model? We will approach this inductively, starting with a predicate applied to a tuple of constants. The truth of such a sentence depends on whether the tuple of denotations of the constants is in the denotation of the predicate. For example, CAPITAL(GEORGIA,ATLANTA) is true in model  $\mathcal{M}$  iff,

$$(\llbracket GEORGIA \rrbracket_{\mathcal{M}}, \llbracket ATLANTA \rrbracket_{\mathcal{M}}) \in \llbracket CAPITAL \rrbracket_{\mathcal{M}}.$$
[12.8]

The Boolean operators  $\land, \lor, \ldots$  provide ways to construct more complicated sentences, and the truth of such statements can be assessed based on the truth tables associated with these operators. The statement  $\exists x\phi$  is true if there is some assignment of the variable x to an entity in the model such that  $\phi$  is true; the statement  $\forall x\phi$  is true if  $\phi$  is true under all possible assignments of x. More formally, we would say that  $\phi$  is **satisfied** under  $\mathcal{M}$ , written as  $\mathcal{M} \models \phi$ .

Truth conditional semantics allows us to define several other properties of sentences and pairs of sentences. Suppose that in every  $\mathcal{M}$  under which  $\phi$  is satisfied, another formula  $\psi$  is also satisfied; then  $\phi$  **entails**  $\psi$ , which is also written as  $\phi \models \psi$ . For example,

$$CAPITAL(GEORGIA, ATLANTA) \models \exists x CAPITAL(GEORGIA, x).$$
[12.9]

A statement that is satisfied under any model, such as  $\phi \lor \neg \phi$ , is **valid**, written  $\models (\phi \lor \neg \phi)$ . A statement that is not satisfied under any model, such as  $\phi \land \neg \phi$ , is **unsatisfiable**,

<span id="page-308-1"></span>or **inconsistent**. A **model checker** is a program that determines whether a sentence φ is satisfied in M. A **model builder** is a program that constructs a model in which φ is satisfied. The problems of checking for consistency and validity in first-order logic are **undecidable**, meaning that there is no algorithm that can automatically determine whether an FOL formula is valid or inconsistent.

#### **Inference in first-order logic**

Our original goal was to support inferences that combine general statements *If anyone is making noise, then Max can't sleep* with specific statements like *Abigail is making noise*. We can now represent such statements in first-order logic, but how are we to perform the inference that *Max can't sleep*? One approach is to use "generalized" versions of propositional inference rules like modus ponens, which can be applied to FOL formulas. By repeatedly applying such inference rules to a knowledge base of facts, it is possible to produce proofs of desired propositions. To find the right sequence of inferences to derive a desired theorem, classical artificial intelligence search algorithms like backward chaining can be applied. Such algorithms are implemented in interpreters for the prolog logic programming language [\(Pereira and Shieber, 2002\)](#page-554-6).

# <span id="page-308-0"></span>**12.3 Semantic parsing and the lambda calculus**

The previous section laid out a lot of formal machinery; the remainder of this chapter links these formalisms back to natural language. Given an English sentence like *Alex likes Brit*, how can we obtain the desired first-order logical representation, LIKES(ALEX,BRIT)? This is the task of **semantic parsing**. Just as a syntactic parser is a function from a natural language sentence to a syntactic structure such as a phrase structure tree, a semantic parser is a function from natural language to logical formulas.

As in syntactic analysis, semantic parsing is difficult because the space of inputs and outputs is very large, and their interaction is complex. Our best hope is that, like syntactic parsing, semantic parsing can somehow be decomposed into simpler sub-problems. This idea, usually attributed to the German philosopher Gottlob Frege, is called the **principle of compositionality**: the meaning of a complex expression is a function of the meanings of that expression's constituent parts. We will define these "constituent parts" as syntactic constituents: noun phrases and verb phrases. These constituents are combined using function application: if the syntactic parse contains the production x → y z, then the semantics of x, written x.sem, will be computed as a function of the semantics of the

<span id="page-309-5"></span><span id="page-309-3"></span>![](_page_309_Figure_1.jpeg)

Figure 12.1: The principle of compositionality requires that we identify meanings for the constituents *likes* and *likes Brit* that will make it possible to compute the meaning for the entire sentence.

constituents, y.sem and z.sem.[3](#page-309-1) [4](#page-309-2)

#### <span id="page-309-0"></span>**12.3.1 The lambda calculus**

Let's see how this works for a simple sentence like *Alex likes Brit*, whose syntactic structure is shown in [Figure 12.1.](#page-309-3) Our goal is the formula, LIKES(ALEX,BRIT), and it is clear that the meaning of the constituents *Alex* and *Brit* should be ALEX and BRIT. That leaves two more constituents: the verb *likes*, and the verb phrase *likes Brit*. The meanings of these units must be defined in a way that makes it possible to recover the desired meaning for the entire sentence by function application. If the meanings of *Alex* and *Brit* are constants, then the meanings of *likes* and *likes Brit* must be functional expressions, which can be applied to their siblings to produce the desired analyses.

Modeling these partial analyses requires extending the first-order logic meaning representation. We do this by adding **lambda expressions**, which are descriptions of anonymous functions,[5](#page-309-4) e.g.,

$$\lambda x. LIKES(x, BRIT).$$
 [12.10]

This functional expression is the meaning of the verb phrase *likes Brit*; it takes a single argument, and returns the result of substituting that argument for x in the expression

<span id="page-309-1"></span><sup>3</sup> § [9.3.2](#page-237-0) briefly discusses Combinatory Categorial Grammar (CCG) as an alternative to a phrase-structure analysis of syntax. CCG is argued to be particularly well-suited to semantic parsing [\(Hockenmaier and](#page-535-6) [Steedman, 2007\)](#page-535-6), and is used in much of the contemporary work on machine learning for semantic parsing, summarized in § [12.4.](#page-313-0)

<span id="page-309-2"></span><sup>4</sup>The approach of algorithmically building up meaning representations from a series of operations on the syntactic structure of a sentence is generally attributed to the philosopher Richard Montague, who published a series of influential papers on the topic in the early 1970s (e.g., [Montague, 1973\)](#page-550-6).

<span id="page-309-4"></span><sup>5</sup> Formally, all first-order logic formulas are lambda expressions; in addition, if φ is a lambda expression, then λx.φ is also a lambda expression. Readers who are familiar with functional programming will recognize lambda expressions from their use in programming languages such as Lisp and Python.

<span id="page-310-3"></span>LIKES(x, BRIT). We write this substitution as,

<span id="page-310-1"></span>
$$(\lambda x.LIKES(x, BRIT))@ALEX = LIKES(ALEX, BRIT),$$
 [12.11]

with the symbol "@" indicating function application. Function application in the lambda calculus is sometimes called β**-reduction** or β-conversion. The expression φ@ψ indicates a function application to be performed by β-reduction, and φ(ψ) indicates a function or predicate in the final logical form.

[Equation 12.11](#page-310-1) shows how to obtain the desired semantics for the sentence *Alex likes Brit*: by applying the lambda expression λx.LIKES(x, BRIT) to the logical constant ALEX. This rule of composition can be specified in a **syntactic-semantic grammar**, in which syntactic productions are paired with semantic operations. For the syntactic production S → NP VP, we have the semantic rule VP.sem@NP.sem.

The meaning of the transitive verb phrase *likes Brit* can also be obtained by function application on its syntactic constituents. For the syntactic production VP → V NP, we apply the semantic rule,

$$VP.sem = (V.sem) @NP.sem [12.12]$$

$$= (\lambda y.\lambda x.LIKES(x, y))@(BRIT)$$
[12.13]

$$=\lambda x. LIKES(x, BRIT).$$
 [12.14]

Thus, the meaning of the transitive verb *likes* is a lambda expression whose output is *another* lambda expression: it takes y as an argument to fill in one of the slots in the LIKES relation, and returns a lambda expression that is ready to take an argument to fill in the other slot.[6](#page-310-2)

[Table 12.1](#page-311-0) shows a minimal syntactic-semantic grammar fragment, G1. The complete **derivation** of *Alex likes Brit* in G<sup>1</sup> is shown in [Figure 12.2.](#page-311-1) In addition to the transitive verb *likes*, the grammar also includes the intransitive verb *sleeps*; it should be clear how to derive the meaning of sentences like *Alex sleeps*. For verbs that can be either transitive or intransitive, such as *eats*, we would have two terminal productions, one for each sense (terminal productions are also called the **lexical entries**). Indeed, most of the grammar is in the **lexicon** (the terminal productions), since these productions select the basic units of the semantic interpretation.

#### <span id="page-310-0"></span>**12.3.2 Quantification**

Things get more complicated when we move from sentences about named entities to sentences that involve more general noun phrases. Let's consider the example, *A dog sleeps*,

<span id="page-310-2"></span><sup>6</sup>This can be written in a few different ways. The notation λy, x.LIKES(x, y)is a somewhat informal way to indicate a lambda expression that takes two arguments; this would be acceptable in functional programming. Logicians (e.g., [Carpenter, 1997\)](#page-518-6) often prefer the more formal notation λy.λx.LIKES(x)(y), indicating that each lambda expression takes exactly one argument.

<span id="page-311-1"></span>![](_page_311_Figure_1.jpeg)

<span id="page-311-0"></span>Figure 12.2: Derivation of the semantic representation for *Alex likes Brit* in the grammar  $G_1$ .

 $\rightarrow$  NP VP VP.sem@NP.sem S  $VP \rightarrow V_t NP$ V<sub>t</sub>.sem@NP.sem  $VP \rightarrow V_i$  $V_i$ .sem  $V_{t}$  $\rightarrow$  likes  $\lambda y.\lambda x.LIKES(x,y)$  $\mathbf{V}_i$  $\rightarrow$  sleeps  $\lambda x. SLEEPS(x)$ NP  $\rightarrow Alex$ ALEX NP  $\rightarrow Brit$ BRIT

Table 12.1:  $G_1$ , a minimal syntactic-semantic context-free grammar

which has the meaning  $\exists x \text{DOG}(x) \land \text{SLEEPS}(x)$ . Clearly, the DOG relation will be introduced by the word *dog*, and the SLEEP relation will be introduced by the word *sleeps*. The existential quantifier  $\exists$  must be introduced by the lexical entry for the determiner a.<sup>7</sup> However, this seems problematic for the compositional approach taken in the grammar  $G_1$ : if the semantics of the noun phrase *a dog* is an existentially quantified expression, how can it be the argument to the semantics of the verb *sleeps*, which expects an entity? And where does the logical conjunction come from?

There are a few different approaches to handling these issues.<sup>8</sup> We will begin by reversing the semantic relationship between subject NPs and VPs, so that the production  $S \rightarrow NP VP$  has the semantics NP.sem@VP.sem: the meaning of the sentence is now the semantics of the noun phrase applied to the verb phrase. The implications of this change are best illustrated by exploring the derivation of the example, shown in Figure 12.3. Let's

<span id="page-311-2"></span><sup>&</sup>lt;sup>7</sup>Conversely, the sentence *Every dog sleeps* would involve a universal quantifier,  $\forall x DOG(x) \Rightarrow SLEEPS(x)$ . The definite article *the* requires more consideration, since *the dog* must refer to some dog which is uniquely identifiable, perhaps from contextual information external to the sentence. Carpenter (1997, pp. 96-100) summarizes recent approaches to handling definite descriptions.

<span id="page-311-3"></span><sup>&</sup>lt;sup>8</sup>Carpenter (1997) offers an alternative treatment based on combinatory categorial grammar.

<span id="page-312-3"></span><span id="page-312-0"></span>![](_page_312_Figure_1.jpeg)

![](_page_312_Figure_2.jpeg)

start with the indefinite article *a*, to which we assign the rather intimidating semantics,

$$\lambda P.\lambda Q. \exists x P(x) \land Q(x).$$
[12.15]

This is a lambda expression that takes two **relations** as arguments, P and Q. The relation P is scoped to the outer lambda expression, so it will be provided by the immediately adjacent noun, which in this case is DOG. Thus, the noun phrase *a dog* has the semantics,

$$NP.sem = DET.sem@NN.sem$$
[12.16]

$$= (\lambda P.\lambda Q.\exists x P(x) \land Q(x)) @(\text{DOG})$$
[12.17]

$$=\lambda Q.\exists x \text{DOG}(x) \land Q(x).$$
 [12.18]

This is a lambda expression that is expecting another relation, Q, which will be provided by the verb phrase, SLEEPS. This gives the desired analysis, ∃xDOG(x) ∧ SLEEPS(x). [9](#page-312-1)

If noun phrases like *a dog* are interpreted as lambda expressions, then proper nouns like *Alex* must be treated in the same way. This is achieved by **type-raising** from constants to lambda expressions, x ⇒ λP.P(x). After type-raising, the semantics of *Alex* is λP.P(ALEX) — a lambda expression that expects a relation to tell us something about ALEX. [10](#page-312-2) Again, make sure you see how the analysis in [Figure 12.3](#page-312-0) can be applied to the sentence *Alex sleeps*.

<span id="page-312-1"></span><sup>9</sup>When applying β-reduction to arguments that are themselves lambda expressions, be sure to use unique variable names to avoid confusion. For example, it is important to distinguish the x in the semantics for *a* from the x in the semantics for *likes*. Variable names are abstractions, and can always be changed — this is known as α**-conversion**. For example, λx.P(x) can be converted to λy.P(y), etc.

<span id="page-312-2"></span><sup>10</sup>Compositional semantic analysis is often supported by **type systems**, which make it possible to check whether a given function application is valid. The base types are entities e and truth values t. A property, such as DOG, is a function from entities to truth values, so its type is written he, ti. A transitive verb has type he,he, tii: after receiving the first entity (the direct object), it returns a function from entities to truth values, which will be applied to the subject of the sentence. The type-raising operation x ⇒ λP.P(x) corresponds to a change in type from e to hhe, ti, ti: it expects a function from entities to truth values, and returns a truth value.

<span id="page-313-2"></span>S : ∃xdog(x) ∧ likes(x, alex) VP : λx.likes(x, alex) NP : λP.P(alex) NNP : alex Alex V<sup>t</sup> : λP.λx.P(λy.likes(x, y)) likes NP : λQ.∃xdog(x) ∧ Q(x) NN : dog dog DT : λP.λQ.∃xP(x) ∧ Q(x) A

Figure 12.4: Derivation of the semantic representation for *A dog likes Alex*.

Direct objects are handled by applying the same type-raising operation to transitive verbs: the meaning of verbs such as *likes* is raised to,

<span id="page-313-1"></span>
$$\lambda P.\lambda x.P(\lambda y.LIKES(x,y))$$
 [12.19]

As a result, we can keep the verb phrase production VP.sem = V.sem@NP.sem, knowing that the direct object will provide the function P in [Equation 12.19.](#page-313-1) To see how this works, let's analyze the verb phrase *likes a dog*. After uniquely relabeling each lambda variable,

$$\begin{aligned} \text{VP.sem} = & \text{V.sem} \\ = & (\lambda P.\lambda x.P(\lambda y.\text{LIKES}(x,y)))@(\lambda Q.\exists z \text{DOG}(z) \land Q(z)) \\ = & \lambda x.(\lambda Q.\exists z \text{DOG}(z) \land Q(z))@(\lambda y.\text{LIKES}(x,y)) \\ = & \lambda x.\exists z \text{DOG}(z) \land (\lambda y.\text{LIKES}(x,y))@z \\ = & \lambda x.\exists z \text{DOG}(z) \land \text{LIKES}(x,z). \end{aligned}$$

These changes are summarized in the revised grammar G2, shown in [Table 12.2.](#page-314-1) [Fig](#page-313-2)[ure 12.4](#page-313-2) shows a derivation that involves a transitive verb, an indefinite noun phrase, and a proper noun.

# <span id="page-313-0"></span>**12.4 Learning semantic parsers**

As with syntactic parsing, any syntactic-semantic grammar with sufficient coverage risks producing many possible analyses for any given sentence. Machine learning is the dominant approach to selecting a single analysis. We will focus on algorithms that learn to score logical forms by attaching weights to features of their derivations [\(Zettlemoyer](#page-572-3) [and Collins, 2005\)](#page-572-3). Alternative approaches include transition-based parsing [\(Zelle and](#page-572-8) [Mooney, 1996;](#page-572-8) [Misra and Artzi, 2016\)](#page-549-6) and methods inspired by machine translation [\(Wong](#page-569-2) [and Mooney, 2006\)](#page-569-2). Methods also differ in the form of supervision used for learning, which can range from complete derivations to much more limited training signals. We will begin with the case of complete supervision, and then consider how learning is still possible even when seemingly key information is missing.

<span id="page-314-1"></span>

| S              | $\rightarrow$ NP VP   | NP.sem@VP.sem                                            |
|----------------|-----------------------|----------------------------------------------------------|
| VP             | $\rightarrow V_t NP$  | V <sub>t</sub> .sem@NP.sem                               |
| VP             | $ ightarrow { m V}_i$ | V <sub>i</sub> .sem                                      |
| NP             | ightarrow Det Nn      | Det.sem@Nn.sem                                           |
| NP             | ightarrow NNP         | $\lambda P.P(NNP.sem)$                                   |
| Det            | $\rightarrow a$       | $\lambda P.\lambda Q. \exists x P(x) \land Q(x)$         |
| Det            | $\rightarrow$ every   | $\lambda P.\lambda Q. \forall x (P(x) \Rightarrow Q(x))$ |
| $\mathrm{V}_t$ | $\rightarrow$ likes   | $\lambda P.\lambda x. P(\lambda y. \text{LIKES}(x, y))$  |
| $\mathrm{V}_i$ | $\rightarrow$ sleeps  | $\lambda x. \text{SLEEPS}(x)$                            |
| NN             | $\rightarrow dog$     | DOG                                                      |
| Nnp            | $\rightarrow Alex$    | ALEX                                                     |
| Nnp            | $\rightarrow Brit$    | BRIT                                                     |

Table 12.2:  $G_2$ , a syntactic-semantic context-free grammar fragment, which supports quantified noun phrases

**Datasets** Early work on semantic parsing focused on natural language expressions of geographical database queries, such as *What states border Texas*. The GeoQuery dataset of Zelle and Mooney (1996) was originally coded in prolog, but has subsequently been expanded and converted into the SQL database query language by Popescu et al. (2003) and into first-order logic with lambda calculus by Zettlemoyer and Collins (2005), providing logical forms like  $\lambda x.STATE(x) \land BORDERS(x, TEXAS)$ . Another early dataset consists of instructions for RoboCup robot soccer teams (Kate et al., 2005). More recent work has focused on broader domains, such as the Freebase database (Bollacker et al., 2008), for which queries have been annotated by Krishnamurthy and Mitchell (2012) and Cai and Yates (2013). Other recent datasets include child-directed speech (Kwiatkowski et al., 2012) and elementary school science exams (Krishnamurthy, 2016).

#### <span id="page-314-0"></span>12.4.1 Learning from derivations

Let  $w^{(i)}$  indicate a sequence of text, and let  $y^{(i)}$  indicate the desired logical form. For example:

$$m{w}^{(i)}=$$
Alex eats shoots and leaves $m{y}^{(i)}=$ EATS(ALEX,SHOOTS)  $\wedge$  EATS(ALEX,LEAVES)

In the standard supervised learning paradigm that was introduced in § 2.3, we first define a feature function, f(w, y), and then learn weights on these features, so that  $y^{(i)} = \operatorname{argmax}_{y} \theta \cdot f(w, y)$ . The weight vector  $\theta$  is learned by comparing the features of the true label  $f(w^{(i)}, y^{(i)})$  against either the features of the predicted label  $f(w^{(i)}, \hat{y})$  (perceptron,

 $S : EATS(ALEX, SHOOTS) \land EATS(ALEX, LEAVES_n)$ 

<span id="page-315-1"></span><span id="page-315-0"></span>![](_page_315_Figure_2.jpeg)

Figure 12.5: Derivation for gold semantic analysis of Alex eats shoots and leaves

support vector machine) or the expected feature vector  $E_{y|w}[f(w^{(i)}, y)]$  (logistic regression).

While this basic framework seems similar to discriminative syntactic parsing, there is a crucial difference. In (context-free) syntactic parsing, the annotation  $y^{(i)}$  contains all of the syntactic productions; indeed, the task of identifying the correct set of productions is identical to the task of identifying the syntactic structure. In semantic parsing, this is not the case: the logical form EATS(ALEX,SHOOTS)  $\land$  EATS(ALEX,LEAVES) does not reveal the syntactic-semantic productions that were used to obtain it. Indeed, there may be **spurious ambiguity**, so that a single logical form can be reached by multiple derivations. (We previously encountered spurious ambiguity in transition-based dependency parsing, § 11.3.2.)

These ideas can be formalized by introducing an additional variable z, representing the derivation of the logical form y from the text w. Assume that the feature function decomposes across the productions in the derivation,  $f(w, z, y) = \sum_{t=1}^{T} f(w, z_t, y)$ , where  $z_t$  indicates a single syntactic-semantic production. For example, we might have a feature for the production  $S \rightarrow NP VP : NP.sem@VP.sem$ , as well as for terminal productions like  $NNP \rightarrow Alex : ALEX$ . Under this decomposition, it is possible to compute scores for each semantically-annotated subtree in the analysis of w, so that bottom-up parsing algorithms like CKY (§ 10.1) can be applied to find the best-scoring semantic analysis.

Figure 12.5 shows a derivation of the correct semantic analysis of the sentence *Alex eats shoots and leaves*, in a simplified grammar in which the plural noun phrases *shoots* and *leaves* are interpreted as logical constants SHOOTS and LEAVES<sub>n</sub>. Figure 12.6 shows a derivation of an incorrect analysis. Assuming one feature per production, the perceptron update is shown in Table 12.3. From this update, the parser would learn to prefer the noun interpretation of *leaves* over the verb interpretation. It would also learn to prefer noun phrase coordination over verb phrase coordination.

While the update is explained in terms of the perceptron, it would be easy to replace the perceptron with a conditional random field. In this case, the online updates would be

<span id="page-316-4"></span><span id="page-316-1"></span>

| NP : $\lambda P. P(ALEX)$ |                                                             | $\mathrm{VP}:\lambda x.\mathrm{EA}$ | $\operatorname{TS}(x, \operatorname{SHOOTS}) \wedge \operatorname{LEAVES}_{v}(x)$ |                               |
|---------------------------|-------------------------------------------------------------|-------------------------------------|-----------------------------------------------------------------------------------|-------------------------------|
|                           | _                                                           |                                     |                                                                                   |                               |
| Alex                      | $VP: \lambda x. EATS($                                      | x, shoots)                          | $\mathrm{CC}: \lambda P.\lambda Q.\lambda x. P(x) \wedge Q(x)$                    | $VP: \lambda x.LEAVES_v(x)$   |
|                           |                                                             |                                     |                                                                                   |                               |
|                           | $V_t: \lambda P.\lambda x. P(\lambda y. \text{EATS}(x, y))$ | NP : $\lambda P.P(\text{SHOOTS})$   | and                                                                               | $V_i: \lambda x. LEAVES_v(x)$ |
|                           |                                                             |                                     |                                                                                   |                               |
|                           | eats                                                        | shoots                              |                                                                                   | leaves                        |

 $S : EATS(ALEX, SHOOTS) \land LEAVES_v(ALEX)$ 

<span id="page-316-2"></span>Figure 12.6: Derivation for incorrect semantic analysis of Alex eats shoots and leaves

| $NP_1 \rightarrow NP_2 CC NP_3$       | $(Cc.sem@(NP_2.sem))@(NP_3.sem)$ | +1 |
|---------------------------------------|----------------------------------|----|
| $VP_1 \rightarrow VP_2 \ CC \ VP_3$   | $(Cc.sem@(VP_2.sem))@(VP_3.sem)$ | -1 |
| $NP \rightarrow leaves$               | LEAVES <sub>n</sub>              | +1 |
| $\mathrm{VP}  ightarrow \mathrm{V}_i$ | V <sub>i</sub> .sem              | -1 |
| $V_i \rightarrow leaves$              | $\lambda x. \texttt{LEAVES}_v$   | -1 |

Table 12.3: Perceptron update for analysis in Figure 12.5 (gold) and Figure 12.6 (predicted)

based on feature expectations, which can be computed using the inside-outside algorithm (§ 10.6).

#### <span id="page-316-0"></span>**12.4.2** Learning from logical forms

Complete derivations are expensive to annotate, and are rarely available.<sup>11</sup> One solution is to focus on learning from logical forms directly, while treating the derivations as **latent variables** (Zettlemoyer and Collins, 2005). In a conditional probabilistic model over logical forms y and derivations z, we have,

$$p(\boldsymbol{y}, \boldsymbol{z} \mid \boldsymbol{w}) = \frac{\exp(\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}, \boldsymbol{z}, \boldsymbol{y}))}{\sum_{\boldsymbol{y}', \boldsymbol{z}'} \exp(\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}, \boldsymbol{z}', \boldsymbol{y}'))},$$
[12.20]

which is the standard log-linear model, applied to the logical form y and the derivation z.

Since the derivation z unambiguously determines the logical form y, it may seem silly to model the joint probability over y and z. However, since z is unknown, it can be marginalized out,

$$p(\boldsymbol{y} \mid \boldsymbol{w}) = \sum_{\boldsymbol{z}} p(\boldsymbol{y}, \boldsymbol{z} \mid \boldsymbol{w}).$$
 [12.21]

<span id="page-316-3"></span><sup>&</sup>lt;sup>11</sup>An exception is the work of Ge and Mooney (2005), who annotate the meaning of each syntactic constituents for several hundred sentences.

<span id="page-317-2"></span>The semantic parser can then select the logical form with the maximum log marginal probability,

$$\log \sum_{\boldsymbol{z}} p(\boldsymbol{y}, \boldsymbol{z} \mid \boldsymbol{w}) = \log \sum_{\boldsymbol{z}} \frac{\exp(\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}, \boldsymbol{z}, \boldsymbol{y}))}{\sum \boldsymbol{y}', \boldsymbol{z}' \exp(\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}, \boldsymbol{z}', \boldsymbol{y}'))}$$
[12.22]

$$\propto \log \sum_{\boldsymbol{z}} \exp(\boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}, \boldsymbol{z}', \boldsymbol{y}'))$$
 [12.23]

$$\geq \max_{\boldsymbol{z}} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{w}, \boldsymbol{z}, \boldsymbol{y}).$$
[12.24]

It is impossible to push the log term inside the sum over z, so our usual linear scoring function does not apply. We can recover this scoring function only in approximation, by taking the max (rather than the sum) over derivations z, which provides a lower bound.

Learning can be performed by maximizing the log marginal likelihood,

$$\ell(\boldsymbol{\theta}) = \sum_{i=1}^{N} \log p(\boldsymbol{y}^{(i)} \mid \boldsymbol{w}^{(i)}; \boldsymbol{\theta})$$
[12.25]

<span id="page-317-1"></span><span id="page-317-0"></span>
$$=\sum_{i=1}^{N}\log\sum_{\boldsymbol{z}}p(\boldsymbol{y}^{(i)},\boldsymbol{z}^{(i)}\mid\boldsymbol{w}^{(i)};\boldsymbol{\theta}).$$
[12.26]

This log-likelihood is not **convex** in  $\theta$ , unlike the log-likelihood of a fully-observed conditional random field. This means that learning can give different results depending on the initialization.

The derivative of Equation 12.26 is,

$$\frac{\partial \ell_i}{\partial \boldsymbol{\theta}} = \sum_{\boldsymbol{z}} p(\boldsymbol{z} \mid \boldsymbol{y}, \boldsymbol{w}; \boldsymbol{\theta}) \boldsymbol{f}(\boldsymbol{w}, \boldsymbol{z}, \boldsymbol{y}) - \sum_{\boldsymbol{y}', \boldsymbol{z}'} p(\boldsymbol{y}', \boldsymbol{z}' \mid \boldsymbol{w}; \boldsymbol{\theta}) \boldsymbol{f}(\boldsymbol{w}, \boldsymbol{z}', \boldsymbol{y}')$$
[12.27]

$$=E_{\boldsymbol{z}|\boldsymbol{y},\boldsymbol{w}}\boldsymbol{f}(\boldsymbol{w},\boldsymbol{z},\boldsymbol{y}) - E_{\boldsymbol{y},\boldsymbol{z}|\boldsymbol{w}}\boldsymbol{f}(\boldsymbol{w},\boldsymbol{z},\boldsymbol{y})$$
[12.28]

Both expectations can be computed via bottom-up algorithms like inside-outside. Alternatively, we can again maximize rather than marginalize over derivations for an approximate solution. In either case, the first term of the gradient requires us to identify derivations z that are compatible with the logical form y. This can be done in a bottom-up dynamic programming algorithm, by having each cell in the table t[i, j, X] include the set of all possible logical forms for  $X \rightsquigarrow w_{i+1:j}$ . The resulting table may therefore be much larger than in syntactic parsing. This can be controlled by using pruning to eliminate intermediate analyses that are incompatible with the final logical form y (Zettlemoyer and Collins, 2005), or by using beam search and restricting the size of each cell to some fixed constant (Liang et al., 2013).

If we replace each expectation in Equation 12.28 with argmax and then apply stochastic gradient descent to learn the weights, we obtain the **latent variable perceptron**, a simple

<span id="page-318-2"></span><span id="page-318-1"></span>

| Al | gorith | m 16 | Latent | variab | ole | perceptron |
|----|--------|------|--------|--------|-----|------------|
|----|--------|------|--------|--------|-----|------------|

| 1: <b>p</b> | rocedure LatentVariablePerceptron( $m{w}^{(1:N)},m{y}^{(1:N)})$                                                                                                                                  |
|-------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 2:          | $oldsymbol{	heta} \leftarrow 0$                                                                                                                                                                  |
| 3:          | repeat                                                                                                                                                                                           |
| 4:          | Select an instance <i>i</i>                                                                                                                                                                      |
| 5:          | $oldsymbol{z}^{(i)} \gets \mathrm{argmax}_{oldsymbol{z}} oldsymbol{	heta} \cdot oldsymbol{f}(oldsymbol{w}^{(i)}, oldsymbol{z}, oldsymbol{y}^{(i)})$                                              |
| 6:          | $\hat{m{y}}, \hat{m{z}} \leftarrow \mathrm{argmax}_{m{y}',m{z}'} m{	heta} \cdot m{f}(m{w}^{(i)},m{z}',m{y}')$                                                                                    |
| 7:          | $oldsymbol{	heta} \leftarrow oldsymbol{	heta} + oldsymbol{f}(oldsymbol{w}^{(i)},oldsymbol{z}^{(i)},oldsymbol{y}^{(i)}) - oldsymbol{f}(oldsymbol{w}^{(i)},\hat{oldsymbol{z}},\hat{oldsymbol{y}})$ |
| 8:          | until tired                                                                                                                                                                                      |
| 9:          | return $	heta$                                                                                                                                                                                   |

and general algorithm for learning with missing data. The algorithm is shown in its most basic form in Algorithm 16, but the usual tricks such as averaging and margin loss can be applied (Yu and Joachims, 2009). Aside from semantic parsing, the latent variable perceptron has been used in tasks such as machine translation (Liang et al., 2006) and named entity recognition (Sun et al., 2009). In **latent conditional random fields**, we use the full expectations rather than maximizing over the hidden variable. This model has also been employed in a range of problems beyond semantic parsing, including parse reranking (Koo and Collins, 2005) and gesture recognition (Quattoni et al., 2007).

#### <span id="page-318-0"></span>**12.4.3** Learning from denotations

Logical forms are easier to obtain than complete derivations, but the annotation of logical forms still requires considerable expertise. However, it is relatively easy to obtain denotations for many natural language sentences. For example, in the geography domain, the denotation of a question would be its answer (Clarke et al., 2010; Liang et al., 2013):

Text :What states border Georgia? Logical form : $\lambda x.STATE(x) \land BORDER(x, GEORGIA)$ Denotation :{Alabama, Florida, North Carolina, South Carolina, Tennessee}

Similarly, in a robotic control setting, the denotation of a command would be an action or sequence of actions (Artzi and Zettlemoyer, 2013). In both cases, the idea is to reward the semantic parser for choosing an analysis whose denotation is correct: the right answer to the question, or the right action.

Learning from logical forms was made possible by summing or maxing over derivations. This idea can be carried one step further, summing or maxing over all logical forms with the correct denotation. Let  $v_i(y) \in \{0, 1\}$  be a **validation function**, which assigns a

<span id="page-319-0"></span>binary score indicating whether the denotation <sup>J</sup>y<sup>K</sup> for the text <sup>w</sup>(i) is correct. We can then learn by maximizing a conditional-likelihood objective,

$$\ell^{(i)}(\boldsymbol{\theta}) = \log \sum_{\boldsymbol{y}} v_i(\boldsymbol{y}) \times p(\boldsymbol{y} \mid \boldsymbol{w}; \boldsymbol{\theta})$$
[12.29]

$$= \log \sum_{\boldsymbol{y}} v_i(\boldsymbol{y}) \times \sum_{\boldsymbol{z}} p(\boldsymbol{y}, \boldsymbol{z} \mid \boldsymbol{w}; \boldsymbol{\theta}), \qquad [12.30]$$

which sums over all derivations z of all valid logical forms, {y : vi(y) = 1}. This corresponds to the log-probability that the semantic parser produces a logical form with a valid denotation.

Differentiating with respect to θ, we obtain,

$$\frac{\partial \ell^{(i)}}{\partial \boldsymbol{\theta}} = \sum_{\boldsymbol{y}, \boldsymbol{z}: v_i(\boldsymbol{y})=1} p(\boldsymbol{y}, \boldsymbol{z} \mid \boldsymbol{w}) \boldsymbol{f}(\boldsymbol{w}, \boldsymbol{z}, \boldsymbol{y}) - \sum_{\boldsymbol{y}', \boldsymbol{z}'} p(\boldsymbol{y}', \boldsymbol{z}' \mid \boldsymbol{w}) \boldsymbol{f}(\boldsymbol{w}, \boldsymbol{z}', \boldsymbol{y}'), \quad [12.31]$$

which is the usual difference in feature expectations. The positive term computes the expected feature expectations conditioned on the denotation being valid, while the second term computes the expected feature expectations according to the current model, without regard to the ground truth. Large-margin learning formulations are also possible for this problem. For example, [Artzi and Zettlemoyer](#page-511-10) [\(2013\)](#page-511-10) generate a set of valid and invalid derivations, and then impose a constraint that all valid derivations should score higher than all invalid derivations. This constraint drives a perceptron-like learning rule.

# **Additional resources**

A key issue not considered here is how to handle **semantic underspecification**: cases in which there are multiple semantic interpretations for a single syntactic structure. Quantifier scope ambiguity is a classic example. [Blackburn and Bos](#page-514-10) [\(2005\)](#page-514-10) enumerate a number of approaches to this issue, and also provide links between natural language semantics and computational inference techniques. Much of the contemporary research on semantic parsing uses the framework of combinatory categorial grammar (CCG). [Carpenter](#page-518-6) [\(1997\)](#page-518-6) provides a comprehensive treatment of how CCG can support compositional semantic analysis. Another recent area of research is the semantics of multi-sentence texts. This can be handled with models of **dynamic semantics**, such as dynamic predicate logic [\(Groe](#page-532-5)[nendijk and Stokhof, 1991\)](#page-532-5).

Alternative readings on formal semantics include an "informal" reading from [Levy](#page-543-2) [and Manning](#page-543-2) [\(2009\)](#page-543-2), and a more involved introduction from [Briscoe](#page-516-6) [\(2011\)](#page-516-6). To learn more about ongoing research on data-driven semantic parsing, readers may consult the survey

#### <span id="page-320-3"></span>12.4. LEARNING SEMANTIC PARSERS

article by Liang and Potts (2015), tutorial slides and videos by Artzi and Zettlemoyer (2013),<sup>12</sup> and the source code by Yoav Artzi<sup>13</sup> and Percy Liang.<sup>14</sup>

# Exercises

- 1. The **modus ponens** inference rule states that if we know  $\phi \Rightarrow \psi$  and  $\phi$ , then  $\psi$  must be true. Justify this rule, using the definition of the  $\Rightarrow$  operator and some of the laws provided in § 12.2.1, plus one additional identity:  $\perp \lor \phi = \phi$ .
- 2. Convert the following examples into first-order logic, using the relations CAN-SLEEP, MAKES-NOISE, and BROTHER.
  - If Abigail makes noise, no one can sleep.
  - If Abigail makes noise, someone cannot sleep.
  - None of Abigail's brothers can sleep.
  - If one of Abigail's brothers makes noise, Abigail cannot sleep.
- 3. Extend the grammar fragment  $G_1$  to include the ditransitive verb *teaches* and the proper noun *Swahili*. Show how to derive the interpretation for the sentence *Alex teaches Brit Swahili*, which should be TEACHES(ALEX, BRIT, SWAHILI). The grammar need not be in Chomsky Normal Form. For the ditransitive verb, use NP<sub>1</sub> and NP<sub>2</sub> to indicate the two direct objects.
- 4. Derive the semantic interpretation for the sentence *Alex likes every dog*, using grammar fragment  $G_2$ .
- 5. Extend the grammar fragment  $G_2$  to handle adjectives, so that the meaning of *an* angry dog is  $\lambda P.\exists x DOG(x) \land ANGRY(x) \land P(x)$ . Specifically, you should supply the lexical entry for the adjective *angry*, and you should specify the syntactic-semantic productions NP  $\rightarrow$  DET NOM, NOM  $\rightarrow$  JJ NOM, and NOM  $\rightarrow$  NN.
- 6. Extend your answer to the previous question to cover copula constructions with predicative adjectives, such as *Alex is angry*. The interpretation should be ANGRY(ALEX). You should add a verb phrase production  $VP \rightarrow V_{cop}$  JJ, and a terminal production  $V_{cop} \rightarrow is$ . Show why your grammar extensions result in the correct interpretation.
- 7. In Figure 12.5 and Figure 12.6, we treat the plurals *shoots* and *leaves* as entities. Revise  $G_2$  so that the interpretation of *Alex eats leaves* is  $\forall x.(\text{LEAF}(x) \Rightarrow \text{EATS}(\text{ALEX}, x))$ , and show the resulting perceptron update.

<span id="page-320-2"></span><span id="page-320-1"></span><span id="page-320-0"></span><sup>&</sup>lt;sup>12</sup>Videos are currently available at http://yoavartzi.com/tutorial/ <sup>13</sup>http://yoavartzi.com/spf <sup>14</sup>https://github.com/percyliang/sempre

8. Statements like *every student eats a pizza* have two possible interpretations, depending on quantifier scope:

$$\forall x \exists y \text{PIZZA}(y) \land (\text{STUDENT}(x) \Rightarrow \text{EATS}(x, y))$$
[12.32]

$$\exists y \forall x \text{PIZZA}(y) \land (\text{STUDENT}(x) \Rightarrow \text{EATS}(x, y))$$
[12.33]

- a) Explain why these interpretations really are different.
- b) Which is generated by grammar G2? Note that you may have to manipulate the logical form to exactly align with the grammar.
- 9. \*Modify G<sup>2</sup> so that produces the second interpretation in the previous problem. **Hint**: one possible solution involves changing the semantics of the sentence production and one other production.
- 10. In the GeoQuery domain, give a natural language query that has multiple plausible semantic interpretations with the same denotation. List both interpretaions and the denotation.

**Hint:** There are many ways to do this, but one approach involves using toponyms (place names) that could plausibly map to several different entities in the model.

# <span id="page-322-3"></span><span id="page-322-0"></span>**Chapter 13**

# **Predicate-argument semantics**

This chapter considers more "lightweight" semantic representations, which discard some aspects of first-order logic, but focus on predicate-argument structures. Let's begin by thinking about the semantics of events, with a simple example:

<span id="page-322-1"></span>(13.1) Asha gives Boyang a book.

A first-order logical representation of this sentence is,

<span id="page-322-2"></span>
$$\exists x. \mathsf{BOOK}(x) \land \mathsf{GIVE}(\mathsf{ASHA}, \mathsf{BOYANG}, x)$$
[13.1]

In this representation, we define variable x for the book, and we link the strings *Asha* and *Boyang* to entities ASHA and BOYANG. Because the action of giving involves a giver, a recipient, and a gift, the predicate GIVE must take three arguments.

Now suppose we have additional information about the event:

(13.2) Yesterday, Asha reluctantly gave Boyang a book.

One possible to solution is to extend the predicate GIVE to take additional arguments,

∃x.BOOK(x) ∧ GIVE(ASHA, BOYANG, x, YESTERDAY, RELUCTANTLY) [13.2]

But this is clearly unsatisfactory: *yesterday* and *relunctantly* are optional arguments, and we would need a different version of the GIVE predicate for every possible combination of arguments. **Event semantics** solves this problem by **reifying** the event as an existentially quantified variable e,

∃e, x.GIVE-EVENT(e) ∧ GIVER(e, ASHA) ∧ GIFT(e, x) ∧ BOOK(e, x) ∧ RECIPIENT(e, BOYANG) ∧ TIME(e, YESTERDAY) ∧ MANNER(e, RELUCTANTLY)

<span id="page-323-2"></span>In this way, each argument of the event — the giver, the recipient, the gift — can be represented with a relation of its own, linking the argument to the event e. The expression GIVER(e, ASHA) says that ASHA plays the **role** of GIVER in the event. This reformulation handles the problem of optional information such as the time or manner of the event, which are called **adjuncts**. Unlike arguments, adjuncts are not a mandatory part of the relation, but under this representation, they can be expressed with additional logical relations that are conjoined to the semantic interpretation of the sentence. [1](#page-323-0)

The event semantic representation can be applied to nested clauses, e.g.,

(13.3) Chris sees Asha pay Boyang.

This is done by using the event variable as an argument:

$$\exists e_1 \exists e_2 \operatorname{SEE-EVENT}(e_1) \land \operatorname{SEER}(e_1, \operatorname{CHRIS}) \land \operatorname{SIGHT}(e_1, e_2) \\ \land \operatorname{PAY-EVENT}(e_2) \land \operatorname{PAYER}(e_2, \operatorname{ASHA}) \land \operatorname{PAYEE}(e_2, \operatorname{BOYANG})$$
[13.3]

As with first-order logic, the goal of event semantics is to provide a representation that generalizes over many surface forms. Consider the following paraphrases of [\(13.1\)](#page-322-1):

- (13.4) a. Asha gives a book to Boyang.
  - b. A book is given to Boyang by Asha.
  - c. A book is given by Asha to Boyang.
  - d. The gift of a book from Asha to Boyang . . .

<span id="page-323-1"></span>All have the same event semantic meaning as [Equation 13.1,](#page-322-2) but the ways in which the meaning can be expressed are diverse. The final example does not even include a verb: events are often introduced by verbs, but as shown by [\(13.4d\)](#page-323-1), the noun *gift* can introduce the same predicate, with the same accompanying arguments.

**Semantic role labeling** (SRL) is a relaxed form of semantic parsing, in which each semantic role is filled by a set of tokens from the text itself. This is sometimes called "shallow semantics" because, unlike model-theoretic semantic parsing, role fillers need not be symbolic expressions with denotations in some world model. A semantic role labeling system is required to identify all predicates, and then specify the spans of text that fill each role. To give a sense of the task, here is a more complicated example:

(13.5) Boyang wants Asha to give him a linguistics book.

<span id="page-323-0"></span><sup>1</sup>This representation is often called **Neo-Davidsonian event semantics**. The use of existentiallyquantified event variables was proposed by [Davidson](#page-523-8) [\(1967\)](#page-523-8) to handle the issue of optional adjuncts. In Neo-Davidsonian semantics, this treatment of adjuncts is extended to mandatory arguments as well (e.g., [Parsons, 1990\)](#page-554-7).

#### <span id="page-324-4"></span>13.1. SEMANTIC ROLES 307

In this example, there are two predicates, expressed by the verbs *want* and *give*. Thus, a semantic role labeler might return the following output:

- (PREDICATE : *wants*, WANTER : *Boyang*, DESIRE : *Asha to give him a linguistics book*)
- (PREDICATE : *give*, GIVER : *Asha*, RECIPIENT : *him*, GIFT : *a linguistics book*)

*Boyang* and *him* may refer to the same person, but the semantic role labeling is not required to resolve this reference. Other predicate-argument representations, such as **Abstract Meaning Representation (AMR)**, do require reference resolution. We will return to AMR in § [13.3,](#page-335-0) but first, let us further consider the definition of semantic roles.

# <span id="page-324-0"></span>**13.1 Semantic roles**

In event semantics, it is necessary to specify a number of additional logical relations to link arguments to events: GIVER, RECIPIENT, SEER, SIGHT, etc. Indeed, every predicate requires a set of logical relations to express its own arguments. In contrast, adjuncts such as TIME and MANNER are shared across many types of events. A natural question is whether it is possible to treat mandatory arguments more like adjuncts, by identifying a set of generic argument types that are shared across many event predicates. This can be further motivated by examples involving related verbs:

- <span id="page-324-2"></span>(13.6) a. Asha gave Boyang a book.
  - b. Asha loaned Boyang a book.
  - c. Asha taught Boyang a lesson.
  - d. Asha gave Boyang a lesson.

<span id="page-324-3"></span>The respective roles of Asha, Boyang, and the book are nearly identical across the first two examples. The third example is slightly different, but the fourth example shows that the roles of GIVER and TEACHER can be viewed as related.

One way to think about the relationship between roles such as GIVER and TEACHER is by enumerating the set of properties that an entity typically possesses when it fulfills these roles: givers and teachers are usually **animate** (they are alive and sentient) and **volitional** (they choose to enter into the action).[2](#page-324-1) In contrast, the thing that gets loaned or taught is usually not animate or volitional; furthermore, it is unchanged by the event.

Building on these ideas, **thematic roles** generalize across predicates by leveraging the shared semantic properties of typical role fillers [\(Fillmore, 1968\)](#page-527-3). For example, in examples [\(13.6a-](#page-324-2)[13.6d\)](#page-324-3), Asha plays a similar role in all four sentences, which we will call the

<span id="page-324-1"></span><sup>2</sup>There are always exceptions. For example, in the sentence *The C programming language has taught me a lot about perseverance*, the "teacher" is the *The C programming language*, which is presumably not animate or volitional.

<span id="page-325-3"></span><span id="page-325-1"></span>

| VerbNet<br>PropBank<br>FrameNet | Asha<br>AGENT<br>ARG0: giver<br>DONOR     | gave   | Boyang<br>RECIPIENT<br>ARG2: entity given to<br>RECIPIENT | a book<br>THEME<br>ARG1: thing given<br>THEME |
|---------------------------------|-------------------------------------------|--------|-----------------------------------------------------------|-----------------------------------------------|
| VerbNet<br>PropBank<br>FrameNet | Asha<br>AGENT<br>ARG0: teacher<br>TEACHER | taught | Boyang<br>RECIPIENT<br>ARG2: student<br>STUDENT           | algebra<br>TOPIC<br>ARG1: subject<br>SUBJECT  |

Figure 13.1: Example semantic annotations according to VerbNet, PropBank, and FrameNet

**agent**. This reflects several shared semantic properties: she is the one who is actively and intentionally performing the action, while Boyang is a more passive participant; the book and the lesson would play a different role, as non-animate participants in the event.

Example annotations from three well known systems are shown in [Figure 13.1.](#page-325-1) We will now discuss these systems in more detail.

#### <span id="page-325-0"></span>**13.1.1 VerbNet**

**VerbNet** [\(Kipper-Schuler, 2005\)](#page-540-4) is a lexicon of verbs, and it includes thirty "core" thematic roles played by arguments to these verbs. Here are some example roles, accompanied by their definitions from the VerbNet Guidelines.[3](#page-325-2)

- AGENT: "ACTOR in an event who initiates and carries out the event intentionally or consciously, and who exists independently of the event."
- PATIENT: "UNDERGOER in an event that experiences a change of state, location or condition, that is causally involved or directly affected by other participants, and exists independently of the event."
- RECIPIENT: "DESTINATION that is animate"
- THEME: "UNDERGOER that is central to an event or state that does not have control over the way the event occurs, is not structurally changed by the event, and/or is characterized as being in a certain position or condition throughout the state."
- TOPIC: "THEME characterized by information content transferred to another participant."

<span id="page-325-2"></span><sup>3</sup>[http://verbs.colorado.edu/verb-index/VerbNet\\_Guidelines.pdf](http://verbs.colorado.edu/verb-index/VerbNet_Guidelines.pdf)

#### <span id="page-326-4"></span>13.1. SEMANTIC ROLES 309

VerbNet roles are organized in a hierarchy, so that a TOPIC is a type of THEME, which in turn is a type of UNDERGOER, which is a type of PARTICIPANT, the top-level category.

In addition, VerbNet organizes verb senses into a class hierarchy, in which verb senses that have similar meanings are grouped together. Recall from § [4.2](#page-90-0) that multiple meanings of the same word are called **senses**, and that WordNet identifies senses for many English words. VerbNet builds on WordNet, so that verb classes are identified by the WordNet senses of the verbs that they contain. For example, the verb class give-13.1 includes the first WordNet sense of *loan* and the second WordNet sense of *lend*.

Each VerbNet class or subclass takes a set of thematic roles. For example, give-13.1 takes arguments with the thematic roles of AGENT, THEME, and RECIPIENT; [4](#page-326-1) the predicate TEACH takes arguments with the thematic roles AGENT, TOPIC, RECIPIENT, and SOURCE. [5](#page-326-2) So according to VerbNet, *Asha* and *Boyang* play the roles of AGENT and RECIP-IENT in the sentences,

- (13.7) a. Asha gave Boyang a book.
  - b. Asha taught Boyang algebra.

The *book* and *algebra* are both THEMES, but *algebra* is a subcategory of THEME — a TOPIC — because it consists of information content that is given to the receiver.

#### <span id="page-326-0"></span>**13.1.2 Proto-roles and PropBank**

Detailed thematic role inventories of the sort used in VerbNet are not universally accepted. For example, [Dowty](#page-525-6) [\(1991,](#page-525-6) pp. 547) notes that "Linguists have often found it hard to agree on, and to motivate, the location of the boundary between role types." He argues that a solid distinction can be identified between just two **proto-roles**:

- **Proto-Agent.** Characterized by volitional involvement in the event or state; sentience and/or perception; causing an event or change of state in another participant; movement; exists independently of the event.
- **Proto-Patient.** Undergoes change of state; causally affected by another participant; stationary relative to the movement of another participant; does not exist independently of the event.[6](#page-326-3)

<span id="page-326-1"></span><sup>4</sup><https://verbs.colorado.edu/verb-index/vn/give-13.1.php>

<span id="page-326-3"></span><span id="page-326-2"></span><sup>5</sup>[https://verbs.colorado.edu/verb-index/vn/transfer\\_mesg-37.1.1.php](https://verbs.colorado.edu/verb-index/vn/transfer_mesg-37.1.1.php)

<sup>6</sup>[Reisinger et al.](#page-557-8) [\(2015\)](#page-557-8) ask crowd workers to annotate these properties directly, finding that annotators tend to agree on the properties of each argument. They also find that in English, arguments having more proto-agent properties tend to appear in subject position, while arguments with more proto-patient properties appear in object position.

<span id="page-327-2"></span>In the examples in [Figure 13.1,](#page-325-1) Asha has most of the proto-agent properties: in giving the book to Boyang, she is acting volitionally (as opposed to *Boyang got a book from Asha*, in which it is not clear whether Asha gave up the book willingly); she is sentient; she causes a change of state in Boyang; she exists independently of the event. Boyang has some protoagent properties: he is sentient and exists independently of the event. But he also has some proto-patient properties: he is the one who is causally affected and who undergoes change of state. The book that Asha gives Boyang has even fewer of the proto-agent properties: it is not volitional or sentient, and it has no causal role. But it also lacks many of the proto-patient properties: it does not undergo change of state, exists independently of the event, and is not stationary.

The **Proposition Bank**, or PropBank [\(Palmer et al., 2005\)](#page-553-8), builds on this basic agentpatient distinction, as a middle ground between generic thematic roles and roles that are specific to each predicate. Each verb is linked to a list of numbered arguments, with ARG0 as the proto-agent and ARG1 as the proto-patient. Additional numbered arguments are verb-specific. For example, for the predicate TEACH, [7](#page-327-1) the arguments are:

- ARG0: the teacher
- ARG1: the subject
- ARG2: the student(s)

Verbs may have any number of arguments: for example, WANT and GET have five, while EAT has only ARG0 and ARG1. In addition to the semantic arguments found in the frame files, roughly a dozen general-purpose adjuncts may be used in combination with any verb. These are shown in [Table 13.1.](#page-328-0)

PropBank-style semantic role labeling is annotated over the entire Penn Treebank. This annotation includes the sense of each verbal predicate, as well as the argument spans.

#### <span id="page-327-0"></span>**13.1.3 FrameNet**

Semantic **frames** are descriptions of situations or events. Frames may be *evoked* by one of their **lexical units** (often a verb, but not always), and they include some number of **frame elements**, which are like roles [\(Fillmore, 1976\)](#page-528-5). For example, the act of teaching is a frame, and can be evoked by the verb *taught*; the associated frame elements include the teacher, the student(s), and the subject being taught. Frame semantics has played a significant role in the history of artificial intelligence, in the work of [Minsky](#page-549-7) [\(1974\)](#page-549-7) and [Schank and Abelson](#page-560-5) [\(1977\)](#page-560-5). In natural language processing, the theory of frame semantics has been implemented in **FrameNet** [\(Fillmore and Baker, 2009\)](#page-528-6), which consists of a lexicon

<span id="page-327-1"></span><sup>7</sup>http://verbs.colorado.edu/propbank/framesets-english-aliases/teach.html

Jacob Eisenstein. Draft of November 13, 2018.

<span id="page-328-0"></span>

| TMP | time                 | Boyang ate a bagel<br>[AM-TMP<br>yesterday].                    |
|-----|----------------------|-----------------------------------------------------------------|
| LOC | location             | Asha studies in<br>[AM-LOC<br>Stuttgart]                        |
| MOD | modal verb           | Asha<br>[AM-MOD<br>will]<br>study in Stuttgart                  |
| ADV | general purpose      | [AM-ADV<br>Luckily], Asha knew algebra.                         |
| MNR | manner               | Asha ate<br>[AM-MNR<br>aggressively].                           |
| DIS | discourse connective | [AM-DIS<br>However], Asha prefers algebra.                      |
| PRP | purpose              | [AM-PRP<br>to pass the bar].<br>Barry studied                   |
| DIR | direction            | [AM-DIR<br>into a bin].<br>Workers dumped burlap sacks          |
| NEG | negation             | [AM-NEG<br>not]<br>Asha does<br>speak Albanian.                 |
| EXT | extent               | [AM-EXT<br>4%].<br>Prices increased                             |
| CAU | cause                | Boyang returned the book<br>[AM-CAU<br>because it was overdue]. |

Table 13.1: PropBank adjuncts [\(Palmer et al., 2005\)](#page-553-8), sorted by frequency in the corpus

of roughly 1000 frames, and a corpus of more than 200,000 "exemplar sentences," in which the frames and their elements are annotated.[8](#page-328-1)

Rather than seeking to link semantic roles such as TEACHER and GIVER into thematic roles such as AGENT, FrameNet aggressively groups verbs into frames, and links semantically-related roles across frames. For example, the following two sentences would be annotated identically in FrameNet:

(13.8) a. Asha taught Boyang algebra.

b. Boyang learned algebra from Asha.

This is because *teach* and *learn* are both lexical units in the EDUCATION TEACHING frame. Furthermore, roles can be shared even when the frames are distinct, as in the following two examples:

- (13.9) a. Asha gave Boyang a book.
  - b. Boyang got a book from Asha.

The GIVING and GETTING frames both have RECIPIENT and THEME elements, so Boyang and the book would play the same role. Asha's role is different: she is the DONOR in the GIVING frame, and the SOURCE in the GETTING frame. FrameNet makes extensive use of multiple inheritance to share information across frames and frame elements: for example, the COMMERCE SELL and LENDING frames inherit from GIVING frame.

<span id="page-328-1"></span><sup>8</sup>Current details and data can be found at <https://framenet.icsi.berkeley.edu/>

# <span id="page-329-0"></span>13.2 Semantic role labeling

The task of semantic role labeling is to identify the parts of the sentence comprising the semantic roles. In English, this task is typically performed on the PropBank corpus, with the goal of producing outputs in the following form:

(13.10)  $[_{ARG0} Asha] [_{GIVE.01} gave] [_{ARG2} Boyang's mom] [_{ARG1} a book] [_{AM-TMP} yesterday].$ 

Note that a single sentence may have multiple verbs, and therefore a given word may be part of multiple role-fillers:

(13.11)  $[_{ARG0} Asha] [_{WANT.01} wanted]$ Asha wanted  $[_{ARG1} Boyang to give her the book].$  $[_{ARG0} Boyang] [_{GIVE.01} to give] [_{ARG2} her] [_{ARG1} the book].$ 

#### <span id="page-329-1"></span>13.2.1 Semantic role labeling as classification

PropBank is annotated on the Penn Treebank, and annotators used phrasal constituents (§ 9.2.2) to fill the roles. PropBank semantic role labeling can be viewed as the task of assigning to each phrase a label from the set  $\mathcal{R} = \{\emptyset, \text{PRED}, \text{ARG0}, \text{ARG1}, \text{ARG2}, \dots, \text{AM-LOC}, \text{AM-TMP}, .$  with respect to each predicate. If we treat semantic role labeling as a classification problem, we obtain the following functional form:

$$\hat{y}_{(i,j)} = \operatorname*{argmax}_{y} \psi(\boldsymbol{w}, y, i, j, \rho, \tau),$$
[13.4]

where,

- (i, j) indicates the span of a phrasal constituent  $(w_{i+1}, w_{i+2}, \ldots, w_j)$ ;
- *w* represents the sentence as a sequence of tokens;
- *ρ* is the index of the predicate verb in *w*;
- $\tau$  is the structure of the phrasal constituent parse of w.

Early work on semantic role labeling focused on discriminative feature-based models, where  $\psi(w, y, i, j, \rho, \tau) = \theta \cdot f(w, y, i, j, \rho, \tau)$ . Table 13.2 shows the features used in a seminal paper on FrameNet semantic role labeling (Gildea and Jurafsky, 2002). By 2005 there

Jacob Eisenstein. Draft of November 13, 2018.

<span id="page-329-3"></span>312

<span id="page-329-2"></span><sup>&</sup>lt;sup>9</sup>PropBank roles can also be filled by **split constituents**, which are discontinuous spans of text. This situation most frequently in reported speech, e.g. [ARG1 By addressing these problems], Mr. Maxwell said, [ARG1 the new funds have become extremely attractive.] (example adapted from Palmer et al., 2005). This issue is typically addressed by defining "continuation arguments", e.g. C-ARG1, which refers to the continuation of ARG1 after the split.

<span id="page-330-1"></span><span id="page-330-0"></span>

| Predicate lemma and<br>POS tag | The lemma of the predicate verb and its part-of-speech tag                                                                                                                                       |
|--------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Voice                          | Whether the predicate is in active or passive voice, as deter-<br>mined by a set of syntactic patterns for identifying passive<br>voice constructions                                            |
| Phrase type                    | The constituent phrase type for the proposed argument in the parse tree, e.g. NP, PP                                                                                                             |
| Headword and POS tag           | The head word of the proposed argument and its POS tag, identified using the Collins (1997) rules                                                                                                |
| Position                       | Whether the proposed argument comes before or after the predicate in the sentence                                                                                                                |
| Syntactic path                 | The set of steps on the parse tree from the proposed argument to the predicate (described in detail in the text)                                                                                 |
| Subcategorization              | The syntactic production from the first branching node above the predicate. For example, in Figure 13.2, the subcategorization feature around <i>taught</i> would be VP $\rightarrow$ VBD NP PP. |

Table 13.2: Features used in semantic role labeling by Gildea and Jurafsky (2002).

were several systems for PropBank semantic role labeling, and their approaches and feature sets are summarized by Carreras and Màrquez (2005). Typical features include: the phrase type, head word, part-of-speech, boundaries, and neighbors of the proposed argument  $w_{i+1:j}$ ; the word, lemma, part-of-speech, and voice of the verb  $w_{\rho}$  (active or passive), as well as features relating to its frameset; the distance and path between the verb and the proposed argument. In this way, semantic role labeling systems are high-level "consumers" in the NLP stack, using features produced from lower-level components such as part-of-speech taggers and parsers. More comprehensive feature sets are enumerated by Das et al. (2014) and Täckström et al. (2015).

A particularly powerful class of features relate to the **syntactic path** between the argument and the predicate. These features capture the sequence of moves required to get from the argument to the verb by traversing the phrasal constituent parse of the sentence. The idea of these features is to capture syntactic regularities in how various arguments are realized. Syntactic path features are best illustrated by example, using the parse tree in Figure 13.2:

• The path from *Asha* to the verb *taught* is NNP↑NP↑S↓VP↓VBD. The first part of the path, NNP↑NP↑S, means that we must travel up the parse tree from the NNP tag (proper noun) to the S (sentence) constituent. The second part of the path, S↓VP↓VBD, means that we reach the verb by producing a VP (verb phrase) from

<span id="page-331-1"></span><span id="page-331-0"></span>![](_page_331_Figure_1.jpeg)

Figure 13.2: Semantic role labeling on the phrase-structure parse tree for a sentence. The dashed line indicates the syntactic path from *Asha* to the predicate verb *taught*.

the S constituent, and then by producing a VBD (past tense verb). This feature is consistent with *Asha* being in subject position, since the path includes the sentence root S.

• The path from *the class* to *taught* is NP↑VP↓VBD. This is consistent with *the class* being in object position, since the path passes through the VP node that dominates the verb *taught*.

Because there are many possible path features, it can also be helpful to look at smaller parts: for example, the upward and downward parts can be treated as separate features; another feature might consider whether S appears anywhere in the path.

Rather than using the constituent parse, it is also possible to build features from the **dependency path** (see § [11.4\)](#page-294-0) between the head word of each argument and the verb [\(Prad](#page-556-5)[han et al., 2005\)](#page-556-5). Using the Universal Dependency part-of-speech tagset and dependency relations [\(Nivre et al., 2016\)](#page-552-3), the dependency path from *Asha* to *taught* is PROPN ← NSUBJ VERB, because *taught* is the head of a relation of type ← NSUBJ with *Asha*. Similarly, the dependency path from *class* to *taught* is NOUN ← DOBJ VERB, because *class* heads the noun phrase that is a direct object of *taught*. A more interesting example is *Asha wanted to teach the class*, where the path from *Asha* to *teach* is PROPN ← NSUBJ VERB → XCOMP VERB. The right-facing arrow in second relation indicates that *wanted* is the head of its XCOMP relation with *teach*.

#### <span id="page-332-2"></span><span id="page-332-0"></span>**13.2.2 Semantic role labeling as constrained optimization**

A potential problem with treating SRL as a classification problem is that there are a number of sentence-level **constraints**, which a classifier might violate.

- For a given verb, there can be only one argument of each type (ARG0, ARG1, etc.)
- Arguments cannot overlap. This problem arises when we are labeling the phrases in a constituent parse tree, as shown in [Figure 13.2:](#page-331-0) if we label the PP *about algebra* as an argument or adjunct, then its children *about* and *algebra* must be labeled as ∅. The same constraint also applies to the syntactic ancestors of this phrase.

These constraints introduce dependencies across labeling decisions. In structure prediction problems such as sequence labeling and parsing, such dependencies are usually handled by defining a scoring over the entire structure, y. Efficient inference requires that the global score decomposes into local parts: for example, in sequence labeling, the scoring function decomposes into scores of pairs of adjacent tags, permitting the application of the Viterbi algorithm for inference. But the constraints that arise in semantic role labeling are less amenable to local decomposition.[10](#page-332-1) We therefore consider **constrained optimization** as an alternative solution.

Let the set C(τ ) refer to all labelings that obey the constraints introduced by the parse τ . The semantic role labeling problem can be reformulated as a constrained optimization over y ∈ C(τ ),

$$\max_{\boldsymbol{y}} \quad \sum_{(i,j)\in\tau} \psi(\boldsymbol{w}, y_{i,j}, i, j, \rho, \tau)$$
  
s.t.  $\boldsymbol{y} \in \mathcal{C}(\tau).$  [13.5]

In this formulation, the objective (shown on the first line) is a separable function of each individual labeling decision, but the constraints (shown on the second line) apply to the overall labeling. The sum P (i,j)∈τ indicates that we are summing over all constituent spans in the parse τ . The expression s.t. in the second line means that we maximize the objective *subject to* the constraint y ∈ C(τ ).

A number of practical algorithms exist for restricted forms of constrained optimization. One such restricted form is **integer linear programming**, in which the objective and constraints are linear functions of integer variables. To formulate SRL as an integer linear program, we begin by rewriting the labels as a set of binary variables z = {zi,j,r} [\(Pun](#page-556-6)[yakanok et al., 2008\)](#page-556-6),

$$z_{i,j,r} = \begin{cases} 1, & y_{i,j} = r \\ 0, & \text{otherwise,} \end{cases}$$
[13.6]

<span id="page-332-1"></span><sup>10</sup>Dynamic programming solutions have been proposed by [Tromble and Eisner](#page-566-4) [\(2006\)](#page-566-4) and Tackstr ¨ [om et al.](#page-565-8) ¨ [\(2015\)](#page-565-8), but they involves creating a trellis structure whose size is exponential in the number of labels.

where  $r \in \mathcal{R}$  is a label in the set {ARG0, ARG1, ..., AM-LOC, ...,  $\emptyset$ }. Thus, the variables z are a binarized version of the semantic role labeling y.

The objective can then be formulated as a linear function of *z*.

$$\sum_{(i,j)\in\tau}\psi(\boldsymbol{w}, y_{i,j}, i, j, \rho, \tau) = \sum_{i,j,r}\psi(\boldsymbol{w}, r, i, j, \rho, \tau) \times z_{i,j,r},$$
[13.7]

which is the sum of the scores of all relations, as indicated by  $z_{i,j,r}$ .

**Constraints** Integer linear programming permits linear inequality constraints, of the general form  $Az \leq b$ , where the parameters **A** and **b** define the constraints. To make this more concrete, let's start with the constraint that each non-null role type can occur only once in a sentence. This constraint can be written,

<span id="page-333-1"></span><span id="page-333-0"></span>
$$\forall r \neq \emptyset, \quad \sum_{(i,j)\in\tau} z_{i,j,r} \le 1.$$
 [13.8]

Recall that  $z_{i,j,r} = 1$  iff the span (i, j) has label r; this constraint says that for each possible label  $r \neq \emptyset$ , there can be at most one (i, j) such that  $z_{i,j,r} = 1$ . Rewriting this constraint can be written in the form  $\mathbf{A}\mathbf{z} \leq \mathbf{b}$ , as you will find if you complete the exercises at the end of the chapter.

Now consider the constraint that labels cannot overlap. Let's define the convenience function o((i, j), (i', j')) = 1 iff (i, j) overlaps (i', j'), and zero otherwise. Thus, o will indicate if a constituent (i', j') is either an ancestor or descendant of (i, j). The constraint is that if two constituents overlap, only one can have a non-null label:

$$\forall (i,j) \in \tau, \quad \sum_{(i',j') \in \tau} \sum_{r \neq \emptyset} o((i,j), (i',j')) \times z_{i',j',r} \le 1,$$
[13.9]

where o((i, j), (i, j)) = 1.

In summary, the semantic role labeling problem can thus be rewritten as the following integer linear program,

$$\max_{z \in \{0,1\}^{|\tau|}} \sum_{(i,j) \in \tau} \sum_{r \in \mathcal{R}} z_{i,j,r} \psi_{i,j,r}$$
[13.10]

s.t. 
$$\forall r \neq \emptyset$$
,  $\sum_{(i,j)\in\tau} z_{i,j,r} \le 1.$  [13.11]

$$\forall (i,j) \in \tau, \quad \sum_{(i',j') \in \tau} \sum_{r \neq \emptyset} o((i,j), (i',j')) \times z_{i',j',r} \le 1.$$
 [13.12]

<span id="page-334-4"></span>**Learning with constraints** Learning can be performed in the context of constrained optimization using the usual perceptron or large-margin classification updates. Because constrained inference is generally more time-consuming, a key question is whether it is necessary to apply the constraints during learning. Chang et al. (2008) find that better performance can be obtained by learning *without* constraints, and then applying constraints only when using the trained model to predict semantic roles for unseen data.

**How important are the constraints?** Das et al. (2014) find that an unconstrained, classificationbased method performs nearly as well as constrained optimization for FrameNet parsing: while it commits many violations of the "no-overlap" constraint, the overall  $F_1$  score is less than one point worse than the score at the constrained optimum. Similar results were obtained for PropBank semantic role labeling by Punyakanok et al. (2008). He et al. (2017) find that constrained inference makes a bigger impact if the constraints are based on manually-labeled "gold" syntactic parses. This implies that errors from the syntactic parser may limit the effectiveness of the constraints. Punyakanok et al. (2008) hedge against parser error by including constituents from several different parsers; any constituent can be selected from any parse, and additional constraints ensure that overlapping constituents are not selected.

**Implementation** Integer linear programming solvers such as glpk,<sup>11</sup> cplex,<sup>12</sup> and Gurobi<sup>13</sup> allow inequality constraints to be expressed directly in the problem definition, rather than in the matrix form  $Az \leq b$ . The time complexity of integer linear programming is theoretically exponential in the number of variables |z|, but in practice these off-the-shelf solvers obtain good solutions efficiently. Using a standard desktop computer, Das et al. (2014) report that the cplex solver requires 43 seconds to perform inference on the FrameNet test set, which contains 4,458 predicates.

Recent work has shown that many constrained optimization problems in natural language processing can be solved in a highly parallelized fashion, using optimization techniques such as **dual decomposition**, which are capable of exploiting the underlying problem structure (Rush et al., 2010). Das et al. (2014) apply this technique to FrameNet semantic role labeling, obtaining an order-of-magnitude speedup over cplex.

#### <span id="page-334-0"></span>13.2.3 Neural semantic role labeling

Neural network approaches to SRL have tended to treat it as a sequence labeling task, using a labeling scheme such as the **BIO notation**, which we previously saw in named entity recognition ( $\S$  8.3). In this notation, the first token in a span of type ARG1 is labeled

<span id="page-334-1"></span><sup>&</sup>lt;sup>11</sup>https://www.gnu.org/software/glpk/

<span id="page-334-2"></span><sup>&</sup>lt;sup>12</sup>https://www-01.ibm.com/software/commerce/optimization/cplex-optimizer/

<span id="page-334-3"></span><sup>&</sup>lt;sup>13</sup>http://www.gurobi.com/

<span id="page-335-2"></span>B-ARG1; all remaining tokens in the span are *inside*, and are therefore labeled I-ARG1. Tokens outside any argument are labeled O. For example:

#### (13.12) Asha taught Boyang 's mom about algebra B-Arg0 Pred B-Arg2 I-Arg2 I-Arg2 B-Arg1 I-Arg1

Recurrent neural networks (§ 7.6) are a natural approach to this tagging task. For example, Zhou and Xu (2015) apply a deep bidirectional multilayer LSTM (see § 7.6) to PropBank semantic role labeling. In this model, each bidirectional LSTM serves as input for another, higher-level bidirectional LSTM, allowing complex non-linear transformations of the original input embeddings,  $\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_M]$ . The hidden state of the final LSTM is  $\mathbf{Z}^{(K)} = [\mathbf{z}_1^{(K)}, \mathbf{z}_2^{(K)}, \dots, \mathbf{z}_M^{(K)}]$ . The "emission" score for each tag  $Y_m = y$  is equal to the inner product  $\boldsymbol{\theta}_y \cdot \mathbf{z}_m^{(K)}$ , and there is also a transition score for each pair of adjacent tags. The complete model can be written,

$$\mathbf{Z}^{(1)} = \operatorname{BiLSTM}(\mathbf{X})$$
[13.13]

$$\mathbf{Z}^{(i)} = \operatorname{BiLSTM}(\mathbf{Z}^{(i-1)})$$
[13.14]

$$\hat{\boldsymbol{y}} = \underset{\boldsymbol{y}}{\operatorname{argmax}} \sum_{m-1}^{M} \boldsymbol{\Theta}^{(y)} \boldsymbol{z}_{m}^{(K)} + \psi_{y_{m-1}, y_{m}}.$$
[13.15]

Note that the final step maximizes over the entire labeling y, and includes a score for each tag transition  $\psi_{y_{m-1},y_m}$ . This combination of LSTM and pairwise potentials on tags is an example of an **LSTM-CRF**. The maximization over y is performed by the Viterbi algorithm.

This model strongly outperformed alternative approaches at the time, including constrained decoding and convolutional neural networks.<sup>14</sup> More recent work has combined recurrent neural network models with constrained decoding, using the *A*<sup>\*</sup> search algorithm to search over labelings that are feasible with respect to the constraints (He et al., 2017). This yields small improvements over the method of Zhou and Xu (2015). He et al. (2017) obtain larger improvements by creating an **ensemble** of SRL systems, each trained on an 80% subsample of the corpus. The average prediction across this ensemble is more robust than any individual model.

## <span id="page-335-0"></span>**13.3** Abstract Meaning Representation

Semantic role labeling transforms the task of semantic parsing to a labeling task. Consider the sentence,

<span id="page-335-1"></span><sup>&</sup>lt;sup>14</sup>The successful application of **convolutional neural networks** to semantic role labeling by Collobert and Weston (2008) was an influential early result in the current wave of neural networks in natural language processing.

<span id="page-336-2"></span><span id="page-336-0"></span>![](_page_336_Figure_1.jpeg)

Figure 13.3: Two views of the AMR representation for the sentence *The whale wants the captain to pursue him.*

<span id="page-336-1"></span>(13.13) The whale wants the captain to pursue him.

The PropBank semantic role labeling analysis is:

- (PREDICATE : *wants*, ARG0 : *the whale*, ARG1 : *the captain to pursue him*)
- (PREDICATE : *pursue*, ARG0 : *the captain*, ARG1 : *him*)

The **Abstract Meaning Representation (AMR)** unifies this analysis into a graph structure, in which each node is a **variable**, and each edge indicates a **concept** [\(Banarescu](#page-512-7) [et al., 2013\)](#page-512-7). This can be written in two ways, as shown in [Figure 13.3.](#page-336-0) On the left is the PENMAN notation [\(Matthiessen and Bateman, 1991\)](#page-547-9), in which each set of parentheses introduces a variable. Each variable is an **instance** of a concept, which is indicated with the slash notation: for example, w / want-01 indicates that the variable w is an instance of the concept want-01, which in turn refers to the PropBank frame for the first sense of the verb *want*; pursue-02 refers to the second sense of *pursue*. Relations are introduced with colons: for example, :ARG0 (c / captain) indicates a relation of type ARG0 with the newly-introduced variable c. Variables can be reused, so that when the variable h appears again as an argument to p, it is understood to refer to the same whale in both cases. This arrangement is indicated compactly in the graph structure on the right, with edges indicating concepts.

One way in which AMR differs from PropBank-style semantic role labeling is that it reifies each entity as a variable: for example, the *whale* in [\(13.13\)](#page-336-1) is reified in the variable h, which is reused as ARG0 in its relationship with w / want-01, and as ARG1 in its relationship with p / pursue-02. Reifying entities as variables also makes it possible to represent the substructure of noun phrases more explicitly. For example, *Asha borrowed the algebra book* would be represented as:

```
(b / borrow-01
   :ARG0 (p / person
            :name (n / name
                      :op1 "Asha"))
```

```
:ARG1 (b2 / book
         :topic (a / algebra)))
```

This indicates that the variable p is a person, whose name is the variable n; that name has one token, the string *Asha*. Similarly, the variable b2 is a book, and the topic of b2 is a variable a whose type is algebra. The relations name and topic are examples of "non-core roles", which are similar to adjunct modifiers in PropBank. However, AMR's inventory is more extensive, including more than 70 non-core roles, such as negation, time, manner, frequency, and location. Lists and sequences — such as the list of tokens in a name — are described using the roles op1, op2, etc.

Another feature of AMR is that a semantic predicate can be introduced by any syntactic element, as in the following examples from [Banarescu et al.](#page-512-7) [\(2013\)](#page-512-7):

(13.14) a. The boy destroyed the room.

- b. the destruction of the room by the boy . . .
- c. the boy's destruction of the room . . .

All these examples have the same semantics in AMR,

```
(d / destroy-01
   :ARG0 (b / boy)
   :ARG1 (r / room))
```

The noun *destruction* is linked to the verb *destroy*, which is captured by the PropBank frame destroy-01. This can happen with adjectives as well: in the phrase *the attractive spy*, the adjective *attractive* is linked to the PropBank frame attract-01:

```
(s / spy
   :ARG0-of (a / attract-01))
```

In this example, ARG0-of is an **inverse relation**, indicating that s is the ARG0 of the predicate a. Inverse relations make it possible for all AMR parses to have a single root concept.

While AMR goes farther than semantic role labeling, it does not link semanticallyrelated frames such as buy/sell (as FrameNet does). AMR also does not handle quantification (as first-order predicate calculus does), and it makes no attempt to handle noun number and verb tense (as PropBank does).

#### <span id="page-338-1"></span>13.3. ABSTRACT MEANING REPRESENTATION 321

#### <span id="page-338-0"></span>**13.3.1 AMR Parsing**

Abstract Meaning Representation is not a labeling of the original text — unlike PropBank semantic role labeling, and most of the other tagging and parsing tasks that we have encountered thus far. The AMR for a given sentence may include multiple concepts for single words in the sentence: as we have seen, the sentence *Asha likes algebra* contains both person and name concepts for the word *Asha*. Conversely, words in the sentence may not appear in the AMR: in *Boyang made a tour of campus*, the **light verb** *make* would not appear in the AMR, which would instead be rooted on the predicate tour. As a result, AMR is difficult to parse, and even evaluating AMR parsing involves considerable algorithmic complexity [\(Cai and Yates, 2013\)](#page-517-3).

A further complexity is that AMR labeled datasets do not explicitly show the **alignment** between the AMR annotation and the words in the sentence. For example, the link between the word *wants* and the concept want-01 is not annotated. To acquire training data for learning-based parsers, it is therefore necessary to first perform an alignment between the training sentences and their AMR parses. [Flanigan et al.](#page-528-7) [\(2014\)](#page-528-7) introduce a rule-based parser, which links text to concepts through a series of increasingly high-recall steps.

As with dependency parsing, AMR can be parsed by graph-based methods that explore the space of graph structures, or by incremental transition-based algorithms. One approach to graph-based AMR parsing is to first group adjacent tokens into local substructures, and then to search the space of graphs over these substructures [\(Flanigan et al.,](#page-528-7) [2014\)](#page-528-7). The identification of concept subgraphs can be formulated as a sequence labeling problem, and the subsequent graph search can be solved using integer linear programming (§ [13.2.2\)](#page-332-0). Various transition-based parsing algorithms have been proposed. [Wang](#page-568-5) [et al.](#page-568-5) [\(2015\)](#page-568-5) construct an AMR graph by incrementally modifying the *syntactic* dependency graph. At each step, the parser performs an action: for example, adding an AMR relation label to the current dependency edge, swapping the direction of a syntactic dependency edge, or cutting an edge and reattaching the orphaned subtree to a new parent.

# **Additional resources**

Practical semantic role labeling was first made possible by the PropBank annotations on the Penn Treebank [\(Palmer et al., 2005\)](#page-553-8). [Abend and Rappoport](#page-509-5) [\(2017\)](#page-509-5) survey several semantic representation schemes, including semantic role labeling and AMR. Other linguistic features of AMR are summarized in the original paper [\(Banarescu et al., 2013\)](#page-512-7) and the tutorial slides by [Schneider et al.](#page-560-6) [\(2015\)](#page-560-6). Recent shared tasks have undertaken semantic dependency parsing, in which the goal is to identify semantic relationships between pairs of words [\(Oepen et al., 2014\)](#page-552-8); see [Ivanova et al.](#page-536-5) [\(2012\)](#page-536-5) for an overview of connections between syntactic and semantic dependencies.

#### <span id="page-339-3"></span>**Exercises**

- <span id="page-339-0"></span>1. Write out an event semantic representation for the following sentences. You may make up your own predicates.
  - (13.15) *Abigail shares with Max.*
  - (13.16) *Abigail reluctantly shares a toy with Max.*
  - (13.17) *Abigail hates to share with Max.*
- <span id="page-339-1"></span>2. Find the PropBank framesets for*share* and *hate* at [http://verbs.colorado.edu/](http://verbs.colorado.edu/propbank/framesets-english-aliases/) [propbank/framesets-english-aliases/](http://verbs.colorado.edu/propbank/framesets-english-aliases/), and rewrite your answers from the previous question, using the thematic roles ARG0, ARG1, and ARG2.
- 3. Compute the syntactic path features for Abigail and Max in each of the example sentences [\(13.15\)](#page-339-0) and [\(13.17\)](#page-339-1) in Question 1, with respect to the verb *share*. If you're not sure about the parse, you can try an online parser such as [http://nlp.stanford.](http://nlp.stanford.edu:8080/parser/) [edu:8080/parser/](http://nlp.stanford.edu:8080/parser/).
- 4. Compute the dependency path features for Abigail and Max in each of the example sentences [\(13.15\)](#page-339-0) and [\(13.17\)](#page-339-1) in Question 1, with respect to the verb *share*. Again, if you're not sure about the parse, you can try an online parser such as [http://nlp.](http://nlp.stanford.edu:8080/parser/) [stanford.edu:8080/parser/](http://nlp.stanford.edu:8080/parser/). As a hint, the dependency relation between *share* and *Max* is OBL according to the Universal Dependency treebank.
- 5. PropBank semantic role labeling includes **reference arguments**, such as,
  - (13.18) [AM-LOC The bed] on [R-AM-LOC which] I slept broke.[15](#page-339-2)

The label R-AM-LOC indicates that the word *which* is a reference to *The bed*, which expresses the location of the event. Reference arguments must have referents: the tag R-AM-LOC can appear only when AM-LOC also appears in the sentence. Show how to express this as a linear constraint, specifically for the tag R-AM-LOC. Be sure to correctly handle the case in which neither AM-LOC nor R-AM-LOC appear in the sentence.

- 6. Explain how to express the constraints on semantic role labeling in [Equation 13.8](#page-333-0) and [Equation 13.9](#page-333-1) in the general form Az ≥ b.
- 7. Produce the AMR annotations for the following examples:
  - (13.19) a. The girl likes the boy.

<span id="page-339-2"></span><sup>15</sup>Example from 2013 NAACL tutorial slides by Shumin Wu

- <span id="page-340-0"></span>b. The girl was liked by the boy.
- c. Abigail likes Maxwell Aristotle.
- d. The spy likes the attractive boy.
- e. The girl doesn't like the boy.
- f. The girl likes her dog.

<span id="page-340-2"></span><span id="page-340-1"></span>For [\(13.19c\)](#page-340-0), recall that multi-token names are created using op1, op2, etc. You will need to consult [Banarescu et al.](#page-512-7) [\(2013\)](#page-512-7) for [\(13.19e\)](#page-340-1), and [Schneider et al.](#page-560-6) [\(2015\)](#page-560-6) for [\(13.19f\)](#page-340-2). You may assume that *her* refers to *the girl* in this example.

8. In this problem, you will build a FrameNet sense classifier for the verb *can*, which can evoke two frames: POSSIBILITY (can you order a salad with french fries?) and CAPABILITY(can you eat a salad with chopsticks?).

To build the dataset, access the FrameNet corpus in NLTK:

```
import nltk
nltk.download('framenet_v17')
from nltk.corpus import framenet as fn
```

Next, find instances in which the lexical unit can.v (the verb form of *can*) evokes a frame. Do this by iterating over fn.docs(), and then over sentences, and then

```
for doc in fn.docs():
    if 'sentence' in doc:
        for sent in doc['sentence']:
            for anno_set in sent['annotationSet']:
                if 'luName' in anno_set and anno_set['luName'] == 'can.v':
                    pass # your code here
```

Use the field frameName as a label, and build a set of features from the field text. Train a classifier to try to accurately predict the frameName, disregarding cases other than CAPABILITY and POSSIBILITY. Treat the first hundred instances as a training set, and the remaining instances as the test set. Can you do better than a classifier that simply selects the most common class?

- 9. \*Download the PropBank sample data, using NLTK ([http://www.nltk.org/](http://www.nltk.org/howto/propbank.html) [howto/propbank.html](http://www.nltk.org/howto/propbank.html)).
  - a) Use a deep learning toolkit such as PyTorch to train a BiLSTM sequence labeling model (§ [7.6\)](#page-184-0) to identify words or phrases that are predicates, e.g., *we*/<sup>O</sup> *took*/B-PRED *a*/I-PRED *walk*/I-PRED *together*/O. Your model should compute the tag score from the BiLSTM hidden state ψ(ym) = β<sup>y</sup> · hm.
  - b) Optionally, implement Viterbi to improve the predictions of the model in the previous section.

- c) Try to identify ARG0 and ARG1 for each predicate. You should again use the BiLSTM and BIO notation, but you may want to include the BiLSTM hidden state at the location of the predicate in your prediction model, e.g., ψ(ym) = β<sup>y</sup> ·[hm; hrˆ], where rˆ is the predicted location of the (first word of the) predicate.
- 10. Using an off-the-shelf PropBank SRL system,[16](#page-341-0) build a simplified question answering system in the style of [Shen and Lapata](#page-560-7) [\(2007\)](#page-560-7). Specifically, your system should do the following:
  - For each document in a collection, it should apply the semantic role labeler, and should store the output as a tuple.
  - For a question, your system should again apply the semantic role labeler. If any of the roles are filled by a *wh*-pronoun, you should mark that role as the expected answer phrase (EAP).
  - To answer the question, search for a stored tuple which matches the question as well as possible (same predicate, no incompatible semantic roles, and as many matching roles as possible). Align the EAP against its role filler in the stored tuple, and return this as the answer.

To evaluate your system, download a set of three news articles on the same topic, and write down five factoid questions that should be answerable from the articles. See if your system can answer these questions correctly. (If this problem is assigned to an entire class, you can build a large-scale test set and compare various approaches.)

<span id="page-341-0"></span><sup>16</sup>At the time of writing, the following systems are availabe: SENNA ([http://ronan.collobert.](http://ronan.collobert.com/senna/) [com/senna/](http://ronan.collobert.com/senna/)), Illinois Semantic Role Labeler ([https://cogcomp.cs.illinois.edu/page/](https://cogcomp.cs.illinois.edu/page/software_view/SRL) [software\\_view/SRL](https://cogcomp.cs.illinois.edu/page/software_view/SRL)), and mate-tools (<https://code.google.com/archive/p/mate-tools/>).

# <span id="page-342-6"></span><span id="page-342-0"></span>**Chapter 14**

# **Distributional and distributed semantics**

A recurring theme in natural language processing is the complexity of the mapping from words to meaning. In [chapter 4,](#page-86-0) we saw that a single word form, like *bank*, can have multiple meanings; conversely, a single meaning may be created by multiple surface forms, a lexical semantic relationship known as **synonymy**. Despite this complex mapping between words and meaning, natural language processing systems usually rely on words as the basic unit of analysis. This is especially true in semantics: the logical and frame semantic methods from the previous two chapters rely on hand-crafted lexicons that map from words to semantic predicates. But how can we analyze texts that contain words that we haven't seen before? This chapter describes methods that learn representations of word meaning by analyzing unlabeled data, vastly improving the generalizability of natural language processing systems. The theory that makes it possible to acquire meaningful representations from unlabeled data is the **distributional hypothesis**.

# <span id="page-342-1"></span>**14.1 The distributional hypothesis**

Here's a word you may not know: *tezg ¨uino* (the example is from [Lin, 1998\)](#page-544-7). If you do not know the meaning of *tezg ¨uino*, then you are in the same situation as a natural language processing system when it encounters a word that did not appear in its training data. Now suppose you see that *tezg ¨uino* is used in the following contexts:

- <span id="page-342-2"></span>(14.1) A bottle of is on the table.
- <span id="page-342-3"></span>(14.2) Everybody likes .
- <span id="page-342-4"></span>(14.3) Don't have before you drive.
- <span id="page-342-5"></span>(14.4) We make out of corn.

<span id="page-343-2"></span><span id="page-343-0"></span>

|            | (14.1) | (14.2) | (14.3) | (14.4) |  |
|------------|--------|--------|--------|--------|--|
| tezg ¨uino | 1      | 1      | 1      | 1      |  |
| loud       | 0      | 0      | 0      | 0      |  |
| motor oil  | 1      | 0      | 0      | 1      |  |
| tortillas  | 0      | 1      | 0      | 1      |  |
| choices    | 0      | 1      | 0      | 0      |  |
| wine       | 1      | 1      | 1      | 0      |  |

Table 14.1: Distributional statistics for *tezg ¨uino* and five related terms

<span id="page-343-1"></span>![](_page_343_Figure_3.jpeg)

Figure 14.1: Lexical semantic relationships have regular linear structures in two dimensional projections of distributional statistics [\(Pennington et al., 2014\)](#page-554-8).

What other words fit into these contexts? How about: *loud, motor oil, tortillas, choices, wine*? Each row of [Table 14.1](#page-343-0) is a vector that summarizes the contextual properties for each word, with a value of one for contexts in which the word can appear, and a value of zero for contexts in which it cannot. Based on these vectors, we can conclude: *wine* is very similar to *tezg ¨uino*; *motor oil* and *tortillas* are fairly similar to *tezg ¨uino*; *loud* is completely different.

These vectors, which we will call **word representations**, describe the **distributional** properties of each word. Does vector similarity imply semantic similarity? This is the **distributional hypothesis**, stated by [Firth](#page-528-8) [\(1957\)](#page-528-8) as: "You shall know a word by the company it keeps." The distributional hypothesis has stood the test of time: distributional statistics are a core part of language technology today, because they make it possible to leverage large amounts of unlabeled data to learn about rare words that do not appear in labeled training data.

Distributional statistics have a striking ability to capture lexical semantic relationships

<span id="page-344-2"></span>such as analogies. [Figure 14.1](#page-343-1) shows two examples, based on two-dimensional projections of distributional **word embeddings**, discussed later in this chapter. In each case, wordpair relationships correspond to regular linear patterns in this two dimensional space. No labeled data about the nature of these relationships was required to identify this underlying structure.

**Distributional** semantics are computed from context statistics. **Distributed** semantics are a related but distinct idea: that meaning can be represented by numerical vectors rather than symbolic structures. Distributed representations are often estimated from distributional statistics, as in latent semantic analysis and WORD2VEC, described later in this chapter. However, distributed representations can also be learned in a supervised fashion from labeled data, as in the neural classification models encountered in [chapter 3.](#page-64-0)

# <span id="page-344-0"></span>**14.2 Design decisions for word representations**

There are many approaches for computing word representations, but most can be distinguished on three main dimensions: the nature of the representation, the source of contextual information, and the estimation procedure.

#### <span id="page-344-1"></span>**14.2.1 Representation**

Today, the dominant word representations are k-dimensional vectors of real numbers, known as **word embeddings**. (The name is due to the fact that each discrete word is embedded in a continuous vector space.) This representation dates back at least to the late 1980s [\(Deerwester et al., 1990\)](#page-524-7), and is used in popular techniques such as WORD2VEC [\(Miko](#page-548-4)lov [et al., 2013\)](#page-548-4).

Word embeddings are well suited for neural networks, where they can be plugged in as inputs. They can also be applied in linear classifiers and structure prediction models [\(Turian et al., 2010\)](#page-566-5), although it can be difficult to learn linear models that employ real-valued features [\(Kummerfeld et al., 2015\)](#page-541-2). A popular alternative is bit-string representations, such as **Brown clusters** (§ [14.4\)](#page-348-0), in which each word is represented by a variable-length sequence of zeros and ones [\(Brown et al., 1992\)](#page-516-7).

Another representational question is whether to estimate one embedding per surface form (e.g., *bank*), or to estimate distinct embeddings for each word sense or synset. Intuitively, if word representations are to capture the meaning of individual words, then words with multiple meanings should have multiple embeddings. This can be achieved by integrating unsupervised clustering with word embedding estimation [\(Huang and](#page-536-6) [Yates, 2012;](#page-536-6) [Li and Jurafsky, 2015\)](#page-543-3). However, [Arora et al.](#page-511-11) [\(2018\)](#page-511-11) argue that it is unnecessary to model distinct word senses explicitly, because the embeddings for each surface form are a linear combination of the embeddings of the underlying senses.

<span id="page-345-3"></span><span id="page-345-1"></span>

| The moment one learns English, complications set in (Alfau, 1999) |                                                                   |  |  |  |
|-------------------------------------------------------------------|-------------------------------------------------------------------|--|--|--|
| Brown Clusters                                                    | {one}                                                             |  |  |  |
| WORD2VEC, $h = 2$                                                 | {moment, one, English, complications}                             |  |  |  |
| Structured WORD2VEC, $h = 2$                                      | $\{(moment, -2), (one, -1), (English, +1), (complications, +2)\}$ |  |  |  |
| Dependency contexts,                                              | $\{(one, NSUBJ), (English, DOBJ), (moment, ACL^{-1})\}$           |  |  |  |

Table 14.2: Contexts for the word *learns*, according to various word representations. For dependency context, (*one*, NSUBJ) means that there is a relation of type NSUBJ (nominal subject) *to* the word *one*, and (*moment*,  $ACL^{-1}$ ) means that there is a relation of type ACL (adjectival clause) *from* the word *moment*.

#### <span id="page-345-0"></span>14.2.2 Context

The distributional hypothesis says that word meaning is related to the "contexts" in which the word appears, but context can be defined in many ways. In the *tezgüino* example, contexts are entire sentences, but in practice there are far too many sentences. At the opposite extreme, the context could be defined as the immediately preceding word; this is the context considered in Brown clusters. WORD2VEC takes an intermediate approach, using local neighborhoods of words (e.g., h = 5) as contexts (Mikolov et al., 2013). Contexts can also be much larger: for example, in **latent semantic analysis**, each word's context vector includes an entry per document, with a value of one if the word appears in the document (Deerwester et al., 1990); in **explicit semantic analysis**, these documents are Wikipedia pages (Gabrilovich and Markovitch, 2007).

In structured WORD2VEC, context words are labeled by their position with respect to the target word  $w_m$  (e.g., two words before, one word after), which makes the resulting word representations more sensitive to syntactic differences (Ling et al., 2015). Another way to incorporate syntax is to perform parsing as a preprocessing step, and then form context vectors from the dependency edges (Levy and Goldberg, 2014) or predicateargument relations (Lin, 1998). The resulting context vectors for several of these methods are shown in Table 14.2.

The choice of context has a profound effect on the resulting representations, which can be viewed in terms of word similarity. Applying latent semantic analysis (§ 14.3) to contexts of size h = 2 and h = 30 yields the following nearest-neighbors for the word dog:<sup>1</sup>

<span id="page-345-2"></span>• (h = 2): cat, horse, fox, pet, rabbit, pig, animal, mongrel, sheep, pigeon

<sup>&</sup>lt;sup>1</sup>The example is from lecture slides by Marco Baroni, Alessandro Lenci, and Stefan Evert, who applied latent semantic analysis to the British National Corpus. You can find an online demo here: http://clic.cimec.unitn.it/infomap-query/

#### <span id="page-346-2"></span>14.3. LATENT SEMANTIC ANALYSIS

• (h = 30): kennel, puppy, pet, bitch, terrier, rottweiler, canine, cat, to bark, Alsatian

Which word list is better? Each word in the h = 2 list is an animal, reflecting the fact that locally, the word *dog* tends to appear in the same contexts as other animal types (e.g., *pet the dog, feed the dog*). In the h = 30 list, nearly everything is dog-related, including specific breeds such as *rottweiler* and *Alsatian*. The list also includes words that are not animals (*kennel*), and in one case (*to bark*), is not a noun at all. The 2-word context window is more sensitive to syntax, while the 30-word window is more sensitive to topic.

#### <span id="page-346-0"></span>14.2.3 Estimation

Word embeddings are estimated by optimizing some objective: the likelihood of a set of unlabeled data (or a closely related quantity), or the reconstruction of a matrix of context counts, similar to Table 14.1.

**Maximum likelihood estimation** Likelihood-based optimization is derived from the objective  $\log p(w; U)$ , where  $U \in \mathbb{R}K \times V$  is matrix of word embeddings, and  $w = \{w_m\}_{m=1}^M$  is a corpus, represented as a list of M tokens. Recurrent neural network language models (§ 6.3) optimize this objective directly, backpropagating to the input word embeddings through the recurrent structure. However, state-of-the-art word embeddings employ huge corpora with hundreds of billions of tokens, and recurrent architectures are difficult to scale to such data. As a result, likelihood-based word embeddings are usually based on simplified likelihoods or heuristic approximations.

**Matrix factorization** The matrix  $\mathbf{C} = \{\text{count}(i, j)\}\$  stores the co-occurrence counts of word *i* and context *j*. Word representations can be obtained by approximately factoring this matrix, so that count(i, j) is approximated by a function of a word embedding  $u_i$  and a context embedding  $v_j$ . These embeddings can be obtained by minimizing the norm of the reconstruction error,

$$\min_{\boldsymbol{u},\boldsymbol{v}} ||\mathbf{C} - \tilde{\mathbf{C}}(\boldsymbol{u},\boldsymbol{v})||_F,$$
[14.1]

where C(u, v) is the approximate reconstruction resulting from the embeddings u and v, and  $||\mathbf{X}||_F$  indicates the Frobenius norm,  $\sum_{i,j} x_{i,j}^2$ . Rather than factoring the matrix of word-context counts directly, it is often helpful to transform these counts using information-theoretic metrics such as **pointwise mutual information** (PMI), described in the next section.

### <span id="page-346-1"></span>14.3 Latent semantic analysis

Latent semantic analysis (LSA) is one of the oldest approaches to distributed semantics (Deerwester et al., 1990). It induces continuous vector representations of words by <span id="page-347-1"></span>factoring a matrix of word and context counts, using **truncated singular value decomposition** (SVD),

$$\min_{\mathbf{U}\in\mathbb{R}^{V\times K}, \mathbf{S}\in\mathbb{R}^{K\times K}, \mathbf{V}\in\mathbb{R}^{|\mathcal{C}|\times K}} ||\mathbf{C}-\mathbf{U}\mathbf{S}\mathbf{V}^{\top}||_{F}$$
[14.2]

s.t. 
$$\mathbf{U}^{\top}\mathbf{U} = \mathbb{I}$$
 [14.3]

$$\mathbf{V}^{\top}\mathbf{V} = \mathbb{I}$$
 [14.4]

$$\forall i \neq j, \mathbf{S}_{i,j} = 0,$$
 [14.5]

where V is the size of the vocabulary, |C| is the number of contexts, and K is size of the resulting embeddings, which are set equal to the rows of the matrix U. The matrix S is constrained to be diagonal (these diagonal elements are called the singular values), and the columns of the product SV<sup>&</sup>gt; provide descriptions of the contexts. Each element ci,j is then reconstructed as a **bilinear product**,

$$c_{i,j} \approx \sum_{k=1}^{K} u_{i,k} s_k v_{j,k}.$$
 [14.6]

The objective is to minimize the sum of squared approximation errors. The orthonormality constraints U>U = V>V = I ensure that all pairs of dimensions in U and V are uncorrelated, so that each dimension conveys unique information. Efficient implementations of truncated singular value decomposition are available in numerical computing packages such as SCIPY and MATLAB. [2](#page-347-0)

Latent semantic analysis is most effective when the count matrix is transformed before the application of SVD. One such transformation is **pointwise mutual information** (PMI; [Church and Hanks, 1990\)](#page-520-11), which captures the degree of association between word i and context j,

$$PMI(i, j) = \log \frac{p(i, j)}{p(i)p(j)} = \log \frac{p(i \mid j)p(j)}{p(i)p(j)} = \log \frac{p(i \mid j)}{p(i)}$$
[14.7]

$$= \log \operatorname{count}(i, j) - \log \sum_{i'=1}^{V} \operatorname{count}(i', j)$$
[14.8]

$$-\log \sum_{j' \in \mathcal{C}} \operatorname{count}(i, j') + \log \sum_{i'=1}^{V} \sum_{j' \in \mathcal{C}} \operatorname{count}(i', j').$$
[14.9]

The pointwise mutual information can be viewed as the logarithm of the ratio of the conditional probability of word i in context j to the marginal probability of word i in all

<span id="page-347-0"></span><sup>2</sup>An important implementation detail is to represent C as a **sparse matrix**, so that the storage cost is equal to the number of non-zero entries, rather than the size V × |C|.

#### <span id="page-348-2"></span><span id="page-348-1"></span>14.4. BROWN CLUSTERS 331

![](_page_348_Figure_1.jpeg)

Figure 14.2: Subtrees produced by bottom-up Brown clustering on news text [\(Miller et al.,](#page-549-8) [2004\)](#page-549-8).

contexts. When word i is statistically associated with context j, the ratio will be greater than one, so PMI(i, j) > 0. The PMI transformation focuses latent semantic analysis on reconstructing strong word-context associations, rather than on reconstructing large counts.

The PMI is negative when a word and context occur together less often than if they were independent, but such negative correlations are unreliable because counts of rare events have high variance. Furthermore, the PMI is undefined when count(i, j) = 0. One solution to these problems is to use the **Positive PMI** (PPMI),

$$PPMI(i, j) = \begin{cases} PMI(i, j), & p(i \mid j) > p(i) \\ 0, & \text{otherwise.} \end{cases}$$
[14.10]

[Bullinaria and Levy](#page-517-4) [\(2007\)](#page-517-4) compare a range of matrix transformations for latent semantic analysis, using a battery of tasks related to word meaning and word similarity (for more on evaluation, see § [14.6\)](#page-355-0). They find that PPMI-based latent semantic analysis yields strong performance on a battery of tasks related to word meaning: for example, PPMI-based LSA vectors can be used to solve multiple-choice word similarity questions from the Test of English as a Foreign Language (TOEFL), obtaining 85% accuracy.

# <span id="page-348-0"></span>**14.4 Brown clusters**

Learning algorithms like perceptron and conditional random fields often perform better with discrete feature vectors. A simple way to obtain discrete representations from distri-

<span id="page-349-1"></span><span id="page-349-0"></span>

| bitstring    | ten most frequent words                                                                          |
|--------------|--------------------------------------------------------------------------------------------------|
| 011110100111 | excited thankful grateful stoked pumped anxious hyped psyched<br>exited geeked                   |
| 01111010100  | talking talkin complaining talkn bitching tlkn tlkin bragging rav<br>ing +k                      |
| 011110101010 | thinking thinkin dreaming worrying thinkn speakin reminiscing<br>dreamin daydreaming fantasizing |
| 011110101011 | saying sayin suggesting stating sayn jokin talmbout implying<br>insisting 5'2                    |
| 011110101100 | wonder dunno wondered duno donno dno dono wonda wounder<br>dunnoe                                |
| 011110101101 | wondering wonders debating deciding pondering unsure won<br>derin debatin woundering wondern     |
| 011110101110 | sure suree suuure suure sure- surre sures shuree                                                 |

Table 14.3: Fragment of a Brown clustering of Twitter data [\(Owoputi et al., 2013\)](#page-553-9). Each row is a leaf in the tree, showing the ten most frequent words. This part of the tree emphasizes verbs of communicating and knowing, especially in the present participle. Each leaf node includes orthographic variants (*thinking, thinkin, thinkn*), semantically related terms (*excited, thankful, grateful*), and some outliers (*5'2, +k*). See [http:](http://www.cs.cmu.edu/~ark/TweetNLP/cluster_viewer.html) [//www.cs.cmu.edu/˜ark/TweetNLP/cluster\\_viewer.html](http://www.cs.cmu.edu/~ark/TweetNLP/cluster_viewer.html) for more.

butional statistics is by clustering (§ [5.1.1\)](#page-113-0), so that words in the same cluster have similar distributional statistics. This can help in downstream tasks, by sharing features between all words in the same cluster. However, there is an obvious tradeoff: if the number of clusters is too small, the words in each cluster will not have much in common; if the number of clusters is too large, then the learner will not see enough examples from each cluster to generalize.

A solution to this problem is **hierarchical clustering**: using the distributional statistics to induce a tree-structured representation. Fragments of **Brown cluster** trees are shown in [Figure 14.2](#page-348-1) and [Table 14.3.](#page-349-0) Each word's representation consists of a binary string describing a path through the tree: 0 for taking the left branch, and 1 for taking the right branch. In the subtree in the upper right of the figure, the representation of the word *conversation* is 10; the representation of the word *assessment* is 0001. Bitstring prefixes capture similarity at varying levels of specificity, and it is common to use the first eight, twelve, sixteen, and twenty bits as features in tasks such as named entity recognition [\(Miller et al., 2004\)](#page-549-8) and dependency parsing [\(Koo et al., 2008\)](#page-541-12).

Hierarchical trees can be induced from a likelihood-based objective, using a discrete

<span id="page-350-2"></span>latent variable  $k_i \in \{1, 2, ..., K\}$  to represent the cluster of word *i*:

$$\log \mathbf{p}(\boldsymbol{w}; \boldsymbol{k}) \approx \sum_{m=1}^{M} \log \mathbf{p}(w_m \mid w_{m-1}; \boldsymbol{k})$$
[14.11]

<span id="page-350-0"></span>
$$\triangleq \sum_{m=1}^{M} \log \mathbf{p}(w_m \mid k_{w_m}) + \log \mathbf{p}(k_{w_m} \mid k_{w_{m-1}}).$$
 [14.12]

This is similar to a hidden Markov model, with the crucial difference that each word can be emitted from only a single cluster:  $\forall k \neq k_{w_m}$ ,  $p(w_m \mid k) = 0$ .

Using the objective in Equation 14.12, the Brown clustering tree can be constructed from the bottom up: begin with each word in its own cluster, and incrementally merge clusters until only a single cluster remains. At each step, we merge the pair of clusters such that the objective in Equation 14.12 is maximized. Although the objective seems to involve a sum over the entire corpus, the score for each merger can be computed from the cluster-to-cluster co-occurrence counts. These counts can be updated incrementally as the clustering proceeds. The optimal merge at each step can be shown to maximize the **average mutual information**,

<span id="page-350-1"></span>
$$I(\mathbf{k}) = \sum_{k_1=1}^{K} \sum_{k_2=1}^{K} p(k_1, k_2) \times PMI(k_1, k_2)$$
[14.13]  
$$p(k_1, k_2) = \frac{count(k_1, k_2)}{\sum_{k_1'=1}^{K} \sum_{k_2'=1}^{K} count(k_{1'}, k_{2'})},$$

where  $p(k_1, k_2)$  is the joint probability of a bigram involving a word in cluster  $k_1$  followed by a word in  $k_2$ . This probability and the PMI are both computed from the co-occurrence counts between clusters. After each merger, the co-occurrence vectors for the merged clusters are simply added up, so that the next optimal merger can be found efficiently.

This bottom-up procedure requires iterating over the entire vocabulary, and evaluating  $K_t^2$  possible mergers at each step, where  $K_t$  is the current number of clusters at step tof the algorithm. Furthermore, computing the score for each merger involves a sum over  $K_t^2$  clusters. The maximum number of clusters is  $K_0 = V$ , which occurs when every word is in its own cluster at the beginning of the algorithm. The time complexity is thus  $O(V^5)$ .

To avoid this complexity, practical implementations use a heuristic approximation called **exchange clustering**. The *K* most common words are placed in clusters of their own at the beginning of the process. We then consider the next most common word, and merge it with one of the existing clusters. This continues until the entire vocabulary has been incorporated, at which point the *K* clusters are merged down to a single cluster, forming a tree. The algorithm never considers more than K + 1 clusters at any step, and the complexity is  $O(VK + V \log V)$ , with the second term representing the cost of sorting

<span id="page-351-2"></span>![](_page_351_Figure_1.jpeg)

Figure 14.3: The CBOW and skipgram variants of WORD2VEC. The parameter U is the matrix of word embeddings, and each v<sup>m</sup> is the context embedding for word wm.

the words at the beginning of the algorithm. For more details on the algorithm, see [Liang](#page-544-8) [\(2005\)](#page-544-8).

# <span id="page-351-0"></span>**14.5 Neural word embeddings**

Neural word embeddings combine aspects of the previous two methods: like latent semantic analysis, they are a continuous vector representation; like Brown clusters, they are trained from a likelihood-based objective. Let the vector u<sup>i</sup> represent the K-dimensional **embedding** for word i, and let v<sup>j</sup> represent the K-dimensional embedding for context j. The inner product u<sup>i</sup> · v<sup>j</sup> represents the compatibility between word i and context j. By incorporating this inner product into an approximation to the log-likelihood of a corpus, it is possible to estimate both parameters by backpropagation. WORD2VEC [\(Mikolov](#page-548-4) [et al., 2013\)](#page-548-4) includes two such approximations: continuous bag-of-words (CBOW) and skipgrams.

#### <span id="page-351-1"></span>**14.5.1 Continuous bag-of-words (CBOW)**

In recurrent neural network language models, each word w<sup>m</sup> is conditioned on a recurrentlyupdated state vector, which is based on word representations going all the way back to the beginning of the text. The **continuous bag-of-words (CBOW)** model is a simplification: the local context is computed as an average of embeddings for words in the immediate neighborhood m − h, m − h + 1, . . . , m + h − 1, m + h,

$$\overline{v}_m = \frac{1}{2h} \sum_{n=1}^h v_{w_{m+n}} + v_{w_{m-n}}.$$
 [14.14]

Thus, CBOW is a bag-of-words model, because the order of the context words does not matter; it is continuous, because rather than conditioning on the words themselves, we condition on a continuous vector constructed from the word embeddings. The parameter h determines the neighborhood size, which [Mikolov et al.](#page-548-4) [\(2013\)](#page-548-4) set to h = 4.

<span id="page-352-2"></span>The CBOW model optimizes an approximation to the corpus log-likelihood,

$$\log \mathbf{p}(\boldsymbol{w}) \approx \sum_{m=1}^{M} \log \mathbf{p}(w_m \mid w_{m-h}, w_{m-h+1}, \dots, w_{m+h-1}, w_{m+h})$$
[14.15]

$$=\sum_{m=1}^{M}\log\frac{\exp\left(\boldsymbol{u}_{w_{m}}\cdot\overline{\boldsymbol{v}}_{m}\right)}{\sum_{j=1}^{V}\exp\left(\boldsymbol{u}_{j}\cdot\overline{\boldsymbol{v}}_{m}\right)}$$
[14.16]

$$= \sum_{m=1}^{M} \boldsymbol{u}_{w_m} \cdot \overline{\boldsymbol{v}}_m - \log \sum_{j=1}^{V} \exp\left(\boldsymbol{u}_j \cdot \overline{\boldsymbol{v}}_m\right).$$
[14.17]

#### <span id="page-352-0"></span>14.5.2 Skipgrams

In the CBOW model, words are predicted from their context. In the **skipgram** model, the context is predicted from the word, yielding the objective:

$$\log \mathbf{p}(\mathbf{w}) \approx \sum_{m=1}^{M} \sum_{n=1}^{h_m} \log \mathbf{p}(w_{m-n} \mid w_m) + \log \mathbf{p}(w_{m+n} \mid w_m)$$
[14.18]

$$= \sum_{m=1}^{M} \sum_{n=1}^{h_m} \log \frac{\exp(\boldsymbol{u}_{w_{m-n}} \cdot \boldsymbol{v}_{w_m})}{\sum_{j=1}^{V} \exp(\boldsymbol{u}_j \cdot \boldsymbol{v}_{w_m})} + \log \frac{\exp(\boldsymbol{u}_{w_{m+n}} \cdot \boldsymbol{v}_{w_m})}{\sum_{j=1}^{V} \exp(\boldsymbol{u}_j \cdot \boldsymbol{v}_{w_m})}$$
[14.19]

$$=\sum_{m=1}^{M}\sum_{n=1}^{h_m}\boldsymbol{u}_{w_{m-n}}\cdot\boldsymbol{v}_{w_m}+\boldsymbol{u}_{w_{m+n}}\cdot\boldsymbol{v}_{w_m}-2\log\sum_{j=1}^{V}\exp\left(\boldsymbol{u}_j\cdot\boldsymbol{v}_{w_m}\right).$$
 [14.20]

In the skipgram approximation, each word is generated multiple times; each time it is conditioned only on a single word. This makes it possible to avoid averaging the word vectors, as in the CBOW model. The local neighborhood size  $h_m$  is randomly sampled from a uniform categorical distribution over the range  $\{1, 2, ..., h_{max}\}$ ; Mikolov et al. (2013) set  $h_{max} = 10$ . Because the neighborhood grows outward with h, this approach has the effect of weighting near neighbors more than distant ones. Skipgram performs better on most evaluations than CBOW (see § 14.6 for details of how to evaluate word representations), but CBOW is faster to train (Mikolov et al., 2013).

#### <span id="page-352-1"></span>14.5.3 Computational complexity

The WORD2VEC models can be viewed as an efficient alternative to recurrent neural network language models, which involve a recurrent state update whose time complexity is quadratic in the size of the recurrent state vector. CBOW and skipgram avoid this computation, and incur only a linear time complexity in the size of the word and context representations. However, all three models compute a normalized probability over word tokens; a naïve implementation of this probability requires summing over the entire

<span id="page-353-1"></span><span id="page-353-0"></span>![](_page_353_Figure_1.jpeg)

Figure 14.4: A fragment of a hierarchical softmax tree. The probability of each word is computed as a product of probabilities of local branching decisions in the tree.

vocabulary. The time complexity of this sum is O(V ×K), which dominates all other computational costs. There are two solutions: **hierarchical softmax**, a tree-based computation that reduces the cost to a logarithm of the size of the vocabulary; and **negative sampling**, an approximation that eliminates the dependence on vocabulary size. Both methods are also applicable to RNN language models.

#### **Hierarchical softmax**

In Brown clustering, the vocabulary is organized into a binary tree. [Mnih and Hin](#page-550-7)[ton](#page-550-7) [\(2008\)](#page-550-7) show that the normalized probability over words in the vocabulary can be reparametrized as a probability over paths through such a tree. This hierarchical softmax probability is computed as a product of binary decisions over whether to move left or right through the tree, with each binary decision represented as a sigmoid function of the inner product between the context embedding v<sup>c</sup> and an output embedding associated with the node un,

$$\Pr(\text{left at } n \mid c) = \sigma(\boldsymbol{u}_n \cdot \boldsymbol{v}_c)$$
[14.21]

$$\Pr(\text{right at } n \mid c) = 1 - \sigma(\boldsymbol{u}_n \cdot \boldsymbol{v}_c) = \sigma(-\boldsymbol{u}_n \cdot \boldsymbol{v}_c), \qquad [14.22]$$

where σ refers to the sigmoid function, σ(x) = <sup>1</sup> 1+exp(−x) . The range of the sigmoid is the interval (0, 1), and 1 − σ(x) = σ(−x).

As shown in [Figure 14.4,](#page-353-0) the probability of generating each word is redefined as the product of the probabilities across its path. The sum of all such path probabilities is guaranteed to be one, for any context vector v<sup>c</sup> ∈ R <sup>K</sup>. In a balanced binary tree, the depth is logarithmic in the number of leaf nodes, and thus the number of multiplications is equal to O(log V ). The number of non-leaf nodes is equal to O(2V − 1), so the number of parameters to be estimated increases by only a small multiple. The tree can be constructed using an incremental clustering procedure similar to hierarchical Brown clusters [\(Mnih](#page-550-7)

<span id="page-354-2"></span>and Hinton, 2008), or by using the Huffman (1952) encoding algorithm for lossless compression.

#### Negative sampling

Likelihood-based methods are computationally intensive because each probability must be normalized over the vocabulary. These probabilities are based on scores for each word in each context, and it is possible to design an alternative objective that is based on these scores more directly: we seek word embeddings that maximize the score for the word that was really observed in each context, while minimizing the scores for a set of randomly selected **negative samples**:

<span id="page-354-1"></span>
$$\psi(i,j) = \log \sigma(\boldsymbol{u}_i \cdot \boldsymbol{v}_j) + \sum_{i' \in \mathcal{W}_{\text{neg}}} \log(1 - \sigma(\boldsymbol{u}_{i'} \cdot \boldsymbol{v}_j)), \quad [14.23]$$

where  $\psi(i, j)$  is the score for word *i* in context *j*, and  $W_{\text{neg}}$  is the set of negative samples. The objective is to maximize the sum over the corpus,  $\sum_{m=1}^{M} \psi(w_m, c_m)$ , where  $w_m$  is token *m* and  $c_m$  is the associated context.

The set of negative samples  $W_{neg}$  is obtained by sampling from a unigram language model. Mikolov et al. (2013) construct this unigram language model by exponentiating the empirical word probabilities, setting  $\hat{p}(i) \propto (\operatorname{count}(i))^{\frac{3}{4}}$ . This has the effect of redistributing probability mass from common to rare words. The number of negative samples increases the time complexity of training by a constant factor. Mikolov et al. (2013) report that 5-20 negative samples works for small training sets, and that two to five samples suffice for larger corpora.

#### <span id="page-354-0"></span>14.5.4 Word embeddings as matrix factorization

The negative sampling objective in Equation 14.23 can be justified as an efficient approximation to the log-likelihood, but it is also closely linked to the matrix factorization objective employed in latent semantic analysis. For a matrix of word-context pairs in which all counts are non-zero, negative sampling is equivalent to factorization of the matrix  $\mathbf{M}$ , where  $M_{ij} = \text{PMI}(i, j) - \log k$ : each cell in the matrix is equal to the pointwise mutual information of the word and context, shifted by  $\log k$ , with k equal to the number of negative samples (Levy and Goldberg, 2014). For word-context pairs that are not observed in the data, the pointwise mutual information is  $-\infty$ , but this can be addressed by considering only PMI values that are greater than  $\log k$ , resulting in a matrix of **shifted positive pointwise mutual information**,

$$M_{ij} = \max(0, \text{PMI}(i, j) - \log k).$$
 [14.24]

Word embeddings are obtained by factoring this matrix with truncated singular value decomposition.

<span id="page-355-2"></span>

| word 1      | word 2  | similarity |
|-------------|---------|------------|
| love        | sex     | 6.77       |
| stock       | jaguar  | 0.92       |
| money       | cash    | 9.15       |
| development | issue   | 3.97       |
| lad         | brother | 4.46       |

<span id="page-355-1"></span>Table 14.4: Subset of the WS-353 (Finkelstein et al., 2002) dataset of word similarity ratings (examples from Faruqui et al. (2016)).

GloVe ("global vectors") are a closely related approach (Pennington et al., 2014), in which the matrix to be factored is constructed from log co-occurrence counts,  $M_{ij} = \log \operatorname{count}(i, j)$ . The word embeddings are estimated by minimizing the sum of squares,

$$\min_{\boldsymbol{u},\boldsymbol{v},b,\tilde{b}} \quad \sum_{j=1}^{V} \sum_{j \in \mathcal{C}} f(M_{ij}) \left( \widehat{\log M_{ij}} - \log M_{ij} \right)^2$$
s.t. 
$$\widehat{\log M_{ij}} = \boldsymbol{u}_i \cdot \boldsymbol{v}_j + b_i + \tilde{b}_j, \qquad [14.25]$$

where  $b_i$  and  $b_j$  are offsets for word *i* and context *j*, which are estimated jointly with the embeddings *u* and *v*. The weighting function  $f(M_{ij})$  is set to be zero at  $M_{ij} = 0$ , thus avoiding the problem of taking the logarithm of zero counts; it saturates at  $M_{ij} = m_{max}$ , thus avoiding the problem of overcounting common word-context pairs. This heuristic turns out to be critical to the method's performance.

The time complexity of sparse matrix reconstruction is determined by the number of non-zero word-context counts. Pennington et al. (2014) show that this number grows sublinearly with the size of the dataset: roughly  $O(N^{0.8})$  for typical English corpora. In contrast, the time complexity of WORD2VEC is linear in the corpus size. Computing the co-occurrence counts also requires linear time in the size of the corpus, but this operation can easily be parallelized using MapReduce-style algorithms (Dean and Ghemawat, 2008).

## <span id="page-355-0"></span>14.6 Evaluating word embeddings

Distributed word representations can be evaluated in two main ways. **Intrinsic** evaluations test whether the representations cohere with our intuitions about word meaning. **Extrinsic** evaluations test whether they are useful for downstream tasks, such as sequence labeling.

#### <span id="page-356-2"></span><span id="page-356-0"></span>14.6.1 Intrinsic evaluations

A basic question for word embeddings is whether the similarity of words i and j is reflected in the similarity of the vectors  $u_i$  and  $u_j$ . **Cosine similarity** is typically used to compare two word embeddings,

$$\cos(\boldsymbol{u}_i, \boldsymbol{u}_j) = \frac{\boldsymbol{u}_i \cdot \boldsymbol{u}_j}{||\boldsymbol{u}_i||_2 \times ||\boldsymbol{u}_j||_2}.$$
[14.26]

For any embedding method, we can evaluate whether the cosine similarity of word embeddings is correlated with human judgments of word similarity. The WS-353 dataset (Finkelstein et al., 2002) includes similarity scores for 353 word pairs (Table 14.4). To test the accuracy of embeddings for rare and morphologically complex words, Luong et al. (2013) introduce a dataset of "rare words." Outside of English, word similarity resources are limited, mainly consisting of translations of WS-353 and the related SimLex-999 dataset (Hill et al., 2015).

Word analogies (e.g., *king:queen :: man:woman*) have also been used to evaluate word embeddings (Mikolov et al., 2013). In this evaluation, the system is provided with the first three parts of the analogy  $(i_1 : j_1 :: i_2 :?)$ , and the final element is predicted by finding the word embedding most similar to  $u_{i_1} - u_{j_1} + u_{i_2}$ . Another evaluation tests whether word embeddings are related to broad lexical semantic categories called **supersenses** (Ciaramita and Johnson, 2003): verbs of motion, nouns that describe animals, nouns that describe body parts, and so on. These supersenses are annotated for English synsets in Word-Net (Fellbaum, 2010). This evaluation is implemented in the QVEC metric, which tests whether the matrix of supersenses can be reconstructed from the matrix of word embeddings (Tsvetkov et al., 2015).

Levy et al. (2015) compared several dense word representations for English — including latent semantic analysis, WORD2VEC, and GloVe — using six word similarity metrics and two analogy tasks. None of the embeddings outperformed the others on every task, but skipgrams were the most broadly competitive. Hyperparameter tuning played a key role: any method will perform badly if the wrong hyperparameters are used. Relevant hyperparameters include the embedding size, as well as algorithm-specific details such as the neighborhood size and the number of negative samples.

#### <span id="page-356-1"></span>14.6.2 Extrinsic evaluations

Word representations contribute to downstream tasks like sequence labeling and document classification by enabling generalization across words. The use of distributed representations as features is a form of **semi-supervised learning**, in which performance on a supervised learning problem is augmented by learning distributed representations from unlabeled data (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010). These **pre-trained word representations** can be used as features in a linear prediction model, or as the input

<span id="page-357-1"></span>layer in a neural network, such as a Bi-LSTM tagging model (§ [7.6\)](#page-184-0). Word representations can be evaluated by the performance of the downstream systems that consume them: for example, GloVe embeddings are convincingly better than Latent Semantic Analysis as features in the downstream task of named entity recognition [\(Pennington et al., 2014\)](#page-554-8). Unfortunately, extrinsic and intrinsic evaluations do not always point in the same direction, and the best word representations for one downstream task may perform poorly on another task [\(Schnabel et al., 2015\)](#page-560-8).

When word representations are updated from labeled data in the downstream task, they are said to be **fine-tuned**. When labeled data is plentiful, pre-training may be unnecessary; when labeled data is scarce, fine-tuning may lead to overfitting. Various combinations of pre-training and fine-tuning can be employed. Pre-trained embeddings can be used as initialization before fine-tuning, and this can substantially improve performance [\(Lample et al., 2016\)](#page-542-3). Alternatively, both fine-tuned and pre-trained embeddings can be used as inputs in a single model [\(Kim, 2014\)](#page-539-6).

In semi-supervised scenarios, pretrained word embeddings can be replaced by "contextualized" word representations [\(Peters et al., 2018\)](#page-554-9). These contextualized representations are set to the hidden states of a deep bi-directional LSTM, which is trained as a bi-directional language model, motivating the name **ELMo (embeddings from language models)**. By running the language model, we obtain contextualized word representations, which can then be used as the base layer in a supervised neural network for any task. This approach yields significant gains over pretrained word embeddings on several tasks, presumably because the contextualized embeddings use unlabeled data to learn how to integrate linguistic context into the base layer of the supervised neural network.

#### <span id="page-357-0"></span>**14.6.3 Fairness and bias**

[Figure 14.1](#page-343-1) shows how word embeddings can capture analogies such as *man:woman :: king:queen*. While *king* and *queen* are gender-specific by definition, other professions or titles are associated with genders and other groups merely by statistical tendency. This statistical tendency may be a fact about the world (e.g., professional baseball players are usually men), or a fact about the text corpus (e.g., there are professional basketball leagues for both women and men, but the men's basketball is written about far more often).

There is now considerable evidence that word embeddings do indeed encode such biases. [Bolukbasi et al.](#page-515-7) [\(2016\)](#page-515-7) show that the words most aligned with the vector difference *she* − *he* are stereotypically female professions *homemaker, nurse, receptionist*; in the other direction are *maestro, skipper, protege*. [Caliskan et al.](#page-517-5) [\(2017\)](#page-517-5) systematize this observation by showing that biases in word embeddings align with well-validated gender stereotypes. [Garg et al.](#page-530-7) [\(2018\)](#page-530-7) extend these results to ethnic stereotypes of Asian Americans, and provide a historical perspective on how stereotypes evolve over 100 years of text data.

Because word embeddings are the input layer for many other natural language pro-

<span id="page-358-4"></span>cessing systems, these findings highlight the risk that natural language processing will replicate and amplify biases in the world, as well as in text. If, for example, word embeddings encode the belief that women are as unlikely to be *programmers* as they are to be *nephews*, then software is unlikely to successfully parse, translate, index, and generate texts in which women do indeed program computers. For example, contemporary NLP systems often fail to properly resolve pronoun references in texts that cut against gender stereotypes [\(Rudinger et al., 2018;](#page-559-5) [Zhao et al., 2018\)](#page-572-10). (The task of pronoun resolution is described in depth in [chapter 15.](#page-368-0)) Such biases can have profound consequences: for example, search engines are more likely to yield personalized advertisements for public arrest records when queried with names that are statistically associated with African Americans [\(Sweeney, 2013\)](#page-564-6). There is now an active research literature on "debiasing" machine learning and natural language processing, as evidenced by the growth of annual meetings such as Fairness, Accountability, and Transparency in Machine Learning (FAT/ML). However, given that the ultimate source of these biases is the text itself, it may be too much to hope for a purely algorithmic solution. There is no substitute for critical thought about the inputs to natural language processing systems – and the uses of their outputs.

# <span id="page-358-0"></span>**14.7 Distributed representations beyond distributional statistics**

Distributional word representations can be estimated from huge unlabeled datasets, thereby covering many words that do not appear in labeled data: for example, GloVe embeddings are estimated from 800 billion tokens of web data,[3](#page-358-2) while the largest labeled datasets for NLP tasks are on the order of millions of tokens. Nonetheless, even a dataset of hundreds of billions of tokens will not cover every word that may be encountered in the future. Furthermore, many words will appear only a few times, making their embeddings unreliable. Many languages exceed English in morphological complexity, and thus have lower token-to-type ratios. When this problem is coupled with small training corpora, it becomes especially important to leverage other sources of information beyond distributional statistics.

# <span id="page-358-1"></span>**14.7.1 Word-internal structure**

One solution is to incorporate word-internal structure into word embeddings. Purely distributional approaches consider words as atomic units, but in fact, many words have internal structure, so that their meaning can be **composed** from the representations of sub-word units. Consider the following terms, all of which are missing from Google's pre-trained WORD2VEC embeddings:[4](#page-358-3)

<span id="page-358-2"></span><sup>3</sup><http://commoncrawl.org/>

<span id="page-358-3"></span><sup>4</sup><https://code.google.com/archive/p/word2vec/>, accessed September 20, 2017

<span id="page-359-1"></span><span id="page-359-0"></span>![](_page_359_Figure_1.jpeg)

Figure 14.5: Two architectures for building word embeddings from subword units. On the left, morpheme embeddings u (m) are combined by addition with the non-compositional word embedding u˜ [\(Botha and Blunsom, 2014\)](#page-515-4). On the right, morpheme embeddings are combined in a recursive neural network [\(Luong et al., 2013\)](#page-546-5).

- *millicuries* This word has **morphological** structure (see § [9.1.2](#page-211-0) for more on morphology): the prefix *milli-* indicates an amount, and the suffix *-s* indicates a plural. (A *millicurie* is an unit of radioactivity.)
- *caesium* This word is a single morpheme, but the characters *-ium* are often associated with chemical elements. (*Caesium* is the British spelling of a chemical element, spelled *cesium* in American English.)
- *IAEA* This term is an acronym, as suggested by the use of capitalization. The prefix *I-* frequently refers to international organizations, and the suffix *-A* often refers to agencies or associations. (*IAEA* is the International Atomic Energy Agency.)
- *Zhezhgan* This term is in title case, suggesting the name of a person or place, and the character bigram *zh* indicates that it is likely a transliteration. (*Zhezhgan* is a mining facility in Kazakhstan.)

How can word-internal structure be incorporated into word representations? One approach is to construct word representations from embeddings of the characters or morphemes. For example, if word i has morphological segments M<sup>i</sup> , then its embedding can be constructed by addition [\(Botha and Blunsom, 2014\)](#page-515-4),

$$\boldsymbol{u}_i = \tilde{\boldsymbol{u}}_i + \sum_{j \in \mathcal{M}_i} \boldsymbol{u}_j^{(M)},$$
[14.27]

where u (M) <sup>m</sup> is a morpheme embedding and u˜<sup>i</sup> is a non-compositional embedding of the whole word, which is an additional free parameter of the model [\(Figure 14.5,](#page-359-0) left side). All embeddings are estimated from a **log-bilinear language model** [\(Mnih and Hinton,](#page-550-3) [2007\)](#page-550-3), which is similar to the CBOW model (§ [14.5\)](#page-351-0), but includes only contextual information from preceding words. The morphological segments are obtained using an unsupervised segmenter [\(Creutz and Lagus, 2007\)](#page-522-6). For words that do not appear in the training

#### <span id="page-360-1"></span>14.7. DISTRIBUTED REPRESENTATIONS BEYOND DISTRIBUTIONAL STATISTICS343

data, the embedding can be constructed directly from the morphemes, assuming that each morpheme appears in some other word in the training data. The free parameter u˜ adds flexibility: words with similar morphemes are encouraged to have similar embeddings, but this parameter makes it possible for them to be different.

Word-internal structure can be incorporated into word representations in various other ways. Here are some of the main parameters.

- **Subword units.** Examples like *IAEA* and *Zhezhgan* are not based on morphological composition, and a morphological segmenter is unlikely to identify meaningful subword units for these terms. Rather than using morphemes for subword embeddings, one can use characters [\(Santos and Zadrozny, 2014;](#page-559-3) [Ling et al., 2015;](#page-545-3) [Kim et al., 2016\)](#page-539-1), character n-grams [\(Wieting et al., 2016a;](#page-569-3) [Bojanowski et al., 2017\)](#page-515-8), and **byte-pair encodings**, a compression technique which captures frequent substrings [\(Gage, 1994;](#page-529-10) [Sennrich et al., 2016\)](#page-560-9).
- **Composition.** Combining the subword embeddings by addition does not differentiate between orderings, nor does it identify any particular morpheme as the root. A range of more flexible compositional models have been considered, including recurrence [\(Ling et al., 2015\)](#page-545-3), convolution [\(Santos and Zadrozny, 2014;](#page-559-3) [Kim et al.,](#page-539-1) [2016\)](#page-539-1), and **recursive neural networks** [\(Luong et al., 2013\)](#page-546-5), in which representations of progressively larger units are constructed over a morphological parse, e.g. ((*milli+curie*)+*s*), ((*in*+*flam*)+*able*), (*in*+(*vis*+*ible*)). A recursive embedding model is shown in the right panel of [Figure 14.5.](#page-359-0)
- **Estimation.** Estimating subword embeddings from a full dataset is computationally expensive. An alternative approach is to train a subword model to match pre-trained word embeddings [\(Cotterell et al., 2016;](#page-522-10) [Pinter et al., 2017\)](#page-555-5). To train such a model, it is only necessary to iterate over the vocabulary, and not the corpus.

### <span id="page-360-0"></span>**14.7.2 Lexical semantic resources**

Resources such as WordNet provide another source of information about word meaning: if we know that *caesium* is a synonym of *cesium*, or that a *millicurie* is a type of *measurement unit*, then this should help to provide embeddings for the unknown words, and to smooth embeddings of rare words. One way to do this is to **retrofit** pre-trained word embeddings across a network of lexical semantic relationships [\(Faruqui et al., 2015\)](#page-527-5) by minimizing the following objective,

$$\min_{\mathbf{U}} \quad \sum_{j=1}^{V} ||\boldsymbol{u}_i - \hat{\boldsymbol{u}}_i||_2 + \sum_{(i,j) \in \mathcal{L}} \beta_{ij} ||\boldsymbol{u}_i - \boldsymbol{u}_j||_2, \quad [14.28]$$

<span id="page-361-2"></span>where  $\hat{u}_i$  is the pretrained embedding of word i, and  $\mathcal{L} = \{(i, j)\}$  is a lexicon of word relations. The hyperparameter  $\beta_{ij}$  controls the importance of adjacent words having similar embeddings; Faruqui et al. (2015) set it to the inverse of the degree of word i,  $\beta_{ij} = |\{j : (i, j) \in \mathcal{L}\}|^{-1}$ . Retrofitting improves performance on a range of intrinsic evaluations, and gives small improvements on an extrinsic document classification task.

# <span id="page-361-0"></span>14.8 Distributed representations of multiword units

Can distributed representations extend to phrases, sentences, paragraphs, and beyond? Before exploring this possibility, recall the distinction between distributed and distributional representations. Neural embeddings such as WORD2VEC are both distributed (vector-based) and distributional (derived from counts of words in context). As we consider larger units of text, the counts decrease: in the limit, a multi-paragraph span of text would never appear twice, except by plagiarism. Thus, the meaning of a large span of text cannot be determined from distributional statistics alone; it must be computed compositionally from smaller spans. But these considerations are orthogonal to the question of whether distributed representations — dense numerical vectors — are sufficiently expressive to capture the meaning of phrases, sentences, and paragraphs.

#### <span id="page-361-1"></span>14.8.1 Purely distributional methods

Some multiword phrases are non-compositional: the meaning of such phrases is not derived from the meaning of the individual words using typical compositional semantics. This includes proper nouns like *San Francisco* as well as idiomatic expressions like *kick the bucket* (Baldwin and Kim, 2010). For these cases, purely distributional approaches can work. A simple approach is to identify multiword units that appear together frequently, and then treat these units as words, learning embeddings using a technique such as WORD2VEC.

The problem of identifying multiword units is sometimes called **collocation extraction**. A good collocation has high **pointwise mutual information** (PMI; see § 14.3). For example, *Naïve Bayes* is a good collocation because  $p(w_t = Bayes | w_{t-1} = naïve)$  is much larger than  $p(w_t = Bayes)$ . Collocations of more than two words can be identified by a greedy incremental search: for example, *mutual information* might first be extracted as a collocation and grouped into a single word type *mutual\_information*; then *pointwise mutual\_information* can be extracted later. After identifying such units, they can be treated as words when estimating skipgram embeddings. Mikolov et al. (2013) show that the resulting embeddings perform reasonably well on a task of solving phrasal analogies, e.g. *New York : New York Times :: Baltimore : Baltimore Sun.* 

<span id="page-362-2"></span><span id="page-362-1"></span>**this was the only way** it was the only way it was her turn to blink it was hard to tell it was time to move on he had to do it again they all looked at each other they all turned to look back they both turned to face him **they both turned and walked away**

Figure 14.6: By interpolating between the distributed representations of two sentences (in bold), it is possible to generate grammatical sentences that combine aspects of both [\(Bow](#page-515-9)[man et al., 2016\)](#page-515-9)

#### <span id="page-362-0"></span>**14.8.2 Distributional-compositional hybrids**

To move beyond short multiword phrases, composition is necessary. A simple but surprisingly powerful approach is to represent a sentence with the average of its word embeddings [\(Mitchell and Lapata, 2010\)](#page-549-11). This can be considered a hybrid of the distributional and compositional approaches to semantics: the word embeddings are computed distributionally, and then the sentence representation is computed by composition.

The WORD2VEC approach can be stretched considerably further, embedding entire sentences using a model similar to skipgrams, in the "skip-thought" model of [Kiros et al.](#page-540-5) [\(2015\)](#page-540-5). Each sentence is *encoded* into a vector using a recurrent neural network: the encoding of sentence t is set to the RNN hidden state at its final token, h (t) Mt . This vector is then a parameter in a *decoder* model that is used to generate the previous and subsequent sentences: the decoder is another recurrent neural network, which takes the encoding of the neighboring sentence as an additional parameter in its recurrent update. (This **encoderdecoder model** is discussed at length in [chapter 18.](#page-448-0)) The encoder and decoder are trained simultaneously from a likelihood-based objective, and the trained encoder can be used to compute a distributed representation of any sentence. Skip-thought can also be viewed as a hybrid of distributional and compositional approaches: the vector representation of each sentence is computed compositionally from the representations of the individual words, but the training objective is distributional, based on sentence co-occurrence across a corpus.

**Autoencoders** are a variant of encoder-decoder models in which the decoder is trained to produce the same text that was originally encoded, using only the distributed encoding vector [\(Li et al., 2015\)](#page-543-6). The encoding acts as a bottleneck, so that generalization is necessary if the model is to successfully fit the training data. In **denoising autoencoders**,

<span id="page-363-2"></span>the input is a corrupted version of the original sentence, and the auto-encoder must reconstruct the uncorrupted original [\(Vincent et al., 2010;](#page-567-4) [Hill et al., 2016\)](#page-534-6). By interpolating between distributed representations of two sentences, αui+(1−α)u<sup>j</sup> , it is possible to generate sentences that combine aspects of the two inputs, as shown in [Figure 14.6](#page-362-1) [\(Bowman](#page-515-9) [et al., 2016\)](#page-515-9).

Autoencoders can also be applied to longer texts, such as paragraphs and documents. This enables applications such as **question answering**, which can be performed by matching the encoding of the question with encodings of candidate answers [\(Miao et al., 2016\)](#page-548-5).

#### <span id="page-363-0"></span>**14.8.3 Supervised compositional methods**

Given a supervision signal, such as a label describing the sentiment or meaning of a sentence, a wide range of compositional methods can be applied to compute a distributed representation that then predicts the label. The simplest is to average the embeddings of each word in the sentence, and pass this average through a feedforward neural network [\(Iyyer et al., 2015\)](#page-536-8). Convolutional and recurrent neural networks go further, with the ability to effectively capturing multiword phenomena such as negation [\(Kalchbrenner](#page-539-4) [et al., 2014;](#page-539-4) [Kim, 2014;](#page-539-6) [Li et al., 2015;](#page-544-9) [Tang et al., 2015\)](#page-565-9). Another approach is to incorporate the syntactic structure of the sentence into a **recursive neural network**, in which the representation for each syntactic constituent is computed from the representations of its children [\(Socher et al., 2012\)](#page-562-8). However, in many cases, recurrent neural networks perform as well or better than recursive networks [\(Li et al., 2015\)](#page-544-9).

Whether convolutional, recurrent, or recursive, a key question is whether supervised sentence representations are task-specific, or whether a single supervised sentence representation model can yield useful performance on other tasks. [Wieting et al.](#page-569-4) [\(2016b\)](#page-569-4) train a variety of sentence embedding models for the task of labeling pairs of sentences as **paraphrases**. They show that the resulting sentence embeddings give good performance for sentiment analysis. The **Stanford Natural Language Inference corpus** classifies sentence pairs as **entailments** (the truth of sentence i implies the truth of sentence j), **contradictions** (the truth of sentence i implies the falsity of sentence j), and neutral (i neither entails nor contradicts j). Sentence embeddings trained on this dataset transfer to a wide range of classification tasks [\(Conneau et al., 2017\)](#page-522-11).

#### <span id="page-363-1"></span>**14.8.4 Hybrid distributed-symbolic representations**

The power of distributed representations is in their generality: the distributed representation of a unit of text can serve as a summary of its meaning, and therefore as the input for downstream tasks such as classification, matching, and retrieval. For example, distributed sentence representations can be used to recognize the paraphrase relationship between closely related sentences like the following:

- <span id="page-364-3"></span><span id="page-364-1"></span><span id="page-364-0"></span>(14.5) a. Donald thanked Vlad profusely.
  - b. Donald conveyed to Vlad his profound appreciation.
  - c. Vlad was showered with gratitude by Donald.

Symbolic representations are relatively brittle to this sort of variation, but are better suited to describe individual entities, the things that they do, and the things that are done to them. In examples [\(14.5a\)](#page-364-0)-[\(14.5c\)](#page-364-1), we not only know that somebody thanked someone else, but we can make a range of inferences about what has happened between the entities named *Donald* and *Vlad*. Because distributed representations do not treat entities symbolically, they lack the ability to reason about the roles played by entities across a sentence or larger discourse.[5](#page-364-2) A hybrid between distributed and symbolic representations might give the best of both worlds: robustness to the many different ways of describing the same event, plus the expressiveness to support inferences about entities and the roles that they play.

A "top-down" hybrid approach is to begin with logical semantics (of the sort described in the previous two chapters), and but replace the predefined lexicon with a set of distributional word clusters [\(Poon and Domingos, 2009;](#page-555-8) [Lewis and Steedman, 2013\)](#page-543-7). A "bottom-up" approach is to add minimal symbolic structure to existing distributed representations, such as vector representations for each entity [\(Ji and Eisenstein, 2015;](#page-537-5) [Wiseman](#page-569-5) [et al., 2016\)](#page-569-5). This has been shown to improve performance on two problems that we will encounter in the following chapters: classification of **discourse relations** between adjacent sentences [\(chapter 16;](#page-396-0) [Ji and Eisenstein, 2015\)](#page-537-5), and **coreference resolution** of entity mentions [\(chapter 15;](#page-368-0) [Wiseman et al., 2016;](#page-569-5) [Ji et al., 2017\)](#page-537-6). Research on hybrid semantic representations is still in an early stage, and future representations may deviate more boldly from existing symbolic and distributional approaches.

# **Additional resources**

[Turney and Pantel](#page-566-7) [\(2010\)](#page-566-7) survey a number of facets of vector word representations, focusing on matrix factorization methods. [Schnabel et al.](#page-560-8) [\(2015\)](#page-560-8) highlight problems with similarity-based evaluations of word embeddings, and present a novel evaluation that controls for word frequency. [Baroni et al.](#page-513-6) [\(2014\)](#page-513-6) address linguistic issues that arise in attempts to combine distributed and compositional representations.

In bilingual and multilingual distributed representations, embeddings are estimated for translation pairs or tuples, such as (*dog, perro, chien*). These embeddings can improve machine translation [\(Zou et al., 2013;](#page-573-2) [Klementiev et al., 2012\)](#page-540-6), transfer natural language

<span id="page-364-2"></span><sup>5</sup>At a 2014 workshop on semantic parsing, this critique of distributed representations was expressed by Ray Mooney — a leading researcher in computational semantics — in a now well-known quote, "you can't cram the meaning of a whole sentence into a single vector!"

processing models across languages (Täckström et al., 2012), and make monolingual word embeddings more accurate (Faruqui and Dyer, 2014). A typical approach is to learn a projection that maximizes the correlation of the distributed representations of each element in a translation pair, which can be obtained from a bilingual dictionary. Distributed representations can also be linked to perceptual information, such as image features. Bruni et al. (2014) use textual descriptions of images to obtain visual contextual information for various words, which supplements traditional distributional context. Image features can also be inserted as contextual information in log bilinear language models (Kiros et al., 2014), making it possible to automatically generate text descriptions of images.

### Exercises

- 1. Prove that the sum of probabilities of paths through a hierarchical softmax tree is equal to one.
- 2. In skipgram word embeddings, the negative sampling objective can be written as,

$$\mathcal{L} = \sum_{i \in \mathcal{V}} \sum_{j \in \mathcal{C}} \operatorname{count}(i, j) \psi(i, j),$$
[14.29]

with  $\psi(i, j)$  is defined in Equation 14.23.

Suppose we draw the negative samples from the empirical unigram distribution  $\hat{p}(i) = p_{unigram}(i)$ . First, compute the expectation of  $\mathcal{L}$  with respect the negative samples, using this probability.

Next, take the derivative of this expectation with respect to the score of a single word context pair  $\sigma(u_i \cdot v_j)$ , and solve for the pointwise mutual information PMI(i, j). You should be able to show that at the optimum, the PMI is a simple function of  $\sigma(u_i \cdot v_j)$  and the number of negative samples.

(This exercise is part of a proof that shows that skipgram with negative sampling is closely related to PMI-weighted matrix factorization.)

- 3. \* In Brown clustering, prove that the cluster merge that maximizes the average mutual information (Equation 14.13) also maximizes the log-likelihood objective (Equation 14.12).
- 4. A simple way to compute a distributed phrase representation is to add up the distributed representations of the words in the phrase. Consider a sentiment analysis model in which the predicted sentiment is,  $\psi(\boldsymbol{w}) = \boldsymbol{\theta} \cdot (\sum_{m=1}^{M} \boldsymbol{x}_m)$ , where  $\boldsymbol{x}_m$  is the vector representation of word m. Prove that in such a model, the following two

#### 14.8. DISTRIBUTED REPRESENTATIONS OF MULTIWORD UNITS 349

inequalities cannot both hold:

$$\psi(good) > \psi(not \ good)$$
 [14.30]

$$\psi(bad) < \psi(not \ bad).$$
[14.31]

Then construct a similar example pair for the case in which phrase representations are the *average* of the word representations.

5. Now let's consider a slight modification to the prediction model in the previous problem:

$$\psi(\boldsymbol{w}) = \boldsymbol{\theta} \cdot \operatorname{ReLU}(\sum_{m=1}^{M} \boldsymbol{x}_m)$$
 [14.32]

Show that in this case, it *is* possible to achieve the inequalities above. Your solution should provide the weights θ and the embeddings xgood, xbad, and xnot.

For the next two problems, download a set of pre-trained word embeddings, such as the WORD2VEC or polyglot embeddings.

- 6. Use cosine similarity to find the most similar words to: *dog, whale, before, however, fabricate*.
- 7. Use vector addition and subtraction to compute target vectors for the analogies below. After computing each target vector, find the top three candidates by cosine similarity.
  - *dog:puppy :: cat: ?*
  - *speak:speaker :: sing:?*
  - *France:French :: England:?*
  - *France:wine :: England:?*

The remaining problems will require you to build a classifier and test its properties. Pick a text classification dataset, such as the Cornell Movie Review data.[6](#page-366-0) Divide your data into training (60%), development (20%), and test sets (20%), if no such division already exists.

8. Train a convolutional neural network, with inputs set to pre-trained word embeddings from the previous two problems. Use an additional, fine-tuned embedding for out-of-vocabulary words. Train until performance on the development set does not improve. You can also use the development set to tune the model architecture, such as the convolution width and depth. Report F -MEASURE and accuracy, as well as training time.

<span id="page-366-0"></span><sup>6</sup>http://www.cs.cornell.edu/people/pabo/movie-review-data/

- 9. Now modify your model from the previous problem to fine-tune the word embeddings. Report F -MEASURE, accuracy, and training time.
- 10. Try a simpler approach, in which word embeddings in the document are averaged, and then this average is passed through a feed-forward neural network. Again, use the development data to tune the model architecture. How close is the accuracy to the convolutional networks from the previous problems?

# <span id="page-368-2"></span><span id="page-368-0"></span>**Chapter 15 Reference Resolution**

References are one of the most noticeable forms of linguistic ambiguity, afflicting not just automated natural language processing systems, but also fluent human readers. Warnings to avoid "ambiguous pronouns" are ubiquitous in manuals and tutorials on writing style. But referential ambiguity is not limited to pronouns, as shown in the text in [Fig](#page-369-2)[ure 15.1.](#page-369-2) Each of the bracketed substrings refers to an entity that is introduced earlier in the passage. These references include the pronouns *he* and *his*, but also the shortened name *Cook*, and **nominals** such as *the firm* and *the firm's biggest growth market*.

**Reference resolution** subsumes several subtasks. This chapter will focus on **coreference resolution**, which is the task of grouping spans of text that refer to a single underlying entity, or, in some cases, a single event: for example, the spans *Tim Cook*, *he*, and *Cook* are all **coreferent**. These individual spans are called **mentions**, because they mention an entity; the entity is sometimes called the **referent**. Each mention has a set of **antecedents**, which are preceding mentions that are coreferent; for the first mention of an entity, the antecedent set is empty. The task of **pronominal anaphora resolution** requires identifying only the antecedents of pronouns. In **entity linking**, references are resolved not to other spans of text, but to entities in a knowledge base. This task is discussed in [chapter 17.](#page-420-0)

Coreference resolution is a challenging problem for several reasons. Resolving different types of **referring expressions** requires different types of reasoning: the features and methods that are useful for resolving pronouns are different from those that are useful to resolve names and nominals. Coreference resolution involves not only linguistic reasoning, but also world knowledge and pragmatics: you may not have known that China was Apple's biggest growth market, but it is likely that you effortlessly resolved this reference while reading the passage in [Figure 15.1.](#page-369-2)[1](#page-368-1) A further challenge is that coreference

<span id="page-368-1"></span><sup>1</sup>This interpretation is based in part on the assumption that a **cooperative** author would not use the expression *the firm's biggest growth market* to refer to an entity not yet mentioned in the article [\(Grice, 1975\)](#page-532-6). **Pragmatics** is the discipline of linguistics concerned with the formalization of such assumptions [\(Huang,](#page-536-9)

<span id="page-369-4"></span><span id="page-369-2"></span>(15.1) *[[Apple Inc] Chief Executive Tim Cook] has jetted into [China] for talks with government officials as [he] seeks to clear up a pile of problems in [[the firm] 's biggest growth market] ... [Cook] is on [his] first trip to [the country] since taking over...*

Figure 15.1: Running example [\(Yee and Jones, 2012\)](#page-571-8). Coreferring entity mentions are in brackets.

resolution decisions are often entangled: each mention adds information about the entity, which affects other coreference decisions. This means that coreference resolution must be addressed as a structure prediction problem. But as we will see, there is no dynamic program that allows the space of coreference decisions to be searched efficiently.

# <span id="page-369-0"></span>**15.1 Forms of referring expressions**

There are three main forms of referring expressions — pronouns, names, and nominals.

### <span id="page-369-1"></span>**15.1.1 Pronouns**

Pronouns are a closed class of words that are used for references. A natural way to think about pronoun resolution is SMASH [\(Kehler, 2007\)](#page-539-11):

- **S**earch for candidate antecedents;
- **M**atch against hard agreement constraints;
- **A**nd **S**elect using **H**euristics, which are "soft" constraints such as recency, syntactic prominence, and parallelism.

#### **Search**

In the search step, candidate antecedents are identified from the preceding text or speech.[2](#page-369-3) Any noun phrase can be a candidate antecedent, and pronoun resolution usually requires

(15.1) Many years later, as [he] faced the firing squad, [Colonel Aureliano Buend´ıa] was to remember that distant afternoon when [his] father took him to discover ice.

<span id="page-369-3"></span>[<sup>2015\)</sup>](#page-536-9).

<sup>2</sup>Pronouns whose referents come later are known as **cataphora**, as in the opening line from a novel by [Marquez](#page-547-10) [\(1970\)](#page-547-10): ´

#### <span id="page-370-1"></span>15.1. FORMS OF REFERRING EXPRESSIONS 353

parsing the text to identify all such noun phrases.[3](#page-370-0) Filtering heuristics can help to prune the search space to noun phrases that are likely to be coreferent [\(Lee et al., 2013;](#page-542-5) [Durrett](#page-525-7) [and Klein, 2013\)](#page-525-7). In nested noun phrases, mentions are generally considered to be the largest unit with a given **head word** (see § [10.5.2\)](#page-261-0): thus, *Apple Inc. Chief Executive Tim Cook* would be included as a mention, but *Tim Cook* would not, since they share the same head word, *Cook*.

#### **Matching constraints for pronouns**

References and their antecedents must agree on semantic features such as number, person, gender, and animacy. Consider the pronoun *he* in this passage from the running example:

(15.2) Tim Cook has jetted in for talks with officials as [he] seeks to clear up a pile of problems...

The pronoun and possible antecedents have the following features:

- *he*: singular, masculine, animate, third person
- *officials*: plural, animate, third person
- *talks*: plural, inanimate, third person
- *Tim Cook*: singular, masculine, animate, third person

The SMASH method searches backwards from *he*, discarding *officials* and *talks* because they do not satisfy the agreements constraints.

Another source of constraints comes from syntax — specifically, from the phrase structure trees discussed in [chapter 10.](#page-242-0) Consider a parse tree in which both x and y are phrasal constituents. The constituent x **c-commands** the constituent y iff the first branching node above x also dominates y. For example, in [Figure 15.2a,](#page-371-0) *Abigail* c-commands *her*, because the first branching node above *Abigail*, S, also dominates *her*. Now, if x c-commands y, **government and binding theory** [\(Chomsky, 1982\)](#page-520-12) states that y can refer to x only if it is a **reflexive pronoun** (e.g., *herself*). Furthermore, if y is a reflexive pronoun, then its antecedent must c-command it. Thus, in [Figure 15.2a,](#page-371-0) *her* cannot refer to *Abigail*; conversely, if we replace *her* with *herself*, then the reflexive pronoun *must* refer to *Abigail*, since this is the only candidate antecedent that c-commands it.

Now consider the example shown in [Figure 15.2b.](#page-371-0) Here, *Abigail* does not c-command *her*, but *Abigail's mom* does. Thus, *her* can refer to *Abigail* — and we cannot use reflexive

<span id="page-370-0"></span><sup>3</sup> In the OntoNotes coreference annotations, verbs can also be antecedents, if they are later referenced by nominals [\(Pradhan et al., 2011\)](#page-556-7):

<sup>(15.1)</sup> Sales of passenger cars [grew] 22%. [The strong growth] followed year-to-year increases.

<span id="page-371-0"></span>![](_page_371_Figure_1.jpeg)

Figure 15.2: In (a), *Abigail* c-commands *her*; in (b), *Abigail* does not c-command *her*, but *Abigail's mom* does; in (c), the scope of *Abigail* is limited by the S non-terminal, so that *she* or *her* can bind to *Abigail*, but not both.

*herself* in this context, unless we are talking about Abigail's mom. However, *her* does not have to refer to *Abigail*. Finally, [Figure 15.2c](#page-371-0) shows the how these constraints are limited. In this case, the pronoun *she* can refer to *Abigail*, because the S non-terminal puts *Abigail* outside the domain of *she*. Similarly, *her* can also refer to *Abigail*. But *she* and *her* cannot be coreferent, because *she* c-commands *her*.

#### **Heuristics**

After applying constraints, heuristics are applied to select among the remaining candidates. Recency is a particularly strong heuristic. All things equal, readers will prefer the more recent referent for a given pronoun, particularly when comparing referents that occur in different sentences. [Jurafsky and Martin](#page-538-11) [\(2009\)](#page-538-11) offer the following example:

<span id="page-371-2"></span>(15.3) The doctor found an old map in the captain's chest. Jim found an even older map hidden on the shelf. [It] described an island.

Readers are expected to prefer the older map as the referent for the pronoun *it*.

However, subjects are often preferred over objects, and this can contradict the preference for recency when two candidate referents are in the same sentence. For example,

<span id="page-371-1"></span>(15.4) Abigail loaned Lucia a book on Spanish. [She] is always trying to help people.

Here, we may prefer to link *she* to *Abigail* rather than *Lucia*, because of *Abigail*'s position in the subject role of the preceding sentence. (Arguably, this preference would not be strong enough to select *Abigail* if the second sentence were *She is visiting Valencia next month*.)

A third heuristic is parallelism:

(15.5) Abigail loaned Lucia a book on Spanish. Ozlem loaned [her] a book on Por- ¨ tuguese.

<span id="page-372-1"></span><span id="page-372-0"></span>![](_page_372_Figure_1.jpeg)

Figure 15.3: Left-to-right breadth-first tree traversal [\(Hobbs, 1978\)](#page-535-7), indicating that the search for an antecedent for *it* (NP1) would proceed in the following order: *536*; *the castle in Camelot*; *the residence of the king*; *Camelot*; *the king*. [Hobbs](#page-535-7) [\(1978\)](#page-535-7) proposes semantic constraints to eliminate *536* and *the castle in Camelot* as candidates, since they are unlikely to be the direct object of the verb *move*.

Here *Lucia* is preferred as the referent for *her*, contradicting the preference for the subject *Abigail* in the preceding example.

The recency and subject role heuristics can be unified by traversing the document in a syntax-driven fashion [\(Hobbs, 1978\)](#page-535-7): each preceding sentence is traversed breadth-first, left-to-right [\(Figure 15.3\)](#page-372-0). This heuristic successfully handles [\(15.4\)](#page-371-1): *Abigail* is preferred as the referent for *she* because the subject NP is visited first. It also handles [\(15.3\)](#page-371-2): the older map is preferred as the referent for *it* because the more recent sentence is visited first. (An alternative unification of recency and syntax is proposed by **centering theory** [\(Grosz et al.,](#page-532-7) [1995\)](#page-532-7), which is discussed in detail in [chapter 16.](#page-396-0))

In early work on reference resolution, the number of heuristics was small enough that a set of numerical weights could be set by hand [\(Lappin and Leass, 1994\)](#page-542-6). More recent work uses machine learning to quantify the importance of each of these factors. However, pronoun resolution cannot be completely solved by constraints and heuristics alone. This is shown by the classic example pair [\(Winograd, 1972\)](#page-569-6):

(15.6) The [city council] denied [the protesters] a permit because [they] advocated/feared violence.

Without reasoning about the motivations of the city council and protesters, it is unlikely that any system could correctly resolve both versions of this example.

#### <span id="page-373-1"></span><span id="page-373-0"></span>**Non-referential pronouns**

While pronouns are generally used for reference, they need not refer to entities. The following examples show how pronouns can refer to propositions, events, and speech acts.

- (15.7) a. They told me that I was too ugly for show business, but I didn't believe [it].
  - b. Elifsu saw Berthold get angry, and I saw [it] too.
  - c. Emmanuel said he worked in security. I suppose [that]'s one way to put it.

These forms of reference are generally not annotated in large-scale coreference resolution datasets such as OntoNotes [\(Pradhan et al., 2011\)](#page-556-7).

Pronouns may also have **generic referents**:

- (15.8) a. A poor carpenter blames [her] tools.
  - b. On the moon, [you] have to carry [your] own oxygen.
  - c. Every farmer who owns a donkey beats [it]. [\(Geach, 1962\)](#page-530-8)

In the OntoNotes dataset, coreference is not annotated for generic referents, even in cases like these examples, in which the same generic entity is mentioned multiple times.

Some pronouns do not refer to anything at all:

- (15.9) a. *[It]'s raining.*
  - [Il] pleut. (Fr)
  - b. [It] 's money that she's really after.
  - c. [It] is too bad that we have to work so hard.

How can we automatically distinguish these usages of *it* from referential pronouns? Consider the the difference between the following two examples [\(Bergsma et al., 2008\)](#page-514-11):

- (15.10) a. You can make [it] in advance.
  - b. You can make [it] in showbiz.

In the second example, the pronoun *it* is non-referential. One way to see this is by substituting another pronoun, like *them*, into these examples:

- (15.11) a. You can make [them] in advance.
  - b. ? You can make [them] in showbiz.

The questionable grammaticality of the second example suggests that *it* is not referential. [Bergsma et al.](#page-514-11) [\(2008\)](#page-514-11) operationalize this idea by comparing distributional statistics for the n-grams around the word *it*, testing how often other pronouns or nouns appear in the same context. In cases where nouns and other pronouns are infrequent, the *it* is unlikely to be referential.

#### <span id="page-374-2"></span>15.1. FORMS OF REFERRING EXPRESSIONS 357

### <span id="page-374-0"></span>**15.1.2 Proper Nouns**

If a proper noun is used as a referring expression, it often corefers with another proper noun, so that the coreference problem is simply to determine whether the two names match. Subsequent proper noun references often use a shortened form, as in the running example [\(Figure 15.1\)](#page-369-2):

(15.12) Apple Inc Chief Executive [Tim Cook] has jetted into China . . . [Cook] is on his first business trip to the country . . .

A typical solution for proper noun coreference is to match the syntactic head words of the reference with the referent. In § [10.5.2,](#page-261-0) we saw that the head word of a phrase can be identified by applying head percolation rules to the phrasal parse tree; alternatively, the head can be identified as the root of the dependency subtree covering the name. For sequences of proper nouns, the head word will be the final token.

There are a number of caveats to the practice of matching head words of proper nouns.

- In the European tradition, family names tend to be more specific than given names, and family names usually come last. However, other traditions have other practices: for example, in Chinese names, the family name typically comes first; in Japanese, honorifics come after the name, as in *Nobu-San* (*Mr. Nobu*).
- In organization names, the head word is often not the most informative, as in *Georgia Tech* and *Virginia Tech*. Similarly, *Lebanon* does not refer to the same entity as *Southern Lebanon*, necessitating special rules for the specific case of geographical modifiers [\(Lee et al., 2011\)](#page-542-7).
- Proper nouns can be nested, as in *[the CEO of [Microsoft]]*, resulting in head word match without coreference.

Despite these difficulties, proper nouns are the easiest category of references to resolve [\(Stoyanov et al., 2009\)](#page-563-10). In machine learning systems, one solution is to include a range of matching features, including exact match, head match, and string inclusion. In addition to matching features, competitive systems (e.g., [Bengtson and Roth, 2008\)](#page-513-7) include large lists, or **gazetteers**, of acronyms (e.g, *the National Basketball Association*/*NBA*), demonyms (e.g., *the Israelis*/*Israel*), and other aliases (e.g., *the Georgia Institute of Technology*/*Georgia Tech*).

### <span id="page-374-1"></span>**15.1.3 Nominals**

In coreference resolution, noun phrases that are neither pronouns nor proper nouns are referred to as **nominals**. In the running example [\(Figure 15.1\)](#page-369-2), nominal references include: *the firm* (*Apple Inc*); *the firm's biggest growth market* (*China*); and *the country* (*China*).

<span id="page-375-2"></span>Nominals are especially difficult to resolve (Denis and Baldridge, 2007; Durrett and Klein, 2013), and the examples above suggest why this may be the case: world knowledge is required to identify *Apple Inc* as a *firm*, and *China* as a *growth market*. Other difficult examples include the use of colloquial expressions, such as coreference between *Clinton campaign officials* and *the Clinton camp* (Soon et al., 2001).

# <span id="page-375-0"></span>15.2 Algorithms for coreference resolution

The ground truth training data for coreference resolution is a set of mention sets, where all mentions within each set refer to a single entity.<sup>4</sup> In the running example from Figure 15.1, the ground truth coreference annotation is:

$$c_1 = \{Apple \ Inc_{1:2}, the \ firm_{27:28}\}$$
 [15.1]

$$c_2 = \{Apple Inc Chief Executive Tim Cook_{1:6}, he_{17}, Cook_{33}, his_{36}\}$$
 [15.2]

$$c_3 = \{China_{10}, the firm 's biggest growth market_{27:32}, the country_{40:41}\}$$
 [15.3]

Each row specifies the token spans that mention an entity. ("Singleton" entities, which are mentioned only once (e.g., *talks*, *government officials*), are excluded from the annotations.) Equivalently, if given a set of M mentions,  $\{m_i\}_{i=1}^M$ , each mention i can be assigned to a cluster  $z_i$ , where  $z_i = z_j$  if i and j are coreferent. The cluster assignments z are invariant under permutation. The unique clustering associated with the assignment z is written c(z).

Coreference resolution can thus be viewed as a structure prediction problem, involving two subtasks: identifying which spans of text mention entities, and then clustering those spans.

**Mention identification** The task of identifying mention spans for coreference resolution is often performed by applying a set of heuristics to the phrase structure parse of each sentence. A typical approach is to start with all noun phrases and named entities, and then apply filtering rules to remove nested noun phrases with the same head (e.g., [*Apple CEO* [*Tim Cook*]]), numeric entities (e.g., [100 miles], [97%]), non-referential *it*, etc (Lee et al., 2013; Durrett and Klein, 2013). In general, these deterministic approaches err in favor of recall, since the mention clustering component can choose to ignore false positive mentions, but cannot recover from false negatives. An alternative is to consider all spans (up to some finite length) as candidate mentions, performing mention identification and clustering jointly (Daumé III and Marcu, 2005; Lee et al., 2017).

<span id="page-375-1"></span><sup>&</sup>lt;sup>4</sup>In many annotations, the term **markable** is used to refer to spans of text that can *potentially* mention an entity. The set of markables includes non-referential pronouns, which does not mention any entity. Part of the job of the coreference system is to avoid incorrectly linking these non-referential markables to any mention chains.

<span id="page-376-1"></span>**Mention clustering** The subtask of mention clustering will be the focus of the remainder of this chapter. There are two main classes of models. In *mention-based models*, the scoring function for a coreference clustering decomposes over pairs of mentions. These pairwise decisions are then aggregated, using a clustering heuristic. Mention-based coreference clustering can be treated as a fairly direct application of supervised classification or ranking. However, the mention-pair locality assumption can result in incoherent clusters, like  $\{Hillary Clinton \leftarrow Clinton \leftarrow Mr Clinton\}$ , in which the pairwise links score well, but the overall result is unsatisfactory. *Entity-based models* address this issue by scoring entities holistically. This can make inference more difficult, since the number of possible entity groupings is exponential in the number of mentions.

#### <span id="page-376-0"></span>15.2.1 Mention-pair models

In the **mention-pair model**, a binary label  $y_{i,j} \in \{0, 1\}$  is assigned to each pair of mentions (i, j), where i < j. If i and j corefer  $(z_i = z_j)$ , then  $y_{i,j} = 1$ ; otherwise,  $y_{i,j} = 0$ . The mention *he* in Figure 15.1 is preceded by five other mentions: (1) *Apple Inc*; (2) *Apple Inc Chief Executive Tim Cook*; (3) *China*; (4) *talks*; (5) *government officials*. The correct mention pair labeling is  $y_{2,6} = 1$  and  $y_{i\neq 2,6} = 0$  for all other i. If a mention j introduces a new entity, such as mention 3 in the example, then  $y_{i,j} = 0$  for all i. The same is true for "mentions" that do not refer to any entity, such as non-referential pronouns. If mention j refers to an entity that has been mentioned more than once, then  $y_{i,j} = 1$  for all i < j that mention the referent.

By transforming coreference into a set of binary labeling problems, the mention-pair model makes it possible to apply an off-the-shelf binary classifier (Soon et al., 2001). This classifier is applied to each mention j independently, searching backwards from j until finding an antecedent i which corefers with j with high confidence. After identifying a single **antecedent**, the remaining mention pair labels can be computed by transitivity: if  $y_{i,j} = 1$  and  $y_{j,k} = 1$ , then  $y_{i,k} = 1$ .

Since the ground truth annotations give entity chains c but not individual mentionpair labels y, an additional heuristic must be employed to convert the labeled data into training examples for classification. A typical approach is to generate at most one positive labeled instance  $y_{a_j,j} = 1$  for mention j, where  $a_j$  is the index of the most recent antecedent,  $a_j = \max\{i : i < j \land z_i = z_j\}$ . Negative labeled instances are generated for all for all  $i \in \{a_j + 1, \dots, j\}$ . In the running example, the most recent antecedent of the pronoun *he* is  $a_6 = 2$ , so the training data would be  $y_{2,6} = 1$  and  $y_{3,6} = y_{4,6} = y_{5,6} = 0$ . The variable  $y_{1,6}$  is not part of the training data, because the first mention appears before the true antecedent  $a_6 = 2$ .

#### <span id="page-377-1"></span><span id="page-377-0"></span>15.2.2 Mention-ranking models

In **mention ranking** (Denis and Baldridge, 2007), the classifier learns to identify a single antecedent  $a_i \in \{\epsilon, 1, 2, ..., i - 1\}$  for each referring expression *i*,

$$\hat{a}_i = \operatorname*{argmax}_{a \in \{\epsilon, 1, 2, \dots, i-1\}} \psi_M(a, i),$$
[15.4]

where  $\psi_M(a, i)$  is a score for the mention pair (a, i). If  $a = \epsilon$ , then mention *i* does not refer to any previously-introduced entity — it is not **anaphoric**. Mention-ranking is similar to the mention-pair model, but all candidates are considered simultaneously, and at most a single antecedent is selected. The mention-ranking model explicitly accounts for the possibility that mention *i* is not anaphoric, through the score  $\psi_M(\epsilon, i)$ . The determination of anaphoricity can be made by a special classifier in a preprocessing step, so that non- $\epsilon$ antecedents are identified only for spans that are determined to be anaphoric (Denis and Baldridge, 2008).

As a learning problem, ranking can be trained using the same objectives as in discriminative classification. For each mention *i*, we can define a gold antecedent  $a_i^*$ , and an associated loss, such as the hinge loss,  $\ell_i = (1 - \psi_M(a_i^*, i) + \psi_M(\hat{a}, i))_+$  or the negative log-likelihood,  $\ell_i = -\log p(a_i^* | i; \theta)$ . (For more on learning to rank, see § 17.1.1.) But as with the mention-pair model, there is a mismatch between the labeled data, which comes in the form of mention sets, and the desired supervision, which would indicate the specific antecedent of each mention. The antecedent variables  $\{a_i\}_{i=1}^M$  relate to the mention sets in a many-to-one mapping: each set of antecedents induces a single clustering, but a clustering can correspond to many different settings of antecedent variables.

A heuristic solution is to set  $a_i^* = \max\{j : j < i \land z_j = z_i\}$ , the most recent mention in the same cluster as *i*. But the most recent mention may not be the most informative: in the running example, the most recent antecedent of the mention *Cook* is the pronoun *he*, but a more useful antecedent is the earlier mention *Apple Inc Chief Executive Tim Cook*. Rather than selecting a specific antecedent to train on, the antecedent can be treated as a latent variable, in the manner of the **latent variable perceptron** from § 12.4.2 (Fernandes et al., 2014):

$$\hat{\boldsymbol{a}} = \underset{\boldsymbol{a}}{\operatorname{argmax}} \sum_{i=1}^{M} \psi_M(a_i, i)$$
[15.5]

$$\boldsymbol{a}^* = \operatorname*{argmax}_{\boldsymbol{a} \in \mathcal{A}(\boldsymbol{c})} \sum_{i=1}^{M} \psi_M(a_i, i)$$
[15.6]

$$\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} + \sum_{i=1}^{M} \frac{\partial L}{\partial \boldsymbol{\theta}} \psi_M(a_i^*, i) - \sum_{i=1}^{M} \frac{\partial L}{\partial \boldsymbol{\theta}} \psi_M(\hat{a}_i, i)$$
[15.7]

<span id="page-378-1"></span>where  $\mathcal{A}(c)$  is the set of antecedent structures that is compatible with the ground truth coreference clustering *c*. Another alternative is to sum over all the conditional probabilities of antecedent structures that are compatible with the ground truth clustering (Durrett and Klein, 2013; Lee et al., 2017). For the set of mention *m*, we compute the following probabilities:

$$\mathbf{p}(\boldsymbol{c} \mid \boldsymbol{m}) = \sum_{\boldsymbol{a} \in \mathcal{A}(\boldsymbol{c})} \mathbf{p}(\boldsymbol{a} \mid \boldsymbol{m}) = \sum_{\boldsymbol{a} \in \mathcal{A}(\boldsymbol{c})} \prod_{i=1}^{M} \mathbf{p}(a_i \mid i, \boldsymbol{m})$$
[15.8]

$$p(a_i \mid i, m) = \frac{\exp(\psi_M(a_i, i))}{\sum_{a' \in \{\epsilon, 1, 2, \dots, i-1\}} \exp(\psi_M(a', i))}.$$
[15.9]

This objective rewards models that assign high scores for all valid antecedent structures. In the running example, this would correspond to summing the probabilities of the two valid antecedents for *Cook*, *he* and *Apple Inc Chief Executive Tim Cook*. In one of the exercises, you will compute the number of valid antecedent structures for a given clustering.

#### <span id="page-378-0"></span>**15.2.3** Transitive closure in mention-based models

A problem for mention-based models is that individual mention-level decisions may be incoherent. Consider the following mentions:

$$m_1 = Hillary Clinton$$
 [15.10]

$$m_2 = Clinton$$
 [15.11]

$$m_3 = Bill \ Clinton$$
 [15.12]

A mention-pair system might predict  $\hat{y}_{1,2} = 1$ ,  $\hat{y}_{2,3} = 1$ ,  $\hat{y}_{1,3} = 0$ . Similarly, a mentionranking system might choose  $\hat{a}_2 = 1$  and  $\hat{a}_3 = 2$ . Logically, if mentions 1 and 3 are both coreferent with mention 2, then all three mentions must refer to the same entity. This constraint is known as **transitive closure**.

Transitive closure can be applied *post hoc*, revising the independent mention-pair or mention-ranking decisions. However, there are many possible ways to enforce transitive closure: in the example above, we could set  $\hat{y}_{1,3} = 1$ , or  $\hat{y}_{1,2} = 0$ , or  $\hat{y}_{2,3} = 0$ . For documents with many mentions, there may be many violations of transitive closure, and many possible fixes. Transitive closure can be enforced by always adding edges, so that  $\hat{y}_{1,3} = 1$  is preferred (e.g., Soon et al., 2001), but this can result in overclustering, with too many mentions grouped into too few entities.

Mention-pair coreference resolution can be viewed as a constrained optimization prob-

lem,

$$\begin{aligned} \max_{\boldsymbol{y} \in \{0,1\}^M} \quad & \sum_{j=1}^M \sum_{i=1}^j \psi_M(i,j) \times y_{i,j} \\ \text{s.t.} \quad & y_{i,j} + y_{j,k} - 1 \leq y_{i,k}, \quad \forall i < j < k, \end{aligned}$$

with the constraint enforcing transitive closure. This constrained optimization problem is equivalent to graph partitioning with positive and negative edge weights: construct a graph where the nodes are mentions, and the edges are the pairwise scores ψM(i, j); the goal is to partition the graph so as to maximize the sum of the edge weights between all nodes within the same partition [\(McCallum and Wellner, 2004\)](#page-547-11). This problem is NP-hard, motivating approximations such as correlation clustering [\(Bansal et al., 2004\)](#page-512-9) and **integer linear programming** [\(Klenner, 2007;](#page-540-8) [Finkel and Manning, 2008,](#page-528-10) also see § [13.2.2\)](#page-332-0).

#### <span id="page-379-0"></span>**15.2.4 Entity-based models**

A weakness of mention-based models is that they treat coreference resolution as a classification or ranking problem, when it is really a clustering problem: the goal is to group the mentions together into clusters that correspond to the underlying entities. Entity-based approaches attempt to identify these clusters directly. Such methods require a scoring function at the entity level, measuring whether each set of mentions is internally consistent. Coreference resolution can then be viewed as the following optimization,

<span id="page-379-1"></span>
$$\max_{z} \quad \sum_{e=1} \psi_E(\{i : z_i = e\}),$$
[15.13]

where z<sup>i</sup> indicates the entity referenced by mention i, and ψE({i : z<sup>i</sup> = e}) is a scoring function applied to all mentions i that are assigned to entity e.

Entity-based coreference resolution is conceptually similar to the unsupervised clustering problems encountered in [chapter 5:](#page-112-0) the goal is to obtain clusters of mentions that are internally coherent. The number of possible clusterings of n items is the **Bell number**, which is defined by the following recurrence [\(Bell, 1934;](#page-513-8) [Luo et al., 2004\)](#page-546-6),

$$B_n = \sum_{k=0}^{n-1} B_k \binom{n-1}{k} B_0 = B_1 = 1.$$
 [15.14]

This recurrence is illustrated by the Bell tree, which is applied to a short coreference problem in [Figure 15.4.](#page-380-1) The Bell number B<sup>n</sup> grows exponentially with n, making exhaustive search of the space of clusterings impossible. For this reason, entity-based coreference resolution typically involves incremental search, in which clustering decisions are based on local evidence, in the hope of approximately optimizing the full objective in [Equa](#page-379-1)[tion 15.13.](#page-379-1) This approach is sometimes called **cluster ranking**, in contrast to mention ranking.

<span id="page-379-2"></span>

<span id="page-380-2"></span><span id="page-380-1"></span>![](_page_380_Figure_1.jpeg)

Figure 15.4: The Bell Tree for the sentence *Abigail hopes she speaks with her*. Which paths are excluded by the syntactic constraints mentioned in § [15.1.1?](#page-369-1)

**\*Generative models of coreference** Entity-based coreference can be approached through probabilistic **generative models**, in which the mentions in the document are conditioned on a set of latent entities [\(Haghighi and Klein, 2007,](#page-532-8) [2010\)](#page-533-8). An advantage of these methods is that they can be learned from unlabeled data [\(Poon and Domingos, 2008,](#page-555-9) e.g.,); a disadvantage is that probabilistic inference is required not just for learning, but also for prediction. Furthermore, generative models require independence assumptions that are difficult to apply in coreference resolution, where the diverse and heterogeneous features do not admit an easy decomposition into mutually independent subsets.

#### <span id="page-380-0"></span>**Incremental cluster ranking**

The SMASH method (§ [15.1.1\)](#page-369-1) can be extended to entity-based coreference resolution by building up coreference clusters while moving through the document [\(Cardie and Wagstaff,](#page-517-6) [1999\)](#page-517-6). At each mention, the algorithm iterates backwards through possible antecedent clusters; but unlike SMASH, a cluster is selected only if *all* members of its cluster are compatible with the current mention. As mentions are added to a cluster, so are their features (e.g., gender, number, animacy). In this way, incoherent chains like {*Hillary Clinton*, *Clinton*, *Bill Clinton*} can be avoided. However, an incorrect assignment early in the document — a **search error** — might lead to a cascade of errors later on.

More sophisticated search strategies can help to ameliorate the risk of search errors. One approach is **beam search** (first discussed in § [11.3\)](#page-285-0), in which a set of hypotheses is maintained throughout search. Each hypothesis represents a path through the Bell tree [\(Figure 15.4\)](#page-380-1). Hypotheses are "expanded" either by adding the next mention to an existing cluster, or by starting a new cluster. Each expansion receives a score, based on [Equation 15.13,](#page-379-1) and the top K hypotheses are kept on the beam as the algorithm moves to the next step.

<span id="page-381-1"></span>Incremental cluster ranking can be made more accurate by performing multiple passes over the document, applying rules (or "sieves") with increasing recall and decreasing precision at each pass (Lee et al., 2013). In the early passes, coreference links are proposed only between mentions that are highly likely to corefer (e.g., exact string match for full names and nominals). Information can then be shared among these mentions, so that when more permissive matching rules are applied later, agreement is preserved across the entire cluster. For example, in the case of {*Hillary Clinton*, *Clinton*, *she*}, the name-matching sieve would link *Clinton* and *Hillary Clinton*, and the pronoun-matching sieve would then link *she* to the combined cluster. A deterministic multi-pass system won nearly every track of the 2011 CoNLL shared task on coreference resolution (Pradhan et al., 2011). Given the dominance of machine learning in virtually all other areas of natural language processing — and more than fifteen years of prior work on machine learning for coreference — this was a surprising result, even if learning-based methods have subsequently regained the upper hand (e.g., Lee et al., 2018, the state of the art at the time of this writing).

#### **Incremental perceptron**

Incremental coreference resolution can be learned with the **incremental perceptron**, as described in § 11.3.2. At mention *i*, each hypothesis on the beam corresponds to a clustering of mentions  $1 \dots i - 1$ , or equivalently, a path through the Bell tree up to position i - 1. As soon as none of the hypotheses on the beam are compatible with the gold coreference clustering, a perceptron update is made (Daumé III and Marcu, 2005). For concreteness, consider a linear cluster ranking model,

$$\psi_E(\{i: z_i = e\}) = \sum_{i: z_i = e} \boldsymbol{\theta} \cdot \boldsymbol{f}(i, \{j: j < i \land z_j = e\}),$$

$$[15.15]$$

where the score for each cluster is computed as the sum of scores of all mentions that are linked into the cluster, and  $f(i, \emptyset)$  is a set of features for the non-anaphoric mention that initiates the cluster.

Using Figure 15.4 as an example, suppose that the ground truth is,

<span id="page-381-0"></span>
$$c^* = \{Abigail, her\}, \{she\},$$
 [15.16]

but that with a beam of size one, the learner reaches the hypothesis,

$$\hat{\boldsymbol{c}} = \{Abigail, she\}.$$
[15.17]

This hypothesis is incompatible with  $c^*$ , so an update is needed:

$$\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} + \boldsymbol{f}(\boldsymbol{c}^*) - \boldsymbol{f}(\hat{\boldsymbol{c}})$$
 [15.18]

$$= \boldsymbol{\theta} + (\boldsymbol{f}(Abigail, \emptyset) + \boldsymbol{f}(she, \emptyset)) - (\boldsymbol{f}(Abigail, \emptyset) + \boldsymbol{f}(she, \{Abigail\}))$$
[15.19]

$$=\boldsymbol{\theta} + \boldsymbol{f}(she, \emptyset) - \boldsymbol{f}(she, \{Abigail\}).$$
[15.20]

<span id="page-382-2"></span>This style of incremental update can also be applied to a margin loss between the gold clustering and the top clustering on the beam. By backpropagating from this loss, it is also possible to train a more complicated scoring function, such as a neural network in which the score for each entity is a function of embeddings for the entity mentions (Wiseman et al., 2015).

#### <span id="page-382-1"></span>**Reinforcement learning**

**Reinforcement learning** is a topic worthy of a textbook of its own (Sutton and Barto, 1998),<sup>5</sup> so this section will provide only a very brief overview, in the context of coreference resolution. A stochastic **policy** assigns a probability to each possible **action**, conditional on the context. The goal is to learn a policy that achieves a high expected reward, or equivalently, a low expected cost.

In incremental cluster ranking, a complete clustering on M mentions can be produced by a sequence of M actions, in which the action  $z_i$  either merges mention i with an existing cluster or begins a new cluster. We can therefore create a stochastic policy using the cluster scores (Clark and Manning, 2016),

$$\Pr(z_i = e; \boldsymbol{\theta}) = \frac{\exp \psi_E(i \cup \{j : z_j = e\}; \boldsymbol{\theta})}{\sum_{e'} \exp \psi_E(i \cup \{j : z_j = e'\}'; \boldsymbol{\theta})},$$
[15.21]

where  $\psi_E(i \cup \{j : z_j = e\}; \theta)$  is the score under parameters  $\theta$  for assigning mention *i* to cluster *e*. This score can be an arbitrary function of the mention *i*, the cluster *e* and its (possibly empty) set of mentions; it can also include the history of actions taken thus far.

If a policy assigns probability  $p(c; \theta)$  to clustering *c*, then its expected loss is,

$$L(\boldsymbol{\theta}) = \sum_{\boldsymbol{c} \in \mathcal{C}(\boldsymbol{m})} p_{\boldsymbol{\theta}}(\boldsymbol{c}) \times \ell(\boldsymbol{c}), \qquad [15.22]$$

where C(m) is the set of possible clusterings for mentions m. The loss  $\ell(c)$  can be based on any arbitrary scoring function, including the complex evaluation metrics used in coreference resolution (see § 15.4). This is an advantage of reinforcement learning, which can be trained directly on the evaluation metric — unlike traditional supervised learning, which requires a loss function that is differentiable and decomposable across individual decisions.

Rather than summing over the exponentially many possible clusterings, we can approximate the expectation by sampling trajectories of actions,  $z = (z_1, z_2, ..., z_M)$ , from

<span id="page-382-0"></span><sup>&</sup>lt;sup>5</sup>A draft of the second edition can be found here: http://incompleteideas.net/book/ the-book-2nd.html. Reinforcement learning has been used in spoken dialogue systems (Walker, 2000) and text-based game playing (Branavan et al., 2009), and was applied to coreference resolution by Clark and Manning (2015).

<span id="page-383-0"></span>the current policy. Each action  $z_i$  corresponds to a step in the Bell tree: adding mention  $m_i$  to an existing cluster, or forming a new cluster. Each trajectory z corresponds to a single clustering c, and so we can write the loss of an action sequence as  $\ell(c(z))$ . The **policy gradient** algorithm computes the gradient of the expected loss as an expectation over trajectories (Sutton et al., 2000),

$$\frac{\partial}{\partial \boldsymbol{\theta}} L(\boldsymbol{\theta}) = E_{\boldsymbol{z} \sim \mathcal{Z}(\boldsymbol{m})} \ell(\boldsymbol{c}(\boldsymbol{z})) \sum_{i=1}^{M} \frac{\partial}{\partial \boldsymbol{\theta}} \log p(z_i \mid \boldsymbol{z}_{1:i-1}, \boldsymbol{m})$$
[15.23]

$$\approx \frac{1}{K} \sum_{k=1}^{K} \ell(\boldsymbol{c}(\boldsymbol{z}^{(k)})) \sum_{i=1}^{M} \frac{\partial}{\partial \boldsymbol{\theta}} \log p(\boldsymbol{z}_{i}^{(k)} \mid \boldsymbol{z}_{1:i-1}^{(k)}, \boldsymbol{m}), \quad [15.24]$$

where each action sequence  $z^{(k)}$  is sampled from the current policy. Unlike the incremental perceptron, an update is not made until the complete action sequence is available.

#### Learning to search

Policy gradient can suffer from high variance: while the average loss over *K* samples is asymptotically equal to the expected reward of a given policy, this estimate may not be accurate unless *K* is very large. This can make it difficult to allocate credit and blame to individual actions. In **learning to search**, this problem is addressed through the addition of an **oracle** policy, which is known to receive zero or small loss. The oracle policy can be used in two ways:

- The oracle can be used to generate partial hypotheses that are likely to score well, by generating *i* actions from the initial state. These partial hypotheses are then used as starting points for the learned policy. This is known as **roll-in**.
- The oracle can be used to compute the minimum possible loss from a given state, by generating M i actions from the current state until completion. This is known as **roll-out**.

The oracle can be combined with the existing policy during both roll-in and roll-out, sampling actions from each policy (Daumé III et al., 2009). One approach is to gradually decrease the number of actions drawn from the oracle over the course of learning (Ross et al., 2011).

In the context of entity-based coreference resolution, Clark and Manning (2016) use the learned policy for roll-in and the oracle policy for roll-out. Algorithm 17 shows how the gradients on the policy weights are computed in this case. In this application, the oracle is "noisy", because it selects the action that minimizes only the *local* loss — the accuracy of the coreference clustering up to mention i — rather than identifying the action sequence that will lead to the best final coreference clustering on the entire document.

| Alg | Algorithm 17 Learning to search for entity-based coreference resolution                                                        |                                                                               |  |  |  |  |  |
|-----|--------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------|--|--|--|--|--|
| 1:  | 1: <b>procedure</b> COMPUTE-GRADIENT(mentions <i>m</i> , loss function $\ell$ , parameters $\theta$ )                          |                                                                               |  |  |  |  |  |
| 2:  | $L(\boldsymbol{\theta}) \leftarrow 0$                                                                                          |                                                                               |  |  |  |  |  |
| 3:  | 1 1 1 2                                                                                                                        | Sample a trajectory from the current policy                                   |  |  |  |  |  |
| 4:  | for $i \in \{1, 2, \dots M\}$ do                                                                                               |                                                                               |  |  |  |  |  |
| 5:  | for action $z \in \mathcal{Z}(oldsymbol{z}_{1:i-1},oldsymbol{m})$ do                                                           | $\triangleright$ All possible actions after history $oldsymbol{z}_{1:i-1}$    |  |  |  |  |  |
| 6:  | $\boldsymbol{h} \leftarrow \boldsymbol{z}_{1:i-1} \oplus z$                                                                    | $\triangleright$ Concatenate history $\boldsymbol{z}_{1:i-1}$ with action $z$ |  |  |  |  |  |
| 7:  | for $j \in \{i+1, i+2, \dots, M\}$ do                                                                                          | ⊳ Roll-out                                                                    |  |  |  |  |  |
| 8:  | $h_j \leftarrow \operatorname{argmin}_h \ell(\boldsymbol{h}_{1:j-1} \oplus h)$                                                 | Oracle selects action with minimum loss                                       |  |  |  |  |  |
| 9:  | $L(oldsymbol{	heta}) \leftarrow L(oldsymbol{	heta}) + \mathrm{p}(z \mid oldsymbol{z}_{1:i-1}, oldsymbol{m}; oldsymbol{	heta})$ | $(\boldsymbol{\theta}) \times \ell(\boldsymbol{h})$ > Update expected loss    |  |  |  |  |  |
| 10: | return $\frac{\partial}{\partial \boldsymbol{\theta}} L(\boldsymbol{\theta})$                                                  |                                                                               |  |  |  |  |  |

<span id="page-384-2"></span>Algorithm 17 Learning to search for entity-based coreference resolution

When learning from noisy oracles, it can be helpful to mix in actions from the current policy with the oracle during roll-out (Chang et al., 2015).

# <span id="page-384-0"></span>**15.3** Representations for coreference resolution

Historically, coreference resolution has employed an array of hand-engineered features to capture the linguistic constraints and preferences described in § 15.1 (Soon et al., 2001). Later work has documented the utility of lexical and bilexical features on mention pairs (Björkelund and Nugues, 2011; Durrett and Klein, 2013). The most recent and successful methods replace many (but not all) of these features with distributed representations of mentions and entities (Wiseman et al., 2015; Clark and Manning, 2016; Lee et al., 2017).

### <span id="page-384-1"></span>15.3.1 Features

Coreference features generally rely on a preprocessing pipeline to provide part-of-speech tags and phrase structure parses. This pipeline makes it possible to design features that capture many of the phenomena from § 15.1, and is also necessary for typical approaches to mention identification. However, the pipeline may introduce errors that propagate to the downstream coreference clustering system. Furthermore, the existence of such a pipeline presupposes resources such as treebanks, which do not exist for many languages.<sup>6</sup>

<span id="page-384-3"></span><sup>&</sup>lt;sup>6</sup>The Universal Dependencies project has produced dependency treebanks for more than sixty languages. However, coreference features and mention detection are generally based on phrase structure trees, which exist for roughly two dozen languages. A list is available here: https://en.wikipedia.org/wiki/ Treebank

### **Mention features**

Features of individual mentions can help to predict anaphoricity. In systems where mention detection is performed jointly with coreference resolution, these features can also predict whether a span of text is likely to be a mention. For mention i, typical features include:

- **Mention type.** Each span can be identified as a pronoun, name, or nominal, using the part-of-speech of the head word of the mention: both the Penn Treebank and Universal Dependencies tagsets (§ [8.1.1\)](#page-193-0) include tags for pronouns and proper nouns, and all other heads can be marked as nominals [\(Haghighi and Klein, 2009\)](#page-533-9).
- **Mention width.** The number of tokens in a mention is a rough predictor of its anaphoricity, with longer mentions being less likely to refer back to previously-defined entities.
- **Lexical features.** The first, last, and head words can help to predict anaphoricity; they are also useful in conjunction with features such as mention type and part-of-speech, providing a rough measure of agreement [\(Bjorkelund and Nugues, 2011\)](#page-514-12). The num- ¨ ber of lexical features can be very large, so it can be helpful to select only frequentlyoccurring features [\(Durrett and Klein, 2013\)](#page-525-7).
- **Morphosyntactic features.** These features include the part-of-speech, number, gender, and dependency ancestors.

The features for mention i and candidate antecedent a can be conjoined, producing joint features that can help to assess the compatibility of the two mentions. For example, [Durrett and Klein](#page-525-7) [\(2013\)](#page-525-7) conjoin each feature with the mention types of the anaphora and the antecedent. Coreference resolution corpora such as ACE and OntoNotes contain documents from various genres. By conjoining the genre with other features, it is possible to learn genre-specific feature weights.

#### **Mention-pair features**

For any pair of mentions i and j, typical features include:

**Distance.** The number of intervening tokens, mentions, and sentences between i and j can all be used as distance features. These distances can be computed on the surface text, or on a transformed representation reflecting the breadth-first tree traversal [\(Figure 15.3\)](#page-372-0). Rather than using the distances directly, they are typically binned, creating binary features.

- <span id="page-386-1"></span>**String match.** A variety of string match features can be employed: exact match, suffix match, head match, and more complex matching rules that disregard irrelevant modifiers [\(Soon et al., 2001\)](#page-562-0).
- **Compatibility.** Building on the model, features can measure the anaphor and antecedent agree with respect to morphosyntactic attributes such as gender, number, and animacy.
- **Nesting.** If one mention is nested inside another (e.g., *[The President of [France]]*), they generally cannot corefer.
- **Same speaker.** For documents with quotations, such as news articles, personal pronouns can be resolved only by determining the speaker for each mention [\(Lee et al., 2013\)](#page-542-5). Coreference is also more likely between mentions from the same speaker.
- **Gazetteers.** These features indicate that the anaphor and candidate antecedent appear in a gazetteer of acronyms (e.g., *USA*/*United States*, *GATech*/*Georgia Tech*), demonyms (e.g., *Israel*/*Israeli*), or other aliases (e.g., *Knickerbockers*/*New York Knicks*).
- **Lexical semantics.** These features use a lexical resource such as WORDNET to determine whether the head words of the mentions are related through synonymy, antonymy, and hypernymy (§ [4.2\)](#page-90-0).
- **Dependency paths.** The dependency path between the anaphor and candidate antecedent can help to determine whether the pair can corefer, under the government and binding constraints described in § [15.1.1.](#page-369-1)

Comprehensive lists of mention-pair features are offered by [Bengtson and Roth](#page-513-7) [\(2008\)](#page-513-7) and [Rahman and Ng](#page-557-9) [\(2011\)](#page-557-9). Neural network approaches use far fewer mention-pair features: for example, [Lee et al.](#page-542-8) [\(2017\)](#page-542-8) include only speaker, genre, distance, and mention width features.

**Semantics** In many cases, coreference seems to require knowledge and semantic inferences, as in the running example, where we link *China* with a *country* and a *growth market*. Some of this information can be gleaned from WORDNET, which defines a graph over **synsets** (see § [4.2\)](#page-90-0). For example, one of the synsets of *China* is an instance of an Asian nation#1, which in turn is a hyponym of country#2, a synset that includes *country*. [7](#page-386-0) Such paths can be used to measure the similarity between concepts [\(Pedersen](#page-554-10) [et al., 2004\)](#page-554-10), and this similarity can be incorporated into coreference resolution as a feature [\(Ponzetto and Strube, 2006\)](#page-555-10). Similar ideas can be applied to knowledge graphs induced from Wikipedia [\(Ponzetto and Strube, 2007\)](#page-555-11). But while such approaches improve

<span id="page-386-0"></span><sup>7</sup>teletype font is used to indicate wordnet synsets, and *italics* is used to indicate strings.

relatively simple classification-based systems, they have proven less useful when added to the current generation of techniques.[8](#page-387-1) For example, [Durrett and Klein](#page-525-7) [\(2013\)](#page-525-7) employ a range of semantics-based features — WordNet synonymy and hypernymy relations on head words, named entity types (e.g., person, organization), and unsupervised clustering over nominal heads — but find that these features give minimal improvement over a baseline system using surface features.

### **Entity features**

Many of the features for entity-mention coreference are generated by aggregating mentionpair features over all mentions in the candidate entity [\(Culotta et al., 2007;](#page-523-10) [Rahman and](#page-557-9) [Ng, 2011\)](#page-557-9). Specifically, for each binary mention-pair feature f(i, j), we compute the following entity-mention features for mention i and entity e = {j : j < i ∧ z<sup>j</sup> = e}.

- ALL-TRUE: Feature f(i, j) holds for all mentions j ∈ e.
- MOST-TRUE: Feature f(i, j) holds for at least half and fewer than all mentions j ∈ e.
- MOST-FALSE: Feature f(i, j) holds for at least one and fewer than half of all mentions j ∈ e.
- NONE: Feature f(i, j) does not hold for any mention j ∈ e.

For scalar mention-pair features (e.g., distance features), aggregation can be performed by computing the minimum, maximum, and median values across all mentions in the cluster. Additional entity-mention features include the number of mentions currently clustered in the entity, and ALL-X and MOST-X features for each mention type.

# <span id="page-387-0"></span>**15.3.2 Distributed representations of mentions and entities**

Recent work has emphasized distributed representations of both mentions and entities. One potential advantage is that pre-trained embeddings could help to capture the semantic compatibility underlying nominal coreference, helping with difficult cases like (*Apple*, *the firm*) and (*China*, *the firm's biggest growth market*). Furthermore, a distributed representation of entities can be trained to capture semantic features that are added by each mention.

## **Mention embeddings**

Entity mentions can be embedded into a vector space, providing the base layer for neural networks that score coreference decisions [\(Wiseman et al., 2015\)](#page-569-7).

<span id="page-387-1"></span><sup>8</sup>This point was made by Michael Strube at a 2015 workshop, noting that as the quality of the machine learning models in coreference has improved, the benefit of including semantics has become negligible.

<span id="page-388-2"></span><span id="page-388-0"></span>![](_page_388_Figure_1.jpeg)

Figure 15.5: A bidirectional recurrent model of mention embeddings. The mention is represented by its first word, its last word, and an estimate of its head word, which is computed from a weighted average (Lee et al., 2017).

**Constructing the mention embedding** Various approaches for embedding multiword units can be applied (see § 14.8). Figure 15.5 shows a recurrent neural network approach, which begins by running a bidirectional LSTM over the entire text, obtaining hidden states from the left-to-right and right-to-left passes,  $h_m = [\overleftarrow{h}_m; \overrightarrow{h}_m]$ . Each candidate mention span (s, t) is then represented by the vertical concatenation of four vectors:

<span id="page-388-1"></span>
$$\boldsymbol{u}^{(s,t)} = [\boldsymbol{u}_{\text{first}}^{(s,t)}; \boldsymbol{u}_{\text{last}}^{(s,t)}; \boldsymbol{u}_{\text{head}}^{(s,t)}; \boldsymbol{\phi}^{(s,t)}],$$
[15.25]

where  $u_{\text{first}}^{(s,t)} = h_{s+1}$  is the embedding of the first word in the span,  $u_{\text{last}}^{(s,t)} = h_t$  is the embedding of the last word,  $u_{\text{head}}^{(s,t)}$  is the embedding of the "head" word, and  $\phi^{(s,t)}$  is a vector of surface features, such as the length of the span (Lee et al., 2017).

**Attention over head words** Rather than identifying the head word from the output of a parser, it can be computed from a neural **attention mechanism**:

$$\tilde{\alpha}_m = \boldsymbol{\theta}_\alpha \cdot \boldsymbol{h}_m \tag{15.26}$$

$$\boldsymbol{a}^{(s,t)} = \operatorname{SoftMax}\left(\left[\tilde{\alpha}_{s+1}, \tilde{\alpha}_{s+2}, \dots, \tilde{\alpha}_{t}\right]\right)$$
[15.27]

$$\boldsymbol{u}_{\text{head}}^{(s,t)} = \sum_{m=s+1}^{\iota} a_m^{(s,t)} \boldsymbol{h}_m.$$
[15.28]

Each token *m* gets a scalar score  $\tilde{\alpha}_m = \theta_\alpha \cdot h_m$ , which is the dot product of the LSTM hidden state  $h_m$  and a vector of weights  $\theta_\alpha$ . The vector of scores for tokens in the span  $m \in \{s + 1, s + 2, ..., t\}$  is then passed through a softmax layer, yielding a vector  $a^{(s,t)}$  that allocates one unit of attention across the span. This eliminates the need for syntactic parsing to recover the head word; instead, the model learns to identify the most important words in each span. Attention mechanisms were introduced in neural machine translation (Bahdanau et al., 2014), and are described in more detail in § 18.3.1.

<span id="page-389-0"></span>**Using mention embeddings** Given a set of mention embeddings, each mention i and candidate antecedent a is scored as,

$$\psi(a,i) = \psi_S(a) + \psi_S(i) + \psi_M(a,i)$$
[15.29]

$$\psi_S(a) = \text{FeedForward}_S(\boldsymbol{u}^{(a)})$$
 [15.30]

$$\psi_S(i) = \text{FeedForward}_S(\boldsymbol{u}^{(i)})$$
 [15.31]

$$\psi_M(a,i) = \operatorname{FeedForward}_M([\boldsymbol{u}^{(a)};\boldsymbol{u}^{(i)};\boldsymbol{u}^{(a)} \odot \boldsymbol{u}^{(i)};\boldsymbol{f}(a,i,\boldsymbol{w})]), \quad [15.32]$$

where u (a) and u (i) are the embeddings for spans a and i respectively, as defined in [Equa](#page-388-1)[tion 15.25.](#page-388-1)

- The scores ψS(a) quantify whether span a is likely to be a coreferring mention, independent of what it corefers with. This allows the model to learn identify mentions directly, rather than identifying mentions with a preprocessing step.
- The score ψM(a, i) computes the compatibility of spans a and i. Its base layer is a vector that includes the embeddings of spans a and i, their elementwise product u (a)  <sup>u</sup> (i) , and a vector of surface features f(a, i, w), including distance, speaker, and genre information.

[Lee et al.](#page-542-8) [\(2017\)](#page-542-8) provide an error analysis that shows how this method can correctly link a *blaze* and a *fire*, while incorrectly linking *pilots* and *fight attendants*. In each case, the coreference decision is based on similarities in the word embeddings.

Rather than embedding individual mentions, [Clark and Manning](#page-521-11) [\(2016\)](#page-521-11) embed mention pairs. At the base layer, their network takes embeddings of the words in and around each mention, as well as one-hot vectors representing a few surface features, such as the distance and string matching features. This base layer is then passed through a multilayer feedforward network with ReLU nonlinearities, resulting in a representation of the mention pair. The output of the mention pair encoder ui,j is used in the scoring function of a mention-ranking model, ψM(i, j) = θ · ui,j . A similar approach is used to score cluster pairs, constructing a cluster-pair encoding by **pooling** over the mention-pair encodings for all pairs of mentions within the two clusters.

#### **Entity embeddings**

In entity-based coreference resolution, each entity should be represented by properties of its mentions. In a distributed setting, we maintain a set of vector entity embeddings, ve. Each candidate mention receives an embedding u<sup>i</sup> ; [Wiseman et al.](#page-569-5) [\(2016\)](#page-569-5) compute this embedding by a single-layer neural network, applied to a vector of surface features. The decision of whether to merge mention i with entity e can then be driven by a feedforward

network, ψE(i, e) = Feedforward([ve;u<sup>i</sup> ]). If i is added to entity e, then its representation is updated recurrently, v<sup>e</sup> ← f(ve,ui), using a recurrent neural network such as a long short-term memory (LSTM; [chapter 6\)](#page-142-0). Alternatively, we can apply a pooling operation, such as max-pooling or average-pooling [\(chapter 3\)](#page-64-0), setting v<sup>e</sup> ← Pool(ve,ui). In either case, the update to the representation of entity e can be thought of as adding new information about the entity from mention i.

# <span id="page-390-0"></span>**15.4 Evaluating coreference resolution**

The state of coreference evaluation is aggravatingly complex. Early attempts at simple evaluation metrics were found to be susceptible to trivial baselines, such as placing each mention in its own cluster, or grouping all mentions into a single cluster. Following [Denis and Baldridge](#page-524-10) [\(2009\)](#page-524-10), the CoNLL 2011 shared task on coreference [\(Pradhan](#page-556-7) [et al., 2011\)](#page-556-7) formalized the practice of averaging across three different metrics: MUC [\(Vi](#page-567-5)[lain et al., 1995\)](#page-567-5), B-CUBED [\(Bagga and Baldwin, 1998a\)](#page-512-11), and CEAF [\(Luo, 2005\)](#page-546-7). Reference implementations of these metrics are available from [Pradhan et al.](#page-556-8) [\(2014\)](#page-556-8) at [https:](https://github.com/conll/reference-coreference-scorers) [//github.com/conll/reference-coreference-scorers](https://github.com/conll/reference-coreference-scorers).

# **Additional resources**

[Ng](#page-552-9) [\(2010\)](#page-552-9) surveys coreference resolution through 2010. Early work focused exclusively on pronoun resolution, with rule-based [\(Lappin and Leass, 1994\)](#page-542-6) and probabilistic methods [\(Ge et al., 1998\)](#page-530-9). The full coreference resolution problem was popularized in a shared task associated with the sixth Message Understanding Conference, which included coreference annotations for training and test sets of thirty documents each [\(Grishman and](#page-532-9) [Sundheim, 1996\)](#page-532-9). An influential early paper was the decision tree approach of [Soon et al.](#page-562-0) [\(2001\)](#page-562-0), who introduced mention ranking. A comprehensive list of surface features for coreference resolution is offered by [Bengtson and Roth](#page-513-7) [\(2008\)](#page-513-7). [Durrett and Klein](#page-525-7) [\(2013\)](#page-525-7) improved on prior work by introducing a large lexicalized feature set; subsequent work has emphasized neural representations of entities and mentions [\(Wiseman et al., 2015\)](#page-569-7).

# **Exercises**

- 1. Select an article from today's news, and annotate coreference for the first twenty noun phrases and possessive pronouns that appear in the article, include ones that are nested within larger noun phrases. Then specify the mention-pair training data that would result from the first five of these candidate entity mentions.
- 2. Using your annotations from the preceding problem, compute the following statistics:

- The number of times new entities are introduced by each of the three types of referring expressions: pronouns, proper nouns, and nominals. Include "singleton" entities that are mentioned only once.
- For each type of referring expression, compute the fraction of mentions that are anaphoric.
- 3. Apply a simple heuristic to all pronouns in the article from the previous exercise: link each pronoun to the closest preceding noun phrase that agrees in gender, number, animacy, and person. Compute the following evaluation:
  - True positive: a pronoun that is linked to a noun phrase with which it is coreferent, or is labeled as the first mention of an entity when in fact it does not corefer with any preceding mention. In this case, non-referential pronouns can be true positives if they are marked as having no antecedent.
  - False positive: a pronoun that is linked to a noun phrase with which it is not coreferent. This includes mistakenly linking singleton or non-referential pronouns.
  - False negative: a pronoun that has at least one antecedent, but is either labeled as not having an antecednet, or is linked to mention with which it does not corefer.

Compute the F -MEASURE for your method, and for a trivial baseline in which every pronoun refers to the immediately preceding entity mention. Are there any additional heuristics that would have improved the performance of this method?

- 4. [Durrett and Klein](#page-525-7) [\(2013\)](#page-525-7) compute the probability of the gold coreference clustering by summing over all antecedent structures that are compatible with the clustering. For example, if there are three mentions of a single entity, m1, m2, m3, there are two possible antecedent structures: a<sup>2</sup> = 1, a<sup>3</sup> = 1 and a<sup>2</sup> = 1, a<sup>3</sup> = 2. Compute the number of antecedent structures for a single entity with K mentions.
- 5. Suppose that all mentions can be unambiguously divided into C classes, for example by gender and number. Further suppose that mentions from different classes can never corefer. In a document with M mentions, give upper and lower bounds on the total number of possible coreference clusterings, in terms of the Bell numbers and the parameters M and C. Compute numerical upper and lower bounds for the case M = 4, C = 2.
- 6. [Lee et al.](#page-542-8) [\(2017\)](#page-542-8) propose a model that considers all contiguous spans in a document as possible mentions.
  - a) In a document of length M, how many mention pairs must be evaluated? (All answers can be given in asymptotic, big-O notation.)

- <span id="page-392-0"></span>b) To make inference more efficient, Lee et al. (2017) restrict consideration to spans of maximum length  $L \ll M$ . Under this restriction, how many mention pairs must be evaluated?
- c) To further improve inference, one might evaluate coreference only between pairs of mentions whose endpoints are separated by a maximum of *D* tokens. Under this additional restriction, how many mention pairs must be evaluated?
- 7. In Spanish, the subject can be omitted when it is clear from context, e.g.,
  - (15.13) Las ballenas no son peces. Son mamíferos. The whales no are fish. Are mammals.Whales are not fish. They are mammals.

Resolution of such **null subjects** is facilitated by the Spanish system of verb morphology, which includes distinctive suffixes for most combinations of person and number. For example, the verb form *son* ('are') agrees with the third-person plural pronouns *ellos* (masculine) and *ellas* (feminine), as well as the second-person plural *ustedes*.

Suppose that you are given the following components:

- A system that automatically identifies verbs with null subjects.
- A function *c*(*j*, *p*) ∈ {0, 1} that indicates whether pronoun *p* is compatible with null subject *j*, according to the verb morphology.
- A trained mention-pair model, which computes scores  $\psi(w_i, w_j, j i) \in \mathbb{R}$  for all pairs of mentions *i* and *j*, scoring the pair by the antecedent mention  $w_i$ , the anaphor  $w_j$ , and the distance j i.

Describe an integer linear program that simultaneously performs two tasks: resolving coreference among all entity mentions, and identifying suitable pronouns for all null subjects. In the example above, your program should link the null subject with *las ballenas* ('whales'), and identify *ellas* as the correct pronoun. For simplicity, you may assume that null subjects cannot be antecedents, and you need not worry about the transitivity constraint described in § 15.2.3.

- 8. Use the policy gradient algorithm to compute the gradient for the following scenario, based on the Bell tree in Figure 15.4:
  - The gold clustering *c*<sup>\*</sup> is {*Abigail*, *her*}, {*she*}.

• Drawing a single sequence of actions (K = 1) from the current policy, you obtain the following incremental clusterings:

$$oldsymbol{c}(a_1) = \{Abigail\}\ oldsymbol{c}(a_{1:2}) = \{Abigail, she\}\ oldsymbol{c}(a_{1:3}) = \{Abigail, she\}, \{her\}.$$

• At each mention t, the space of actions A<sup>t</sup> includes merging the mention with each existing cluster or with the empty cluster. The probability of merging m<sup>t</sup> with cluster c is proportional to the exponentiated score for the merged cluster,

> p(Merge(m<sup>t</sup> , c))) ∝ exp ψE(m<sup>t</sup> ∪ c), [15.33]

where ψE(m<sup>t</sup> ∪ c) is defined in [Equation 15.15.](#page-381-0)

Compute the gradient <sup>∂</sup> ∂θ L(θ) in terms of the loss `(c(a)) and the features of each (potential) cluster. Explain the differences between the gradient-based update θ ← θ − ∂ ∂θ L(θ) and the incremental perceptron update from this same example.

- 9. As discussed in § [15.1.1,](#page-373-0) some pronouns are not referential. In English, this occurs frequently with the word *it*. Download the text of *Alice in Wonderland* from NLTK, and examine the first ten appearances of *it*. For each occurrence:
  - First, examine a five-token window around the word. In the first example, this window is,
    - , but it had no

Is there another pronoun that could be substituted for *it*? Consider *she*, *they*, and *them*. In this case, both *she* and *they* yield grammatical substitutions. What about the other ten appearances of *it*?

• Now, view an fifteen-word window for each example. Based on this window, mark whether you think the word *it* is referential.

How often does the substitution test predict whether *it* is referential?

10. Now try to automate the test, using the Google n-grams corpus [\(Brants and Franz,](#page-516-9) [2006\)](#page-516-9). Specifically, find the count of each 5-gram containing *it*, and then compute the counts of 5-grams in which *it* is replaced with other third-person pronouns: *he*, *she*, *they*, *her*, *him*, *them*, *herself*, *himself*.

There are various ways to get these counts. One approach is to download the raw data and search it; another is to construct web queries to [https://books.](https://books.google.com/ngrams) [google.com/ngrams](https://books.google.com/ngrams).

#### 15.4. EVALUATING COREFERENCE RESOLUTION 377

Compare the ratio of the counts of the original 5-gram to the summed counts of the 5-grams created by substitution. Is this ratio a good predictor of whether *it* is referential?

# <span id="page-396-2"></span><span id="page-396-0"></span>**Chapter 16**

# **Discourse**

Applications of natural language processing often concern multi-sentence documents: from paragraph-long restaurant reviews, to 500-word newspaper articles, to 500-page novels. Yet most of the methods that we have discussed thus far are concerned with individual sentences. This chapter discusses theories and methods for handling multisentence linguistic phenomena, known collectively as **discourse**. There are diverse characterizations of discourse structure, and no single structure is ideal for every computational application. This chapter covers some of the most well studied discourse representations, while highlighting computational models for identifying and exploiting these structures.

# <span id="page-396-1"></span>**16.1 Segments**

A document or conversation can be viewed as a sequence of **segments**, each of which is **cohesive** in its content and/or function. In Wikipedia biographies, these segments often pertain to various aspects to the subject's life: early years, major events, impact on others, and so on. This segmentation is organized around **topics**. Alternatively, scientific research articles are often organized by **functional themes**: the introduction, a survey of previous research, experimental setup, and results.

Written texts often mark segments with section headers and related formatting devices. However, such formatting may be too coarse-grained to support applications such as the retrieval of specific passages of text that are relevant to a query [\(Hearst, 1997\)](#page-534-7). Unformatted speech transcripts, such as meetings and lectures, are also an application scenario for segmentation [\(Carletta, 2007;](#page-518-10) [Glass et al., 2007;](#page-530-10) [Janin et al., 2003\)](#page-537-7).

<span id="page-397-4"></span><span id="page-397-1"></span>![](_page_397_Figure_1.jpeg)

Figure 16.1: Smoothed cosine similarity among adjacent sentences in a news article. Local minima at m = 10 and m = 29 indicate likely segmentation points.

#### <span id="page-397-0"></span>16.1.1 Topic segmentation

A cohesive topic segment forms a unified whole, using various linguistic devices: repeated references to an entity or event; the use of conjunctions to link related ideas; and the repetition of meaning through lexical choices (Halliday and Hasan, 1976). Each of these cohesive devices can be measured, and then used as features for topic segmentation. A classical example is the use of lexical cohesion in the TEXTTILING method for topic segmentation (Hearst, 1997). The basic idea is to compute the textual similarity between each pair of adjacent blocks of text (sentences or fixed-length units), using a formula such as the smoothed **cosine similarity** of their bag-of-words vectors,

$$s_m = \frac{\boldsymbol{x}_m \cdot \boldsymbol{x}_{m+1}}{||\boldsymbol{x}_m||_2 \times ||\boldsymbol{x}_{m+1}||_2}$$
[16.1]

<span id="page-397-3"></span>
$$\bar{s}_m = \sum_{\ell=0}^{L} k_\ell (s_{m+\ell} + s_{m-\ell}), \qquad [16.2]$$

with  $k_{\ell}$  representing the value of a smoothing kernel of size *L*, e.g.  $\mathbf{k} = [1, 0.5, 0.25]^{\top}$ . Segmentation points are then identified at local minima in the smoothed similarities  $\overline{s}$ , since these points indicate changes in the overall distribution of words in the text. An example is shown in Figure 16.1.

Text segmentation can also be formulated as a probabilistic model, in which each segment has a unique language model that defines the probability over the text in the segment (Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008; Du et al., 2013).<sup>1</sup> A good

<span id="page-397-2"></span><sup>&</sup>lt;sup>1</sup>There is a rich literature on how latent variable models (such as **latent Dirichlet allocation**) can track

<span id="page-398-2"></span>segmentation achieves high likelihood by grouping segments with similar word distributions. This probabilistic approach can be extended to **hierarchical topic segmentation**, in which each topic segment is divided into subsegments [\(Eisenstein, 2009\)](#page-526-14). All of these approaches are unsupervised. While labeled data can be obtained from well-formatted texts such as textbooks, such annotations may not generalize to speech transcripts in alternative domains. Supervised methods have been tried in cases where in-domain labeled data is available, substantially improving performance by learning weights on multiple types of features [\(Galley et al., 2003\)](#page-529-11).

#### <span id="page-398-0"></span>**16.1.2 Functional segmentation**

In some genres, there is a canonical set of communicative *functions*: for example, in scientific research articles, one such function is to communicate the general background for the article, another is to introduce a new contribution, or to describe the aim of the research [\(Teufel et al., 1999\)](#page-565-11). A **functional segmentation** divides the document into contiguous segments, sometimes called **rhetorical zones**, in which each sentence has the same function. [Teufel and Moens](#page-565-12) [\(2002\)](#page-565-12) train a supervised classifier to identify the functional of each sentence in a set of scientific research articles, using features that describe the sentence's position in the text, its similarity to the rest of the article and title, tense and voice of the main verb, and the functional role of the previous sentence. Functional segmentation can also be performed without supervision. Noting that some types of Wikipedia articles have very consistent functional segmentations (e.g., articles about cities or chemical elements), [Chen et al.](#page-519-8) [\(2009\)](#page-519-8) introduce an unsupervised model for functional segmentation, which learns both the language model associated with each function and the typical patterning of functional segments across the article.

# <span id="page-398-1"></span>**16.2 Entities and reference**

Another dimension of discourse relates to which entities are mentioned throughout the text, and how. Consider the examples in [Figure 16.2:](#page-399-1) [Grosz et al.](#page-532-7) [\(1995\)](#page-532-7) argue that the first discourse is more coherent. Do you agree? The examples differ in their choice of **referring expressions** for the protagonist *John*, and in the syntactic constructions in sentences (b) and (d). The examples demonstrate the need for theoretical models to explain how referring expressions are chosen, and where they are placed within sentences. Such models can then be used to help interpret the overall structure of the discourse, to measure discourse coherence, and to generate discourses in which referring expressions are used coherently.

topics across documents [\(Blei et al., 2003;](#page-514-4) [Blei, 2012\)](#page-514-0).

- <span id="page-399-4"></span><span id="page-399-1"></span>(16.1) a. John went to his favorite music (16.2) store to buy a piano.
  - b. He had frequented the store for many years.
  - c. He was excited that he could finally buy a piano.
  - d. He arrived just as the store was closing for the day
- a. John went to his favorite music store to buy a piano.
- b. It was a store John had frequented for many years.
- c. He was excited that he could finally buy a piano.
- <span id="page-399-3"></span>d. It was closing just as John arrived.

<span id="page-399-2"></span>Figure 16.2: Two tellings of the same story (Grosz et al., 1995). The discourse on the left uses referring expressions coherently, while the one on the right does not.

#### <span id="page-399-0"></span>**16.2.1** Centering theory

**Centering theory** presents a unified account of the relationship between discourse structure and entity reference (Grosz et al., 1995). According to the theory, every utterance in the discourse is characterized by a set of entities, known as *centers*.

- The forward-looking centers in utterance m are all the entities that are mentioned in the utterance,  $c_f(w_m) = \{e_1, e_2, \dots, \}$ . The forward-looking centers are partially ordered by their syntactic prominence, favoring subjects over objects, and objects over other positions (Brennan et al., 1987). For example, in example (1.1a) of Figure 16.2, the ordered list of forward-looking centers in the first utterance is John, the music store, and the piano.
- The backward-looking center c<sub>b</sub>(w<sub>m</sub>) is the highest-ranked element in the set of forward-looking centers from the previous utterance c<sub>f</sub>(w<sub>m-1</sub>) that is also mentioned in w<sub>m</sub>. In example (1.1b) of item 16.1, the backward looking center is John.

Given these two definitions, centering theory makes the following predictions about the form and position of referring expressions:

- 1. If a pronoun appears in the utterance  $w_m$ , then the backward-looking center  $c_b(w_m)$  must also be realized as a pronoun. This rule argues against the use of *it* to refer to the piano store in Example (16.2d), since JOHN is the backward looking center of (16.2d), and he is mentioned by name and not by a pronoun.
- 2. Sequences of utterances should retain the same backward-looking center if possible, and ideally, the backward-looking center should also be the top-ranked element in the list of forward-looking centers. This rule argues in favor of the preservation of JOHN as the backward-looking center throughout Example (16.1).

#### <span id="page-400-2"></span>16.2. ENTITIES AND REFERENCE

<span id="page-400-1"></span>

|                                            | Skyler | WALTER | DANGER | A GUY | THE DOOR |
|--------------------------------------------|--------|--------|--------|-------|----------|
|                                            | -      |        |        |       |          |
| You don't know who you're talk-<br>ing to, | S      | -      | -      | -     | -        |
| so let me clue you in.                     | 0      | 0      | -      | -     | -        |
| I am not in danger, Skyler.                | Х      | S      | Х      | -     | -        |
| I am the danger.                           | -      | S      | 0      | -     | -        |
| A guy opens his door and gets shot,        | -      | -      | -      | S     | 0        |
| and you think that of me?                  | S      | Х      | -      | -     | -        |
| No. I am the one who knocks!               | -      | S      | -      | -     | -        |

Figure 16.3: The entity grid representation for a dialogue from the television show *Break*ing Bad.

Centering theory unifies aspects of syntax, discourse, and anaphora resolution. However, it can be difficult to clarify exactly how to rank the elements of each utterance, or even how to partition a text or dialog into utterances (Poesio et al., 2004).

#### <span id="page-400-0"></span>16.2.2 The entity grid

One way to formalize the ideas of centering theory is to arrange the entities in a text or conversation in an **entity grid**. This is a data structure with one row per sentence, and one column per entity (Barzilay and Lapata, 2008). Each cell c(m, i) can take the following values:

|                | S,   | entity $i$ is in subject position in sentence $m$                                                                               |
|----------------|------|---------------------------------------------------------------------------------------------------------------------------------|
| a(m, i) =      | O,   | entity $i$ is in object position in sentence $m$                                                                                |
| $c(m, i) = \{$ | X,   | entity $i$ is in object position in sentence $m$<br>entity $i$ appears in sentence $m$ , in neither subject nor object position |
|                | ( —, | entity $i$ does not appear in sentence $m$ .                                                                                    |

To populate the entity grid, syntactic parsing is applied to identify subject and object positions, and coreference resolution is applied to link multiple mentions of a single entity. An example is shown in Figure 16.3.

After the grid is constructed, the coherence of a document can be measured by the *transitions* between adjacent cells in each column. For example, the transition  $(S \rightarrow S)$  keeps an entity in subject position across adjacent sentences; the transition  $(O \rightarrow S)$  promotes an entity from object position to subject position; the transition  $(S \rightarrow -)$  drops the subject of one sentence from the next sentence. The probabilities of each transition can be

Under contract with MIT Press, shared under CC-BY-NC-ND license.

[16.3]

<span id="page-401-7"></span>estimated from labeled data, and an entity grid can then be scored by the sum of the logprobabilities across all columns and all transitions,  $\sum_{i=1}^{N_e} \sum_{m=1}^{M} \log p(c(m,i) \mid c(m-1,i))$ . The resulting probability can be used as a proxy for the coherence of a text. This has been shown to be useful for a range of tasks: determining which of a pair of articles is more readable (Schwarm and Ostendorf, 2005), correctly ordering the sentences in a scrambled text (Lapata, 2003), and disentangling multiple conversational threads in an online multiparty chat (Elsner and Charniak, 2010).

#### <span id="page-401-0"></span>**16.2.3** \*Formal semantics beyond the sentence level

An alternative view of the role of entities in discourse focuses on formal semantics, and the construction of meaning representations for multi-sentence units. Consider the following two sentences (from Bird et al., 2009):

<span id="page-401-5"></span>(16.3) a. Angus owns a dog.

b. It bit Irene.

We would like to recover the formal semantic representation,

<span id="page-401-1"></span>
$$\exists x. \text{DOG}(x) \land \text{OWN}(\text{ANGUS}, x) \land \text{BITE}(x, \text{IRENE}).$$
[16.4]

However, the semantic representations of each individual sentence are,

<span id="page-401-3"></span>
$$\exists x. \text{DOG}(x) \land \text{OWN}(\text{ANGUS}, x)$$
[16.5]

<span id="page-401-2"></span>
$$BITE(y, IRENE).$$
[16.6]

Unifying these two representations into the form of Equation 16.4 requires linking the unbound variable y from [16.6] with the quantified variable x in [16.5].<sup>2</sup> Discourse understanding therefore requires the reader to update a set of assignments, from variables to entities. This update would (presumably) link the *dog* in the first sentence of [16.3] with the unbound variable y in the second sentence, thereby licensing the conjunction in [16.4].<sup>3</sup> This basic idea is at the root of **dynamic semantics** (Groenendijk and Stokhof, 1991). **Segmented discourse representation theory** links dynamic semantics with a set of **discourse relations**, which explain how adjacent units of text are rhetorically or conceptually related (Lascarides and Asher, 2007). The next section explores the theory of discourse relations in more detail.

<span id="page-401-4"></span><sup>&</sup>lt;sup>2</sup>Groenendijk and Stokhof (1991) treats the *y* variable in Equation 16.6 as unbound. Even if it were bound locally with an existential quantifier ( $\exists y BITE(y, IRENE)$ ), the variable would still need to be reconciled with the quantified variable in Equation 16.5.

<span id="page-401-6"></span><sup>&</sup>lt;sup>3</sup>This linking task is similar to coreference resolution (see chapter 15), but here the connections are between semantic variables, rather than spans of text.

# <span id="page-402-2"></span><span id="page-402-0"></span>**16.3 Relations**

In dependency grammar, sentences are characterized by a graph (usually a tree) of syntactic relations between words, such as NSUBJ and DET. A similar idea can be applied at the document level, identifying relations between discourse units, such as clauses, sentences, or paragraphs. The task of **discourse parsing** involves identifying discourse units and the relations that hold between them. These relations can then be applied to tasks such as document classification and summarization, as discussed in § [16.3.4.](#page-410-0)

## <span id="page-402-1"></span>**16.3.1 Shallow discourse relations**

The existence of discourse relations is hinted by **discourse connectives**, such as *however*, *moreover*, *meanwhile*, and *if . . . then*. These connectives explicitly specify the relationship between adjacent units of text: *however* signals a contrastive relationship, *moreover* signals that the subsequent text elaborates or strengthens the point that was made immediately beforehand, *meanwhile* indicates that two events are contemporaneous, and *if . . . then* sets up a conditional relationship. Discourse connectives can therefore be viewed as a starting point for the analysis of discourse relations.

In **lexicalized tree-adjoining grammar for discourse (D-LTAG)**, each connective anchors a relationship between two units of text [\(Webber, 2004\)](#page-568-7). This model provides the theoretical basis for the **Penn Discourse Treebank (PDTB)**, the largest corpus of discourse relations in English [\(Prasad et al., 2008\)](#page-556-9). It includes a hierarchical inventory of discourse relations (shown in [Table 16.1\)](#page-403-0), which is created by abstracting the meanings implied by the discourse connectives that appear in real texts [\(Knott, 1996\)](#page-540-9). These relations are then annotated on the same corpus of news text used in the Penn Treebank (see § [9.2.2\)](#page-228-0), adding the following information:

- Each connective is annotated for the discourse relation or relations that it expresses, if any — many discourse connectives have senses in which they do not signal a discourse relation [\(Pitler and Nenkova, 2009\)](#page-555-13).
- For each discourse relation, the two arguments of the relation are specified as ARG1 and ARG2, where ARG2 is constrained to be adjacent to the connective. These arguments may be sentences, but they may also smaller or larger units of text.
- Adjacent sentences are annotated for **implicit discourse relations**, which are not marked by any connective. When a connective could be inserted between a pair of sentence, the annotator supplies it, and also labels its sense (e.g., example [16.5\)](#page-404-0). In some cases, there is no relationship at all between a pair of adjacent sentences; in other cases, the only relation is that the adjacent sentences mention one or more shared entity. These phenomena are annotated as NOREL and ENTREL (entity relation), respectively.

- <span id="page-403-1"></span><span id="page-403-0"></span>• TEMPORAL
  - **–** Asynchronous
  - **–** Synchronous: precedence, succession
- CONTINGENCY
  - **–** Cause: result, reason
  - **–** Pragmatic cause: justification
  - **–** Condition: hypothetical, general, unreal present, unreal past, real present, real past
  - **–** Pragmatic condition: relevance, implicit assertion

- COMPARISON
  - **–** Contrast: juxtaposition, opposition
  - **–** Pragmatic contrast
  - **–** Concession: expectation, contra-expectation
  - **–** Pragmatic concession
- EXPANSION
  - **–** Conjunction
  - **–** Instantiation
  - **–** Restatement: specification, equivalence, generalization
  - **–** Alternative: conjunctive, disjunctive, chosen alternative
  - **–** Exception
  - **–** List

Table 16.1: The hierarchy of discourse relation in the Penn Discourse Treebank annotations [\(Prasad et al., 2008\)](#page-556-9). For example, PRECEDENCE is a subtype of SYNCHRONOUS, which is a type of TEMPORAL relation.

Examples of Penn Discourse Treebank annotations are shown in [\(16.4\)](#page-404-1). In [\(16.4\)](#page-404-2), the word *therefore* acts as an explicit discourse connective, linking the two adjacent units of text. The Treebank annotations also specify the "sense" of each relation, linking the connective to a relation in the sense inventory shown in [Table 16.1:](#page-403-0) in [\(16.4\)](#page-404-2), the relation is PRAGMATIC CAUSE:JUSTIFICATION because it relates to the author's communicative intentions. The word *therefore* can also signal causes in the external world (e.g., *He was therefore forced to relinquish his plan*). In **discourse sense classification**, the goal is to determine which discourse relation, if any, is expressed by each connective. A related task is the classification of implicit discourse relations, as in [\(16.5\)](#page-404-0). In this example, the relationship between the adjacent sentences could be expressed by the connective *because*, indicating a CAUSE:REASON relationship.

#### **Classifying explicit discourse relations and their arguments**

As suggested by the examples above, many connectives can be used to invoke multiple types of discourse relations. Similarly, some connectives have senses that are unrelated to discourse: for example, *and* functions as a discourse connective when it links propo-

- <span id="page-404-2"></span><span id="page-404-1"></span>(16.4) . . . *as this business of whaling has somehow come to be regarded among landsmen as a rather unpoetical and disreputable pursuit*; therefore, **I am all anxiety to convince ye, ye landsmen, of the injustice hereby done to us hunters of whales**.
- <span id="page-404-0"></span>(16.5) But a few funds have taken other defensive steps. *Some have raised their cash positions to record levels.* Implicit = BECAUSE **High cash positions help buffer a fund when the market falls.**
- <span id="page-404-4"></span>(16.6) Michelle lives in a hotel room, and although **she drives a canary-colored Porsche**, *she hasn't time to clean or repair it*.
- <span id="page-404-3"></span>(16.7) *Most oil companies*, when **they set exploration and production budgets for this year**, *forecast revenue of* \$*15 for each barrel of crude produced*.

Figure 16.4: Example annotations of discourse relations. In the style of the Penn Discourse Treebank, the discourse connective is underlined, the first argument is shown in italics, and the second argument is shown in bold. Examples [\(16.5-](#page-404-0)[16.7\)](#page-404-3) are quoted from [Prasad](#page-556-9) [et al.](#page-556-9) [\(2008\)](#page-556-9).

sitions, but not when it links noun phrases [\(Lin et al., 2014\)](#page-545-6). Nonetheless, the senses of explicitly-marked discourse relations in the Penn Treebank are relatively easy to classify, at least at the coarse-grained level. When classifying the four top-level PDTB relations, 90% accuracy can be obtained simply by selecting the most common relation for each connective [\(Pitler and Nenkova, 2009\)](#page-555-13). At the more fine-grained levels of the discourse relation hierarchy, connectives are more ambiguous. This fact is reflected both in the accuracy of automatic sense classification [\(Versley, 2011\)](#page-567-6) and in interannotator agreement, which falls to 80% for level-3 discourse relations [\(Prasad et al., 2008\)](#page-556-9).

A more challenging task for explicitly-marked discourse relations is to identify the scope of the arguments. Discourse connectives need not be adjacent to ARG1, as shown in [item 16.6,](#page-404-4) where ARG1 follows ARG2; furthermore, the arguments need not be contiguous, as shown in [\(16.7\)](#page-404-3). For these reasons, recovering the arguments of each discourse connective is a challenging subtask. Because intra-sentential arguments are often syntactic constituents (see [chapter 10\)](#page-242-0), many approaches train a classifier to predict whether each constituent is an appropriate argument for each explicit discourse connective [\(Well](#page-568-8)[ner and Pustejovsky, 2007;](#page-568-8) [Lin et al., 2014,](#page-545-6) e.g.,).

#### <span id="page-404-6"></span>**Classifying implicit discourse relations**

Implicit discourse relations are considerably more difficult to classify and to annotate.[4](#page-404-5) Most approaches are based on an encoding of each argument, which is then used as input

<span id="page-404-5"></span><sup>4</sup> In the dataset for the 2015 shared task on shallow discourse parsing, the interannotator agreement was 91% for explicit discourse relations and 81% for implicit relations, across all levels of detail [\(Xue et al., 2015\)](#page-570-7).

to a nonlinear classifier:

$$\boldsymbol{z}^{(i)} = \text{Encode}(\boldsymbol{w}^{(i)})$$
[16.7]

$$\boldsymbol{z}^{(i+1)} = \text{Encode}(\boldsymbol{w}^{(i+1)})$$
[16.8]

$$\hat{y}_i = \underset{y}{\operatorname{argmax}} \Psi(y, \boldsymbol{z}^{(i)}, \boldsymbol{z}^{(i+1)}).$$
[16.9]

This basic framework can be instantiated in several ways, including both feature-based and neural encoders.

**Feature-based approaches** Each argument can be encoded into a vector of surface features. The encoding typically includes lexical features (all words, or all content words, or a subset of words such as the first three and the main verb), Brown clusters of individual words (§ 14.4), and syntactic features such as terminal productions and dependency arcs (Pitler et al., 2009; Lin et al., 2009; Rutherford and Xue, 2014). The classification function then has two parts. First, it creates a joint feature vector by combining the encodings of each argument, typically by computing the cross-product of all features in each encoding:

$$f(y, z^{(i)}, z^{(i+1)}) = \{(a \times b \times y) : (z_a^{(i)} z_b^{(i+1)})\}$$
[16.10]

The size of this feature set grows with the square of the size of the vocabulary, so it can be helpful to select a subset of features that are especially useful on the training data (Park and Cardie, 2012). After *f* is computed, any classifier can be trained to compute the final score,  $\Psi(y, \mathbf{z}^{(i)}, \mathbf{z}^{(i+1)}) = \boldsymbol{\theta} \cdot \boldsymbol{f}(y, \mathbf{z}^{(i)}, \mathbf{z}^{(i+1)})$ .

**Neural network approaches** In neural network architectures, the encoder is learned jointly with the classifier as an end-to-end model. Each argument can be encoded using a variety of neural architectures (surveyed in § 14.8): recursive (§ 10.6.1; Ji and Eisenstein, 2015), recurrent (§ 6.3; Ji et al., 2016), and convolutional (§ 3.4; Qin et al., 2017). The classification function can then be implemented as a feedforward neural network on the two encodings (chapter 3; for examples, see Rutherford et al., 2017; Qin et al., 2017), or as a simple bilinear product,  $\Psi(y, z^{(i)}, z^{(i+1)}) = (z^{(i)})^{\top} \Theta_y z^{(i+1)}$  (Ji and Eisenstein, 2015). The encoding model can be trained by backpropagation from the classification objective, such as the margin loss. Rutherford et al. (2017) show that neural architectures outperform feature-based approaches in most settings. While neural approaches require engineering the network architecture (e.g., embedding size, number of hidden units in the classifier), feature-based approaches also require significant engineering to incorporate linguistic resources such as Brown clusters and parse trees, and to select a subset of relevant features.

### <span id="page-406-5"></span><span id="page-406-0"></span>**16.3.2 Hierarchical discourse relations**

In sentence parsing, adjacent phrases combine into larger constituents, ultimately producing a single constituent for the entire sentence. The resulting tree structure enables structured analysis of the sentence, with subtrees that represent syntactically coherent chunks of meaning. **Rhetorical Structure Theory (RST)** extends this style of hierarchical analysis to the discourse level [\(Mann and Thompson, 1988\)](#page-546-8).

The basic element of RST is the **discourse unit**, which refers to a contiguous span of text. **Elementary discourse units** (EDUs) are the atomic elements in this framework, and are typically (but not always) clauses.[5](#page-406-1) Each discourse relation combines two or more adjacent discourse units into a larger, composite discourse unit; this process ultimately unites the entire text into a tree-like structure.[6](#page-406-2)

**Nuclearity** In many discourse relations, one argument is primary. For example:

<span id="page-406-4"></span>(16.8) [LaShawn loves animals]<sup>N</sup> [She has nine dogs and one pig]<sup>S</sup>

In this example, the second sentence provides EVIDENCE for the point made in the first sentence. The first sentence is thus the **nucleus** of the discourse relation, and the second sentence is the **satellite**. The notion of nuclearity is similar to the head-modifier structure of dependency parsing (see § [11.1.1\)](#page-275-0). However, in RST, some relations have multiple nuclei. For example, the arguments of the CONTRAST relation are equally important:

(16.9) [The clash of ideologies survives this treatment]<sup>N</sup> [but the nuance and richness of Gorky's individual characters have vanished in the scuffle]<sup>N</sup> [7](#page-406-3)

Relations that have multiple nuclei are called **coordinating**; relations with a single nucleus are called **subordinating**. Subordinating relations are constrained to have only two arguments, while coordinating relations (such as CONJUNCTION) may have more than two.

The appropriateness of tree structures to discourse has been challenged, e.g., by [Wolf and Gibson](#page-569-8) [\(2005\)](#page-569-8), who propose a more general graph-structured representation.

<span id="page-406-3"></span>7 from the RST Treebank [\(Carlson et al., 2002\)](#page-518-12)

<span id="page-406-1"></span><sup>5</sup>Details of discourse segmentation can be found in the RST annotation manual [\(Carlson and Marcu,](#page-518-11) [2001\)](#page-518-11).

<span id="page-406-2"></span><sup>6</sup>While RST analyses are typically trees, this should not be taken as a strong theoretical commitment to the principle that all coherent discourses have a tree structure. [Taboada and Mann](#page-564-9) [\(2006\)](#page-564-9) write:

It is simply the case that trees are convenient, easy to represent, and easy to understand. There is, on the other hand, no theoretical reason to assume that trees are the only possible representation of discourse structure and of coherence relations.

![](_page_407_Figure_1.jpeg)

<span id="page-407-1"></span>[It could have been a great movie]<sup>1A</sup> [It does have beautiful scenery,]<sup>1B</sup> [some of the best since Lord of the Rings.]<sup>1C</sup> [The acting is well done,]<sup>1D</sup> [and I really liked the son of the leader of the Samurai.]<sup>1E</sup> [He was a likable chap,]<sup>1F</sup> [and I hated to see him die.]<sup>1G</sup> [But, other than all that, this movie is nothing more than hidden rip-offs.]<sup>1H</sup>

Figure 16.5: A rhetorical structure theory analysis of a short movie review, adapted from Voll and Taboada (2007). <u>Positive</u> and <u>negative</u> sentiment words are underlined, indicating RST's potential utility in document-level sentiment analysis.

**RST Relations** Rhetorical structure theory features a large inventory of discourse relations, which are divided into two high-level groups: subject matter relations, and presentational relations. Presentational relations are organized around the intended beliefs of the reader. For example, in (16.8), the second discourse unit provides evidence intended to increase the reader's belief in the proposition expressed by the first discourse unit, that *LaShawn loves animals*. In contrast, subject-matter relations are meant to communicate additional facts about the propositions contained in the discourse units that they relate:

(16.10) [the debt plan was rushed to completion]<sub>N</sub> [in order to be announced at the meeting]<sub>S</sub><sup>8</sup>

In this example, the satellite describes a world state that is realized by the action described in the nucleus. This relationship is about the world, and not about the author's communicative intentions.

**Example** Figure 16.5 depicts an RST analysis of a paragraph from a movie review. Asymmetric (subordinating) relations are depicted with an arrow from the satellite to the nucleus; symmetric (coordinating) relations are depicted with lines. The elementary discourse units 1F and 1G are combined into a larger discourse unit with the symmetric CONJUNCTION relation. The resulting discourse unit is then the satellite in a JUSTIFY relation with 1E.

<span id="page-407-0"></span><sup>&</sup>lt;sup>8</sup>from the RST Treebank (Carlson et al., 2002)

#### <span id="page-408-2"></span><span id="page-408-1"></span>Hierarchical discourse parsing

The goal of discourse parsing is to recover a hierarchical structural analysis from a document text, such as the analysis in Figure 16.5. For now, let's assume a segmentation of the document into elementary discourse units (EDUs); segmentation algorithms are discussed below. After segmentation, discourse parsing can be viewed as a combination of two components: the discourse relation classification techniques discussed in § 16.3.1, and algorithms for phrase-structure parsing, such as chart parsing and shift-reduce, which were discussed in chapter 10.

Both chart parsing and shift-reduce require encoding composite discourse units, either in a discrete feature vector or a dense neural representation.<sup>9</sup> Some discourse parsers rely on the **strong compositionality criterion** (Marcu, 1996), which states the assumption that a composite discourse unit can be represented by its nucleus. This criterion is used in feature-based discourse parsing to determine the feature vector for a composite discourse unit (Hernault et al., 2010); it is used in neural approaches to setting the vector encoding for a composite discourse unit equal to the encoding of its nucleus (Ji and Eisenstein, 2014). An alternative neural approach is to learn a composition function over the components of a composite discourse unit (Li et al., 2014), using a recursive neural network (see § 14.8.3).

**Bottom-up discourse parsing** Assume a segmentation of the text into N elementary discourse units with base representations  $\{z^{(i)}\}_{i=1}^{N}$ , and assume a composition function COMPOSE  $(z^{(i)}, z^{(j)}, \ell)$ , which maps two encodings and a discourse relation  $\ell$  into a new encoding. The composition function can follow the strong compositionality criterion and simply select the encoding of the nucleus, or it can do something more complex. We also need a scoring function  $\Psi(z^{(i,k)}, z^{(k,j)}, \ell)$ , which computes a scalar score for the (binarized) discourse relation  $\ell$  with left child covering the span i + 1 : k, and the right child covering the span k + 1 : j. Given these components, we can construct vector representations for each span, and this is the basic idea underlying **compositional vector grammars** (Socher et al., 2013).

These same components can also be used in bottom-up parsing, in a manner that is similar to the CKY algorithm for weighted context-free grammars (see § 10.1): compute the score and best analysis for each possible span of increasing lengths, while storing back-pointers that make it possible to recover the optimal parse of the entire input. However, there is an important distinction from CKY parsing: for each labeled span  $(i, j, \ell)$ , we must use the composition function to construct a representation  $z^{(i,j,\ell)}$ . This representation is then used to combine the discourse unit spanning i + 1 : j in higher-level discourse relations. The representation  $z^{(i,j,\ell)}$  depends on the entire substructure of the unit span-

<span id="page-408-0"></span><sup>&</sup>lt;sup>9</sup>To use these algorithms, is also necessary to binarize all discourse relations during parsing, and then to "unbinarize" them to reconstruct the desired structure (e.g., Hernault et al., 2010).

<span id="page-409-1"></span>ning i + 1 : j, and this violates the locality assumption that underlie CKY's optimality guarantee. Bottom-up parsing with recursively constructed span representations is generally not guaranteed to find the best-scoring discourse parse. This problem is explored in an exercise at the end of the chapter.

**Transition-based discourse parsing** One drawback of bottom-up parsing is its cubic time complexity in the length of the input. For long documents, transition-based parsing is an appealing alternative. The shift-reduce algorithm (see § [10.6.2\)](#page-268-0) can be applied to discourse parsing fairly directly [\(Sagae, 2009\)](#page-559-9): the stack stores a set of discourse units and their representations, and each action is chosen by a function of these representations. This function could be a linear product of weights and features, or it could be a neural network applied to encodings of the discourse units. The REDUCE action then performs composition on the two discourse units at the top of the stack, yielding a larger composite discourse unit, which goes on top of the stack. All of the techniques for integrating learning and transition-based parsing, described in § [11.3,](#page-285-0) are applicable to discourse parsing.

#### **Segmenting discourse units**

In rhetorical structure theory, elementary discourse units do not cross the sentence boundary, so discourse segmentation can be performed within sentences, assuming the sentence segmentation is given. The segmentation of sentences into elementary discourse units is typically performed using features of the syntactic analysis [\(Braud et al., 2017\)](#page-516-11). One approach is to train a classifier to determine whether each syntactic constituent is an EDU, using features such as the production, tree structure, and head words [\(Soricut and Marcu,](#page-562-9) [2003;](#page-562-9) [Hernault et al., 2010\)](#page-534-8). Another approach is to train a sequence labeling model, such as a conditional random field [\(Sporleder and Lapata, 2005;](#page-563-11) [Xuan Bach et al., 2012;](#page-570-8) [Feng](#page-527-8) [et al., 2014\)](#page-527-8). This is done using the BIO formalism for segmentation by sequence labeling, described in § [8.3.](#page-200-0)

## <span id="page-409-0"></span>**16.3.3 Argumentation**

An alternative view of text-level relational structure focuses on **argumentation** [\(Stab and](#page-563-12) [Gurevych, 2014b\)](#page-563-12). Each segment (typically a sentence or clause) may support or rebut another segment, creating a graph structure over the text. In the following example (from [Peldszus and Stede, 2013\)](#page-554-11), segment S2 provides argumentative support for the proposition in the segment S1:

(16.11) [We should tear the building down,]S<sup>1</sup> [because it is full of asbestos]S2.

Assertions may also support or rebut proposed links between two other assertions, creating a **hypergraph**, which is a generalization of a graph to the case in which edges can

<span id="page-410-2"></span>join any number of vertices. This can be seen by introducing another sentence into the example:

(16.12) [In principle it is possible to clean it up,]S<sup>3</sup> [but according to the mayor that is too expensive.]S<sup>4</sup>

S3 acknowledges the validity of S2, but **undercuts** its support of S1. This can be represented by introducing a hyperedge, (S3, S2, S1)undercut, indicating that S3 undercuts the proposed relationship between S2 and S1. S4 then undercuts the relevance of S3.

**Argumentation mining** is the task of recovering such structures from raw texts. At present, annotations of argumentation structure are relatively small: [Stab and Gurevych](#page-563-13) [\(2014a\)](#page-563-13) have annotated a collection of 90 persuasive essays, and [Peldszus and Stede](#page-554-12) [\(2015\)](#page-554-12) have solicited and annotated a set of 112 paragraph-length "microtexts" in German.

# <span id="page-410-0"></span>**16.3.4 Applications of discourse relations**

The predominant application of discourse parsing is to select content within a document. In rhetorical structure theory, the nucleus is considered the more important element of the relation, and is more likely to be part of a summary of the document; it may also be more informative for document classification. The D-LTAG theory that underlies the Penn Discourse Treebank lacks this notion of nuclearity, but arguments may have varying importance, depending on the relation type. For example, the span of text constituting ARG1 of an expansion relation is more likely to appear in a summary, while the sentence constituting ARG2 of an implicit relation is less likely [\(Louis et al., 2010\)](#page-545-7). Discourse relations may also signal segmentation points in the document structure. Explicit discourse markers have been shown to correlate with changes in subjectivity, and identifying such change points can improve document-level sentiment classification, by helping the classifier to focus on the subjective parts of the text [\(Trivedi and Eisenstein, 2013;](#page-565-13) [Yang and](#page-571-9) [Cardie, 2014\)](#page-571-9).

### <span id="page-410-1"></span>**Extractive Summarization**

Text **summarization** is the problem of converting a longer text into a shorter one, while still conveying the key facts, events, ideas, and sentiments from the original. In **extractive summarization**, the summary is a subset of the original text; in **abstractive summarization**, the summary is produced *de novo*, by paraphrasing the original, or by first encoding it into a semantic representation (see § [19.2\)](#page-481-0). The main strategy for extractive summarization is to maximize coverage, choosing a subset of the document that best covers the concepts mentioned in the document as a whole; typically, coverage is approximated by bag-of-words overlap [\(Nenkova and McKeown, 2012\)](#page-551-9). Coverage-based objectives can be supplemented by hierarchical discourse relations, using the principle of nuclearity: in any subordinating discourse relation, the nucleus is more critical to the overall meaning

<span id="page-411-2"></span>of the text, and is therefore more important to include in an extractive summary (Marcu, 1997a).<sup>10</sup> This insight can be generalized from individual relations using the concept of **discourse depth** (Hirao et al., 2013): for each elementary discourse unit e, the discourse depth  $d_e$  is the number of relations in which a discourse unit containing e is the satellite.

Both discourse depth and nuclearity can be incorporated into extractive summarization, using constrained optimization. Let  $x_n$  be a bag-of-words vector representation of elementary discourse unit n, let  $y_n \in \{0, 1\}$  indicate whether n is included in the summary, and let  $d_n$  be the depth of unit n. Furthermore, let each discourse unit have a "head" h, which is defined recursively:

- if a discourse unit is produced by a subordinating relation, then its head is the head of the (unique) nucleus;
- if a discourse unit is produced by a coordinating relation, then its head is the head of the left-most nucleus;
- for each elementary discourse unit, its parent π(n) ∈ {Ø, 1, 2, ..., N} is the head of the smallest discourse unit containing n whose head is not n;
- if *n* is the head of the discourse unit spanning the whole document, then  $\pi(n) = \emptyset$ .

With these definitions in place, discourse-driven extractive summarization can be formalized as (Hirao et al., 2013),

<span id="page-411-1"></span>
$$\max_{\boldsymbol{y}=\{0,1\}^{N}} \sum_{n=1}^{N} y_{n} \frac{\Psi\left(\boldsymbol{x}_{n}, \{\boldsymbol{x}_{1:N}\}\right)}{d_{n}}$$
  
s.t. 
$$\sum_{n=1}^{N} y_{n} (\sum_{j=1}^{V} x_{n,j}) \leq L$$
$$y_{\pi(n)} \geq y_{n}, \quad \forall n \text{ s.t. } \pi(n) \neq \emptyset$$
[16.11]

where  $\Psi(x_n, \{x_{1:N}\})$  measures the coverage of elementary discourse unit *n* with respect to the rest of the document, and  $\sum_{j=1}^{V} x_{n,j}$  is the number of tokens in  $x_n$ . The first constraint ensures that the number of tokens in the summary has an upper bound *L*. The second constraint ensures that no elementary discourse unit is included unless its parent is also included. In this way, the discourse structure is used twice: to downweight the contributions of elementary discourse units that are not central to the discourse, and to ensure that the resulting structure is a subtree of the original discourse parse. The opti-

<span id="page-411-0"></span><sup>&</sup>lt;sup>10</sup>Conversely, the arguments of a multi-nuclear relation should either both be included in the summary, or both excluded (Durrett et al., 2016).

![](_page_412_Figure_1.jpeg)

<span id="page-412-2"></span><span id="page-412-1"></span>Figure 16.6: A discourse depth tree (Hirao et al., 2013) for the discourse parse from Figure 16.5, in which each elementary discourse unit is connected to its parent. The discourse units in one valid summary are underlined.

mization problem in 16.11 can be solved with **integer linear programming**, described in § 13.2.2.<sup>11</sup>

Figure 16.6 shows a discourse depth tree for the RST analysis from Figure 16.5, in which each elementary discourse is connected to (and below) its parent. The underlined discourse units in the figure constitute the following summary:

(16.13) It could have been a great movie, and I really liked the son of the leader of the Samurai. But, other than all that, this movie is nothing more than hidden rip-offs.

#### **Document classification**

Hierarchical discourse structures lend themselves naturally to text classification: in a subordinating discourse relation, the nucleus should play a stronger role in the classification decision than the satellite. Various implementations of this idea have been proposed.

- Focusing on within-sentence discourse relations and lexicon-based classification (see § 4.1.2), Voll and Taboada (2007) simply ignore the text in the satellites of each discourse relation.
- At the document level, elements of each discourse relation argument can be reweighted, favoring words in the nucleus, and disfavoring words in the satellite (Heerschop et al., 2011; Bhatia et al., 2015). This approach can be applied recursively, computing weights across the entire document. The weights can be relation-specific, so that the features from the satellites of contrastive relations are discounted or even reversed.
- Alternatively, the hierarchical discourse structure can define the structure of a **recursive neural network** (see § 10.6.1). In this network, the representation of each

<span id="page-412-0"></span><sup>&</sup>lt;sup>11</sup>Formally, 16.11 is a special case of the **knapsack problem**, in which the goal is to find a subset of items with maximum value, constrained by some maximum weight (Cormen et al., 2009).

<span id="page-413-0"></span>discourse unit is computed from its arguments and from a parameter corresponding to the discourse relation [\(Ji and Smith, 2017\)](#page-537-9).

Shallow, non-hierarchical discourse relations have also been applied to document classification. One approach is to impose a set of constraints on the analyses of individual discourse units, so that adjacent units have the same polarity when they are connected by a discourse relation indicating agreement, and opposite polarity when connected by a contrastive discourse relation, indicating disagreement [\(Somasundaran et al., 2009;](#page-562-10) [Zirn](#page-573-3) [et al., 2011\)](#page-573-3). [Yang and Cardie](#page-571-9) [\(2014\)](#page-571-9) apply explicitly-marked relations from the Penn Discourse Treebank to the problem of sentence-level sentiment polarity classification (see § [4.1\)](#page-86-1). They impose the following soft constraints:

- When a CONTRAST relation appears at the beginning of a sentence, the sentence should have the opposite sentiment polarity as its predecessor.
- When an EXPANSION or CONTINGENCY appears at the beginning of a sentence, it should have the same polarity as its predecessor.
- When a CONTRAST relation appears *within* a sentence, the sentence should have neutral polarity, since it is likely to express both sentiments.

These discourse-driven constraints are shown to improve performance on two datasets of product reviews.

#### **Coherence**

Just as **grammaticality** is the property shared by well-structured sentences, **coherence** is the property shared by well-structured discourses. One application of discourse processing is to measure (and maximize) the coherence of computer-generated texts like translations and summaries [\(Kibble and Power, 2004\)](#page-539-12). Coherence assessment is also used to evaluate human-generated texts, such as student essays (e.g., [Miltsakaki and Kukich, 2004;](#page-549-12) [Burstein et al., 2013\)](#page-517-7).

Coherence subsumes a range of phenomena, many of which have been highlighted earlier in this chapter: e.g., that adjacent sentences should be lexically cohesive [\(Foltz](#page-528-11) [et al., 1998;](#page-528-11) [Ji et al., 2015;](#page-537-10) [Li and Jurafsky, 2017\)](#page-543-10), and that entity references should follow the principles of centering theory [\(Barzilay and Lapata, 2008;](#page-513-9) [Nguyen and Joty, 2017\)](#page-552-10). Discourse relations also bear on the coherence of a text in a variety of ways:

• Hierarchical discourse relations tend to have a "canonical ordering" of the nucleus and satellite [\(Mann and Thompson, 1988\)](#page-546-8): for example, in the ELABORATION relation from rhetorical structure theory, the nucleus always comes first, while in the JUSTIFICATION relation, the satellite tends to be first [\(Marcu, 1997b\)](#page-547-12).

#### 16.3. RELATIONS 397

- Discourse relations should be signaled by connectives that are appropriate to the semantic or functional relationship between the arguments: for example, a coherent text would be more likely to use *however* to signal a COMPARISON relation than a *temporal* relation [\(Kibble and Power, 2004\)](#page-539-12).
- Discourse relations tend to be ordered in appear in predictable sequences: for example, COMPARISON relations tend to immediately precede CONTINGENCY relations [\(Pitler et al., 2008\)](#page-555-15). This observation can be formalized by generalizing the entity grid model (§ [16.2.2\)](#page-400-0), so that each cell (i, j) provides information about the role of the discourse argument containing a mention of entity j in sentence i [\(Lin](#page-545-8) [et al., 2011\)](#page-545-8). For example, if the first sentence is ARG1 of a comparison relation, then any entity mentions in the sentence would be labeled COMP.ARG1. This approach can also be applied to RST discourse relations [\(Feng et al., 2014\)](#page-527-8).

**Datasets** One difficulty with evaluating metrics of discourse coherence is that humangenerated texts usually meet some minimal threshold of coherence. For this reason, much of the research on measuring coherence has focused on synthetic data. A typical setting is to permute the sentences of a human-written text, and then determine whether the original sentence ordering scores higher according to the proposed coherence measure [\(Barzi](#page-513-9)[lay and Lapata, 2008\)](#page-513-9). There are also small datasets of human evaluations of the coherence of machine summaries: for example, human judgments of the summaries from the participating systems in the 2003 Document Understanding Conference are available online.[12](#page-414-0) Researchers from the Educational Testing Service (an organization which administers several national exams in the United States) have studied the relationship between discourse coherence and student essay quality [\(Burstein et al., 2003,](#page-517-8) [2010\)](#page-517-9). A public dataset of essays from second-language learners, with quality annotations, has been made available by researchers at Cambridge University [\(Yannakoudakis et al., 2011\)](#page-571-10). At the other extreme, [Louis and Nenkova](#page-545-9) [\(2013\)](#page-545-9) analyze the structure of professionally written scientific essays, finding that discourse relation transitions help to distinguish prize-winning essays from other articles in the same genre.

# **Additional resources**

For a manuscript-length discussion of discourse processing, see [Stede](#page-563-14) [\(2011\)](#page-563-14). Articlelength surveys are offered by [Webber et al.](#page-568-9) [\(2012\)](#page-568-9) and [Webber and Joshi](#page-568-10) [\(2012\)](#page-568-10).

<span id="page-414-0"></span><sup>12</sup>http://homepages.inf.ed.ac.uk/mlap/coherence/

# <span id="page-415-0"></span>**Exercises**

- 1. Some discourse connectives tend to occur between their arguments; others can precede both arguments, and a few can follow both arguments. Indicate whether the following connectives can occur between, before, and after their arguments: *however*, *but*, *while* (contrastive, not temporal), *although*, *therefore*, *nonetheless*.
- 2. This exercise is to be done in pairs. Each participant selects an article from today's news, and replaces all mentions of individual people with special tokens like PERSON1, PERSON2, and so on. The other participant should then use the rules of centering theory to guess each type of referring expression: full name (*Captain Ahab*), partial name (e.g., *Ahab*), nominal (e.g., *the ship's captain*), or pronoun. Check whether the predictions match the original text, and whether the text conforms to the rules of centering theory.
- 3. In this exercise, you will produce a figure similar to [Figure 16.1.](#page-397-1)
  - a) Implement the smoothed cosine similarity metric from [Equation 16.2,](#page-397-3) using the smoothing kernel k = [.5, .3, .15, .05].
  - b) Download the text of a news article with at least ten paragraphs.
  - c) Compute and plot the smoothed similarity s over the length of the article.
  - d) Identify *local minima* in s as follows: first find all sentences m such that s<sup>m</sup> < sm±1. Then search among these points to find the five sentences with the lowest sm.
  - e) How often do the five local minima correspond to paragraph boundaries?
    - The fraction of local minima that are paragraph boundaries is the **precisionat-**k, where in this case, k = 5.
    - The fraction of paragraph boundaries which are local minima is the **recallat-**k.
    - Compute precision-at-k and recall-at-k for k = 3 and k = 10.
- 4. One way to formulate text segmentation as a probabilistic model is through the use of the **Dirichlet Compound Multinomial** (DCM) distribution, which computes the probability of a bag-of-words, DCM(x; α), where the parameter α is a vector of positive reals. This distribution can be configured to assign high likelihood to bagof-words vectors that are internally coherent, such that individual words appear repeatedly: for example, this behavior can be observed for simple parameterizations, such as α = α1 with α < 1.

Let ψα(i, j) represent the log-probability of a segment wi+1:<sup>j</sup> under a DCM distribution with parameter α. Give a dynamic program for segmenting a text into a total

<span id="page-416-0"></span>of *K* segments maximizing the sum of log-probabilities  $\sum_{k=1}^{K} \psi_{\alpha}(s_{k-1}, s_k)$ , where  $s_k$  indexes the last token of segment *k*, and  $s_0 = 0$ . The time complexity of your dynamic program should not be worse than quadratic in the length of the input and linear in the number of segments.

5. Building on the previous problem, you will now adapt the CKY algorithm to perform hierarchical segmentation. Define a hierarchical segmentation as a set of segmentations  $\{\{s_k^{(\ell)}\}_{k=1}^{K^{(\ell)}}\}_{\ell=1}^L$ , where *L* is the segmentation depth. To ensure that the segmentation is hierarchically valid, we require that each segmentation point  $s_k^{(\ell)}$  at level  $\ell$  is also a segmentation point at level  $\ell - 1$ , where  $\ell > 1$ .

For simplicity, this problem focuses on binary hierarchical segmentation, so that each segment at level  $\ell > 1$  has exactly 2 subsegments. Define the score of a hierarchical segmentation as the sum of the scores of all segments (at all levels), using the the DCM log-probabilities from the previous problem as the segment scores. Give a CKY-like recurrence such that the optimal "parse" of the text is the maximum log-probability binary segmentation with exactly *L* levels.

- 6. The entity grid representation of centering theory can be used to compute a score for adjacent sentences, as described in § 16.2.2. Given a set of sentences, these scores can be used to compute an optimal ordering. Show that finding the ordering with the maximum log probability is NP-complete, by reduction from a well-known problem.
- 7. In § 16.3.2, it is noted that bottom-up parsing with compositional vector representations of each span is not guaranteed to be optimal. In this exercise, you will construct a minimal example proving this point. Consider a discourse with four units, with base representations  $\{z^{(i)}\}_{i=1}^4$ . Construct a scenario in which the parse selected by bottom-up parsing is not optimal, and give the precise mathematical conditions under which this suboptimal parse is selected. You may ignore the relation labels  $\ell$  for the purpose of this example.
- 8. As noted in § 16.3.3, arguments can described by hypergraphs, in which a segment may **undercut** a proposed edge between two other segments. Extend the model of extractive summarization described in § 16.3.4 to arguments, adding the follwoing constraint: if segment *i* undercuts an argumentative relationship between *j* and *k*, then *i* cannot be included in the summary unless both *j* and *k* are included. Your solution should take the form of a set of *linear* constraints on an integer linear program that is, each constraint can only involve addition and subtraction of variables.

In the next two exercises, you will explore the use of discourse connectives in a real corpus. Using NLTK, acquire the Brown corpus, and identify sentences that begin with any of the following connectives: *however, nevertheless, moreover, furthermore, thus*.

- <span id="page-417-0"></span>9. Both lexical consistency and discourse connectives contribute to the **cohesion** of a text. We might therefore expect adjacent sentences that are joined by explicit discourse connectives to also have higher word overlap. Using the Brown corpus, test this theory by computing the average cosine similarity between adjacent sentences that are connected by one of the connectives mentioned above. Compare this to the average cosine similarity of all other adjacent sentences. If you know how, perform a two-sample t-test to determine whether the observed difference is statistically significant.
- 10. Group the above connectives into the following three discourse relations:
  - Expansion: *moreover, furthermore*
  - Comparison: *however, nevertheless*
  - Contingency: *thus*

Focusing on pairs of sentences which are joined by one of these five connectives, build a classifier to predict the discourse relation from the text of the two adjacent sentences — taking care to ignore the connective itself. Use the first 30000 sentences of the Brown corpus as the training set, and the remaining sentences as the test set. Compare the performance of your classifier against simply choosing the most common class. Using a bag-of-words classifier, it is hard to do much better than this baseline, so consider more sophisticated alternatives!

# <span id="page-418-0"></span>**Part IV Applications**

# <span id="page-420-1"></span><span id="page-420-0"></span>**Chapter 17 Information extraction**

Computers offer powerful capabilities for searching and reasoning about structured records and relational data. Some have argued that the most important limitation of artificial intelligence is not inference or learning, but simply having too little knowledge [\(Lenat et al.,](#page-543-11) [1990\)](#page-543-11). Natural language processing provides an appealing solution: automatically construct a structured **knowledge base** by reading natural language text.

For example, many Wikipedia pages have an "infobox" that provides structured information about an entity or event. An example is shown in [Figure 17.1a:](#page-421-0) each row represents one or more properties of the entity IN THE AEROPLANE OVER THE SEA, a record album. The set of properties is determined by a predefined **schema**, which applies to all record albums in Wikipedia. As shown in [Figure 17.1b,](#page-421-0) the values for many of these fields are indicated directly in the first few sentences of text on the same Wikipedia page.

The task of automatically constructing (or "populating") an infobox from text is an example of **information extraction**. Much of information extraction can be described in terms of **entities**, **relations**, and **events**.

- **Entities** are uniquely specified objects in the world, such as people (JEFF MANGUM), places (ATHENS, GEORGIA), organizations (MERGE RECORDS), and times (FEBRUARY 10, 1998). Chapter [8](#page-192-0) described the task of **named entity recognition**, which labels tokens as parts of entity spans. Now we will see how to go further, **linking** each entity **mention** to an element in a **knowledge base**.
- **Relations** include a **predicate** and two **arguments**: for example, CAPITAL(GEORGIA, ATLANTA).
- **Events** involve multiple typed arguments. For example, the production and release

#### <span id="page-421-1"></span><span id="page-421-0"></span>Studio album by Neutral Milk Hotel

| Released | February 10, 1998                        |
|----------|------------------------------------------|
| Recorded | July-September 1997                      |
| Studio   | Pet Sounds Studio, Denver,<br>Colorado   |
| Genre    | Indie rock • psychedelic folk •<br>lo-fi |
| Length   | 39:55                                    |
| Label    | Merge • Domino                           |
| Producer | Robert Schneider                         |
|          |                                          |

(a) A Wikipedia infobox

(17.1) In the Aeroplane Over the Sea is the second and final studio album by the American indie rock band Neutral Milk Hotel.

- (17.2) It was released in the United States on <u>February 10, 1998</u> on <u>Merge Records</u> and <u>May 1998</u> on <u>Blue Rose Records</u> in the United Kingdom.
- (17.3) Jeff Mangum moved from Athens, Georgia to Denver, Colorado to prepare the bulk of the album's material with producer Robert Schneider, this time at Schneider's newly created Pet Sounds Studio at the home of Jim McIntyre.

(b) The first few sentences of text. Strings that match fields or field names in the infobox are <u>underlined</u>; strings that mention other entities are wavy <u>underlined</u>.

Figure 17.1: From the Wikipedia page for the album "In the Aeroplane Over the Sea", retrieved October 26, 2017.

of the album described in Figure 17.1 is described by the event,

 $\langle \text{title} : \text{In the Aeroplane Over the Sea},$ artist : Neutral Milk Hotel, Release-date : 1998-Feb-10, . . .  $\rangle$ 

The set of arguments for an event type is defined by a **schema**. Events often refer to time-delimited occurrences: weddings, protests, purchases, terrorist attacks.

Information extraction is similar to semantic role labeling (chapter 13): we may think of predicates as corresponding to events, and the arguments as defining slots in the event representation. However, the goals of information extraction are different. Rather than accurately parsing every sentence, information extraction systems often focus on recognizing a few key relation or event types, or on the task of identifying all properties of a given entity. Information extraction is often evaluated by the correctness of the resulting knowledge base, and not by how many sentences were accurately parsed. The goal is sometimes described as **macro-reading**, as opposed to **micro-reading**, in which each sentence must be analyzed correctly. Macro-reading systems are not penalized for ignoring difficult sentences, as long as they can recover the same information from other, easierto-read sources. However, macro-reading systems must resolve apparent inconsistencies

<span id="page-422-3"></span>(was the album released on MERGE RECORDS or BLUE ROSE RECORDS?), requiring reasoning across the entire dataset.

In addition to the basic tasks of recognizing entities, relations, and events, information extraction systems must handle negation, and must be able to distinguish statements of fact from hopes, fears, hunches, and hypotheticals. Finally, information extraction is often paired with the problem of **question answering**, which requires accurately parsing a query, and then selecting or generating a textual answer. Question answering systems can be built on knowledge bases that are extracted from large text corpora, or may attempt to identify answers directly from the source texts.

# <span id="page-422-0"></span>**17.1 Entities**

The starting point for information extraction is to identify mentions of entities in text. Consider the following example:

<span id="page-422-1"></span>(17.4) *The United States Army captured a hill overlooking Atlanta on May 14, 1864*.

For this sentence, there are two goals:

- 1. *Identify* the spans *United States Army*, *Atlanta*, and *May 14, 1864* as entity mentions. (The hill is not uniquely identified, so it is not a *named* entity.) We may also want to recognize the **named entity types**: organization, location, and date. This is **named entity recognition**, and is described in [chapter 8.](#page-192-0)
- 2. *Link* these spans to entities in a knowledge base: U.S. ARMY, ATLANTA, and 1864- MAY-14. This task is known as **entity linking**.

The strings to be linked to entities are **mentions** — similar to the use of this term in coreference resolution. In some formulations of the entity linking task, only named entities are candidates for linking. This is sometimes called **named entity linking** [\(Ling et al.,](#page-545-10) [2015\)](#page-545-10). In other formulations, such as **Wikification** [\(Milne and Witten, 2008\)](#page-549-13), any string can be a mention. The set of target entities often corresponds to Wikipedia pages, and Wikipedia is the basis for more comprehensive knowledge bases such as YAGO [\(Suchanek](#page-564-10) [et al., 2007\)](#page-564-10), DBPedia [\(Auer et al., 2007\)](#page-511-12), and Freebase [\(Bollacker et al., 2008\)](#page-515-6). Entity linking may also be performed in more "closed" settings, where a much smaller list of targets is provided in advance. The system must also determine if a mention does not refer to any entity in the knowledge base, sometimes called a **NIL entity** [\(McNamee and Dang,](#page-548-6) [2009\)](#page-548-6).

Returning to [\(17.4\)](#page-422-1), the three entity mentions may seem unambiguous. But the Wikipedia disambiguation page for the string *Atlanta* says otherwise:[1](#page-422-2) there are more than twenty

<span id="page-422-2"></span><sup>1</sup>[https://en.wikipedia.org/wiki/Atlanta\\_\(disambiguation\)](https://en.wikipedia.org/wiki/Atlanta_(disambiguation)), retrieved November 1, 2017.

<span id="page-423-2"></span>different towns and cities, five United States Navy vessels, a magazine, a television show, a band, and a singer — each prominent enough to have its own Wikipedia page. We now consider how to choose among these dozens of possibilities. In this chapter we will focus on supervised approaches. Unsupervised entity linking is closely related to the problem of **cross-document coreference resolution**, where the task is to identify pairs of mentions that corefer, across document boundaries [\(Bagga and Baldwin, 1998b;](#page-512-12) [Singh et al., 2011\)](#page-561-7).

#### <span id="page-423-0"></span>**17.1.1 Entity linking by learning to rank**

Entity linking is often formulated as a **ranking** problem,

$$\hat{y} = \operatorname*{argmax}_{y \in \mathcal{Y}(\boldsymbol{x})} \Psi(y, \boldsymbol{x}, \boldsymbol{c}),$$
[17.1]

where y is a target entity, x is a description of the mention, Y(x) is a set of candidate entities, and c is a description of the context — such as the other text in the document, or its metadata. The function Ψ is a scoring function, which could be a linear model, Ψ(y, x, c) = θ · f(y, x, c), or a more complex function such as a neural network. In either case, the scoring function can be learned by minimizing a margin-based **ranking loss**,

<span id="page-423-1"></span>
$$\ell(\hat{y}, y^{(i)}, \boldsymbol{x}^{(i)}, \boldsymbol{c}^{(i)}) = \left(\Psi(\hat{y}, \boldsymbol{x}^{(i)}, \boldsymbol{c}^{(i)}) - \Psi(y^{(i)}, \boldsymbol{x}^{(i)}, \boldsymbol{c}^{(i)}) + 1\right)_{+}, \quad [17.2]$$

where y (i) is the ground truth and yˆ 6= y (i) is the predicted target for mention x (i) in context c (i) [\(Joachims, 2002;](#page-537-11) [Dredze et al., 2010\)](#page-525-10).

**Candidate identification** For computational tractability, it is helpful to restrict the set of candidates, Y(x). One approach is to use a **name dictionary**, which maps from strings to the entities that they might mention. This mapping is many-to-many: a string such as *Atlanta* can refer to multiple entities, and conversely, an entity such as ATLANTA can be referenced by multiple strings. A name dictionary can be extracted from Wikipedia, with links between each Wikipedia entity page and the anchor text of all hyperlinks that point to the page [\(Bunescu and Pasca, 2006;](#page-517-10) [Ratinov et al., 2011\)](#page-557-10). To improve recall, the name dictionary can be augmented by partial and approximate matching [\(Dredze et al., 2010\)](#page-525-10), but as the set of candidates grows, the risk of false positives increases. For example, the string *Atlanta* is a partial match to *the Atlanta Fed* (a name for the FEDERAL RESERVE BANK OF ATLANTA), and a noisy match (edit distance of one) from *Atalanta* (a heroine in Greek mythology and an Italian soccer team).

**Features** Feature-based approaches to entity ranking rely on three main types of local information [\(Dredze et al., 2010\)](#page-525-10):

- <span id="page-424-1"></span>• The similarity of the mention string to the canonical entity name, as quantified by string similarity. This feature would elevate the city ATLANTA over the basketball team ATLANTA HAWKS for the string *Atlanta*.
- The popularity of the entity, which can be measured by Wikipedia page views or PageRank in the Wikipedia link graph. This feature would elevate ATLANTA, GEOR-GIA over the unincorporated community of ATLANTA, OHIO.
- The entity type, as output by the named entity recognition system. This feature would elevate the city of ATLANTA over the magazine ATLANTA in contexts where the mention is tagged as a location.

In addition to these local features, the document context can also help. If *Jamaica* is mentioned in a document about the Caribbean, it is likely to refer to the island nation; in the context of New York, it is likely to refer to the neighborhood in Queens; in the context of a menu, it might refer to a hibiscus tea beverage. Such hints can be formalized by computing the similarity between the Wikipedia page describing each candidate entity and the mention context  $c^{(i)}$ , which may include the bag-of-words representing the document (Dredze et al., 2010; Hoffart et al., 2011) or a smaller window of text around the mention (Ratinov et al., 2011). For example, we can compute the cosine similarity between bag-of-words vectors for the context and entity description, typically weighted using **inverse document frequency** to emphasize rare words.<sup>2</sup>

**Neural entity linking** An alternative approach is to compute the score for each entity candidate using distributed vector representations of the entities, mentions, and context. For example, for the task of entity linking in Twitter, Yang et al. (2016) employ the bilinear scoring function,

$$\Psi(y, \boldsymbol{x}, \boldsymbol{c}) = \boldsymbol{v}_{\boldsymbol{y}}^{\top} \boldsymbol{\Theta}^{(y, \boldsymbol{x})} \boldsymbol{x} + \boldsymbol{v}_{\boldsymbol{y}}^{\top} \boldsymbol{\Theta}^{(y, \boldsymbol{c})} \boldsymbol{c}, \qquad [17.3]$$

with  $v_y \in \mathbb{R}^{K_y}$  as the vector embedding of entity  $y, x \in \mathbb{R}^{K_x}$  as the embedding of the mention,  $c \in \mathbb{R}^{K_c}$  as the embedding of the context, and the matrices  $\Theta^{(y,x)}$  and  $\Theta^{(y,c)}$  as parameters that score the compatibility of each entity with respect to the mention and context. Each of the vector embeddings can be learned from an end-to-end objective, or pre-trained on unlabeled data.

• Pretrained **entity embeddings** can be obtained from an existing knowledge base (Bordes et al., 2011, 2013), or by running a word embedding algorithm such as WORD2VEC

<span id="page-424-0"></span><sup>&</sup>lt;sup>2</sup>The **document frequency** of word *j* is  $DF(j) = \frac{1}{N} \sum_{i=1}^{N} \delta\left(x_{j}^{(i)} > 0\right)$ , equal to the number of documents in which the word appears. The contribution of each word to the cosine similarity of two bag-of-words vectors can be weighted by the **inverse document frequency**  $\frac{1}{DF(j)}$  or  $\log \frac{1}{DF(j)}$ , to emphasize rare words (Spärck Jones, 1972).

<span id="page-425-2"></span>on the text of Wikipedia, with hyperlinks substituted for the anchor text.[3](#page-425-1)

- The embedding of the mention x can be computed by averaging the embeddings of the words in the mention [\(Yang et al., 2016\)](#page-571-11), or by the compositional techniques described in § [14.8.](#page-361-0)
- The embedding of the context c can also be computed from the embeddings of the words in the context. A **denoising autoencoder** learns a function from raw text to dense K-dimensional vector encodings by minimizing a reconstruction loss [\(Vin](#page-567-4)[cent et al., 2010\)](#page-567-4),

$$\min_{\boldsymbol{\theta}_g, \boldsymbol{\theta}_h} \sum_{i=1}^{N} ||\boldsymbol{x}^{(i)} - g(h(\tilde{\boldsymbol{x}}^{(i)}; \boldsymbol{\theta}_h); \boldsymbol{\theta}_g)||^2,$$
[17.4]

where x˜ (i) is a noisy version of the bag-of-words counts x (i) , which is produced by randomly setting some counts to zero; h : R <sup>V</sup> <sup>→</sup> <sup>R</sup> <sup>K</sup> is an encoder with parameters θh; and g : R <sup>K</sup> <sup>→</sup> <sup>R</sup> V , with parameters θg. The encoder and decoder functions are typically implemented as feedforward neural networks. To apply this model to entity linking, each entity and context are initially represented by the encoding of their bag-of-words vectors, h(e) and g(c), and these encodings are then fine-tuned from labeled data [\(He et al., 2013\)](#page-534-10). The context vector c can also be obtained by convolution (§ [3.4\)](#page-79-0) on the embeddings of words in the document [\(Sun et al., 2015\)](#page-564-11), or by examining metadata such as the author's social network [\(Yang et al., 2016\)](#page-571-11).

The remaining parameters Θ(y,x) and Θ(y,c) can be trained by backpropagation from the margin loss in [Equation 17.2.](#page-423-1)

#### <span id="page-425-0"></span>**17.1.2 Collective entity linking**

Entity linking can be more accurate when it is performed jointly across a document. To see why, consider the following lists:

- (17.5) a. California, Oregon, Washington
  - b. Baltimore, Washington, Philadelphia
  - c. Washington, Adams, Jefferson

In each case, the term *Washington* refers to a different entity, and this reference is strongly suggested by the other entries on the list. In the last list, all three names are highly ambiguous — there are dozens of other *Adams* and *Jefferson* entities in Wikipedia. But a

<span id="page-425-1"></span><sup>3</sup>Pre-trained entity embeddings can be downloaded from [https://code.google.com/archive/p/](https://code.google.com/archive/p/word2vec/.) [word2vec/.](https://code.google.com/archive/p/word2vec/.)

#### <span id="page-426-1"></span>17.1. ENTITIES

preference for coherence motivates **collectively** linking these references to the first three U.S. presidents.

A general approach to collective entity linking is to introduce a compatibility score  $\psi_c(y)$ . Collective entity linking is then performed by optimizing the global objective,

$$\hat{\boldsymbol{y}} = \operatorname*{argmax}_{\boldsymbol{y} \in \mathbb{Y}(\boldsymbol{x})} \Psi_c(\boldsymbol{y}) + \sum_{i=1}^N \Psi_\ell(\boldsymbol{y}^{(i)}, \boldsymbol{x}^{(i)}, \boldsymbol{c}^{(i)}), \qquad [17.5]$$

where  $\mathbb{Y}(\boldsymbol{x})$  is the set of all possible collective entity assignments for the mentions in  $\boldsymbol{x}$ , and  $\psi_{\ell}$  is the local scoring function for each entity *i*. The compatibility function is typically decomposed into a sum of pairwise scores,  $\Psi_c(\boldsymbol{y}) = \sum_{i=1}^N \sum_{j\neq i}^N \Psi_c(y^{(i)}, y^{(j)})$ . These scores can be computed in a number of different ways:

- Wikipedia defines high-level categories for entities (e.g., *living people, Presidents of the United States, States of the United States*), and  $\Psi_c$  can reward entity pairs for the number of categories that they have in common (Cucerzan, 2007).
- Compatibility can be measured by the number of incoming hyperlinks shared by the Wikipedia pages for the two entities (Milne and Witten, 2008).
- In a neural architecture, the compatibility of two entities can be set equal to the inner product of their embeddings, Ψ<sub>c</sub>(y<sup>(i)</sup>, y<sup>(j)</sup>) = v<sub>y<sup>(i)</sup></sub> · v<sub>y<sup>(j)</sup></sub>.
- A non-pairwise compatibility score can be defined using a type of latent variable model known as a **probabilistic topic model** (Blei et al., 2003; Blei, 2012). In this framework, each latent topic is a probability distribution over entities, and each document has a probability distribution over topics. Each entity helps to determine the document's distribution over topics, and in turn these topics help to resolve ambiguous entity mentions (Newman et al., 2006). Inference can be performed using the sampling techniques described in chapter 5.

Unfortunately, collective entity linking is **NP-hard** even for pairwise compatibility functions, so exact optimization is almost certainly intractable. Various approximate inference techniques have been proposed, including **integer linear programming** (Cheng and Roth, 2013), **Gibbs sampling** (Han and Sun, 2012), and graph-based algorithms (Hoffart et al., 2011; Han et al., 2011).

#### <span id="page-426-0"></span>17.1.3 \*Pairwise ranking loss functions

The loss function defined in Equation 17.2 considers only the highest-scoring prediction  $\hat{y}$ , but in fact, the true entity  $y^{(i)}$  should outscore *all* other entities. A loss function based on this idea would give a gradient against the features or representations of several entities,

<span id="page-427-0"></span>Algorithm 18 WARP approximate ranking loss

| 1: <b>p</b> | rocedure $\mathrm{WARP}(y^{(i)}, oldsymbol{x}^{(i)})$                         |                                      |
|-------------|-------------------------------------------------------------------------------|--------------------------------------|
| 2:          | $N \leftarrow 0$                                                              |                                      |
| 3:          | repeat                                                                        |                                      |
| 4:          | Randomly sample $y \sim \mathcal{Y}(oldsymbol{x}^{(i)})$                      |                                      |
| 5:          | $N \leftarrow N + 1$                                                          |                                      |
| 6:          | if $\psi(y, oldsymbol{x}^{(i)}) + 1 > \psi(y^{(i)}, oldsymbol{x}^{(i)})$ then | check for margin violation           |
| 7:          | $r \leftarrow \left\lfloor  \mathcal{Y}(oldsymbol{x}^{(i)}) /N ight floor$    | compute approximate rank             |
| 8:          | return $L_{\text{rank}}(r) \times (\psi(y, \boldsymbol{x}^{(i)}) + 1 + 1)$    | $-\psi(y^{(i)},oldsymbol{x}^{(i)}))$ |
| 9:          | until $N \ge  \mathcal{Y}(\boldsymbol{x}^{(i)})  - 1$                         | ⊳ no violation found                 |
| 10:         | return 0                                                                      | ⊳ return zero loss                   |

not just the top-scoring prediction. Usunier et al. (2009) define a general ranking error function,

$$L_{\text{rank}}(k) = \sum_{j=1}^{k} \alpha_j, \quad \text{with } \alpha_1 \ge \alpha_2 \ge \dots \ge 0,$$
[17.6]

where *k* is equal to the number of labels ranked higher than the correct label  $y^{(i)}$ . This function defines a class of ranking errors: if  $\alpha_j = 1$  for all *j*, then the ranking error is equal to the rank of the correct entity; if  $\alpha_1 = 1$  and  $\alpha_{j>1} = 0$ , then the ranking error is one whenever the correct entity is not ranked first; if  $\alpha_j$  decreases smoothly with *j*, as in  $\alpha_j = \frac{1}{i}$ , then the error is between these two extremes.

This ranking error can be integrated into a margin objective. Remember that large margin classification requires not only the correct label, but also that the correct label outscores other labels by a substantial margin. A similar principle applies to ranking: we want a high rank for the correct entity, and we want it to be separated from other entities by a substantial margin. We therefore define the margin-augmented rank,

$$r(y^{(i)}, \boldsymbol{x}^{(i)}) \triangleq \sum_{y \in \mathcal{Y}(\boldsymbol{x}^{(i)}) \setminus y^{(i)}} \delta\left(1 + \psi(y, \boldsymbol{x}^{(i)}) \ge \psi(y^{(i)}, \boldsymbol{x}^{(i)})\right),$$
[17.7]

where  $\delta(\cdot)$  is a delta function, and  $\mathcal{Y}(\boldsymbol{x}^{(i)}) \setminus y^{(i)}$  is the set of all entity candidates minus the true entity  $y^{(i)}$ . The margin-augmented rank is the rank of the true entity, after augmenting every other candidate with a margin of one, under the current scoring function  $\psi$ . (The context *c* is omitted for clarity, and can be considered part of *x*.)

For each instance, a hinge loss is computed from the ranking error associated with this

<span id="page-428-2"></span>margin-augmented rank, and the violation of the margin constraint,

<span id="page-428-1"></span>
$$\ell(y^{(i)}, \boldsymbol{x}^{(i)}) = \frac{L_{\text{rank}}(r(y^{(i)}, \boldsymbol{x}^{(i)}))}{r(y^{(i)}, \boldsymbol{x}^{(i)})} \sum_{y \in \mathcal{Y}(\boldsymbol{x}) \setminus y^{(i)}} \left(\psi(y, \boldsymbol{x}^{(i)}) - \psi(y^{(i)}, \boldsymbol{x}^{(i)}) + 1\right)_{+}, \quad [17.8]$$

The sum in Equation 17.8 includes non-zero values for every label that is ranked at least as high as the true entity, after applying the margin augmentation. Dividing by the margin-augmented rank of the true entity thus gives the average violation.

The objective in Equation 17.8 is expensive to optimize when the label space is large, as is usually the case for entity linking against large knowledge bases. This motivates a randomized approximation called **WARP** (Weston et al., 2011), shown in Algorithm 18. In this procedure, we sample random entities until one violates the pairwise margin constraint,  $\psi(y, \mathbf{x}^{(i)}) + 1 \geq \psi(y^{(i)}, \mathbf{x}^{(i)})$ . The number of samples N required to find such a violation yields an approximation of the margin-augmented rank of the true entity,  $r(y^{(i)}, \mathbf{x}^{(i)}) \approx \lfloor \frac{|\mathcal{Y}(\mathbf{x})|}{N} \rfloor$ . If a violation is found immediately, N = 1, the correct entity probably ranks below many others,  $r \approx |\mathcal{Y}(\mathbf{x})|$ . If many samples are required before a violation is found,  $N \rightarrow |\mathcal{Y}(\mathbf{x})|$ , then the correct entity is probably highly ranked,  $r \rightarrow 1$ . A computational advantage of WARP is that it is not necessary to find the highest-scoring label, which can impose a non-trivial computational cost when  $\mathcal{Y}(\mathbf{x}^{(i)})$  is large. The objective is conceptually similar to the **negative sampling** objective in WORD2VEC (chapter 14), which compares the observed word against randomly sampled alternatives.

### <span id="page-428-0"></span>17.2 Relations

After identifying the entities that are mentioned in a text, the next step is to determine how they are related. Consider the following example:

(17.6) George Bush traveled to France on Thursday for a summit.

This sentence introduces a relation between the entities referenced by *George Bush* and *France*. In the Automatic Content Extraction (ACE) ontology (Linguistic Data Consortium, 2005), the type of this relation is PHYSICAL, and the subtype is LOCATED. This relation would be written,

Relations take exactly two arguments, and the order of the arguments matters.

In the ACE datasets, relations are annotated between entity mentions, as in the example above. Relations can also hold between nominals, as in the following example from the SemEval-2010 shared task (Hendrickx et al., 2009):

<span id="page-429-2"></span><span id="page-429-1"></span>

| CAUSE-EFFECT        | those cancers were caused by radiation exposures |
|---------------------|--------------------------------------------------|
| INSTRUMENT-AGENCY   | phone operator                                   |
| PRODUCT-PRODUCER    | a factory manufactures suits                     |
| CONTENT-CONTAINER   | a bottle of honey was weighed                    |
| ENTITY-ORIGIN       | letters from foreign countries                   |
| ENTITY-DESTINATION  | the boy went to bed                              |
| COMPONENT-WHOLE     | my apartment has a large kitchen                 |
| MEMBER-COLLECTION   | there are many trees in the forest               |
| COMMUNICATION-TOPIC | the lecture was about semantics                  |

Table 17.1: Relations and example sentences from the SemEval-2010 dataset (Hendrickx et al., 2009)

(17.7) The cup contained tea from dried ginseng.

This sentence describes a relation of type ENTITY-ORIGIN between *tea* and *ginseng*. Nominal relation extraction is closely related to **semantic role labeling** (chapter 13). The main difference is that relation extraction is restricted to a relatively small number of relation types; for example, Table 17.1 shows the ten relation types from SemEval-2010.

#### <span id="page-429-0"></span>17.2.1 Pattern-based relation extraction

Early work on relation extraction focused on hand-crafted patterns (Hearst, 1992). For example, the appositive *Starbuck*, *a native of Nantucket* signals the relation ENTITY-ORIGIN between *Starbuck* and *Nantucket*. This pattern can be written as,

```
PERSON, a native of LOCATION \Rightarrow ENTITY-ORIGIN(PERSON, LOCATION). [17.10]
```

This pattern will be "triggered" whenever the literal string , *a native of* occurs between an entity of type PERSON and an entity of type LOCATION. Such patterns can be generalized beyond literal matches using techniques such as lemmatization, which would enable the words (*buy, buys, buying*) to trigger the same patterns (see § 4.3.1). A more aggressive strategy would be to group all words in a WordNet synset (§ 4.2), so that, e.g., *buy* and *purchase* trigger the same patterns.

Relation extraction patterns can be implemented in finite-state automata (§ 9.1). If the named entity recognizer is also a finite-state machine, then the systems can be combined by finite-state transduction (Hobbs et al., 1997). This makes it possible to propagate uncertainty through the finite-state cascade, and disambiguate from higher-level context. For example, suppose the entity recognizer cannot decide whether *Starbuck* refers to either a PERSON or a LOCATION; in the composed transducer, the relation extractor would be free to select the PERSON annotation when it appears in the context of an appropriate pattern.

#### <span id="page-430-1"></span><span id="page-430-0"></span>17.2.2 Relation extraction as a classification task

Relation extraction can be formulated as a classification problem,

$$\hat{r}_{(i,j),(m,n)} = \operatorname*{argmax}_{r \in \mathcal{R}} \Psi(r,(i,j),(m,n),\boldsymbol{w}),$$
[17.11]

where  $r \in \mathcal{R}$  is a relation type (possibly NIL),  $w_{i+1:j}$  is the span of the first argument, and  $w_{m+1:n}$  is the span of the second argument. The argument  $w_{m+1:n}$  may appear before or after  $w_{i+1:j}$  in the text, or they may overlap; we stipulate only that  $w_{i+1:j}$  is the first argument of the relation. We now consider three alternatives for computing the scoring function.

#### Feature-based classification

In a feature-based classifier, the scoring function is defined as,

$$\Psi(r, (i, j), (m, n), w) = \theta \cdot f(r, (i, j), (m, n), w),$$
[17.12]

with  $\theta$  representing a vector of weights, and  $f(\cdot)$  a vector of features. The pattern-based methods described in § 17.2.1 suggest several features:

- Local features of  $w_{i+1:j}$  and  $w_{m+1:n}$ , including: the strings themselves; whether they are recognized as entities, and if so, which type; whether the strings are present in a **gazetteer** of entity names; each string's syntactic head (§ 9.2.2).
- Features of the span between the two arguments,  $w_{j+1:m}$  or  $w_{n+1:i}$  (depending on which argument appears first): the length of the span; the specific words that appear in the span, either as a literal sequence or a bag-of-words; the wordnet synsets (§ 4.2) that appear in the span between the arguments.
- Features of the syntactic relationship between the two arguments, typically the **dependency path** between the arguments (§ 13.2.1). Example dependency paths are shown in Table 17.2.

#### Kernels

Suppose that the first line of Table 17.2 is a labeled example, and the remaining lines are instances to be classified. A feature-based approach would have to decompose the dependency paths into features that capture individual edges, with or without their labels, and then learn weights for each of these features: for example, the second line contains identical dependencies, but different arguments; the third line contains a different inflection of the word *travel*; the fourth and fifth lines each contain an additional edge on the dependency path; and the sixth example uses an entirely different path. Rather than attempting to create local features that capture all of the ways in which these dependencies paths

<span id="page-431-3"></span><span id="page-431-0"></span>

| 1. | George Bush traveled to France          | $\begin{array}{c} \textit{George Bush} \leftarrow \textit{traveled} \rightarrow \textit{France} \\ \text{NSUBJ} & \text{OBL} \end{array}$ |
|----|-----------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|
| 2. | Ahab traveled to Nantucket              | Ahab $\leftarrow traveled \rightarrow Nantucket$                                                                                          |
| 3. | George Bush will travel to France       | George Bush ← travel → France                                                                                                             |
| 4. | George Bush wants to travel to France   | $\begin{array}{c} George \ Bush \leftarrow wants \rightarrow travel \rightarrow France \\ NSUBJ XCOMP OBL \end{array}$                    |
| 5. | Ahab traveled to a city in France       | Ahab $\leftarrow traveled \rightarrow city \rightarrow France$                                                                            |
| 6. | We await <b>Ahab</b> 's visit to France | $\begin{array}{ccc} Ahab & \leftarrow & visit \rightarrow France \\ & & \text{NMOD} \end{array}$                                          |

Table 17.2: Candidates instances for the PHYSICAL.LOCATED relation, and their dependency paths

are similar and different, we can instead define a similarity function  $\kappa$ , which computes a score for any pair of instances,  $\kappa : \mathcal{X} \times \mathcal{X} \to \mathbb{R}_+$ . The score for any pair of instances (i, j) is  $\kappa(\boldsymbol{x}^{(i)}, \boldsymbol{x}^{(j)}) \geq 0$ , with  $\kappa(i, j)$  being large when instances  $\boldsymbol{x}^{(i)}$  and  $\boldsymbol{x}^{(j)}$  are similar. If the function  $\kappa$  obeys a few key properties it is a valid **kernel function**.<sup>4</sup>

Given a valid kernel function, we can build a non-linear classifier without explicitly defining a feature vector or neural network architecture. For a binary classification problem  $y \in \{-1, 1\}$ , we have the decision function,

<span id="page-431-2"></span>
$$\hat{y} = \text{Sign}(b + \sum_{i=1}^{N} y^{(i)} \alpha^{(i)} \kappa(\boldsymbol{x}^{(i)}, \boldsymbol{x}))$$
[17.13]

where *b* and  $\{\alpha^{(i)}\}_{i=1}^{N}$  are parameters that must be learned from the training set, under the constraint  $\forall_i, \alpha^{(i)} \geq 0$ . Intuitively, each  $\alpha_i$  specifies the importance of the instance  $x^{(i)}$ towards the classification rule. Kernel-based classification can be viewed as a weighted form of the **nearest-neighbor** classifier (Hastie et al., 2009), in which test instances are assigned the most common label among their near neighbors in the training set. This results in a non-linear classification boundary. The parameters are typically learned from a margin-based objective (see § 2.4), leading to the **kernel support vector machine**. To generalize to multi-class classification, we can train separate binary classifiers for each label (sometimes called **one-versus-all**), or train binary classifiers for each pair of possible labels (**one-versus-one**).

Dependency kernels are particularly effective for relation extraction, due to their ability to capture syntactic properties of the path between the two candidate arguments. One class of dependency tree kernels is defined recursively, with the score for a pair of trees

<span id="page-431-1"></span><sup>&</sup>lt;sup>4</sup>The **Gram matrix K** arises from computing the kernel function between all pairs in a set of instances. For a valid kernel, the Gram matrix must be symmetric ( $\mathbf{K} = \mathbf{K}^{\top}$ ) and positive semi-definite ( $\forall a, a^{\top} \mathbf{K} a \ge 0$ ). For more on kernel-based classification, see chapter 14 of Murphy (2012).

<span id="page-432-0"></span>equal to the similarity of the root nodes and the sum of similarities of matched pairs of child subtrees (Zelenko et al., 2003; Culotta and Sorensen, 2004). Alternatively, Bunescu and Mooney (2005) define a kernel function over sequences of unlabeled dependency edges, in which the score is computed as a product of scores for each pair of words in the sequence: identical words receive a high score, words that share a synset or part-of-speech receive a small non-zero score (e.g., *travel / visit*), and unrelated words receive a score of zero.

#### Neural relation extraction

**Convolutional neural networks** (§ 3.4) were an early neural architecture for relation extraction (Zeng et al., 2014; dos Santos et al., 2015). For the sentence  $(w_1, w_2, \ldots, w_M)$ , obtain a matrix of word embeddings **X**, where  $\boldsymbol{x}_m \in \mathbb{R}^K$  is the embedding of  $w_m$ . Now, suppose the candidate arguments appear at positions  $a_1$  and  $a_2$ ; then for each word in the sentence, its position with respect to each argument is  $m - a_1$  and  $m - a_2$ . (Following Zeng et al. (2014), this is a restricted version of the relation extraction task in which the arguments are single tokens.) To capture any information conveyed by these positions, the word embeddings are concatenated with vector encodings of the positional offsets,  $\boldsymbol{x}_{m-a_1}^{(p)}$  and  $\boldsymbol{x}_{m-a_2}^{(p)}$ . (For more on **positional encodings**, see § 18.3.2.) The complete base representation of the sentence is,

$$\mathbf{X}(a_1, a_2) = \begin{pmatrix} \mathbf{x}_1 & \mathbf{x}_2 & \cdots & \mathbf{x}_M \\ \mathbf{x}_{1-a_1}^{(p)} & \mathbf{x}_{2-a_1}^{(p)} & \cdots & \mathbf{x}_{M-a_1}^{(p)} \\ \mathbf{x}_{1-a_2}^{(p)} & \mathbf{x}_{2-a_2}^{(p)} & \cdots & \mathbf{x}_{M-a_2}^{(p)} \end{pmatrix},$$
[17.14]

where each column is a vertical concatenation of a word embedding, represented by the column vector  $x_m$ , and two positional encodings, specifying the position with respect to  $a_1$  and  $a_2$ . The matrix  $\mathbf{X}(a_1, a_2)$  is then taken as input to a convolutional layer (see § 3.4), and max-pooling is applied to obtain a vector. The final scoring function is then,

$$\Psi(r, i, j, \mathbf{X}) = \boldsymbol{\theta}_r \cdot \text{MaxPool}(\text{ConvNet}(\mathbf{X}(i, j); \boldsymbol{\phi})), \qquad [17.15]$$

where  $\phi$  defines the parameters of the convolutional operator, and the  $\theta_r$  defines a set of weights for relation *r*. The model can be trained using a margin objective,

$$\hat{r} = \underset{r}{\operatorname{argmax}} \Psi(r, i, j, \mathbf{X})$$
[17.16]

$$\ell = (1 + \psi(\hat{r}, i, j, \mathbf{X}) - \psi(r, i, j, \mathbf{X}))_{+}.$$
[17.17]

**Recurrent neural networks** (§ 6.3) have also been applied to relation extraction, using a network such as a bidirectional LSTM to encode the words or dependency path between the two arguments. Xu et al. (2015) segment each dependency path into left and

<span id="page-433-1"></span>right subpaths: the path *George Bush* ← NSUBJ *wants* → XCOMP *travel* → OBL *France* is segmented into the subpaths, *George Bush* ← NSUBJ *wants* and *wants* → XCOMP *travel* → OBL *France*. In each path, a recurrent neural network is run from the argument to the root word (in this case, *wants*). The final representation by max pooling (§ [3.4\)](#page-79-0) across all the recurrent states along each path. This process can be applied across separate "channels", in which the inputs consist of embeddings for the words, parts-of-speech, dependency relations, and WordNet hypernyms (e.g., *France*-*nation*; see § [4.2\)](#page-90-0). To define the model formally, let s(m) define the successor of word m in either the left or right subpath (in a dependency path, each word can have a successor in at most one subpath). Let x (c) m indicate the embedding of word (or relation) m in channel c, and let ←− h (c) <sup>m</sup> and −→<sup>h</sup> (c) m indicate the associated recurrent states in the left and right subtrees respectively. Then the complete model is specified as follows,

$$h_{s(m)}^{(c)} = \text{RNN}(x_{s(m)}^{(c)}, h_m^{(c)})$$
[17.18]

$$\boldsymbol{z}^{(c)} = \operatorname{MaxPool}\left(\overleftarrow{\boldsymbol{h}}_{i}^{(c)}, \overleftarrow{\boldsymbol{h}}_{s(i)}^{(c)}, \dots, \overleftarrow{\boldsymbol{h}}_{root}^{(c)}, \overrightarrow{\boldsymbol{h}}_{s(j)}^{(c)}, \dots, \overrightarrow{\boldsymbol{h}}_{root}^{(c)}\right)$$
[17.19]

$$\Psi(r, i, j) = \boldsymbol{\theta} \cdot \left[ \boldsymbol{z}^{(\text{word})}; \boldsymbol{z}^{(\text{POS})}; \boldsymbol{z}^{(\text{dependency})}; \boldsymbol{z}^{(\text{hypernym})} \right].$$
[17.20]

Note that z is computed by applying max-pooling to the *matrix* of horizontally concatenated vectors h, while Ψ is computed from the *vector* of vertically concatenated vectors z. [Xu et al.](#page-570-9) [\(2015\)](#page-570-9) pass the score Ψ through a **softmax** layer to obtain a probability p(r | i, j, w), and train the model by regularized **cross-entropy**. [Miwa and Bansal](#page-550-8) [\(2016\)](#page-550-8) show that a related model can solve the more challenging "end-to-end" relation extraction task, in which the model must simultaneously detect entities and then extract their relations.

#### <span id="page-433-0"></span>**17.2.3 Knowledge base population**

In many applications, what matters is not what fraction of sentences are analyzed correctly, but how much accurate knowledge can be extracted. **Knowledge base population (KBP)** refers to the task of filling in Wikipedia-style infoboxes, as shown in [Figure 17.1a.](#page-421-0) Knowledge base population can be decomposed into two subtasks: **entity linking** (described in § [17.1\)](#page-422-0), and **slot filling** [\(Ji and Grishman, 2011\)](#page-537-12). Slot filling has two key differences from the formulation of relation extraction presented above: the relations hold between entities rather than spans of text, and the performance is evaluated at the *type level* (on entity pairs), rather than on the *token level* (on individual sentences).

From a practical standpoint, there are three other important differences between slot filling and per-sentence relation extraction.

• KBP tasks are often formulated from the perspective of identifying attributes of a few "query" entities. As a result, these systems often start with an **information**

#### 17.2. RELATIONS 417

**retrieval** phase, in which relevant passages of text are obtained by search.

- For many entity pairs, there will be multiple passages of text that provide evidence. Slot filling systems must aggregate this evidence to predict a single relation type (or set of relations).
- Labeled data is usually available in the form of pairs of related entities, rather than annotated passages of text. Training from such type-level annotations is a challenge: two entities may be linked by several relations, or they may appear together in a passage of text that nonetheless does not describe their relation to each other.

Information retrieval is beyond the scope of this text (see [Manning et al., 2008\)](#page-546-0). The remainder of this section describes approaches to information fusion and learning from type-level annotations.

#### **Information fusion**

In knowledge base population, there will often be multiple pieces of evidence for (and sometimes against) a single relation. For example, a search for the entity MAYNARD JACK-SON, JR. may return several passages that reference the entity ATLANTA: [5](#page-434-0)

- <span id="page-434-1"></span>(17.8) a. Elected mayor of **Atlanta** in 1973, **Maynard Jackson** was the first African American to serve as mayor of a major southern city.
  - b. **Atlanta**'s airport will be renamed to honor **Maynard Jackson**, the city's first Black mayor.
  - c. Born in Dallas, Texas in 1938, **Maynard Holbrook Jackson, Jr.** moved to **Atlanta** when he was 8.
  - d. **Maynard Jackson** has gone from one of the worst high schools in **Atlanta** to one of the best.

The first and second examples provide evidence for the relation MAYOR holding between the entities ATLANTA and MAYNARD JACKSON, JR.. The third example provides evidence for a different relation between these same entities, LIVED-IN. The fourth example poses an entity linking problem, referring to MAYNARD JACKSON HIGH SCHOOL. Knowledge base population requires aggregating this sort of textual evidence, and predicting the relations that are most likely to hold.

One approach is to run a single-document relation extraction system (using the techniques described in § [17.2.2\)](#page-430-0), and then aggregate the results [\(Li et al., 2011\)](#page-544-11). Relations

<span id="page-434-0"></span><sup>5</sup> First three examples from: [http://www.georgiaencyclopedia.org/articles/](http://www.georgiaencyclopedia.org/articles/government-politics/maynard-jackson-1938-2003) [government-politics/maynard-jackson-1938-2003](http://www.georgiaencyclopedia.org/articles/government-politics/maynard-jackson-1938-2003); JET magazine, November 10, 2003; <www.todayingeorgiahistory.org/content/maynard-jackson-elected>

<span id="page-435-1"></span>that are detected with high confidence in multiple documents are more likely to be valid, motivating the heuristic,

$$\psi(r, e_1, e_2) = \sum_{i=1}^{N} (\mathbf{p}(r(e_1, e_2) \mid \boldsymbol{w}^{(i)}))^{\alpha},$$
[17.21]

where  $p(r(e_1, e_2) | \boldsymbol{w}^{(i)})$  is the probability of relation r between entities  $e_1$  and  $e_2$  conditioned on the text  $\boldsymbol{w}^{(i)}$ , and  $\alpha \gg 1$  is a tunable hyperparameter. Using this heuristic, it is possible to rank all candidate relations, and trace out a **precision-recall curve** as more relations are extracted.<sup>6</sup> Alternatively, features can be aggregated across multiple passages of text, feeding a single type-level relation extraction system (Wolfe et al., 2017).

Precision can be improved by introducing constraints across multiple relations. For example, if we are certain of the relation  $PARENT(e_1, e_2)$ , then it cannot also be the case that  $PARENT(e_2, e_1)$ . Integer linear programming makes it possible to incorporate such constraints into a global optimization (Li et al., 2011). Other pairs of relations have positive correlations, such  $MAYOR(e_1, e_2)$  and  $LIVED-IN(e_1, e_2)$ . Compatibility across relation types can be incorporated into probabilistic graphical models (e.g., Riedel et al., 2010).

#### **Distant supervision**

Relation extraction is "annotation hungry," because each relation requires its own labeled data. Rather than relying on annotations of individual documents, it would be preferable to use existing knowledge resources — such as the many facts that are already captured in knowledge bases like DBPedia. However such annotations raise the inverse of the information fusion problem considered above: the existence of the relation MAYOR(MAYNARD JACKSON JR., ATLANTA) provides only **distant supervision** for the example texts in which this entity pair is mentioned.

One approach is to treat the entity pair as the instance, rather than the text itself (Mintz et al., 2009). Features are then aggregated across all sentences in which both entities are mentioned, and labels correspond to the relation (if any) between the entities in a knowledge base, such as FreeBase. Negative instances are constructed from entity pairs that are not related in the knowledge base. In some cases, two entities are related, but the knowledge base is missing the relation; however, because the number of possible entity pairs is huge, these missing relations are presumed to be relatively rare. This approach is shown in Figure 17.2.

In **multiple instance learning**, labels are assigned to *sets* of instances, of which only an unknown subset are actually relevant (Dietterich et al., 1997; Maron and Lozano-Pérez, 1998). This formalizes the framework of distant supervision: the relation REL(A, B) acts

<span id="page-435-0"></span><sup>&</sup>lt;sup>6</sup>The precision-recall curve is similar to the ROC curve shown in Figure 4.4, but it includes the precision  $\frac{TP}{TP+FP}$  rather than the false positive rate  $\frac{FP}{FP+TN}$ .

- <span id="page-436-2"></span><span id="page-436-1"></span>• **Label** : MAYOR(ATLANTA, MAYNARD JACKSON)
  - **–** Elected mayor of **Atlanta** in 1973, **Maynard Jackson** . . .
  - **– Atlanta**'s airport will be renamed to honor **Maynard Jackson**, the city's first Black mayor
  - **–** Born in Dallas, Texas in 1938, **Maynard Holbrook Jackson, Jr.** moved to **Atlanta** when he was 8.
- **Label** : MAYOR(NEW YORK, FIORELLO L<sup>A</sup> GUARDIA)
  - **– Fiorello La Guardia** was Mayor of **New York** for three terms . . .
  - **– Fiorello La Guardia**, then serving on the **New York** City Board of Aldermen. . .
- **Label** : BORN-IN(DALLAS, MAYNARD JACKSON)
  - **–** Born in **Dallas**, Texas in 1938, **Maynard Holbrook Jackson, Jr.** moved to Atlanta when he was 8.
  - **– Maynard Jackson** was raised in **Dallas** . . .
- **Label** : NIL(NEW YORK, MAYNARD JACKSON)
  - **– Jackson** married Valerie Richardson, whom he had met in **New York**. . .
  - **– Jackson** was a member of the Georgia and **New York** bars . . .

Figure 17.2: Four training instances for relation classification using **distant supervision** [Mintz et al.](#page-549-3) [\(2009\)](#page-549-3). The first two instances are positive for the MAYOR relation, and the third instance is positive for the BORN-IN relation. The fourth instance is a negative example, constructed from a pair of entities (NEW YORK, MAYNARD JACKSON) that do not appear in any Freebase relation. Each instance's features are computed by aggregating across all sentences in which the two entities are mentioned.

as a label for the entire set of sentences mentioning entities A and B, even when only a subset of these sentences actually describes the relation. One approach to multi-instance learning is to introduce a binary **latent variable** for each sentence, indicating whether the sentence expresses the labeled relation [\(Riedel et al., 2010\)](#page-558-6). A variety of inference techniques have been employed for this probabilistic model of relation extraction: [Surdeanu](#page-564-12) [et al.](#page-564-12) [\(2012\)](#page-564-12) use expectation maximization, [Riedel et al.](#page-558-6) [\(2010\)](#page-558-6) use sampling, and [Hoff](#page-535-10)[mann et al.](#page-535-10) [\(2011\)](#page-535-10) use a custom graph-based algorithm. Expectation maximization and sampling are surveyed in [chapter 5,](#page-112-0) and are covered in more detail by [Murphy](#page-551-1) [\(2012\)](#page-551-1); graph-based methods are surveyed by [Mihalcea and Radev](#page-548-7) [\(2011\)](#page-548-7).

## <span id="page-436-0"></span>**17.2.4 Open information extraction**

In classical relation extraction, the set of relations is defined in advance, using a **schema**. The relation for any pair of entities can then be predicted using multi-class classification. In **open information extraction** (OpenIE), a relation can be any triple of text. The example

<span id="page-437-1"></span>

| Task                            | Relation ontology      | Supervision                |
|---------------------------------|------------------------|----------------------------|
| PropBank semantic role labeling | VerbNet                | sentence                   |
| FrameNet semantic role labeling | FrameNet               | sentence                   |
| Relation extraction             | ACE, TAC, SemEval, etc | sentence                   |
| Slot filling                    | ACE, TAC, SemEval, etc | relation                   |
| Open Information Extraction     | open                   | seed relations or patterns |

Table 17.3: Various relation extraction tasks and their properties. VerbNet and FrameNet are described in [chapter 13.](#page-322-0) ACE [\(Linguistic Data Consortium, 2005\)](#page-545-11), TAC [\(McNamee](#page-548-6) [and Dang, 2009\)](#page-548-6), and SemEval [\(Hendrickx et al., 2009\)](#page-534-11) refer to shared tasks, each of which involves an ontology of relation types.

sentence [\(17.8a\)](#page-434-1) instantiates several "relations" of this sort, e.g.,

- (*mayor of*, *Maynard Jackson*, *Atlanta*),
- (*elected*, *Maynard Jackson*, *mayor of Atlanta*),
- (*elected in*, *Maynard Jackson*, *1973*).

Extracting such tuples can be viewed as a lightweight version of **semantic role labeling** [\(chapter 13\)](#page-322-0), with only two argument types: first slot and second slot. The task is generally evaluated on the relation level, rather than on the level of sentences: precision is measured by the number of extracted relations that are accurate, and recall is measured by the number of true relations that were successfully extracted. OpenIE systems are trained from distant supervision or bootstrapping, rather than from labeled sentences.

An early example is the TEXTRUNNER system [\(Banko et al., 2007\)](#page-512-13), which identifies relations with a set of handcrafted syntactic rules. The examples that are acquired from the handcrafted rules are then used to train a classification model that uses part-of-speech patterns as features. Finally, the relations that are extracted by the classifier are aggregated, removing redundant relations and computing the number of times that each relation is mentioned in the corpus. TEXTRUNNER was the first in a series of systems that performed increasingly accurate open relation extraction by incorporating more precise linguistic features [\(Etzioni et al., 2011\)](#page-527-9), distant supervision from Wikipedia infoboxes [\(Wu](#page-570-10) [and Weld, 2010\)](#page-570-10), and better learning algorithms [\(Zhu et al., 2009\)](#page-572-13).

# <span id="page-437-0"></span>**17.3 Events**

Relations link pairs of entities, but many real-world situations involve more than two entities. Consider again the example sentence [\(17.8a\)](#page-434-1), which describes the event of an election,

<span id="page-438-0"></span>with four properties: the office (MAYOR), the district (ATLANTA), the date (1973), and the person elected (MAYNARD JACKSON, JR.). In **event detection**, a schema is provided for each event type (e.g., an election, a terrorist attack, or a chemical reaction), indicating all the possible properties of the event. The system is then required to fill in as many of these properties as possible [\(Doddington et al., 2004\)](#page-525-12).

Event detection systems generally involve a retrieval component (finding relevant documents and passages of text) and an extraction component (determining the properties of the event based on the retrieved texts). Early approaches focused on finite-state patterns for identify event properties [\(Hobbs et al., 1997\)](#page-535-2); such patterns can be automatically induced by searching for patterns that are especially likely to appear in documents that match the event query [\(Riloff, 1996\)](#page-558-7). Contemporary approaches employ techniques that are similar to FrameNet semantic role labeling (§ [13.2\)](#page-329-0), such as structured prediction over local and global features [\(Li et al., 2013\)](#page-544-12) and bidirectional recurrent neural networks [\(Feng](#page-527-10) [et al., 2016\)](#page-527-10). These methods detect whether an event is described in a sentence, and if so, what are its properties.

**Event coreference** Because multiple sentences may describe unique properties of a single event, **event coreference** is required to link event mentions across a single passage of text, or between passages [\(Humphreys et al., 1997\)](#page-536-10). [Bejan and Harabagiu](#page-513-10) [\(2014\)](#page-513-10) define event coreference as the task of identifying event mentions that share the same event participants (i.e., the slot-filling entities) and the same event properties (e.g., the time and location), within or across documents. Event coreference resolution can be performed using supervised learning techniques in a similar way to entity coreference, as described in [chapter 15:](#page-368-0) move left-to-right through the document, and use a classifier to decide whether to link each event reference to an existing cluster of coreferent events, or to create a new cluster [\(Ahn, 2006\)](#page-509-6). Each clustering decision is based on the compatibility of features describing the participants and properties of the event. Due to the difficulty of annotating large amounts of data for entity coreference, unsupervised approaches are especially desirable [\(Chen and Ji, 2009;](#page-519-9) [Bejan and Harabagiu, 2014\)](#page-513-10).

**Relations between events** Just as entities are related to other entities, events may be related to other events: for example, the event of winning an election both *precedes* and *causes* the event of serving as mayor; moving to Atlanta *precedes* and *enables* the event of becoming mayor of Atlanta; moving from Dallas to Atlanta *prevents* the event of later becoming mayor of Dallas. As these examples show, events may be related both temporally and causally. The **TimeML** annotation scheme specifies a set of six temporal relations between events [\(Pustejovsky et al., 2005\)](#page-556-11), derived in part from **interval algebra** [\(Allen,](#page-510-9) [1984\)](#page-510-9). The TimeBank corpus provides TimeML annotations for 186 documents [\(Puste](#page-556-12)[jovsky et al., 2003\)](#page-556-12). Methods for detecting these temporal relations combine supervised

<span id="page-439-3"></span><span id="page-439-2"></span>

|                    | Positive (+)  | Negative (-)      | Underspecified (u)         |
|--------------------|---------------|-------------------|----------------------------|
| Certain (CT)       | Fact: CT+     | Counterfact: CT-  | Certain, but unknown: CTU  |
| Probable (PR)      | Probable: PR+ | Not probable: PR- | (NA)                       |
| Possible (PS)      | Possible: PS+ | Not possible: PS- | (NA)                       |
| Underspecified (U) | (NA)          | (NA)              | Unknown or uncommitted: UU |

Table 17.4: Table of factuality values from the FactBank corpus [\(Saur´ı and Pustejovsky,](#page-560-11) [2009\)](#page-560-11). The entry (NA) indicates that this combination is not annotated.

machine learning with temporal constraints, such as transitivity (e.g. [Mani et al., 2006;](#page-546-11) [Chambers and Jurafsky, 2008\)](#page-518-13).

More recent annotation schemes and datasets combine temporal and causal relations [\(M](#page-549-14)irza [et al., 2014;](#page-549-14) [Dunietz et al., 2017\)](#page-525-13): for example, the CaTeRS dataset includes annotations of 320 five-sentence short stories [\(Mostafazadeh et al., 2016\)](#page-550-9). Abstracting still further, **processes** are networks of causal relations between multiple events. A small dataset of biological processes is annotated in the ProcessBank dataset [\(Berant et al., 2014\)](#page-513-11), with the goal of supporting automatic question answering on scientific textbooks.

# <span id="page-439-0"></span>**17.4 Hedges, denials, and hypotheticals**

The methods described thus far apply to **propositions** about the way things are in the real world. But natural language can also describe events and relations that are likely or unlikely, possible or impossible, desired or feared. The following examples hint at the scope of the problem [\(Prabhakaran et al., 2010\)](#page-556-13):

- (17.9) a. GM will lay off workers.
  - b. A spokesman for GM said GM will lay off workers.
  - c. GM may lay off workers.
  - d. The politician claimed that GM will lay off workers.
  - e. Some wish GM would lay off workers.
  - f. Will GM lay off workers?
  - g. Many wonder whether GM will lay off workers.

Accurate information extraction requires handling these **extra-propositional** aspects of meaning, which are sometimes summarized under the terms **modality** and **negation**. [7](#page-439-1)

<span id="page-439-1"></span><sup>7</sup>The classification of negation as extra-propositional is controversial: [Packard et al.](#page-553-11) [\(2014\)](#page-553-11) argue that negation is a "core part of compositionally constructed logical-form representations." Negation is an element of the semantic parsing tasks discussed in [chapter 12](#page-302-0) and [chapter 13](#page-322-0) — for example, negation markers are

<span id="page-440-1"></span>Modality refers to expressions of the speaker's attitude towards her own statements, including "degree of certainty, reliability, subjectivity, sources of information, and perspective" [\(Morante and Sporleder, 2012\)](#page-550-10). Various systematizations of modality have been proposed (e.g., [Palmer, 2001\)](#page-553-12), including categories such as future, interrogative, imperative, conditional, and subjective. Information extraction is particularly concerned with negation and certainty. For example, [Saur´ı and Pustejovsky](#page-560-11) [\(2009\)](#page-560-11) link negation with a modal calculus of certainty, likelihood, and possibility, creating the two-dimensional schema shown in [Table 17.4.](#page-439-2) This is the basis for the FactBank corpus, with annotations of the **factuality** of all sentences in 208 documents of news text.

A related concept is **hedging**, in which speakers limit their commitment to a proposition [\(Lakoff, 1973\)](#page-542-11):

- (17.10) a. These results **suggest** that expression of c-jun, jun B and jun D genes **might** be involved in terminal granulocyte differentiation. . . [\(Morante and Daelemans,](#page-550-11) [2009\)](#page-550-11)
  - b. A whale is **technically** a mammal [\(Lakoff, 1973\)](#page-542-11)

In the first example, the hedges *suggest* and *might* communicate uncertainty; in the second example, there is no uncertainty, but the hedge *technically* indicates that the evidence for the proposition will not fully meet the reader's expectations. Hedging has been studied extensively in scientific texts [\(Medlock and Briscoe, 2007;](#page-548-8) [Morante and Daelemans, 2009\)](#page-550-11), where the goal of large-scale extraction of scientific facts is obstructed by hedges and speculation. Still another related aspect of modality is **evidentiality**, in which speakers mark the source of their information. In many languages, it is obligatory to mark evidentiality through affixes or particles [\(Aikhenvald, 2004\)](#page-509-7); while evidentiality is not grammaticalized in English, authors are expected to express this information in contexts such as journalism [\(Kovach and Rosenstiel, 2014\)](#page-541-13) and Wikipedia.[8](#page-440-0)

Methods for handling negation and modality generally include two phases:

- 1. detecting negated or uncertain events;
- 2. identifying **scope** of the negation or modal operator.

A considerable body of work on negation has employed rule-based techniques such as regular expressions [\(Chapman et al., 2001\)](#page-518-14) to detect negated events. Such techniques

treated as adjuncts in PropBank semantic role labeling. However, many of the relation extraction methods mentioned in this chapter do not handle negation directly. A further consideration is that negation interacts closely with aspects of modality that are generally not considered in propositional semantics, such as certainty and subjectivity.

<span id="page-440-0"></span><sup>8</sup>https://en.wikipedia.org/wiki/Wikipedia:Verifiability

<span id="page-441-3"></span>match lexical cues (e.g., *Norwood was not elected Mayor*), while avoiding "double negatives" (e.g., *surely all this is not without meaning*). Supervised techniques involve classifiers over lexical and syntactic features [\(Uzuner et al., 2009\)](#page-566-10) and sequence labeling [\(Prab](#page-556-13)[hakaran et al., 2010\)](#page-556-13).

The scope refers to the elements of the text whose propositional meaning is negated or modulated [\(Huddleston and Pullum, 2005\)](#page-536-11), as elucidated in the following example from [Morante and Sporleder](#page-550-10) [\(2012\)](#page-550-10):

(17.11) [ After his habit he said ] **nothing**, and after mine I asked no questions. After his habit he said nothing, and [ after mine I asked ] **no** [ questions ].

In this sentence, there are two negation cues (*nothing* and *no*). Each negates an event, indicated by the underlined verbs *said* and *asked*, and each occurs within a scope: *after his habit he said* and *after mine I asked questions*. Scope identification is typically formalized as sequence labeling problems, with each word token labeled as beginning, inside, or outside of a cue, focus, or scope span (see § [8.3\)](#page-200-0). Conventional sequence labeling approaches can then be applied, using surface features as well as syntax [\(Velldal et al., 2012\)](#page-567-8) and semantic analysis [\(Packard et al., 2014\)](#page-553-11). Labeled datasets include the BioScope corpus of biomedical texts [\(Vincze et al., 2008\)](#page-567-9) and a shared task dataset of detective stories by Arthur Conan Doyle [\(Morante and Blanco, 2012\)](#page-550-12).

# <span id="page-441-0"></span>**17.5 Question answering and machine reading**

The victory of the Watson question-answering system against three top human players on the game show *Jeopardy!* was a landmark moment for natural language processing [\(Fer](#page-527-11)[rucci et al., 2010\)](#page-527-11). Game show questions are usually answered by **factoids**: entity names and short phrases.[9](#page-441-2) The task of factoid question answering is therefore closely related to information extraction, with the additional problem of accurately parsing the question.

### <span id="page-441-1"></span>**17.5.1 Formal semantics**

Semantic parsing is an effective method for question-answering in restricted domains such as questions about geography and airline reservations [\(Zettlemoyer and Collins,](#page-572-3) [2005\)](#page-572-3), and has also been applied in "open-domain" settings such as question answering on Freebase [\(Berant et al., 2013\)](#page-513-12) and biomedical research abstracts [\(Poon and Domingos,](#page-555-8) [2009\)](#page-555-8). One approach is to convert the question into a lambda calculus expression that returns a boolean value: for example, the question *who is the mayor of the capital of Georgia?*

<span id="page-441-2"></span><sup>9</sup>The broader landscape of question answering includes "why" questions (*Why did Ahab continue to pursue the white whale?*), "how questions" (*How did Queequeg die?*), and requests for summaries (*What was Ishmael's attitude towards organized religion?*). For more, see [Hirschman and Gaizauskas](#page-535-11) [\(2001\)](#page-535-11).

<span id="page-442-2"></span>would be converted to,

$$\lambda x. \exists y \text{ CAPITAL}(\text{GEORGIA}, y) \land \text{MAYOR}(y, x).$$
 [17.22]

This lambda expression can then be used to query an existing knowledge base, returning "true" for all entities that satisfy it.

# <span id="page-442-0"></span>**17.5.2 Machine reading**

Recent work has focused on answering questions about specific textual passages, similar to the reading comprehension examinations for young students [\(Hirschman et al., 1999\)](#page-535-12). This task has come to be known as **machine reading**.

## **Datasets**

The machine reading problem can be formulated in a number of different ways. The most important distinction is what form the answer should take.

- <span id="page-442-1"></span>• **Multiple-choice question answering**, as in the MCTest dataset of stories [\(Richard](#page-558-8)[son et al., 2013\)](#page-558-8) and the New York Regents Science Exams [\(Clark, 2015\)](#page-521-13). In MCTest, the answer is deducible from the text alone, while in the science exams, the system must make inferences using an existing model of the underlying scientific phenomena. Here is an example from MCTest:
  - (17.12) James the turtle was always getting into trouble. Sometimes he'd reach into the freezer and empty out all the food . . .
    - Q: What is the name of the trouble making turtle?
    - (a) Fries
    - (b) Pudding
    - (c) James
    - (d) Jane
- **Cloze**-style "fill in the blank" questions, as in the CNN/Daily Mail comprehension task [\(Hermann et al., 2015\)](#page-534-12), the Children's Book Test [\(Hill et al., 2016\)](#page-534-13), and the Whodid-What dataset [\(Onishi et al., 2016\)](#page-553-13). In these tasks, the system must guess which word or entity completes a sentence, based on reading a passage of text. Here is an example from Who-did-What:
  - (17.13) Q: Tottenham manager Juande Ramos has hinted he will allow to leave if the Bulgaria striker makes it clear he is unhappy. [\(Onishi et al., 2016\)](#page-553-13)

The query sentence may be selected either from the story itself, or from an external summary. In either case, datasets can be created automatically by processing large

<span id="page-443-1"></span>quantities existing documents. An additional constraint is that that missing element from the cloze must appear in the main passage of text: for example, in Who-did-What, the candidates include all entities mentioned in the main passage. In the CNN/Daily Mail dataset, each entity name is replaced by a unique identifier, e.g., ENTITY37. This ensures that correct answers can only be obtained by accurately reading the text, and not from external knowledge about the entities.

- Extractive question answering, in which the answer is drawn from the original text. In WikiQA, answers are sentences (Yang et al., 2015). In the Stanford Question Answering Dataset (SQuAD), answers are words or short phrases (Rajpurkar et al., 2016):
  - (17.14) In metereology, precipitation is any product of the condensation of atmospheric water vapor that falls under gravity.

Q: What causes precipitation to fall? A: gravity

In both WikiQA and SQuAD, the original texts are Wikipedia articles, and the questions are generated by crowdworkers.

#### <span id="page-443-0"></span>Methods

A baseline method is to search the text for sentences or short passages that overlap with both the query and the candidate answer (Richardson et al., 2013). In example (17.12), this baseline would select the correct answer, since *James* appears in a sentence that includes the query terms *trouble* and *turtle*.

This baseline can be implemented as a neural architecture, using an **attention mechanism** (see § 18.3.1), which scores the similarity of the query to each part of the source text (Chen et al., 2016). The first step is to encode the passage  $w^{(p)}$  and the query  $w^{(q)}$ , using two bidirectional LSTMs (§ 7.6).

$$\boldsymbol{h}^{(q)} = \text{BiLSTM}(\boldsymbol{w}^{(q)}; \boldsymbol{\Theta}^{(q)})$$
[17.23]

$$\boldsymbol{h}^{(p)} = \text{BiLSTM}(\boldsymbol{w}^{(p)}; \boldsymbol{\Theta}^{(p)}).$$
[17.24]

The query is represented by vertically concatenating the final states of the left-to-right and right-to-left passes:

$$\boldsymbol{u} = [\overrightarrow{\boldsymbol{h}^{(q)}}_{M_q}; \overleftarrow{\boldsymbol{h}^{(q)}}_0].$$
 [17.25]

The attention vector is computed as a softmax over a vector of bilinear products, and the expected representation is computed by summing over attention values,

$$\tilde{\alpha}_m = (\boldsymbol{u}^{(q)})^\top \mathbf{W}_a \boldsymbol{h}_m^{(p)}$$
[17.26]

$$\boldsymbol{\alpha} = \operatorname{SoftMax}(\tilde{\boldsymbol{\alpha}})$$
[17.27]

$$\boldsymbol{o} = \sum_{m=1}^{M} \alpha_m \boldsymbol{h}_m^{(p)}.$$
 [17.28]

Each candidate answer c is represented by a vector xc. Assuming the candidate answers are spans from the original text, these vectors can be set equal to the corresponding element in h (p) . The score for each candidate answer a is computed by the inner product,

$$\hat{c} = \operatorname*{argmax}_{c} \boldsymbol{o} \cdot \boldsymbol{x}_{c}.$$
[17.29]

This architecture can be trained end-to-end from a loss based on the log-likelihood of the correct answer. A number of related architectures have been proposed (e.g., [Hermann](#page-534-12) [et al., 2015;](#page-534-12) [Kadlec et al., 2016;](#page-538-12) [Dhingra et al., 2017;](#page-524-11) [Cui et al., 2017\)](#page-523-12), and these methods are surveyed by [Wang et al.](#page-568-11) [\(2017\)](#page-568-11).

# **Additional resources**

The field of information extraction is surveyed in course notes by [Grishman](#page-532-0) [\(2012\)](#page-532-0), and more recently in a short survey paper [\(Grishman, 2015\)](#page-532-10). [Shen et al.](#page-561-8) [\(2015\)](#page-561-8) survey the task of entity linking, and [Ji and Grishman](#page-537-12) [\(2011\)](#page-537-12) survey work on knowledge base population. This chapter's discussion of non-propositional meaning was strongly influenced by [Morante and Sporleder](#page-550-10) [\(2012\)](#page-550-10), who introduced a special issue of the journal *Computational Linguistics* dedicated to recent work on modality and negation.

# **Exercises**

- 1. Go to the Wikipedia page for your favorite movie. For each record in the info box (e.g., *Screenplay by: Stanley Kubrick*), report whether there is a sentence in the article containing both the field and value (e.g., *The screenplay was written by Stanley Kubrick*). If not, is there is a sentence in the article containing just the value? (For records with more than one value, just use the first value.)
- 2. Building on your answer in the previous question, report the dependency path between the head words of the field and value for at least three records.
- 3. Consider the following heuristic for entity linking:

- Among all entities that have the same type as the mention (e.g., LOC, PER), choose the one whose name has the lowest edit distance from the mention.
- If more than one entity has the right type and the lowest edit distance from the mention, choose the most popular one.
- If no candidate entity has the right type, choose NIL.

Now suppose you have the following feature function:

f(y, x) = [edit-dist(name(y), x), same-type(y, x), popularity(y), δ (y = NIL)]

Design a set of ranking weights θ that match the heuristic. You may assume that edit distance and popularity are always in the range [0, 100], and that the NIL entity has values of zero for all features except δ (y = NIL).

- 4. Now consider another heuristic:
  - Among all candidate entities that have edit distance zero from the mention, and are the right type, choose the most popular one.
  - If no entity has edit distance zero from the mention, choose the one with the right type that is most popular, regardless of edit distance.
  - If no entity has the right type, choose NIL.

Using the same features and assumptions from the previous problem, prove that there is no set of weights that could implement this heuristic. Then show that the heuristic can be implemented by adding a single feature. Your new feature should consider only the edit distance.

5. Download the Reuters corpus in NLTK, and iterate over the tokens in the corpus:

```
import nltk
nltk.corpus.download('reuters')
from nltk.corpus import reuters
for word in reuters.words():
 #your code here
```

- a) Apply the pattern *, such as* to obtain candidates for the IS-A relation, e.g. IS-A(ROMANIA, COUNTRY). What are three pairs that this method identifies correctly? What are three different pairs that it gets wrong?
- b) Design a pattern for the PRESIDENT relation, e.g. PRESIDENT(PHILIPPINES, CORAZON AQUINO). In this case, you may want to augment your pattern matcher with the ability to match multiple token wildcards, perhaps using case information to detect proper names. Again, list three correct

#### 17.5. QUESTION ANSWERING AND MACHINE READING

- c) Preprocess the Reuters data by running a named entity recognizer, replacing tokens with named entity spans when applicable — e.g., your pattern can now match on *the United States* if the NER system tags it. Apply your PRESIDENT matcher to this preprocessed data. Does the accuracy improve? Compare 20 randomly-selected pairs from this pattern and the one you designed in the previous part.
- 6. Using the same NLTK Reuters corpus, apply distant supervision to build a training set for detecting the relation between nations and their capitals. Start with the following known relations: (JAPAN, TOKYO), (FRANCE, PARIS), (ITALY, ROME). How many positive and negative examples are you able to extract?
- 7. Represent the dependency path  $x^{(i)}$  as a sequence of words and dependency arcs of length  $M_i$ , ignoring the endpoints of the path. In example 1 of Table 17.2, the dependency path is,

$$\boldsymbol{x}^{(1)} = (\underset{\text{NSUBJ}}{\leftarrow}, traveled, \xrightarrow{OBL})$$
[17.30]

If  $x_m^{(i)}$  is a word, then let  $pos(x_m^{(i)})$  be its part-of-speech, using the tagset defined in chapter 8.

We can define the following kernel function over pairs of dependency paths (Bunescu and Mooney, 2005):

$$\begin{split} \kappa(\boldsymbol{x}^{(i)}, \boldsymbol{x}^{(j)}) &= \begin{cases} 0, & M_i \neq M_j \\ \prod_{m=1}^{M_i} c(x_m^{(i)}, x_m^{(j)}), & M_i = M_j \end{cases} \\ c(x_m^{(i)}, x_m^{(j)}) &= \begin{cases} 2, & x_m^{(i)} = x_m^{(j)} \\ 1, & x_m^{(i)} \neq x_m^{(j)} \text{ and } \operatorname{pos}(x_m^{(i)}) = \operatorname{pos}(x_m^{(j)}) \\ 0, & \text{otherwise.} \end{cases} \end{split}$$

Using this kernel function, compute the kernel similarities of example 1 from Table 17.2 with the other five examples.

8. Continuing from the previous problem, suppose that the instances have the following labels:

$$y_2 = 1, y_3 = -1, y_4 = -1, y_5 = 1, y_6 = 1$$
 [17.31]

Equation 17.13 defines a kernel-based classification in terms of parameters  $\alpha$  and b. Using the above labels for  $y_2, \ldots, y_6$ , identify the values of  $\alpha$  and b under which  $\hat{y}_1 = 1$ . Remember the constraint that  $\alpha_i \ge 0$  for all i.

- 9. Consider the neural QA system described in § 17.5.2, but restrict the set of candidate answers to words in the passage, and set each candidate answer embedding *x* equal to the vector *h*<sup>(p)</sup><sub>m</sub>, representing token *m* in the passage, so that *m* = argmax<sub>m</sub> *o* · *h*<sup>(p)</sup><sub>m</sub>. Suppose the system selects answer *m*, but the correct answer is *m*<sup>\*</sup>. Consider the gradient of the margin loss with respect to the attention:
  - a) Prove that  $\frac{\partial \ell}{\partial \alpha_{\hat{m}}} \geq \frac{\partial \ell}{\partial \alpha_{m}*}$ .
  - b) Assuming that  $||\mathbf{h}_{\hat{m}}|| = ||\mathbf{h}_{m^*}||$ , prove that  $\frac{\partial \ell}{\partial \alpha_{\hat{m}}} \ge 0$  and  $\frac{\partial \ell}{\partial \alpha_{m^*}} \le 0$ . Explain in words what this means about how the attention is expected to change after a gradient-based update.

430

# <span id="page-448-2"></span><span id="page-448-0"></span>**Chapter 18 Machine translation**

Machine translation (MT) is one of the "holy grail" problems in artificial intelligence, with the potential to transform society by facilitating communication between people anywhere in the world. As a result, MT has received significant attention and funding since the early 1950s. However, it has proved remarkably challenging, and while there has been substantial progress towards usable MT systems — especially for high-resource language pairs like English-French — we are still far from translation systems that match the nuance and depth of human translations.

# <span id="page-448-1"></span>**18.1 Machine translation as a task**

Machine translation can be formulated as an optimization problem:

$$\hat{\boldsymbol{w}}^{(t)} = \operatorname*{argmax}_{\boldsymbol{w}^{(t)}} \Psi(\boldsymbol{w}^{(s)}, \boldsymbol{w}^{(t)}), \qquad [18.1]$$

where w(s) is a sentence in a **source** language, w(t) is a sentence in the **target language**, and Ψ is a scoring function. As usual, this formalism requires two components: a decoding algorithm for computing wˆ (t) , and a learning algorithm for estimating the parameters of the scoring function Ψ.

Decoding is difficult for machine translation because of the huge space of possible translations. We have faced large label spaces before: for example, in sequence labeling, the set of possible label sequences is exponential in the length of the input. In these cases, it was possible to search the space quickly by introducing locality assumptions: for example, that each tag depends only on its predecessor, or that each production depends only on its parent. In machine translation, no such locality assumptions seem possible: human translators reword, reorder, and rearrange words; they replace single words with multi-word phrases, and vice versa. This flexibility means that in even relatively simple

<span id="page-449-1"></span><span id="page-449-0"></span>![](_page_449_Figure_1.jpeg)

Figure 18.1: The Vauquois Pyramid

translation models, decoding is NP-hard (Knight, 1999). Approaches for dealing with this complexity are described in § 18.4.

Estimating translation models is difficult as well. Labeled translation data usually comes in the form parallel sentences, e.g.,

 $oldsymbol{w}^{(s)}=\!\!A$  Vinay le gusta las manzanas.  $oldsymbol{w}^{(t)}=\!\!V$ inay likes apples.

A useful feature function would note the translation pairs (*gusta*, *likes*), (*manzanas*, *apples*), and even (*Vinay*, *Vinay*). But this word-to-word **alignment** is not given in the data. One solution is to treat this alignment as a **latent variable**; this is the approach taken by classical **statistical machine translation** (SMT) systems, described in § 18.2. Another solution is to model the relationship between  $w^{(t)}$  and  $w^{(s)}$  through a more complex and expressive function; this is the approach taken by **neural machine translation** (NMT) systems, described in § 18.3.

The **Vauquois Pyramid** is a theory of how translation should be done. At the lowest level, the translation system operates on individual words, but the horizontal distance at this level is large, because languages express ideas differently. If we can move up the triangle to syntactic structure, the distance for translation is reduced; we then need only produce target-language text from the syntactic representation, which can be as simple as reading off a tree. Further up the triangle lies semantics; translating between semantic representations should be easier still, but mapping between semantics and surface text is a difficult, unsolved problem. At the top of the triangle is **interlingua**, a semantic representation that is so generic that it is identical across all human languages. Philosophers debate whether such a thing as interlingua is really possible (e.g., Derrida, 1985). While the first-order logic representations discussed in chapter 12 might be thought to be language independent, they are built on an inventory of predicates that are suspiciously similar to English words (Nirenburg and Wilks, 2001). Nonetheless, the idea of linking translation

<span id="page-450-3"></span>

|                           | Adequate? | Fluent? |
|---------------------------|-----------|---------|
| To Vinay it like Python   | yes       | no      |
| Vinay debugs memory leaks | no        | yes     |
| Vinay likes Python        | yes       | yes     |

<span id="page-450-1"></span>Table 18.1: Adequacy and fluency for translations of the Spanish sentence *A Vinay le gusta Python*.

and semantic understanding may still be a promising path, if the resulting translations better preserve the meaning of the original text.

#### <span id="page-450-0"></span>**18.1.1** Evaluating translations

There are two main criteria for a translation, summarized in Table 18.1.

- Adequacy: The translation  $w^{(t)}$  should adequately reflect the linguistic content of  $w^{(s)}$ . For example, if  $w^{(s)} = A$  Vinay le gusta Python, the reference translation is  $w^{(t)} = Vinay$  likes Python. However, the gloss, or word-for-word translation  $w^{(t)} = To$  Vinay it like Python is also considered adequate because it contains all the relevant content. The output  $w^{(t)} = Vinay$  debugs memory leaks is not adequate.
- Fluency: The translation  $w^{(t)}$  should read like fluent text in the target language. By this criterion, the gloss  $w^{(t)} =$  To Vinay it like Python will score poorly, and  $w^{(t)} =$  Vinay debugs memory leaks will be preferred.

Automated evaluations of machine translations typically merge both of these criteria, by comparing the system translation with one or more **reference translations**, produced by professional human translators. The most popular quantitative metric is **BLEU** (bilingual evaluation understudy; Papineni et al., 2002), which is based on *n*-gram precision: what fraction of *n*-grams in the system translation appear in the reference? Specifically, for each *n*-gram length, the precision is defined as,

<span id="page-450-2"></span>
$$p_n = \frac{\text{number of } n\text{-grams appearing in both reference and hypothesis translations}}{\text{number of } n\text{-grams appearing in the hypothesis translation}}.$$

[18.2]

The *n*-gram precisions for three hypothesis translations are shown in Figure 18.2.

The BLEU score is then based on the average,  $\exp \frac{1}{N} \sum_{n=1}^{N} \log p_n$ . Two modifications of Equation 18.2 are necessary: (1) to avoid computing  $\log 0$ , all precisions are smoothed to ensure that they are positive; (2) each *n*-gram in the reference can be used at most once, so that *to to to to to to to does* not achieve  $p_1 = 1$  against the reference *to be or not to be*. Furthermore, precision-based metrics are biased in favor of short translations, which

<span id="page-451-1"></span><span id="page-451-0"></span>

|           | Translation                            | p1     | p2     | p3     | p4     | BP  | BLEU |
|-----------|----------------------------------------|--------|--------|--------|--------|-----|------|
| Reference | Vinay likes programming in Python      |        |        |        |        |     |      |
| Sys1      | To Vinay it like to program Python     | 2<br>7 | 0      | 0      | 0      | 1   | .21  |
| Sys2      | Vinay likes Python                     | 3<br>3 | 1<br>2 | 0      | 0      | .51 | .33  |
| Sys3      | Vinay likes programming in his pajamas | 4<br>6 | 3<br>5 | 2<br>4 | 1<br>3 | 1   | .76  |

Figure 18.2: A reference translation and three system outputs. For each output, p<sup>n</sup> indicates the precision at each n-gram, and BP indicates the brevity penalty.

can achieve high scores by minimizing the denominator in [\[18.2\]](#page-450-2). To avoid this issue, a **brevity penalty** is applied to translations that are shorter than the reference. This penalty is indicated as "BP" in [Figure 18.2.](#page-451-0)

Automated metrics like BLEU have been validated by correlation with human judgments of translation quality. Nonetheless, it is not difficult to construct examples in which the BLEU score is high, yet the translation is disfluent or carries a completely different meaning from the original. To give just one example, consider the problem of translating pronouns. Because pronouns refer to specific entities, a single incorrect pronoun can obliterate the semantics of the original sentence. Existing state-of-the-art systems generally do not attempt the reasoning necessary to correctly resolve pronominal anaphora [\(Hard](#page-533-13)[meier, 2012\)](#page-533-13). Despite the importance of pronouns for semantics, they have a marginal impact on BLEU, which may help to explain why existing systems do not make a greater effort to translate them correctly.

**Fairness and bias** The problem of pronoun translation intersects with issues of fairness and bias. In many languages, such as Turkish, the third person singular pronoun is gender neutral. Today's state-of-the-art systems produce the following Turkish-English translations [\(Caliskan et al., 2017\)](#page-517-5):

- (18.1) *O* He *bir* is a *doktor.* doctor.
- (18.2) *O* She *bir* is a *hem¸sire.* nurse.

The same problem arises for other professions that have stereotypical genders, such as engineers, soldiers, and teachers, and for other languages that have gender-neutral pronouns. This bias was not directly programmed into the translation model; it arises from statistical tendencies in existing datasets. This highlights a general problem with datadriven approaches, which can perpetuate biases that negatively impact disadvantaged

<span id="page-452-3"></span>groups. Worse, machine learning can *amplify* biases in data (Bolukbasi et al., 2016): if a dataset has even a slight tendency towards men as doctors, the resulting translation model may produce translations in which doctors are always *he*, and nurses are always *she*.

**Other metrics** A range of other automated metrics have been proposed for machine translation. One potential weakness of BLEU is that it only measures precision; **METEOR** is a weighted *F*-MEASURE, which is a combination of recall and precision (see § 4.4.1). **Translation Error Rate (TER)** computes the string **edit distance** (see § 9.1.4) between the reference and the hypothesis (Snover et al., 2006). For language pairs like English and Japanese, there are substantial differences in word order, and word order errors are not sufficiently captured by *n*-gram based metrics. The **RIBES** metric applies rank correlation to measure the similarity in word order between the system and reference translations (Isozaki et al., 2010).

#### <span id="page-452-0"></span>18.1.2 Data

Data-driven approaches to machine translation rely primarily on **parallel corpora**, which are translations at the sentence level. Early work focused on government records, in which fine-grained official translations are often required. For example, the IBM translation systems were based on the proceedings of the Canadian Parliament, called **Hansards**, which are recorded in English and French (Brown et al., 1990). The growth of the European Union led to the development of the **EuroParl corpus**, which spans 21 European languages (Koehn, 2005). While these datasets helped to launch the field of machine translation, they are restricted to narrow domains and a formal speaking style, limiting their applicability to other types of text. As more resources are committed to machine translation, new translation datasets have been commissioned. This has broadened the scope of available data to news,<sup>1</sup> movie subtitles,<sup>2</sup> social media (Ling et al., 2013), dialogues (Fordyce, 2007), TED talks (Paul et al., 2010), and scientific research articles (Nakazawa et al., 2016).

Despite this growing set of resources, the main bottleneck in machine translation data is the need for parallel corpora that are aligned at the sentence level. Many languages have sizable parallel corpora with some high-resource language, but not with each other. The high-resource language can then be used as a "pivot" or "bridge" (Boitet, 1988; Utiyama and Isahara, 2007): for example, De Gispert and Marino (2006) use Spanish as a bridge for translation between Catalan and English. For most of the 6000 languages spoken today, the only source of translation data remains the Judeo-Christian Bible (Resnik et al., 1999). While relatively small, at less than a million tokens, the Bible has been translated into more than 2000 languages, far outpacing any other corpus. Some research has explored

<span id="page-452-1"></span><sup>&</sup>lt;sup>1</sup>https://catalog.ldc.upenn.edu/LDC2010T10, http://www.statmt.org/wmt15/ translation-task.html

<span id="page-452-2"></span><sup>&</sup>lt;sup>2</sup>http://opus.nlpl.eu/

<span id="page-453-2"></span>the possibility of automatically identifying parallel sentence pairs from unaligned parallel texts, such as web pages and Wikipedia articles [\(Kilgarriff and Grefenstette, 2003;](#page-539-13) [Resnik](#page-558-9) [and Smith, 2003;](#page-558-9) [Adafre and De Rijke, 2006\)](#page-509-8). Another approach is to create large parallel corpora through crowdsourcing [\(Zaidan and Callison-Burch, 2011\)](#page-571-13).

# <span id="page-453-0"></span>**18.2 Statistical machine translation**

The previous section introduced adequacy and fluency as the two main criteria for machine translation. A natural modeling approach is to represent them with separate scores,

<span id="page-453-1"></span>
$$\Psi(\boldsymbol{w}^{(s)}, \boldsymbol{w}^{(t)}) = \Psi_A(\boldsymbol{w}^{(s)}, \boldsymbol{w}^{(t)}) + \Psi_F(\boldsymbol{w}^{(t)}).$$
[18.3]

The fluency score Ψ<sup>F</sup> need not even consider the source sentence; it only judges w(t) on whether it is fluent in the target language. This decomposition is advantageous because it makes it possible to estimate the two scoring functions on separate data. While the adequacy model must be estimated from aligned sentences — which are relatively expensive and rare — the fluency model can be estimated from monolingual text in the target language. Large monolingual corpora are now available in many languages, thanks to resources such as Wikipedia.

An elegant justification of the decomposition in [Equation 18.3](#page-453-1) is provided by the **noisy channel model**, in which each scoring function is a log probability:

$$\Psi_A(\boldsymbol{w}^{(s)}, \boldsymbol{w}^{(t)}) \triangleq \log \mathbf{p}_{S|T}(\boldsymbol{w}^{(s)} \mid \boldsymbol{w}^{(t)})$$
[18.4]

$$\Psi_F(\boldsymbol{w}^{(t)}) \triangleq \log \mathbf{p}_T(\boldsymbol{w}^{(t)})$$
[18.5]

$$\Psi(\boldsymbol{w}^{(s)}, \boldsymbol{w}^{(t)}) = \log p_{S|T}(\boldsymbol{w}^{(s)} \mid \boldsymbol{w}^{(t)}) + \log p_T(\boldsymbol{w}^{(t)}) = \log p_{S,T}(\boldsymbol{w}^{(s)}, \boldsymbol{w}^{(t)}).$$
 [18.6]

By setting the scoring functions equal to the logarithms of the prior and likelihood, their sum is equal to log pS,T , which is the logarithm of the joint probability of the source and target. The sentence wˆ (t) that maximizes this joint probability is also the maximizer of the conditional probability pT|<sup>S</sup> , making it the most likely target language sentence, conditioned on the source.

The noisy channel model can be justified by a generative story. The target text is originally generated from a probability model p<sup>T</sup> . It is then encoded in a "noisy channel" pS|<sup>T</sup> , which converts it to a string in the source language. In decoding, we apply Bayes' rule to recover the string w(t) that is maximally likely under the conditional probability pT|<sup>S</sup> . Under this interpretation, the target probability p<sup>T</sup> is just a language model, and can be estimated using any of the techniques from [chapter 6.](#page-142-0) The only remaining learning problem is to estimate the translation model pS|<sup>T</sup> .

<span id="page-454-2"></span><span id="page-454-1"></span>![](_page_454_Figure_1.jpeg)

Figure 18.3: An example word-to-word alignment

#### <span id="page-454-0"></span>**18.2.1 Statistical translation modeling**

The simplest decomposition of the translation model is word-to-word: each word in the source should be aligned to a word in the translation. This approach presupposes an **alignment** <sup>A</sup>(w(s) , w(t) ), which contains a list of pairs of source and target tokens. For example, given w(s) = *A Vinay le gusta Python* and w(t) = *Vinay likes Python*, one possible word-to-word alignment is,

$$\mathcal{A}(\boldsymbol{w}^{(s)}, \boldsymbol{w}^{(t)}) = \{(A, \emptyset), (Vinay, Vinay), (le, likes), (gusta, likes), (Python, Python)\}.$$
 [18.7]

This alignment is shown in [Figure 18.3.](#page-454-1) Another, less promising, alignment is:

$$\mathcal{A}(\boldsymbol{w}^{(s)}, \boldsymbol{w}^{(t)}) = \{(A, Vinay), (Vinay, likes), (le, Python), (gusta, \emptyset), (Python, \emptyset)\}.$$
 [18.8]

Each alignment contains exactly one tuple for each word in the *source*, which serves to explain how the source word could be translated from the target, as required by the translation probability pS|<sup>T</sup> . If no appropriate word in the target can be identified for a source word, it is aligned to ∅ — as is the case for the Spanish function word *a* in the example, which glosses to the English word *to*. Words in the target can align with multiple words in the source, so that the target word *likes* can align to both *le* and *gusta* in the source.

The joint probability of the alignment and the translation can be defined conveniently as,

$$\mathbf{p}(\boldsymbol{w}^{(s)}, \mathcal{A} \mid \boldsymbol{w}^{(t)}) = \prod_{m=1}^{M^{(s)}} \mathbf{p}(w_m^{(s)}, a_m \mid w_{a_m}^{(t)}, m, M^{(s)}, M^{(t)})$$
[18.9]

$$=\prod_{m=1}^{M^{(s)}} \mathbf{p}(a_m \mid m, M^{(s)}, M^{(t)}) \times \mathbf{p}(w_m^{(s)} \mid w_{a_m}^{(t)}).$$
[18.10]

This probability model makes two key assumptions:

• The alignment probability factors across tokens,

$$\mathbf{p}(\mathcal{A} \mid \boldsymbol{w}^{(s)}, \boldsymbol{w}^{(t)}) = \prod_{m=1}^{M^{(s)}} \mathbf{p}(a_m \mid m, M^{(s)}, M^{(t)}).$$
 [18.11]

This means that each alignment decision is independent of the others, and depends only on the index m, and the sentence lengths M(s) and M(t) .

• The translation probability also factors across tokens,

$$\mathbf{p}(\boldsymbol{w}^{(s)} \mid \boldsymbol{w}^{(t)}, \mathcal{A}) = \prod_{m=1}^{M^{(s)}} \mathbf{p}(w_m^{(s)} \mid w_{a_m}^{(t)}),$$
[18.12]

so that each word in w(s) depends only on its aligned word in w(t) . This means that translation is word-to-word, ignoring context. The hope is that the target language model p(w(t) ) will correct any disfluencies that arise from word-to-word translation.

To translate with such a model, we could sum or max over all possible alignments,

$$\mathbf{p}(\boldsymbol{w}^{(s)}, \boldsymbol{w}^{(t)}) = \sum_{\mathcal{A}} \mathbf{p}(\boldsymbol{w}^{(s)}, \boldsymbol{w}^{(t)}, \mathcal{A})$$
[18.13]

$$= \mathbf{p}(\boldsymbol{w}^{(t)}) \sum_{\mathcal{A}} \mathbf{p}(\mathcal{A}) \times \mathbf{p}(\boldsymbol{w}^{(s)} \mid \boldsymbol{w}^{(t)}, \mathcal{A})$$
[18.14]

$$\geq \mathbf{p}(\boldsymbol{w}^{(t)}) \max_{\mathcal{A}} \mathbf{p}(\mathcal{A}) \times \mathbf{p}(\boldsymbol{w}^{(s)} \mid \boldsymbol{w}^{(t)}, \mathcal{A}).$$
 [18.15]

The term p(A) defines the prior probability over alignments. A series of alignment models with increasingly relaxed independence assumptions was developed by researchers at IBM in the 1980s and 1990s, known as IBM Models 1-6 [\(Och and Ney, 2003\)](#page-552-13). IBM Model 1 makes the strongest independence assumption:

$$\mathbf{p}(a_m \mid m, M^{(s)}, M^{(t)}) = \frac{1}{M^{(t)}}.$$
[18.16]

In this model, every alignment is equally likely. This is almost surely wrong, but it results in a convex learning objective, yielding a good initialization for the more complex alignment models [\(Brown et al., 1993;](#page-516-13) [Koehn, 2009\)](#page-541-0).

#### <span id="page-455-0"></span>**18.2.2 Estimation**

Let us define the parameter θu→<sup>v</sup> as the probability of translating target word u to source word v. If word-to-word alignments were annotated, these probabilities could be computed from relative frequencies,

$$\hat{\theta}_{u \to v} = \frac{\operatorname{count}(u, v)}{\operatorname{count}(u)},$$
[18.17]

#### <span id="page-456-5"></span>18.2. STATISTICAL MACHINE TRANSLATION

where count(u, v) is the count of instances in which word v was aligned to word u in the training set, and count(u) is the total count of the target word u. The smoothing techniques mentioned in chapter 6 can help to reduce the variance of these probability estimates.

Conversely, if we had an accurate translation model, we could estimate the likelihood of each alignment decision,

<span id="page-456-1"></span>
$$q_m(a_m \mid \boldsymbol{w}^{(s)}, \boldsymbol{w}^{(t)}) \propto p(a_m \mid m, M^{(s)}, M^{(t)}) \times p(w_m^{(s)} \mid w_{a_m}^{(t)}),$$
[18.18]

where  $q_m(a_m | \boldsymbol{w}^{(s)}, \boldsymbol{w}^{(t)})$  is a measure of our confidence in aligning source word  $w_m^{(s)}$  to target word  $w_{a_m}^{(t)}$ . The relative frequencies could then be computed from the *expected counts*,

<span id="page-456-3"></span><span id="page-456-2"></span>
$$\hat{\theta}_{u \to v} = \frac{E_q \left[ \text{count}(u, v) \right]}{\text{count}(u)}$$
[18.19]

$$E_q \left[ \text{count}(u, v) \right] = \sum_m q_m(a_m \mid \boldsymbol{w}^{(s)}, \boldsymbol{w}^{(t)}) \times \delta(w_m^{(s)} = v) \times \delta(w_{a_m}^{(t)} = u).$$
 [18.20]

The **expectation-maximization** (EM) algorithm proceeds by iteratively updating  $q_m$  and  $\hat{\Theta}$ . The algorithm is described in general form in chapter 5. For statistical machine translation, the steps of the algorithm are:

- 1. E-step: Update beliefs about word alignment using Equation 18.18.
- 2. M-step: Update the translation model using Equations 18.19 and 18.20.

As discussed in chapter 5, the expectation maximization algorithm is guaranteed to converge, but not to a global optimum. However, for IBM Model 1, it can be shown that EM optimizes a convex objective, and global optimality is guaranteed. For this reason, IBM Model 1 is often used as an initialization for more complex alignment models. For more detail, see Koehn (2009).

#### <span id="page-456-0"></span>18.2.3 Phrase-based translation

Real translations are not word-to-word substitutions. One reason is that many multiword expressions are not translated literally, as shown in this example from French:

<span id="page-456-4"></span>(18.3) Nous allons prendre un verre We will take a glass We'll have a drink

<span id="page-457-0"></span>![](_page_457_Figure_1.jpeg)

Figure 18.4: A phrase-based alignment between French and English, corresponding to example (18.3)

The line *we will take a glass* is the word-for-word gloss of the French sentence; the translation *we'll have a drink* is shown on the third line. Such examples are difficult for word-to-word translation models, since they require translating *prendre* to *have* and *verre* to *drink*. These translations are only correct in the context of these specific phrases.

Phrase-based translation generalizes on word-based models by building translation tables and alignments between multiword spans. (These "phrases" are not necessarily syntactic constituents like the noun phrases and verb phrases described in chapters 9 and 10.) The generalization from word-based translation is surprisingly straightforward: the translation tables can now condition on multi-word units, and can assign probabilities to multi-word units; alignments are mappings from spans to spans,  $((i, j), (k, \ell))$ , so that

$$p(\boldsymbol{w}^{(s)} \mid \boldsymbol{w}^{(t)}, \mathcal{A}) = \prod_{((i,j), (k,\ell)) \in \mathcal{A}} p_{w^{(s)} \mid w^{(t)}}(\{w_{i+1}^{(s)}, w_{i+2}^{(s)}, \dots, w_{j}^{(s)}\} \mid \{w_{k+1}^{(t)}, w_{k+2}^{(t)}, \dots, w_{\ell}^{(t)}\}).$$
[18.21]

The phrase alignment  $((i, j), (k, \ell))$  indicates that the span  $w_{i+1:j}^{(s)}$  is the translation of the span  $w_{k+1:\ell}^{(t)}$ . An example phrasal alignment is shown in Figure 18.4. Note that the alignment set  $\mathcal{A}$  is required to cover all of the tokens in the source, just as in word-based translation. The probability model  $p_{w^{(s)}|w^{(t)}}$  must now include translations for all phrase pairs, which can be learned from expectation-maximization just as in word-based statistical machine translation.

#### <span id="page-458-3"></span><span id="page-458-0"></span>18.2.4 \*Syntax-based translation

The Vauquois Pyramid (Figure 18.1) suggests that translation might be easier if we take a higher-level view. One possibility is to incorporate the syntactic structure of the source, the target, or both. This is particularly promising for language pairs that consistent syntactic differences. For example, English adjectives almost always precede the nouns that they modify, while in Romance languages such as French and Spanish, the adjective often follows the noun: thus, *angry fish* would translate to *pez (fish) enojado (angry)* in Spanish. In word-to-word translation, these reorderings cause the alignment model to be overly permissive. It is not that the order of *any* pair of English words can be reversed when translating into Spanish, but only adjectives and nouns within a noun phrase. Similar issues arise when translating between verb-final languages such as Japanese (in which verbs usually follow the subject and object), verb-initial languages like Tagalog and classical Arabic, and verb-medial languages such as English.

An elegant solution is to link parsing and translation in a **synchronous context-free grammar** (SCFG; Chiang, 2007).<sup>3</sup> An SCFG is a set of productions of the form  $X \to (\alpha, \beta, \sim)$ , where X is a non-terminal,  $\alpha$  and  $\beta$  are sequences of terminals or non-terminals, and  $\sim$  is a one-to-one alignment of items in  $\alpha$  with items in  $\beta$ . English-Spanish adjective-noun ordering can be handled by a set of synchronous productions, e.g.,

<span id="page-458-2"></span>
$$NP \rightarrow (DET_1 NN_2 JJ_3, DET_1 JJ_3 NN_2), \qquad [18.22]$$

with subscripts indicating the alignment between the Spanish (left) and English (right) parts of the right-hand side. Terminal productions yield translation pairs,

$$JJ \to (enojado_1, angry_1).$$
 [18.23]

A synchronous derivation begins with the start symbol S, and derives a pair of sequences of terminal symbols.

Given an SCFG in which each production yields at most two symbols in each language (Chomsky Normal Form; see § 9.2.1), a sentence can be parsed using only the CKY algorithm (chapter 10). The resulting derivation also includes productions in the other language, all the way down to the surface form. Therefore, SCFGs make translation very similar to parsing. In a weighted SCFG, the log probability  $\log p_{S|T}$  can be computed from the sum of the log-probabilities of the productions. However, combining SCFGs with a target language model is computationally expensive, necessitating approximate search algorithms (Huang and Chiang, 2007).

Synchronous context-free grammars are an example of **tree-to-tree translation**, because they model the syntactic structure of both the target and source language. In **stringto-tree translation**, string elements are translated into constituent tree fragments, which

<span id="page-458-1"></span><sup>&</sup>lt;sup>3</sup>Earlier approaches to syntactic machine translation includes syntax-driven transduction (Lewis II and Stearns, 1968) and stochastic inversion transduction grammars (Wu, 1997).

<span id="page-459-1"></span>are then assembled into a translation [\(Yamada and Knight, 2001;](#page-571-14) [Galley et al., 2004\)](#page-529-12); in **tree-to-string translation**, the source side is parsed, and then transformed into a string on the target side [\(Liu et al., 2006\)](#page-545-13). A key question for syntax-based translation is the extent to which we phrasal constituents align across translations [\(Fox, 2002\)](#page-528-13), because this governs the extent to which we can rely on monolingual parsers and treebanks. For more on syntax-based machine translation, see the monograph by [Williams et al.](#page-569-11) [\(2016\)](#page-569-11).

# <span id="page-459-0"></span>**18.3 Neural machine translation**

Neural network models for machine translation are based on the **encoder-decoder** architecture [\(Cho et al., 2014\)](#page-520-5). The encoder network converts the source language sentence into a vector or matrix representation; the decoder network then converts the encoding into a sentence in the target language.

$$\boldsymbol{z} = \text{ENCODE}(\boldsymbol{w}^{(s)})$$
[18.24]

$$\boldsymbol{w}^{(t)} \mid \boldsymbol{w}^{(s)} \sim \text{DECODE}(\boldsymbol{z}),$$
 [18.25]

where the second line means that the function DECODE(z) defines the conditional probability p(w(t) <sup>|</sup> <sup>w</sup>(s) ).

The decoder is typically a recurrent neural network, which generates the target language sentence one word at a time, while recurrently updating a hidden state. The encoder and decoder networks are trained end-to-end from parallel sentences. If the output layer of the decoder is a logistic function, then the entire architecture can be trained to maximize the conditional log-likelihood,

$$\log p(\boldsymbol{w}^{(t)} \mid \boldsymbol{w}^{(s)}) = \sum_{m=1}^{M^{(t)}} p(w_m^{(t)} \mid \boldsymbol{w}_{1:m-1}^{(t)}, \boldsymbol{z})$$
[18.26]

$$p(w_m^{(t)} \mid \boldsymbol{w}_{1:m-1}^{(t)}, \boldsymbol{w}^{(s)}) \propto \exp\left(\boldsymbol{\beta}_{w_m^{(t)}} \cdot \boldsymbol{h}_{m-1}^{(t)}\right)$$
[18.27]

where the hidden state h (t) m−1 is a recurrent function of the previously generated text w (t) 1:m−1 and the encoding z. The second line is equivalent to writing,

$$w_m^{(t)} \mid \boldsymbol{w}_{1:m-1}^{(t)}, \boldsymbol{w}^{(s)} \sim \text{SoftMax}\left(\boldsymbol{\beta} \cdot \boldsymbol{h}_{m-1}^{(t)}\right), \qquad [18.28]$$

where β ∈ R (V (t)×K) is the matrix of output word vectors for the V (t) words in the target language vocabulary.

The simplest encoder-decoder architecture is the **sequence-to-sequence model** [\(Sutskev](#page-564-13)er [et al., 2014\)](#page-564-13). In this model, the encoder is set to the final hidden state of a **long short-term**

<span id="page-460-3"></span><span id="page-460-0"></span>![](_page_460_Figure_1.jpeg)

Figure 18.5: A deep bidirectional LSTM encoder

**memory (LSTM)** (see  $\S$  6.3.3) on the source sentence:

$$h_m^{(s)} = \text{LSTM}(x_m^{(s)}, h_{m-1}^{(s)})$$
 [18.29]

<span id="page-460-1"></span>
$$\boldsymbol{z} \triangleq \boldsymbol{h}_{M^{(s)}}^{(s)}, \qquad [18.30]$$

where  $x_m^{(s)}$  is the embedding of source language word  $w_m^{(s)}$ . The encoding then provides the initial hidden state for the decoder LSTM:

<span id="page-460-2"></span>
$$\boldsymbol{h}_{0}^{(t)} = \boldsymbol{z} \tag{18.31}$$

$$h_m^{(t)} = \text{LSTM}(\boldsymbol{x}_m^{(t)}, \boldsymbol{h}_{m-1}^{(t)}),$$
 [18.32]

where  $x_m^{(t)}$  is the embedding of the target language word  $w_m^{(t)}$ .

Sequence-to-sequence translation is nothing more than wiring together two LSTMs: one to read the source, and another to generate the target. To make the model work well, some additional tweaks are needed:

- Most notably, the model works much better if the source sentence is reversed, reading from the end of the sentence back to the beginning. In this way, the words at the beginning of the source have the greatest impact on the encoding *z*, and therefore impact the words at the beginning of the target sentence. Later work on more advanced encoding models, such as **neural attention** (see § 18.3.1), has eliminated the need for reversing the source sentence.
- The encoder and decoder can be implemented as **deep LSTMs**, with multiple layers of hidden states. As shown in Figure 18.5, each hidden state  $h_m^{(s,i)}$  at layer *i* is treated

<span id="page-461-2"></span>as the input to an LSTM at layer i + 1:

$$\boldsymbol{h}_{m}^{(s,1)} = \text{LSTM}(\boldsymbol{x}_{m}^{(s)}, \boldsymbol{h}_{m-1}^{(s)})$$
 [18.33]

$$\boldsymbol{h}_{m}^{(s,i+1)} = \text{LSTM}(\boldsymbol{h}_{m}^{(s,i)}, \boldsymbol{h}_{m-1}^{(s,i+1)}), \quad \forall i \ge 1.$$
 [18.34]

The original work on sequence-to-sequence translation used four layers; in 2016, Google's commercial machine translation system used eight layers [\(Wu et al., 2016\)](#page-570-0).[4](#page-461-1)

• Significant improvements can be obtained by creating an **ensemble** of translation models, each trained from a different random initialization. For an ensemble of size N, the per-token decoding probability is set equal to,

$$\mathbf{p}(w^{(t)} \mid \boldsymbol{z}, \boldsymbol{w}_{1:m-1}^{(t)}) = \frac{1}{N} \sum_{i=1}^{N} \mathbf{p}_{i}(w^{(t)} \mid \boldsymbol{z}, \boldsymbol{w}_{1:m-1}^{(t)}), \quad [18.35]$$

where p<sup>i</sup> is the decoding probability for model i. Each translation model in the ensemble includes its own encoder and decoder networks.

• The original sequence-to-sequence model used a fairly standard training setup: stochastic gradient descent with an exponentially decreasing learning rate after the first five epochs; mini-batches of 128 sentences, chosen to have similar length so that each sentence on the batch will take roughly the same amount of time to process; gradient clipping (see § [3.3.4\)](#page-76-0) to ensure that the norm of the gradient never exceeds some predefined value.

#### <span id="page-461-0"></span>**18.3.1 Neural attention**

The sequence-to-sequence model discussed in the previous section was a radical departure from statistical machine translation, in which each word or phrase in the target language is conditioned on a single word or phrase in the source language. Both approaches have advantages. Statistical translation leverages the idea of compositionality — translations of large units should be based on the translations of their component parts — and this seems crucial if we are to scale translation to longer units of text. But the translation of each word or phrase often depends on the larger context, and encoder-decoder models capture this context at the sentence level.

Is it possible for translation to be both contextualized and compositional? One approach is to augment neural translation with an **attention mechanism**. The idea of neural attention was described in § [17.5,](#page-441-0) but its application to translation bears further discussion. In general, attention can be thought of as using a query to select from a memory of key-value pairs. However, the query, keys, and values are all vectors, and the entire

<span id="page-461-1"></span><sup>4</sup>Google reports that this system took six days to train for English-French translation, using 96 NVIDIA K80 GPUs, which would have cost roughly half a million dollars at the time.

<span id="page-462-1"></span>![](_page_462_Figure_1.jpeg)

<span id="page-462-0"></span>Figure 18.6: A general view of neural attention. The dotted box indicates that each  $\alpha_{m \to n}$  can be viewed as a **gate** on value *n*.

operation is differentiable. For each key n in the memory, we compute a score  $\psi_{\alpha}(m, n)$  with respect to the query m. That score is a function of the compatibility of the key and the query, and can be computed using a small feedforward neural network. The vector of scores is passed through an activation function, such as softmax. The output of this activation function is a vector of non-negative numbers  $[\alpha_{m\to 1}, \alpha_{m\to 2}, \dots, \alpha_{m\to N}]^{\top}$ , with length N equal to the size of the memory. Each value in the memory  $v_n$  is multiplied by the attention  $\alpha_{m\to n}$ ; the sum of these scaled values is the output. This process is shown in Figure 18.6. In the extreme case that  $\alpha_{m\to n} = 1$  and  $\alpha_{m\to n'} = 0$  for all other n', then the attention mechanism simply selects the value  $v_n$  from the memory.

Neural attention makes it possible to integrate alignment into the encoder-decoder architecture. Rather than encoding the entire source sentence into a fixed length vector z, it can be encoded into a matrix  $\mathbf{Z} \in \mathbb{R}^{K \times M^{(S)}}$ , where K is the dimension of the hidden state, and  $M^{(S)}$  is the number of tokens in the source input. Each column of  $\mathbf{Z}$  represents the state of a recurrent neural network over the source sentence. These vectors are constructed from a **bidirectional LSTM** (see § 7.6), which can be a deep network as shown in Figure 18.5. These columns are both the keys and the values in the attention mechanism.

At each step *m* in decoding, the attentional state is computed by executing a query, which is equal to the state of the decoder,  $h_m^{(t)}$ . The resulting compatibility scores are,

$$\psi_{\alpha}(m,n) = \boldsymbol{v}_{\alpha} \cdot \tanh(\Theta_{\alpha}[\boldsymbol{h}_{m}^{(t)};\boldsymbol{h}_{n}^{(s)}]).$$
[18.36]

The function  $\psi$  is thus a two layer feedforward neural network, with weights  $v_{\alpha}$  on the output layer, and weights  $\Theta_{\alpha}$  on the input layer. To convert these scores into attention weights, we apply an activation function, which can be vector-wise softmax or an element-wise sigmoid:

Softmax attention

$$\alpha_{m \to n} = \frac{\exp \psi_{\alpha}(m, n)}{\sum_{n'=1}^{M^{(s)}} \exp \psi_{\alpha}(m, n')}$$
[18.37]

#### <span id="page-463-2"></span>**Sigmoid attention**

$$\alpha_{m \to n} = \sigma \left( \psi_{\alpha}(m, n) \right)$$
[18.38]

The attention α is then used to compute a context vector c<sup>m</sup> by taking a weighted average over the columns of Z,

$$\boldsymbol{c}_{m} = \sum_{n=1}^{M^{(s)}} \alpha_{m \to n} \boldsymbol{z}_{n}, \qquad [18.39]$$

where αm→<sup>n</sup> ∈ [0, 1] is the amount of attention from word m of the target to word n of the source. The context vector can be incorporated into the decoder's word output probability model, by adding another layer to the decoder [\(Luong et al., 2015\)](#page-546-12):

$$\tilde{\boldsymbol{h}}_{m}^{(t)} = \tanh\left(\Theta_{c}[\boldsymbol{h}_{m}^{(t)};\boldsymbol{c}_{m}]\right)$$
[18.40]

$$\mathbf{p}(w_{m+1}^{(t)} \mid \boldsymbol{w}_{1:m}^{(t)}, \boldsymbol{w}^{(s)}) \propto \exp\left(\boldsymbol{\beta}_{w_{m+1}^{(t)}} \cdot \tilde{\boldsymbol{h}}_{m}^{(t)}\right).$$
[18.41]

Here the decoder state h (t) m is concatenated with the context vector, forming the input to compute a final output vector h˜ (t) m . The context vector can be incorporated into the decoder recurrence in a similar manner [\(Bahdanau et al., 2014\)](#page-512-10).

#### <span id="page-463-0"></span>**18.3.2 \*Neural machine translation without recurrence**

In the encoder-decoder model, attention's "keys and values" are the hidden state representations in the encoder network, z, and the "queries" are state representations in the decoder network h (t) . It is also possible to completely eliminate recurrence from neural translation, by applying **self-attention** [\(Lin et al., 2017;](#page-544-13) [Kim et al., 2017\)](#page-539-14) within the encoder and decoder, as in the **transformer architecture** [\(Vaswani et al., 2017\)](#page-567-10). For level i, the basic equations of the encoder side of the transformer are:

<span id="page-463-1"></span>
$$\boldsymbol{z}_{m}^{(i)} = \sum_{n=1}^{M^{(s)}} \alpha_{m \to n}^{(i)}(\Theta_{v} \boldsymbol{h}_{n}^{(i-1)})$$
[18.42]

$$\boldsymbol{h}_{m}^{(i)} = \Theta_{2} \operatorname{ReLU} \left( \Theta_{1} \boldsymbol{z}_{m}^{(i)} + \boldsymbol{b}_{1} \right) + \boldsymbol{b}_{2}.$$
[18.43]

For each token m at level i, we compute self-attention over the entire source sentence. The keys, values, and queries are all projections of the vector h (i−1): for example, in [Equa](#page-463-1)[tion 18.42,](#page-463-1) the value v<sup>n</sup> is the projection Θvh (i−1) <sup>n</sup> . The attention scores α (i) m→n are computed using a scaled form of softmax attention,

$$\alpha_{m \to n} \propto \exp(\psi_{\alpha}(m, n)/M),$$
 [18.44]

<span id="page-464-2"></span><span id="page-464-0"></span>![](_page_464_Figure_1.jpeg)

Figure 18.7: The transformer encoder's computation of  $z_m^{(i)}$  from  $h^{(i-1)}$ . The key, value, and **q**uery are shown for token m - 1. For example,  $\psi_{\alpha}^{(i)}(m, m - 1)$  is computed from the key  $\Theta_k h_{m-1}^{(i-1)}$  and the query  $\Theta_q h_m^{(i-1)}$ , and the gate  $\alpha_{m \to m-1}^{(i)}$  operates on the value  $\Theta_v h_{m-1}^{(i-1)}$ . The figure shows a minimal version of the architecture, with a single attention head. With multiple heads, it is possible to attend to different properties of multiple words.

where M is the length of the input. This encourages the attention to be more evenly dispersed across the input. Self-attention is applied across multiple "heads", each using different projections of  $h^{(i-1)}$  to form the keys, values, and queries. This architecture is shown in Figure 18.7. The output of the self-attentional layer is the representation  $z_m^{(i)}$ , which is then passed through a two-layer feed-forward network, yielding the input to the next layer,  $h^{(i)}$ . This self-attentional architecture can be applied in the decoder as well, but this requires that there is zero attention to future words:  $\alpha_{m\to n} = 0$  for all n > m.

To ensure that information about word order in the source is integrated into the model, the encoder augments the base layer of the network with **positional encodings** of the indices of each word in the source. These encodings are vectors for each position  $m \in \{1, 2, ..., M\}$ . The transformer sets these encodings equal to a set of sinusoidal functions of m,

$$e_{2i-1}(m) = \sin(m/(10000^{\frac{2i}{K_e}}))$$
[18.45]

$$e_{2i}(m) = \cos(m/(10000 \frac{K_e}{K_e})), \quad \forall i \in \{1, 2, \dots, K_e/2\}$$
 [18.46]

where  $e_{2i}(m)$  is the value at element 2i of the encoding for index m. As we progress through the encoding, the sinusoidal functions have progressively narrower bandwidths. This enables the model to learn to attend by relative positions of words. The positional encodings are concatenated with the word embeddings  $x_m$  at the base layer of the model.<sup>5</sup>

2i

<span id="page-464-1"></span><sup>&</sup>lt;sup>5</sup>The transformer architecture relies on several additional tricks, including layer normalization (see

<span id="page-465-3"></span><span id="page-465-2"></span>![](_page_465_Figure_1.jpeg)

Figure 18.8: Translation with *unknown words*. The system outputs *unk* to indicate words that are outside its vocabulary. Figure adapted from Luong et al. (2015).

**Convolutional neural networks** (see § 3.4) have also been applied as encoders in neural machine translation (Gehring et al., 2017). For each word  $w_m^{(s)}$ , a convolutional network computes a representation  $h_m^{(s)}$  from the embeddings of the word and its neighbors. This procedure is applied several times, creating a deep convolutional network. The recurrent decoder then computes a set of attention weights over these convolutional representations, using the decoder's hidden state  $h^{(t)}$  as the queries. This attention vector is used to compute a weighted average over the outputs of *another* convolutional neural network of the source, yielding an averaged representation  $c_m$ , which is then fed into the decoder. As with the transformer, speed is the main advantage over recurrent encoding models; another similarity is that word order information is approximated through the use of positional encodings.<sup>6</sup>

#### <span id="page-465-0"></span>18.3.3 Out-of-vocabulary words

Thus far, we have treated translation as a problem at the level of words or phrases. For words that do not appear in the training data, all such models will struggle. There are two main reasons for the presence of out-of-vocabulary (OOV) words:

- New proper nouns, such as family names or organizations, are constantly arising particularly in the news domain. The same is true, to a lesser extent, for technical terminology. This issue is shown in Figure 18.8.
- In many languages, words have complex internal structure, known as **morphology**. An example is German, which uses compounding to form nouns like *Abwasserbehandlungsanlage* (*sewage water treatment plant*; example from Sennrich et al. (2016)).

<sup>§ 3.3.4),</sup> residual connections around the nonlinear activations (see § 3.2.2), and a non-monotonic learning rate schedule.

<span id="page-465-1"></span><sup>&</sup>lt;sup>6</sup>A recent evaluation found that best performance was obtained by using a recurrent network for the decoder, and a transformer for the encoder (Chen et al., 2018). The transformer was also found to significantly outperform a convolutional neural network.

<span id="page-466-3"></span>While compounds could in principle be addressed by better tokenization (see § [8.4\)](#page-202-0), other morphological processes involve more complex transformations of subword units.

Names and technical terms can be handled in a postprocessing step: after first identifying alignments between unknown words in the source and target, we can look up each aligned source word in a dictionary, and choose a replacement [\(Luong et al., 2015\)](#page-546-13). If the word does not appear in the dictionary, it is likely to be a proper noun, and can be copied directly from the source to the target. This approach can also be integrated directly into the translation model, rather than applying it as a postprocessing step [\(Jean et al., 2015\)](#page-537-13).

Words with complex internal structure can be handled by translating subword units rather than entire words. A popular technique for identifying subword units is **byte-pair encoding** (BPE; [Gage, 1994;](#page-529-10) [Sennrich et al., 2016\)](#page-560-9). The initial vocabulary is defined as the set of characters used in the text. The most common character bigram is then merged into a new symbol, the vocabulary is updated, and the merging operation is applied again. For example, given the dictionary {*fish, fished, want, wanted, bike, biked*}, we would first form the subword unit *ed*, since this character bigram appears in three of the six words. Next, there are several bigrams that each appear in a pair of words: *fi*, *is*, *sh*, *wa*, *an*, etc. These can be merged in any order. By iterating this process, we eventually reach the segmentation, {*fish*, *fish+ed*, *want*, *want+ed*, *bik+e*, *bik+ed*}. At this point, there are no bigrams that appear more than once. In real data, merging is performed until the number of subword units reaches some predefined threshold, such as 10<sup>4</sup> .

Each subword unit is treated as a token for translation, in both the encoder (source side) and decoder (target side). BPE can be applied jointly to the union of the source and target vocabularies, identifying subword units that appear in both languages. For languages that have different scripts, such as English and Russian, **transliteration** between the scripts should be applied first.[7](#page-466-1)

# <span id="page-466-0"></span>**18.4 Decoding**

Given a trained translation model, the decoding task is:

<span id="page-466-2"></span>
$$\hat{\boldsymbol{w}}^{(t)} = \operatorname*{argmax}_{\boldsymbol{w} \in \mathcal{V}^*} \Psi(\boldsymbol{w}, \boldsymbol{w}^{(s)}), \qquad [18.47]$$

where w(t) is a sequence of tokens from the target vocabulary V. It is not possible to efficiently obtain exact solutions to the decoding problem, for even minimally effective

<span id="page-466-1"></span><sup>7</sup>Transliteration is crucial for converting names and other foreign words between languages that do not share a single script, such as English and Japanese. It is typically approached using the finite-state methods discussed in [chapter 9](#page-208-0) [\(Knight and Graehl, 1998\)](#page-540-12).

<span id="page-467-3"></span>models in either statistical or neural machine translation. Today's state-of-the-art translation systems use **beam search** (see § 11.3.1), which is an incremental decoding algorithm that maintains a small constant number of competitive hypotheses. Such greedy approximations are reasonably effective in practice, and this may be in part because the decoding objective is only loosely correlated with measures of translation quality, so that exact optimization of [18.47] may not greatly improve the resulting translations.

Decoding in neural machine translation is simpler than in phrase-based statistical machine translation.<sup>8</sup> The scoring function  $\Psi$  is defined,

$$\Psi(\boldsymbol{w}^{(t)}, \boldsymbol{w}^{(s)}) = \sum_{m=1}^{M^{(t)}} \psi(w_m^{(t)}; \boldsymbol{w}_{1:m-1}^{(t)}, \boldsymbol{z})$$
[18.48]

$$\psi(w^{(t)}; \boldsymbol{w}_{1:m-1}^{(t)}, \boldsymbol{z}) = \beta_{w_m^{(t)}} \cdot \boldsymbol{h}_m^{(t)} - \log \sum_{w \in \mathcal{V}} \exp\left(\beta_w \cdot \boldsymbol{h}_m^{(t)}\right), \quad [18.49]$$

where z is the encoding of the source sentence  $w^{(s)}$ , and  $h_m^{(t)}$  is a function of the encoding z and the decoding history  $w_{1:m-1}^{(t)}$ . This formulation subsumes the attentional translation model, where z is a matrix encoding of the source.

Now consider the incremental decoding algorithm,

<span id="page-467-2"></span>
$$\hat{w}_{m}^{(t)} = \operatorname*{argmax}_{w \in \mathcal{V}} \psi(w; \hat{w}_{1:m-1}^{(t)}, z), \quad m = 1, 2, \dots$$
 [18.50]

This algorithm selects the best target language word at position m, assuming that it has already generated the sequence  $\hat{w}_{1:m-1}^{(t)}$ . (Termination can be handled by augmenting the vocabulary  $\mathcal{V}$  with a special end-of-sequence token,  $\blacksquare$ .) The incremental algorithm is likely to produce a suboptimal solution to the optimization problem defined in Equation 18.47, because selecting the highest-scoring word at position m can set the decoder on a "garden path," in which there are no good choices at some later position n > m. We might hope for some dynamic programming solution, as in sequence labeling (§ 7.3). But the Viterbi algorithm and its relatives rely on a Markov decomposition of the objective function into a sum of local scores: for example, scores can consider locally adjacent tags  $(y_m, y_{m-1})$ , but not the entire tagging history  $y_{1:m}$ . This decomposition is not applicable to recurrent neural networks, because the hidden state  $h_m^{(t)}$  is impacted by the entire history  $w_{1:m}^{(t)}$ ; this sensitivity to long-range context is precisely what makes recurrent neural networks so effective.<sup>9</sup> In fact, it can be shown that decoding from any recurrent neural network is NP-complete (Siegelmann and Sontag, 1995; Chen et al., 2018).

<span id="page-467-1"></span><span id="page-467-0"></span><sup>&</sup>lt;sup>8</sup>For more on decoding in phrase-based statistical models, see Koehn (2009).

<sup>&</sup>lt;sup>9</sup>Note that this problem does not impact RNN-based sequence labeling models (see § 7.6). This is because the tags produced by these models do not affect the recurrent state.

<span id="page-468-2"></span>**Beam search Beam search** is a general technique for avoiding search errors when exhaustive search is impossible; it was first discussed in § [11.3.1.](#page-289-0) Beam search can be seen as a variant of the incremental decoding algorithm sketched in [Equation 18.50,](#page-467-2) but at each step m, a set of K different hypotheses are kept on the beam. For each hypothesis <sup>k</sup> ∈ {1, <sup>2</sup>, . . . , K}, we compute both the current score <sup>P</sup>M(t) <sup>m</sup>=1 ψ(w (t) k,m; w (t) k,1:m−1 , z) as well as the current hidden state h (t) k . At each step in the beam search, the K top-scoring children of each hypothesis currently on the beam are "expanded", and the beam is updated. For a detailed description of beam search for RNN decoding, see [Graves](#page-532-11) [\(2012\)](#page-532-11).

**Learning and search** Conventionally, the learning algorithm is trained to predict the right token in the translation, conditioned on the translation history being correct. But if decoding must be approximate, then we might do better by modifying the learning algorithm to be robust to errors in the translation history. **Scheduled sampling** does this by training on histories that sometimes come from the ground truth, and sometimes come from the model's own output [\(Bengio et al., 2015\)](#page-513-13).[10](#page-468-1) As training proceeds, the training wheels come off: we increase the fraction of tokens that come from the model rather than the ground truth. Another approach is to train on an objective that relates directly to beam search performance [\(Wiseman et al., 2016\)](#page-569-5). **Reinforcement learning** has also been applied to decoding of RNN-based translation models, making it possible to directly optimize translation metrics such as BLEU [\(Ranzato et al., 2016\)](#page-557-13).

# <span id="page-468-0"></span>**18.5 Training towards the evaluation metric**

In likelihood-based training, the objective is the maximize the probability of a parallel corpus. However, translations are not evaluated in terms of likelihood: metrics like BLEU consider only the correctness of a single output translation, and not the range of probabilities that the model assigns. It might therefore be better to train translation models to achieve the highest BLEU score possible — to the extent that we believe BLEU measures translation quality. Unfortunately, BLEU and related metrics are not friendly for optimization: they are discontinuous, non-differentiable functions of the parameters of the translation model.

Consider an error function ∆(wˆ (t) , w(t) ), which measures the discrepancy between the system translation wˆ (t) and the reference translation w(t) ; this function could be based on BLEU or any other metric on translation quality. One possible criterion would be to select

<span id="page-468-1"></span><sup>10</sup>Scheduled sampling builds on earlier work on learning to search [\(Daume III et al., 2009;](#page-523-5) [Ross et al.,](#page-559-6) ´ [2011\)](#page-559-6), which are also described in § [15.2.4.](#page-379-0)

<span id="page-469-1"></span>the parameters  $\theta$  that minimize the error of the system's preferred translation,

$$\hat{\boldsymbol{w}}^{(t)} = \operatorname*{argmax}_{\boldsymbol{w}^{(t)}} \Psi(\boldsymbol{w}^{(t)}, \boldsymbol{w}^{(s)}; \boldsymbol{\theta})$$
[18.51]

<span id="page-469-0"></span>
$$\hat{\boldsymbol{\theta}} = \operatorname*{argmin}_{\boldsymbol{\theta}} \Delta(\hat{\boldsymbol{w}}^{(t)}, \boldsymbol{w}^{(s)})$$
[18.52]

However, identifying the top-scoring translation  $\hat{w}^{(t)}$  is usually intractable, as described in the previous section. In **minimum error-rate training (MERT)**,  $\hat{w}^{(t)}$  is selected from a set of candidate translations  $\mathcal{Y}(w^{(s)})$ ; this is typically a strict subset of all possible translations, so that it is only possible to optimize an approximation to the true error rate (Och and Ney, 2003).

A further issue is that the objective function in Equation 18.52 is discontinuous and non-differentiable, due to the argmax over translations: an infinitesimal change in the parameters  $\theta$  could cause another translation to be selected, with a completely different error. To address this issue, we can instead minimize the **risk**, which is defined as the expected error rate,

$$R(\boldsymbol{\theta}) = E_{\hat{\boldsymbol{w}}^{(t)}|\boldsymbol{w}^{(s)};\boldsymbol{\theta}}[\Delta(\hat{\boldsymbol{w}}^{(t)}, \boldsymbol{w}^{(t)})]$$
[18.53]

$$= \sum_{\hat{\boldsymbol{w}}^{(t)} \in \mathcal{Y}(\boldsymbol{w}^{(s)})} p(\hat{\boldsymbol{w}}^{(t)} \mid \boldsymbol{w}^{(s)}) \times \Delta(\hat{\boldsymbol{w}}^{(t)}, \boldsymbol{w}^{(t)}).$$
[18.54]

**Minimum risk training** minimizes the sum of  $R(\theta)$  across all instances in the training set.

The risk can be generalized by exponentiating the translation probabilities,

$$\tilde{p}(\boldsymbol{w}^{(t)};\boldsymbol{\theta},\alpha) \propto \left( p(\boldsymbol{w}^{(t)} \mid \boldsymbol{w}^{(s)};\boldsymbol{\theta}) \right)^{\alpha}$$
[18.55]

$$\tilde{R}(\boldsymbol{\theta}) = \sum_{\hat{\boldsymbol{w}}^{(t)} \in \mathcal{Y}(\boldsymbol{w}^{(s)})} \tilde{p}(\hat{\boldsymbol{w}}^{(t)} \mid \boldsymbol{w}^{(s)}; \alpha, \boldsymbol{\theta}) \times \Delta(\hat{\boldsymbol{w}}^{(t)}, \boldsymbol{w}^{(t)})$$
[18.56]

where  $\mathcal{Y}(\boldsymbol{w}^{(s)})$  is now the set of *all* possible translations for  $\boldsymbol{w}^{(s)}$ . Exponentiating the probabilities in this way is known as **annealing** (Smith and Eisner, 2006). When  $\alpha = 1$ , then  $\tilde{R}(\boldsymbol{\theta}) = R(\boldsymbol{\theta})$ ; when  $\alpha = \infty$ , then  $\tilde{R}(\boldsymbol{\theta})$  is equivalent to the sum of the errors of the maximum probability translations for each sentence in the dataset.

Clearly the set of candidate translations  $\mathcal{Y}(\boldsymbol{w}^{(s)})$  is too large to explicitly sum over. Because the error function  $\Delta$  generally does not decompose into smaller parts, there is no efficient dynamic programming solution to sum over this set. We can approximate the sum  $\sum_{\hat{\boldsymbol{w}}^{(t)} \in \mathcal{Y}(\boldsymbol{w}^{(s)})}$  with a sum over a finite number of samples,  $\{\boldsymbol{w}_1^{(t)}, \boldsymbol{w}_2^{(t)}, \dots, \boldsymbol{w}_K^{(t)}\}$ . If these samples were drawn uniformly at random, then the (annealed) risk would be

<span id="page-470-0"></span>approximated as [\(Shen et al., 2016\)](#page-561-12),

$$\tilde{R}(\boldsymbol{\theta}) \approx \frac{1}{Z} \sum_{k=1}^{K} \tilde{p}(\boldsymbol{w}_{k}^{(t)} \mid \boldsymbol{w}^{(s)}; \boldsymbol{\theta}, \alpha) \times \Delta(\boldsymbol{w}_{k}^{(t)}, \boldsymbol{w}^{(t)})$$
[18.57]

$$Z = \sum_{k=1}^{K} \tilde{p}(\boldsymbol{w}_{k}^{(t)} \mid \boldsymbol{w}^{(s)}; \boldsymbol{\theta}, \alpha).$$
[18.58]

[Shen et al.](#page-561-12) [\(2016\)](#page-561-12) report that performance plateaus at K = 100 for minimum risk training of neural machine translation.

Uniform sampling over the set of all possible translations is undesirable, because most translations have very low probability. A solution from Monte Carlo estimation is **importance sampling**, in which we draw samples from a **proposal distribution** q(w(s) ). This distribution can be set equal to the current translation model p(w(t) <sup>|</sup> <sup>w</sup>(s) ; θ). Each sample is then weighted by an **importance score**, ω<sup>k</sup> = p˜(w (t) k |w(s) ) q(w k ;w(s)) . The effect of this weighting is to correct for any mismatch between the proposal distribution q and the true distribution p˜. The risk can then be approximated as,

$$\boldsymbol{w}_{k}^{(t)} \sim q(\boldsymbol{w}^{(s)})$$
[18.59]

$$\omega_k = \frac{\tilde{p}(\boldsymbol{w}_k^{(t)} \mid \boldsymbol{w}^{(s)})}{q(\boldsymbol{w}_k^{(t)}; \boldsymbol{w}^{(s)})}$$
[18.60]

$$\tilde{R}(\boldsymbol{\theta}) \approx \frac{1}{\sum_{k=1}^{K} \omega_k} \sum_{k=1}^{K} \omega_k \times \Delta(\boldsymbol{w}_k^{(t)}, \boldsymbol{w}^{(t)}).$$
[18.61]

Importance sampling will generally give a more accurate approximation than uniform sampling. The only formal requirement is that the proposal assigns non-zero probability to every <sup>w</sup>(t) ∈ Y(w(s) ). For more on importance sampling and related methods, see [Robert and Casella](#page-558-2) [\(2013\)](#page-558-2).

# **Additional resources**

A complete textbook on machine translation is available from [Koehn](#page-541-0) [\(2009\)](#page-541-0). While this book precedes recent work on neural translation, a more recent draft chapter on neural translation models is also available [\(Koehn, 2017\)](#page-541-14). [Neubig](#page-551-11) [\(2017\)](#page-551-11) provides a comprehensive tutorial on neural machine translation, starting from first principles. The course notes from [Cho](#page-520-4) [\(2015\)](#page-520-4) are also useful. Several neural machine translation libraries are available: LAMTRAM is an implementation of neural machine translation in DYNET [\(Neu](#page-551-2)[big et al., 2017\)](#page-551-2); OPENNMT [\(Klein et al., 2017\)](#page-540-13) and FAIRSEQ are available in PYTORCH;

TENSOR2TENSOR is an implementation of several of the Google translation models in TEN-SORFLOW [\(Abadi et al., 2016\)](#page-509-1).

Literary translation is especially challenging, even for expert human translators. [Mes](#page-548-9)[sud](#page-548-9) [\(2014\)](#page-548-9) describes some of these issues in her review of an English translation of *L'´etranger*, the 1942 French novel by Albert Camus.[11](#page-471-0) She compares the new translation by Sandra Smith against earlier translations by Stuart Gilbert and Matthew Ward, focusing on the difficulties presented by a single word in the first sentence:

Then, too, Smith has reconsidered the book's famous opening. Camus's original is deceptively simple: "*Aujourd'hui, maman est morte*." Gilbert influenced generations by offering us "Mother died today"—inscribing in Meursault [the narrator] from the outset a formality that could be construed as heartlessness. But *maman*, after all, is intimate and affectionate, a child's name for his mother. Matthew Ward concluded that it was essentially untranslatable ("mom" or "mummy" being not quite apt), and left it in the original French: "Maman died today." There is a clear logic in this choice; but as Smith has explained, in an interview in *The Guardian*, *maman* "didn't really tell the reader anything about the connotation." She, instead, has translated the sentence as "My mother died today."

I chose "My mother" because I thought about how someone would tell another person that his mother had died. Meursault is speaking to the reader directly. "My mother died today" seemed to me the way it would work, and also implied the closeness of "maman" you get in the French.

Elsewhere in the book, she has translated *maman* as "mama" — again, striving to come as close as possible to an actual, colloquial word that will carry the same connotations as *maman* does in French.

The passage is a reminder that while the quality of machine translation has improved dramatically in recent years, expert human translations draw on considerations that are beyond the ken of any contemporary computational approach.

# **Exercises**

1. Using Google translate or another online service, translate the following example into two different languages of your choice:

<span id="page-471-0"></span><sup>11</sup>The book review is currently available online at [http://www.nybooks.com/articles/2014/06/](http://www.nybooks.com/articles/2014/06/05/camus-new-letranger/) [05/camus-new-letranger/](http://www.nybooks.com/articles/2014/06/05/camus-new-letranger/).

(18.4) It is not down on any map; true places never are.

Then translate each result back into English. Which is closer to the original? Can you explain the differences?

- 2. Compute the unsmoothed *n*-gram precisions  $p_1 \dots p_4$  for the two back-translations in the previous problem, using the original source as the reference. Your *n*-grams should include punctuation, and you should segment conjunctions like *it's* into two tokens.
- 3. You are given the following dataset of translations from "simple" to "difficult" English:
  - (18.5) a. *Kids like cats.* Children adore felines.
    - b. *Cats hats.* Felines fedoras.

Estimate a word-to-word statistical translation model from simple English (source) to difficult English (target), using the expectation-maximization as described in § 18.2.2. Compute two iterations of the algorithm by hand, starting from a uniform translation model, and using the simple alignment model  $p(a_m \mid m, M^{(s)}, M^{(t)}) = \frac{1}{M^{(t)}}$ . Hint: in the final M-step, you will want to switch from fractions to decimals.

- 4. Building on the previous problem, what will be the converged translation probability table? Can you state a general condition about the data, under which this translation model will fail in the way that it fails here?
- 5. Propose a simple alignment model that would make it possible to recover the correct translation probabilities from the toy dataset in the previous two problems.
- 6. Let  $\ell_{m+1}^{(t)}$  represent the loss at word m+1 of the target, and let  $h_n^{(s)}$  represent the hidden state at word n of the source. Write the expression for the derivative  $\frac{\partial \ell_{m+1}^{(t)}}{\partial h_n^{(s)}}$  in the sequence-to-sequence translation model expressed in Equations [18.29-18.32]. You may assume that both the encoder and decoder are one-layer LSTMs. In general, how many terms are on the shortest backpropagation path from  $\ell_{m+1}^{(t)}$  to  $h_n^{(s)}$ ?
- 7. Now consider the neural attentional model from § 18.3.1, with sigmoid attention. The derivative  $\frac{\partial \ell_{m+1}^{(t)}}{\partial z_n}$  is the sum of many paths through the computation graph; identify the shortest such path. You may assume that the initial state of the decoder recurrence  $h_0^{(t)}$  is *not* tied to the final state of the encoder recurrence  $h_{M^{(s)}}^{(s)}$ .

- 8. Apply byte-pair encoding for the vocabulary *it*, *unit*, *unite*, until no bigram appears more than once.
- 9. This problem relates to the complexity of machine translation. Suppose you have an oracle that returns the list of words to include in the translation, so that your only task is to order the words. Furthermore, suppose that the scoring function over orderings is a sum over bigrams, P<sup>M</sup> <sup>m</sup>=1 ψ(w (t) <sup>m</sup> , w (t) m−1 ). Show that the problem of finding the optimal translation is NP-complete, by reduction from a well-known problem.
- 10. Hand-design an attentional recurrent translation model that simply copies the input from the source to the target. You may assume an arbitrarily large hidden state, and you may assume that there is a finite maximum input length M. Specify all the weights such that the maximum probability translation of any source is the source itself. Hint: it is simplest to use the Elman recurrence h<sup>m</sup> = f(Θhm−<sup>1</sup> + xm) rather than an LSTM.
- 11. Give a synchronized derivation (§ [18.2.4\)](#page-458-0) for the Spanish-English translation,
  - (18.6) *El* The *pez* fish *enojado* angry *atacado.* attacked. The angry fish attacked.

As above, the second line shows a word-for-word gloss, and the third line shows the desired translation. Use the synchronized production rule in [\[18.22\]](#page-458-2), and design the other production rules necessary to derive this sentence pair. You may derive (*atacado, attacked*) directly from VP.

# <span id="page-474-2"></span><span id="page-474-0"></span>**Chapter 19 Text generation**

In many of the most interesting problems in natural language processing, language is the output. The previous chapter described the specific case of machine translation, but there are many other applications, from summarization of research articles, to automated journalism, to dialogue systems. This chapter emphasizes three main scenarios: data-totext, in which text is generated to explain or describe a structured record or unstructured perceptual input; text-to-text, which typically involves fusing information from multiple linguistic sources into a single coherent summary; and dialogue, in which text is generated as part of an interactive conversation with one or more human participants.

# <span id="page-474-1"></span>**19.1 Data-to-text generation**

In data-to-text generation, the input ranges from structured records, such as the description of an weather forecast (as shown in [Figure 19.1\)](#page-475-0), to unstructured perceptual data, such as a raw image or video; the output may be a single sentence, such as an image caption, or a multi-paragraph argument. Despite this diversity of conditions, all data-to-text systems share some of the same challenges [\(Reiter and Dale, 2000\)](#page-557-0):

- determining what parts of the data to describe;
- planning a presentation of this information;
- **lexicalizing** the data into words and phrases;
- organizing words and phrases into well-formed sentences and paragraphs.

The earlier stages of this process are sometimes called **content selection** and **text planning**; the later stages are often called **surface realization**.

Early systems for data-to-text generation were modular, with separate software components for each task. Artificial intelligence **planning** algorithms can be applied to both

<span id="page-475-1"></span><span id="page-475-0"></span>

| Temperature         |          |            |           | Cloud sky cover                    |                               |
|---------------------|----------|------------|-----------|------------------------------------|-------------------------------|
| time<br>06:00-21:00 | min<br>9 | mean<br>15 | max<br>21 | time<br>06:00-09:00<br>09:00-12:00 | percent (%)<br>25-50<br>50-75 |
| Wind speed          |          |            |           | Wind direction                     |                               |
| time                | min      | mean       | max       | time                               | mode                          |
| 06:00-21:00         | 15       | 20         | 30        | 06:00-21:00                        | S                             |

*Cloudy, with temperatures between 10 and 20 degrees. South wind around 20 mph.*

Figure 19.1: An example input-output pair for the task of generating text descriptions of weather forecasts (adapted from [Konstas and Lapata, 2013\)](#page-541-15).

the high-level information structure and the organization of individual sentences, ensuring that communicative goals are met [\(McKeown, 1992;](#page-547-13) [Moore and Paris, 1993\)](#page-550-13). Surface realization can be performed by grammars or templates, which link specific types of data to candidate words and phrases. A simple example template is offered by [Wiseman et al.](#page-569-12) [\(2017\)](#page-569-12), for generating descriptions of basketball games:

(19.1) The <team1> (<wins1>-losses1) defeated the <team2> (<wins2>-<losses2>), <pts1>-<pts2>. The New York Knicks (45-5) defeated the Boston Celtics (11-38), 115-79.

For more complex cases, it may be necessary to apply morphological inflections such as pluralization and tense marking — even in the simple example above, languages such as Russian would require case marking suffixes for the team names. Such inflections can be applied as a postprocessing step. Another difficult challenge for surface realization is the generation of varied **referring expressions** (e.g., *The Knicks*, *New York*, *they*), which is critical to avoid repetition. As discussed in § [16.2.1,](#page-399-0) the form of referring expressions is constrained by the discourse and information structure.

An example at the intersection of rule-based and statistical techniques is the NITRO-GEN system [\(Langkilde and Knight, 1998\)](#page-542-12). The input to NITROGEN is an abstract meaning representation (AMR; see § [13.3\)](#page-335-0) of semantic content to be expressed in a single sentence. In data-to-text scenarios, the abstract meaning representation is the output of a higherlevel text planning stage. A set of rules then converts the abstract meaning representation into various sentence plans, which may differ in both the high-level structure (e.g., active versus passive voice) as well as the low-level details (e.g., word and phrase choice). Some examples are shown in [Figure 19.2.](#page-476-1) To control the combinatorial explosion in the number of possible realizations for any given meaning, the sentence plans are unified into a single finite-state acceptor, in which word tokens are represented by arcs (see § [9.1.1\)](#page-210-0). A bigram

#### <span id="page-476-2"></span>19.1. DATA-TO-TEXT GENERATION 459

<span id="page-476-1"></span>

| (a / admire-01            | • Visitors who came to Japan admire Mount<br>Fuji. |
|---------------------------|----------------------------------------------------|
| :ARG0 (v / visitor        | • Visitors who came in Japan admire Mount          |
| :ARG1-of (c / arrive-01   | Fuji.                                              |
| :ARG4 (j / Japan)))       | • Mount Fuji is admired by the visitor who         |
| :ARG1 (m / "Mount Fuji")) | came in Japan.                                     |

Figure 19.2: Abstract meaning representation and candidate surface realizations from the NITROGEN system. Example adapted from [Langkilde and Knight](#page-542-12) [\(1998\)](#page-542-12).

language model is then used to compute weights on the arcs, so that the shortest path is also the surface realization with the highest bigram language model probability.

More recent systems are unified models that are trained end-to-end using backpropagation. Data-to-text generation shares many properties with machine translation, including a problem of **alignment**: labeled examples provide the data and the text, but they do not specify which parts of the text correspond to which parts of the data. For example, to learn from [Figure 19.1,](#page-475-0) the system must align the word *cloudy* to records in CLOUD SKY COVER, the phrases *10* and *20 degrees* to the MIN and MAX fields in TEMPERATURE, and so on. As in machine translation, both latent variables and neural attention have been proposed as solutions.

#### <span id="page-476-0"></span>**19.1.1 Latent data-to-text alignment**

Given a dataset of texts and associated records {(w(i) , y (i) )} N <sup>i</sup>=1, our goal is to learn a model Ψ, so that

$$\hat{\boldsymbol{w}} = \operatorname*{argmax}_{\boldsymbol{w} \in \mathcal{V}^*} \Psi(\boldsymbol{w}, \boldsymbol{y}; \boldsymbol{\theta}),$$
[19.1]

where V ∗ is the set of strings over a discrete vocabulary, and θ is a vector of parameters. The relationship between w and y is complex: the data y may contain dozens of records, and w may extend to several sentences. To facilitate learning and inference, it would be helpful to decompose the scoring function Ψ into subcomponents. This would be possible if given an **alignment**, specifying which element of y is expressed in each part of w. Specifically, let z<sup>m</sup> indicates the record aligned to word m. For example, in [Figure 19.1,](#page-475-0) z<sup>1</sup> might specify that the word *cloudy* is aligned to the record cloud-sky-cover:percent. The score for this alignment would then be given by the weight on features such as

(*cloudy*, cloud-sky-cover:percent). [19.2]

In general, given an observed set of alignments, the score for a generation can be

<span id="page-477-3"></span>written as sum of local scores (Angeli et al., 2010):

$$\Psi(\boldsymbol{w}, \boldsymbol{y}; \boldsymbol{\theta}) = \sum_{m=1}^{M} \psi_{w,y}(\boldsymbol{w}_m, \boldsymbol{y}_{z_m}) + \psi_w(w_m, w_{m-1}) + \psi_z(z_m, z_{m-1}), \quad [19.3]$$

where  $\psi_w$  can represent a bigram language model, and  $\psi_z$  can be tuned to reward coherence, such as the use of related records in nearby words. <sup>1</sup> The parameters of this model could be learned from labeled data  $\{(\boldsymbol{w}^{(i)}, \boldsymbol{y}^{(i)}, \boldsymbol{z}^{(i)})\}_{i=1}^N$ . However, while several datasets include structured records and natural language text (Barzilay and McKeown, 2005; Chen and Mooney, 2008; Liang and Klein, 2009), the alignments between text and records are usually not available.<sup>2</sup> One solution is to model the problem probabilistically, treating the alignment as a latent variable (Liang et al., 2009; Konstas and Lapata, 2013). The model can then be estimated using expectation maximization or sampling (see chapter 5).

#### <span id="page-477-0"></span>19.1.2 Neural data-to-text generation

The **encoder-decoder model** and **neural attention** were introduced in § 18.3 as methods for neural machine translation. They can also be applied to data-to-text generation, with the data acting as the source language (Mei et al., 2016). In neural machine translation, the attention mechanism linked words in the source to words in the target; in data-to-text generation, the attention mechanism can link each part of the generated text back to a record in the data. The biggest departure from translation is in the encoder, which depends on the form of the data.

#### Data encoders

In some types of structured records, all values are drawn from discrete sets. For example, the birthplace of an individual is drawn from a discrete set of possible locations; the diagnosis and treatment of a patient are drawn from an exhaustive list of clinical codes (Johnson et al., 2016). In such cases, vector embeddings can be estimated for each field and possible value: for example, a vector embedding for the field BIRTHPLACE, and another embedding for the value BERKELEY\_CALIFORNIA (Bordes et al., 2011). The table of such embeddings serves as the encoding of a structured record (He et al., 2017). It is also possible to compress the entire table into a single vector representation, by **pooling** across the embeddings of each field and value (Lebret et al., 2016).

<span id="page-477-1"></span><sup>&</sup>lt;sup>1</sup>More expressive decompositions of  $\Psi$  are possible. For example, Wong and Mooney (2007) use a synchronous context-free grammar (see § 18.2.4) to "translate" between a meaning representation and natural language text.

<span id="page-477-2"></span><sup>&</sup>lt;sup>2</sup>An exception is a dataset of records and summaries from American football games, containing annotations of alignments between sentences and records (Snyder and Barzilay, 2007).

<span id="page-478-0"></span>![](_page_478_Picture_1.jpeg)

A little <u>girl</u> sitting on a bed with a teddy bear.

![](_page_478_Figure_3.jpeg)

A giraffe standing in a forest with trees in the background.

Figure 19.3: Examples of the image captioning task, with attention masks shown for each of the underlined words (Xu et al., 2015).

**Sequences** Some types of structured records have a natural ordering, such as events in a game (Chen and Mooney, 2008) and steps in a recipe (Tutin and Kittredge, 1992). For example, the following records describe a sequence of events in a robot soccer match (Mei et al., 2016):

PASS(arg1 = PURPLE6, arg2 = PURPLE3)KICK(arg1 = PURPLE3) BADPASS(arg1 = PURPLE3, arg2 = PINK9).

Each event is a single record, and can be encoded by a concatenation of vector representations for the event type (e.g., PASS), the field (e.g., arg1), and the values (e.g., PURPLE3), e.g.,

$$\mathbf{X} = \begin{bmatrix} \boldsymbol{u}_{\text{PASS}}, \boldsymbol{u}_{\text{arg1}}, \boldsymbol{u}_{\text{PURPLE6}}, \boldsymbol{u}_{\text{arg2}}, \boldsymbol{u}_{\text{PURPLE3}} \end{bmatrix}.$$
[19.4]

This encoding can then act as the input layer for a recurrent neural network, yielding a sequence of vector representations  $\{z_r\}_{r=1}^R$ , where *r* indexes over records. Interestingly, this sequence-based approach can work even in cases where there is no natural ordering over the records, such as the weather data in Figure 19.1 (Mei et al., 2016).

**Images** Another flavor of data-to-text generation is the generation of text captions for images. Examples from this task are shown in Figure 19.3. Images are naturally represented as tensors: a color image of  $320 \times 240$  pixels would be stored as a tensor with  $320 \times 240 \times 3$  intensity values. The dominant approach to image classification is to encode images as vectors using a combination of convolution and pooling (Krizhevsky et al.,

<span id="page-479-0"></span>![](_page_479_Figure_1.jpeg)

Figure 19.4: Neural attention in text generation. Figure adapted from [Mei et al.](#page-548-10) [\(2016\)](#page-548-10).

[2012\)](#page-541-1). Chapter [3](#page-64-0) explains how to use convolutional networks for text; for images, convolution is applied across the vertical, horizontal, and color dimensions. By pooling the results of successive convolutions, the image is converted to a vector representation, which can then be fed directly into the decoder as the initial state [\(Vinyals et al., 2015\)](#page-567-11), just as in the sequence-to-sequence translation model (see § [18.3\)](#page-459-0). Alternatively, one can apply a set of convolutional networks, yielding vector representations for different parts of the image, which can then be combined using neural attention [\(Xu et al., 2015\)](#page-570-12).

#### **Attention**

Given a set of embeddings of the data {zr} R <sup>r</sup>=1 and a decoder state hm, an attention vector over the data can be computed using the same techniques as in machine translation (see § [18.3.1\)](#page-461-0). When generating word m of the output, attention is computed over the records,

$$\psi_{\alpha}(m,r) = \boldsymbol{\beta}_{\alpha} \cdot f(\boldsymbol{\Theta}_{\alpha}[\boldsymbol{h}_m; \boldsymbol{z}_r])$$
[19.5]

$$\boldsymbol{\alpha}_m = g\left( \left[ \psi_\alpha(m, 1), \psi_\alpha(m, 2), \dots, \psi_\alpha(m, R) \right] \right)$$
[19.6]

$$\boldsymbol{c}_m = \sum_{r=1}^R \alpha_{m \to r} \boldsymbol{z}_r, \qquad [19.7]$$

where f is an elementwise nonlinearity such as tanh or ReLU, and g is a either softmax or elementwise sigmoid. The weighted sum c<sup>m</sup> can then be included in the recurrent update to the decoder state, or in the emission probabilities, as described in § [18.3.1.](#page-461-0) [Figure 19.4](#page-479-0) shows the attention to components of a weather record, while generating the text shown on the x-axis.

Adapting this architecture to image captioning is straightforward. A convolutional neural networks is applied to a set of image locations, and the output at each location ` is represented with a vector z` . Attention can then be computed over the image locations, as shown in the right panels of each pair of images in [Figure 19.3.](#page-478-0)

<span id="page-480-1"></span>Various modifications to this basic mechanism have been proposed. In **coarse-to-fine attention** (Mei et al., 2016), each record receives a global attention  $a_r \in [0, 1]$ , which is independent of the decoder state. This global attention, which represents the overall importance of the record, is multiplied with the decoder-based attention scores, before computing the final normalized attentions. In **structured attention**, the attention vector  $\alpha_{m\rightarrow}$ . can include structural biases, which can favor assigning higher attention values to contiguous segments or to dependency subtrees (Kim et al., 2017). Structured attention vectors can be computed by running the forward-backward algorithm to obtain marginal attention probabilities (see § 7.5.3). Because each step in the forward-backward algorithm is differentiable, it can be encoded in a computation graph, and end-to-end learning can be performed by backpropagation.

#### Decoder

Given the encoding, the decoder can function just as in neural machine translation (see  $\S$  18.3.1), using the attention-weighted encoder representation in the decoder recurrence and/or output computation. As in machine translation, beam search can help to avoid search errors (Lebret et al., 2016).

Many applications require generating words that do not appear in the training vocabulary. For example, a weather record may contain a previously unseen city name; a sports record may contain a previously unseen player name. Such tokens can be generated in the text by copying them over from the input (e.g., Gulcehre et al., 2016).<sup>3</sup> First introduce an additional variable  $s_m \in \{\text{gen}, \text{copy}\}$ , indicating whether token  $w_m^{(t)}$  should be generated or copied. The decoder probability is then,

$$\mathbf{p}(w^{(t)} \mid \boldsymbol{w}_{1:m-1}^{(t)}, \mathbf{Z}, s_m) = \begin{cases} \text{SoftMax}(\boldsymbol{\beta}_{w^{(t)}} \cdot \boldsymbol{h}_{m-1}^{(t)}), & s_m = \text{gen} \\ \sum_{r=1}^R \delta\left(w_r^{(s)} = w^{(t)}\right) \times \alpha_{m \to r}, & s_m = \text{copy}, \end{cases}$$
[19.8]

where  $\delta(w_r^{(s)} = w^{(t)})$  is an indicator function, taking the value 1 iff the text of the record  $w_r^{(s)}$  is identical to the target word  $w^{(t)}$ . The probability of copying record r from the source is  $\delta(s_m = \text{copy}) \times \alpha_{m \to r}$ , the product of the copy probability by the local attention. Note that in this model, the attention weights  $\alpha_m$  are computed from the *previous* decoder state  $h_{m-1}$ . The computation graph therefore remains a feedforward network, with recurrent paths such as  $h_{m-1}^{(t)} \to \alpha_m \to w_m^{(t)} \to h_m^{(t)}$ .

To facilitate end-to-end training, the switching variable  $s_m$  can be represented by a gate  $\pi_m$ , which is computed from a two-layer feedforward network, whose input consists of the concatenation of the decoder state  $h_{m-1}^{(t)}$  and the attention-weighted representation

<span id="page-480-0"></span><sup>&</sup>lt;sup>3</sup>A number of variants of this strategy have been proposed (e.g., Gu et al., 2016; Merity et al., 2017). See Wiseman et al. (2017) for an overview.

of the data, c<sup>m</sup> = P<sup>R</sup> <sup>r</sup>=1 αm→rzr,

$$\pi_m = \sigma(\Theta^{(2)} f(\Theta^{(1)}[\boldsymbol{h}_{m-1}^{(t)}; \boldsymbol{c}_m])).$$
[19.9]

The full generative probability at token m is then,

$$p(w^{(t)} \mid \boldsymbol{w}_{1:m}^{(t)}, \mathbf{Z}) = \pi_m \times \underbrace{\frac{\exp \boldsymbol{\beta}_{w^{(t)}} \cdot \boldsymbol{h}_{m-1}^{(t)}}{\sum_{j=1}^{V} \exp \boldsymbol{\beta}_j \cdot \boldsymbol{h}_{m-1}^{(t)}}_{\text{generate}} + (1 - \pi_m) \times \underbrace{\sum_{r=1}^{R} \delta(w_r^{(s)} = w^{(t)}) \times \alpha_{m \to r}}_{\text{copy}}.$$
[19.10]

# <span id="page-481-0"></span>**19.2 Text-to-text generation**

Text-to-text generation includes problems of summarization and simplification:

- reading a novel and outputting a paragraph-long summary of the plot;[4](#page-481-2)
- reading a set of blog posts about politics, and outputting a bullet list of the various issues and perspectives;
- reading a technical research article about the long-term health consequences of drinking kombucha, and outputting a summary of the article in language that non-experts can understand.

These problems can be approached in two ways: through the encoder-decoder architecture discussed in the previous section, or by operating directly on the input text.

#### <span id="page-481-1"></span>**19.2.1 Neural abstractive summarization**

**Sentence summarization** is the task of shortening a sentence while preserving its meaning, as in the following examples [\(Knight and Marcu, 2000;](#page-540-14) [Rush et al., 2015\)](#page-559-10):

- <span id="page-481-3"></span>(19.2) a. The documentation is typical of Epson quality: excellent. Documentation is excellent.
  - b. Russian defense minister Ivanov called sunday for the creation of a joint front for combating global terrorism. Russia calls for joint front against terrorism.

<span id="page-481-4"></span>

<span id="page-481-2"></span><sup>4</sup> In § [16.3.4,](#page-410-1) we encountered a special case of single-document summarization, which involved extracting the most important sentences or discourse units. We now consider the more challenging problem of **abstractive summarization**, in which the summary can include words that do not appear in the original text.

#### <span id="page-482-1"></span>19.2. TEXT-TO-TEXT GENERATION

Sentence summarization is closely related to **sentence compression**, in which the summary is produced by deleting words or phrases from the original (Clarke and Lapata, 2008). But as shown in (19.2b), a sentence summary can also introduce new words, such as *against*, which replaces the phrase *for combatting*.

Sentence summarization can be treated as a machine translation problem, using the attentional encoder-decoder translation model discussed in § 18.3.1 (Rush et al., 2015). The longer sentence is encoded into a sequence of vectors, one for each token. The decoder then computes attention over these vectors when updating its own recurrent state. As with data-to-text generation, it can be useful to augment the encoder-decoder model with the ability to copy words directly from the source. Rush et al. (2015) train this model by building four million sentence pairs from news articles. In each pair, the longer sentence is the first sentence of the article, and the summary is the article headline. Sentence summarization can also be trained in a semi-supervised fashion, using a probabilistic formulation of the encoder-decoder model called a **variational autoencoder** (Miao and Blunsom, 2016, also see § 14.8.2).

When summarizing longer documents, an additional concern is that the summary not be repetitive: each part of the summary should cover new ground. This can be addressed by maintaining a vector of the sum total of all attention values thus far,  $t_m = \sum_{n=1}^{m} \alpha_n$ . This total can be used as an additional input to the computation of the attention weights,

$$\alpha_{m \to n} \propto \exp\left(\boldsymbol{v}_{\alpha} \cdot \tanh(\Theta_{\alpha}[\boldsymbol{h}_{m}^{(t)}; \boldsymbol{h}_{n}^{(s)}; \boldsymbol{t}_{m}])\right), \qquad [19.11]$$

which enables the model to learn to prefer parts of the source which have not been attended to yet (Tu et al., 2016). To further encourage diversity in the generated summary, See et al. (2017) introduce a **coverage loss** to the objective function,

$$\ell_m = \sum_{n=1}^{M^{(s)}} \min(\alpha_{m \to n}, t_{m \to n}).$$
[19.12]

This loss will be low if  $\alpha_m$  assigns little attention to words that already have large values in  $t_m$ . Coverage loss is similar to the concept of **marginal relevance**, in which the reward for adding new content is proportional to the extent to which it increases the overall amount of information conveyed by the summary (Carbonell and Goldstein, 1998).

#### <span id="page-482-0"></span>**19.2.2** Sentence fusion for multi-document summarization

In **multi-document summarization**, the goal is to produce a summary that covers the content of several documents (McKeown et al., 2002). One approach to this challenging problem is to identify sentences across multiple documents that relate to a single theme, and then to fuse them into a single sentence (Barzilay and McKeown, 2005). As an example, consider the following two sentences (McKeown et al., 2010):

- <span id="page-483-1"></span>(19.3) a. Palin actually turned against the bridge project only after it became a national symbol of wasteful spending.
  - b. Ms. Palin supported the bridge project while running for governor, and abandoned it after it became a national scandal.

An *intersection* preserves only the content that is present in both sentences:

(19.4) Palin turned against the bridge project after it became a national scandal.

A *union* includes information from both sentences:

(19.5) Ms. Palin supported the bridge project while running for governor, but turned against it when it became a national scandal and a symbol of wasteful spending.

Dependency parsing is often used as a technique for sentence fusion. After parsing each sentence, the resulting dependency trees can be aggregated into a lattice (Barzilay and McKeown, 2005) or a graph structure (Filippova and Strube, 2008), in which identical or closely related words (e.g., *Palin, bridge, national*) are fused into a single node. The resulting graph can then be pruned back to a tree by solving an **integer linear program** (see § 13.2.2),

$$\max_{\boldsymbol{y}} \quad \sum_{i,j,r} \psi(i \xrightarrow{r} j, \boldsymbol{w}; \boldsymbol{\theta}) \times y_{i,j,r}$$
[19.13]

s.t. 
$$y \in \mathcal{C}$$
, [19.14]

where the variable  $y_{i,j,r} \in \{0,1\}$  indicates whether there is an edge from *i* to *j* of type *r*, the score of this edge is  $\psi(i \xrightarrow{r} j, w; \theta)$ , and *C* is a set of constraints, which ensures that *y* forms a valid dependency graph. As usual, *w* is the list of words in the graph, and  $\theta$  is a vector of parameters. The score  $\psi(i \xrightarrow{r} j, w; \theta)$  reflects the "importance" of the modifier *j* to the overall meaning: in intersective fusion, this score indicates the extent to which the content in this edge is expressed in all sentences; in union fusion, the score indicates whether the content in the edge is expressed in any sentence. The constraint set *C* can impose additional linguistic constraints: for example, ensuring that coordinated nouns are sufficiently similar. The resulting tree must then be **linearized** into a sentence. Linearization is like the inverse of dependency parsing: instead of parsing from a sequence of tokens into a tree, we must convert the tree back into a sequence of tokens. This is typically done by generating a set of candidate linearizations, and choosing the one with the highest score under a language model (Langkilde and Knight, 1998; Song et al., 2016).

#### <span id="page-483-0"></span>19.3 Dialogue

**Dialogue systems** are capable of conversing with a human interlocutor, often to perform some task (Grosz, 1979), but sometimes just to chat (Weizenbaum, 1966). While re-

<span id="page-484-2"></span><span id="page-484-1"></span>![](_page_484_Figure_1.jpeg)

Figure 19.5: An example dialogue and the associated finite-state model. In the finite-state model, SMALL CAPS indicates that the user must provide information of this type in their answer.

search on dialogue systems goes back several decades [\(Carbonell, 1970;](#page-517-13) [Winograd, 1972\)](#page-569-6), commercial systems such as Alexa and Siri have recently brought this technology into widespread use. Nonetheless, there is a significant gap between research and practice: many practical dialogue systems remain scripted and inflexible, while research systems emphasize abstractive text generation, "on-the-fly" decision making, and probabilistic reasoning about the user's intentions.

#### <span id="page-484-0"></span>**19.3.1 Finite-state and agenda-based dialogue systems**

Finite-state automata were introduced in [chapter 9](#page-208-0) as a formal model of computation, in which string inputs and outputs are linked to transitions between a finite number of discrete states. This model naturally fits simple task-oriented dialogues, such as the one shown in the left panel of [Figure 19.5.](#page-484-1) This (somewhat frustrating) dialogue can be represented with a finite-state transducer, as shown in the right panel of the figure. The accepting state is reached only when the two needed pieces of information are provided, and the human user confirms that the order is correct. In this simple scenario, the TOPPING and ADDRESS are the two **slots** associated with the activity of ordering a pizza, which is called a **frame**. Frame representations can be hierarchical: for example, an ADDRESS could have slots of its own, such as STREET and CITY.

In the example dialogue in [Figure 19.5,](#page-484-1) the user provides the precise inputs that are needed in each turn (e.g., *anchovies*; *the College of Computing building*). Some users may

<span id="page-485-2"></span>prefer to communicate more naturally, with phrases like *I'd, uh, like some anchovies please*. One approach to handling such utterances is to design a custom grammar, with nonterminals for slots such as TOPPING and LOCATION. However, context-free parsing of unconstrained speech input is challenging. A more lightweight alternative is BIO-style sequence labeling (see § [8.3\)](#page-200-0), e.g.:

<span id="page-485-1"></span>(19.7) *I'd* O *like* O *anchovies* B-TOPPING *,* O *and* O *please* O *bring* O *it* O *to* O *the* B-ADDR *College* I-ADDR *of* I-ADDR *Computing* I-ADDR *Building* I-ADDR *.* O

The tagger can be driven by a bi-directional recurrent neural network, similar to recurrent approaches to semantic role labeling described in § [13.2.3.](#page-334-0)

The input in [\(19.7\)](#page-485-1) could not be handled by the finite-state system from [Figure 19.5,](#page-484-1) which forces the user to provide the topping first, and then the location. In this sense, the "initiative" is driven completely by the system. **Agenda-based dialogue systems** extend finite-state architectures by attempting to recognize all slots that are filled by the user's reply, thereby handling these more complex examples. Agenda-based systems dynamically pose additional questions until the frame is complete [\(Bobrow et al., 1977;](#page-515-13) [Allen et al.,](#page-510-10) [1995;](#page-510-10) [Rudnicky and Xu, 1999\)](#page-559-11). Such systems are said to be **mixed-initiative**, because both the user and the system can drive the direction of the dialogue.

### <span id="page-485-0"></span>**19.3.2 Markov decision processes**

The task of dynamically selecting the next move in a conversation is known as **dialogue management**. This problem can be framed as a **Markov decision process**, which is a theoretical model that includes a discrete set of states, a discrete set of actions, a function that computes the probability of transitions between states, and a function that computes the cost or reward of action-state pairs. Let's see how each of these elements pertains to the pizza ordering dialogue system.

• Each state is a tuple of information about whether the topping and address are known, and whether the order has been confirmed. For example,

```
(KNOWN TOPPING, UNKNOWN ADDRESS, NOT CONFIRMED) [19.15]
```

is a possible state. Any state in which the pizza order is confirmed is a terminal state, and the Markov decision process stops after entering such a state.

• The set of actions includes querying for the topping, querying for the address, and requesting confirmation. Each action induces a probability distribution over states, p(s<sup>t</sup> | a<sup>t</sup> , st−1). For example, requesting confirmation of the order is not likely to

<span id="page-486-2"></span>result in a transition to the terminal state if the topping is not yet known. This probability distribution over state transitions may be learned from data, or it may be specified in advance.

• Each state-action-state tuple earns a reward,  $r_a(s_t, s_{t+1})$ . In the context of the pizza ordering system, a simple reward function would be,

<span id="page-486-0"></span>
$$r_a(s_t, s_{t-1}) = \begin{cases} 0, & a = \text{CONFIRM}, s_t = (*, *, \text{CONFIRMED}) \\ -10, & a = \text{CONFIRM}, s_t = (*, *, \text{NOT CONFIRMED}) \\ -1, & a \neq \text{CONFIRM} \end{cases}$$
[19.16]

This function assigns zero reward for successful transitions to the terminal state, a large negative reward to a rejected request for confirmation, and a small negative reward for every other type of action. The system is therefore rewarded for reaching the terminal state in few steps, and penalized for prematurely requesting confirmation.

In a Markov decision process, a **policy** is a function  $\pi : S \to A$  that maps from states to actions (see § 15.2.4). The value of a policy is the expected sum of discounted rewards,  $E_{\pi}[\sum_{t=1}^{T} \gamma^t r_{a_t}(s_t, s_{t+1})]$ , where  $\gamma$  is the discount factor,  $\gamma \in [0, 1)$ . Discounting has the effect of emphasizing rewards that can be obtained immediately over less certain rewards in the distant future.

An optimal policy can be obtained by dynamic programming, by iteratively updating the **value function** V(s), which is the expectation of the cumulative reward from *s* under the optimal action *a*,

<span id="page-486-1"></span>
$$V(s) \leftarrow \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} \mathbf{p}(s' \mid s, a) [r_a(s, s') + \gamma V(s')].$$
[19.17]

The value function V(s) is computed in terms of V(s') for all states  $s' \in S$ . A series of iterative updates to the value function will eventually converge to a stationary point. This algorithm is known as **value iteration**. Given the converged value function V(s), the optimal action at each state is the argmax,

$$\pi(s) = \operatorname*{argmax}_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} \mathbf{p}(s' \mid s, a) [r_a(s, s') + \gamma V(s')]. \tag{19.18}$$

Value iteration and related algorithms are described in detail by Sutton and Barto (1998). For applications to dialogue systems, see Levin et al. (1998) and Walker (2000).

The Markov decision process framework assumes that the current state of the dialogue is known. In reality, the system may misinterpret the user's statements — for example, believing that a specification of the delivery location (PEACHTREE) is in fact a specification

<span id="page-487-2"></span>of the topping (PEACHES). In a **partially observable Markov decision process (POMDP)**, the system receives an *observation* o, which is probabilistically conditioned on the state, p(o | s). It must therefore maintain a distribution of beliefs about which state it is in, with qt(s) indicating the degree of belief that the dialogue is in state s at time t. The POMDP formulation can help to make dialogue systems more robust to errors, particularly in the context of spoken language dialogues, where the speech itself may be misrecognized [\(Roy](#page-559-12) [et al., 2000;](#page-559-12) [Williams and Young, 2007\)](#page-569-14). However, finding the optimal policy in a POMDP is computationally intractable, requiring additional approximations.

### <span id="page-487-0"></span>**19.3.3 Neural chatbots**

It's easier to talk when you don't need to get anything done. **Chatbots** are systems that parry the user's input with a response that keeps the conversation going. They can be built from the encoder-decoder architecture discussed in § [18.3](#page-459-0) and § [19.1.2:](#page-477-0) the encoder converts the user's input into a vector, and the decoder produces a sequence of words as a response. For example, [Shang et al.](#page-560-13) [\(2015\)](#page-560-13) apply the attentional encoder-decoder translation model, training on a dataset of posts and responses from the Chinese microblogging platform Sina Weibo.[5](#page-487-1) This approach is capable of generating replies that relate thematically to the input, as shown in the following examples (translated from Chinese by [Shang](#page-560-13) [et al., 2015\)](#page-560-13).

- (19.8) a. A: High fever attacks me every New Year's day. B: Get well soon and stay healthy!
  - b. A: I gain one more year. Grateful to my group, so happy. B: Getting old now. Time has no mercy.

While encoder-decoder models can generate responses that make sense in the context of the immediately preceding turn, they struggle to maintain coherence over longer conversations. One solution is to model the dialogue context recurrently. This creates a **hierarchical recurrent network**, including both word-level and turn-level recurrences. The turn-level hidden state is then used as additional context in the decoder [\(Serban et al.,](#page-560-14) [2016\)](#page-560-14).

An open question is how to integrate the encoder-decoder architecture into task-oriented dialogue systems. Neural chatbots can be trained end-to-end: the user's turn is analyzed by the encoder, and the system output is generated by the decoder. This architecture can be trained by log-likelihood using backpropagation (e.g., [Sordoni et al., 2015;](#page-562-13) [Serban](#page-560-14) [et al., 2016\)](#page-560-14), or by more elaborate objectives, using reinforcement learning [\(Li et al., 2016\)](#page-544-15). In contrast, the task-oriented dialogue systems described in § [19.3.1](#page-484-0) typically involve a

<span id="page-487-1"></span><sup>5</sup>Twitter is also frequently used for construction of dialogue datasets [\(Ritter et al., 2011;](#page-558-10) [Sordoni et al.,](#page-562-13) [2015\)](#page-562-13). Another source is technical support chat logs from the Ubuntu linux distribution [\(Uthus and Aha,](#page-566-14) [2013;](#page-566-14) [Lowe et al., 2015\)](#page-546-14).

<span id="page-488-0"></span>set of specialized modules: one for recognizing the user input, another for deciding what action to take, and a third for arranging the text of the system output.

Recurrent neural network decoders can be integrated into Markov Decision Process dialogue systems, by conditioning the decoder on a representation of the information that is to be expressed in each turn [\(Wen et al., 2015\)](#page-569-15). Specifically, the long short-term memory (LSTM; § [6.3\)](#page-150-1) architecture is augmented so that the memory cell at turn m takes an additional input dm, which is a representation of the slots and values to be expressed in the next turn. However, this approach still relies on additional modules to recognize the user's utterance and to plan the overall arc of the dialogue.

Another promising direction is to create embeddings for the elements in the domain: for example, the slots in a record and the entities that can fill them. The encoder then encodes not only the words of the user's input, but the embeddings of the elements that the user mentions. Similarly, the decoder is endowed with the ability to refer to specific elements in the knowledge base. [He et al.](#page-533-14) [\(2017\)](#page-533-14) show that such a method can learn to play a collaborative dialogue game, in which both players are given a list of entities and their properties, and the goal is to find an entity that is on both players' lists.

# **Additional resources**

[Gatt and Krahmer](#page-530-12) [\(2018\)](#page-530-12) provide a comprehensive recent survey on text generation. For a book-length treatment of earlier work, see [Reiter and Dale](#page-557-0) [\(2000\)](#page-557-0). For a survey on image captioning, see [Bernardi et al.](#page-514-15) [\(2016\)](#page-514-15); for a survey of pre-neural approaches to dialogue systems, see [Rieser and Lemon](#page-558-11) [\(2011\)](#page-558-11). **Dialogue acts** were introduced in § [8.6](#page-204-0) as a labeling scheme for human-human dialogues; they also play a critical in task-based dialogue systems (e.g., [Allen et al., 1996\)](#page-510-11). The incorporation of theoretical models of dialogue into computational systems is reviewed by [Jurafsky and Martin](#page-538-11) [\(2009,](#page-538-11) chapter 24).

While this chapter has focused on the informative dimension of text generation, another line of research aims to generate text with configurable stylistic properties [\(Walker](#page-568-13) [et al., 1997;](#page-568-13) [Mairesse and Walker, 2011;](#page-546-15) [Ficler and Goldberg, 2017;](#page-527-13) [Hu et al., 2017\)](#page-536-14). This chapter also does not address the generation of creative text such as narratives [\(Riedl and](#page-558-12) [Young, 2010\)](#page-558-12), jokes [\(Ritchie, 2001\)](#page-558-13), poems [\(Colton et al., 2012\)](#page-522-12), and song lyrics [\(Gonc¸alo Oli](#page-531-13)veira [et al., 2007\)](#page-531-13).

#### **Exercises**

1. Find an article about a professional basketball game, with an associated "box score" of statistics. Which are the first three elements in the box score that are expressed in the article? Can you identify template-based patterns that express these elements of the record? Now find a second article about a different basketball game. Does it

<span id="page-489-0"></span>mention the same first three elements of the box score? Do your templates capture how these elements are expressed in the text?

- 2. This exercise is to be done by a pair of students. One student should choose an article from the news or from Wikipedia, and manually perform semantic role labeling (SRL) on three short sentences or clauses. (See [chapter 13](#page-322-0) for a review of SRL.) Identify the main the semantic relation and its arguments and adjuncts. Pass this structured record — but not the original sentence — to the other student, whose job is to generate a sentence expressing the semantics. Then reverse roles, and try to regenerate three sentences from another article, based on the predicate-argument semantics.
- 3. Compute the BLEU scores (see § [18.1.1\)](#page-450-0) for the generated sentences in the previous problem, using the original article text as the reference.
- 4. Align each token in the text of [Figure 19.1](#page-475-0) to a specific single record in the database, or to the null record ∅. For example, the tokens *south wind* would align to the record wind direction: 06:00-21:00: mode=S. How often is each token aligned to the same record as the previous token? How many transitions are there? How might a system learn to output *10 degrees* for the record min=9?
- 5. In sentence compression and fusion, we may wish to preserve contiguous sequences of tokens (n-grams) and/or dependency edges. Find five short news articles with headlines. For each headline, compute the fraction of bigrams that appear in the main text of the article. Then do a manual depenency parse of the headline. For each dependency edge, count how often it appears as a dependency edge in the main text. You may use an automatic dependency parser to assist with this exercise, but check the output, and focus on UD 2.0 dependency grammar, as described in [chapter 11.](#page-274-0)
- 6. § [19.2.2](#page-482-0) presents the idea of generating text from dependency trees, which requires **linearization**. Sometimes there are multiple ways that a dependency tree can be linearized. For example:
  - (19.9) a. The sick kids stayed at home in bed.
    - b. The sick kids stayed in bed at home.

Both sentences have an identical dependency parse: both *home* and *bed* are (oblique) dependents of *stayed*.

Identify two more English dependency trees that can each be linearized in more than one way, and try to use a different pattern of variation in each tree. As usual, specify your trees in the Universal Dependencies 2 style, which is described in [chapter 11.](#page-274-0)

#### 19.3. DIALOGUE 473

7. In § [19.3.2,](#page-485-0) we considered a pizza delivery service. Let's simplify the problem to take-out, where it is only necessary to determine the topping and confirm the order. The state is a tuple in which the first element is T if the topping is specified and ? otherwise, and the second element is either YES or NO, depending on whether the order has been confirmed. The actions are TOPPING? (request information about the topping) and CONFIRM? (request confirmation). The state transition function is:

$$p(s_t \mid s_{t-1} = (?, NO), a = TOPPING?) = \begin{cases} 0.9, & s_t = (T, NO) \\ 0.1, & s_t = (?, NO). \end{cases}$$
[19.19]

$$p(s_t | s_{t-1} = (?, NO), a = CONFIRM?) = \{1, s_t = (?, NO).$$
 [19.20]

$$p(s_t | s_{t-1} = (T, NO), a = TOPPING?) = \begin{cases} 1, & s_t = (T, NO). \end{cases}$$
 [19.21]

$$p(s_t \mid s_{t-1} = (T, NO), a = CONFIRM?) = \begin{cases} 0.9, & s_t = (T, YES) \\ 0.1, & s_t = (T, NO). \end{cases}$$
[19.22]

Using the reward function defined in [Equation 19.16,](#page-486-0) the discount γ = 0.9, and the initialization V (s) = 0, execute three iterations of [Equation 19.17.](#page-486-1) After these three iterations, compute the optimal action in each state. You can assume that for the terminal states, V (\*, YES) = 0, so you only need to compute the values for nonterminal states, V (?, NO) and V (T, NO).

8. There are several toolkits that allow you to train encoder-decoder translation models "out of the box", such as FAIRSEQ [\(Gehring et al., 2017\)](#page-530-11), XNMT [\(Neubig et al., 2018\)](#page-551-12), TENSOR2TENSOR [\(Vaswani et al., 2018\)](#page-567-12), and OPENNMT [\(Klein et al., 2017\)](#page-540-13).[6](#page-490-0) Use one of these toolkits to train a chatbot dialogue system, using either the NPS dialogue corpus that comes with NLTK [\(Forsyth and Martell, 2007\)](#page-528-14), or, if you are feeling more ambitious, the Ubuntu dialogue corpus [\(Lowe et al., 2015\)](#page-546-14).

<span id="page-490-0"></span><sup>6</sup><https://github.com/facebookresearch/fairseq>; <https://github.com/neulab/xnmt>; <https://github.com/tensorflow/tensor2tensor>; <http://opennmt.net/>

# <span id="page-492-2"></span><span id="page-492-0"></span>**Appendix A Probability**

Probability theory provides a way to reason about random events. The sorts of random events that are typically used to explain probability theory include coin flips, card draws, and the weather. It may seem odd to think about the choice of a word as akin to the flip of a coin, particularly if you are the type of person to choose words carefully. But random or not, language has proven to be extremely difficult to model deterministically. Probability offers a powerful tool for modeling and manipulating linguistic data.

Probability can be thought of in terms of **random outcomes**: for example, a single coin flip has two possible outcomes, heads or tails. The set of possible outcomes is the **sample space**, and a subset of the **sample space** is an **event**. For a sequence of two coin flips, there are four possible outcomes, {HH, HT, T H, T T}, representing the ordered sequences heads-head, heads-tails, tails-heads, and tails-tails. The event of getting exactly one head includes two outcomes: {HT, T H}.

Formally, a probability is a function from events to the interval between zero and one: Pr : F → [0, 1], where F is the set of possible events. An event that is certain has probability one; an event that is impossible has probability zero. For example, the probability of getting fewer than three heads on two coin flips is one. Each outcome is also an event (a set with exactly one element), and for two flips of a fair coin, the probability of each outcome is,

$$\Pr(\{HH\}) = \Pr(\{HT\}) = \Pr(\{TH\}) = \Pr(\{TT\}) = \frac{1}{4}.$$
 [A.1]

# <span id="page-492-1"></span>**A.1 Probabilities of event combinations**

Because events are sets of outcomes, we can use set-theoretic operations such as complement, intersection, and union to reason about the probabilities of events and their combinations.

For any event *A*, there is a **complement**  $\neg$ *A*, such that:

- The probability of the union  $A \cup \neg A$  is  $Pr(A \cup \neg A) = 1$ ;
- The intersection  $A \cap \neg A = \emptyset$  is the empty set, and  $Pr(A \cap \neg A) = 0$ .

In the coin flip example, the event of obtaining a single head on two flips corresponds to the set of outcomes  $\{HT, TH\}$ ; the complement event includes the other two outcomes,  $\{TT, HH\}$ .

#### <span id="page-493-0"></span>A.1.1 Probabilities of disjoint events

When two events have an empty intersection,  $A \cap B = \emptyset$ , they are **disjoint**. The probability of the union of two disjoint events is equal to the sum of their probabilities,

$$A \cap B = \emptyset \quad \Rightarrow \quad \Pr(A \cup B) = \Pr(A) + \Pr(B).$$
 [A.2]

This is the **third axiom of probability**, and it can be generalized to any countable sequence of disjoint events.

In the coin flip example, this axiom can derive the probability of the event of getting a single head on two flips. This event is the set of outcomes  $\{HT, TH\}$ , which is the union of two simpler events,  $\{HT, TH\} = \{HT\} \cup \{TH\}$ . The events  $\{HT\}$  and  $\{TH\}$  are disjoint. Therefore,

$$\Pr(\{HT, TH\}) = \Pr(\{HT\} \cup \{TH\}) = \Pr(\{HT\}) + \Pr(\{TH\})$$
 [A.3]

<span id="page-493-1"></span>
$$=\frac{1}{4} + \frac{1}{4} = \frac{1}{2}.$$
 [A.4]

In the general, the probability of the union of two events is,

$$\Pr(A \cup B) = \Pr(A) + \Pr(B) - \Pr(A \cap B).$$
[A.5]

This can be seen visually in Figure A.1, and it can be derived from the third axiom of probability. Consider an event that includes all outcomes in *B* that are not in *A*, denoted as  $B - (A \cap B)$ . By construction, this event is disjoint from *A*. We can therefore apply the additive rule,

$$\Pr(A \cup B) = \Pr(A) + \Pr(B - (A \cap B)).$$
[A.6]

Furthermore, the event *B* is the union of two disjoint events:  $A \cap B$  and  $B - (A \cap B)$ .

$$\Pr(B) = \Pr(B - (A \cap B)) + \Pr(A \cap B).$$
[A.7]

Reorganizing and subtituting into Equation A.6 gives the desired result:

$$\Pr(B - (A \cap B)) = \Pr(B) - \Pr(A \cap B)$$
[A.8]

$$\Pr(A \cup B) = \Pr(A) + \Pr(B) - \Pr(A \cap B).$$
[A.9]

Jacob Eisenstein. Draft of November 13, 2018.

<span id="page-493-2"></span>476

<span id="page-494-4"></span><span id="page-494-2"></span>![](_page_494_Figure_1.jpeg)

Figure A.1: A visualization of the probability of non-disjoint events A and B.

#### <span id="page-494-0"></span>A.1.2 Law of total probability

A set of events  $\mathcal{B} = \{B_1, B_2, \dots, B_N\}$  is a **partition** of the sample space iff each pair of events is disjoint  $(B_i \cap B_j = \emptyset)$ , and the union of the events is the entire sample space. The law of total probability states that we can **marginalize** over these events as follows,

$$\Pr(A) = \sum_{B_n \in \mathcal{B}} \Pr(A \cap B_n).$$
 [A.10]

For any event *B*, the union  $B \cup \neg B$  is a partition of the sample space. Therefore, a special case of the law of total probability is,

$$\Pr(A) = \Pr(A \cap B) + \Pr(A \cap \neg B).$$
[A.11]

# <span id="page-494-1"></span>A.2 Conditional probability and Bayes' rule

A **conditional probability** is an expression like  $Pr(A \mid B)$ , which is the probability of the event *A*, assuming that event *B* happens too. For example, we may be interested in the probability of a randomly selected person answering the phone by saying *hello*, conditioned on that person being a speaker of English. Conditional probability is defined as the ratio,

<span id="page-494-3"></span>
$$\Pr(A \mid B) = \frac{\Pr(A \cap B)}{\Pr(B)}.$$
[A.12]

The chain rule of probability states that  $Pr(A \cap B) = Pr(A \mid B) \times Pr(B)$ , which is just

<span id="page-495-0"></span>a rearrangement of terms from [Equation A.12.](#page-494-3) The chain rule can be applied repeatedly:

$$Pr(A \cap B \cap C) = Pr(A \mid B \cap C) \times Pr(B \cap C)$$
$$= Pr(A \mid B \cap C) \times Pr(B \mid C) \times Pr(C).$$

**Bayes' rule** (sometimes called Bayes' law or Bayes' theorem) gives us a way to convert between Pr(A | B) and Pr(B | A). It follows from the definition of conditional probability and the chain rule:

$$\Pr(A \mid B) = \frac{\Pr(A \cap B)}{\Pr(B)} = \frac{\Pr(B \mid A) \times \Pr(A)}{\Pr(B)}$$
[A.13]

Each term in Bayes rule has a name, which we will occasionally use:

- Pr(A) is the **prior**, since it is the probability of event A without knowledge about whether B happens or not.
- Pr(B | A) is the **likelihood**, the probability of event B given that event A has occurred.
- Pr(A | B) is the **posterior**, the probability of event A with knowledge that B has occurred.

**Example** The classic examples for Bayes' rule involve tests for rare diseases, but [Man](#page-546-3)[ning and Schutze](#page-546-3) [\(1999\)](#page-546-3) reframe this example in a linguistic setting. Suppose that you are ¨ is interested in a rare syntactic construction, such as *parasitic gaps*, which occur on average once in 100,000 sentences. Here is an example of a parasitic gap:

#### (A.1) *Which class did you attend without registering for ?*

Lana Linguist has developed a complicated pattern matcher that attempts to identify sentences with parasitic gaps. It's pretty good, but it's not perfect:

- If a sentence has a parasitic gap, the pattern matcher will find it with probability 0.95. (This is the **recall**, which is one minus the **false negative rate**.)
- If the sentence doesn't have a parasitic gap, the pattern matcher will wrongly say it does with probability 0.005. (This is the **false positive rate**, which is one minus the **precision**.)

Suppose that Lana's pattern matcher says that a sentence contains a parasitic gap. What is the probability that this is true?

#### <span id="page-496-1"></span>A.3. INDEPENDENCE

Let *G* be the event of a sentence having a parasitic gap, and *T* be the event of the test being positive. We are interested in the probability of a sentence having a parasitic gap given that the test is positive. This is the conditional probability Pr(G | T), and it can be computed by Bayes' rule:

$$\Pr(G \mid T) = \frac{\Pr(T \mid G) \times \Pr(G)}{\Pr(T)}.$$
[A.14]

We already know both terms in the numerator:  $Pr(T \mid G)$  is the recall, which is 0.95; Pr(G) is the prior, which is  $10^{-5}$ .

We are not given the denominator, but it can be computed using tools developed earlier in this section. First apply the law of total probability, using the partition  $\{G, \neg G\}$ :

$$\Pr(T) = \Pr(T \cap G) + \Pr(T \cap \neg G).$$
[A.15]

This says that the probability of the test being positive is the sum of the probability of a **true positive**  $(T \cap G)$  and the probability of a **false positive**  $(T \cap \neg G)$ . The probability of each of these events can be computed using the chain rule:

$$\Pr(T \cap G) = \Pr(T \mid G) \times \Pr(G) = 0.95 \times 10^{-5}$$
[A.16]

$$\Pr(T \cap \neg G) = \Pr(T \mid \neg G) \times \Pr(\neg G) = 0.005 \times (1 - 10^{-5}) \approx 0.005$$
 [A.17]

$$\Pr(T) = \Pr(T \cap G) + \Pr(T \cap \neg G)$$
[A.18]

$$=0.95 \times 10^{-5} + 0.005.$$
 [A.19]

Plugging these terms into Bayes' rule gives the desired posterior probability,

$$\Pr(G \mid T) = \frac{\Pr(T \mid G) \Pr(G)}{\Pr(T)}$$
[A.20]

$$=\frac{0.95 \times 10^{-5}}{0.95 \times 10^{-5} + 0.005 \times (1 - 10^{-5})}$$
 [A.21]

Lana's pattern matcher seems accurate, with false positive and false negative rates below 5%. Yet the extreme rarity of the phenomenon means that a positive result from the detector is most likely to be wrong.

# <span id="page-496-0"></span>A.3 Independence

Two events are independent if the probability of their intersection is equal to the product of their probabilities:  $Pr(A \cap B) = Pr(A) \times Pr(B)$ . For example, for two flips of a fair

<span id="page-497-2"></span>coin, the probability of getting heads on the first flip is independent of the probability of getting heads on the second flip:

$$\Pr(\{HT, HH\}) = \Pr(HT) + \Pr(HH) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}$$
 [A.23]

$$\Pr(\{HH, TH\}) = \Pr(HH) + \Pr(TH) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}$$
 [A.24]

$$\Pr(\{HT, HH\}) \times \Pr(\{HH, TH\}) = \frac{1}{2} \times \frac{1}{2} = \frac{1}{4}$$
[A.25]

$$\Pr(\{HT, HH\} \cap \{HH, TH\}) = \Pr(HH) = \frac{1}{4}$$
 [A.26]

$$= \Pr(\{HT, HH\}) \times \Pr(\{HH, TH\}).$$
 [A.27]

If  $Pr(A \cap B \mid C) = Pr(A \mid C) \times Pr(B \mid C)$ , then the events *A* and *B* are **conditionally independent**, written  $A \perp B \mid C$ . Conditional independence plays a important role in probabilistic models such as Naïve Bayes chapter 2.

## <span id="page-497-0"></span>A.4 Random variables

**Random variables** are functions from events to  $\mathbb{R}^n$ , where  $\mathbb{R}$  is the set of real numbers. This subsumes several useful special cases:

- An indicator random variable is a function from events to the set  $\{0, 1\}$ . In the coin flip example, we can define *Y* as an indicator random variable, taking the value 1 when the coin has come up heads on at least one flip. This would include the outcomes  $\{HH, HT, TH\}$ . The probability Pr(Y = 1) is the sum of the probabilities of these outcomes,  $Pr(Y = 1) = \frac{1}{4} + \frac{1}{4} + \frac{1}{4} = \frac{3}{4}$ .
- A discrete random variable is a function from events to a discrete subset of  $\mathbb{R}$ . Consider the coin flip example: the number of heads on two flips, X, can be viewed as a discrete random variable,  $X \in 0, 1, 2$ . The event probability Pr(X = 1) can again be computed as the sum of the probabilities of the events in which there is one head,  $\{HT, TH\}$ , giving  $Pr(X = 1) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}$ .

Each possible value of a random variable is associated with a subset of the sample space. In the coin flip example, X = 0 is associated with the event  $\{TT\}$ , X = 1 is associated with the event  $\{HT, TH\}$ , and X = 2 is associated with the event  $\{HH\}$ . Assuming a fair coin, the probabilities of these events are, respectively, 1/4, 1/2, and 1/4. This list of numbers represents the **probability distribution** over X, written  $p_X$ , which maps from the possible values of X to the non-negative reals. For a specific value x, we write  $p_X(x)$ , which is equal to the event probability Pr(X = x).<sup>1</sup> The function  $p_X$  is called

<span id="page-497-1"></span><sup>&</sup>lt;sup>1</sup>In general, capital letters (e.g., X) refer to random variables, and lower-case letters (e.g., x) refer to specific values. When the distribution is clear from context, I will simply write p(x).

<span id="page-498-1"></span>a probability **mass** function (pmf) if X is discrete; it is called a probability **density** function (pdf) if X is continuous. In either case, the function must sum to one, and all values must be non-negative:

$$\int_{x} \mathbf{p}_{X}(x) dx = 1$$
 [A.28]

$$\forall x, \mathbf{p}_X(x) \ge 0. \tag{A.29}$$

Probabilities over multiple random variables can written as **joint probabilities**, e.g., pA,B(a, b) = Pr(A = a ∩ B = b). Several properties of event probabilities carry over to probability distributions over random variables:

- The **marginal probability distribution** is p<sup>A</sup> (a) = P <sup>b</sup> pA,B(a, b).
- The **conditional probability distribution** is pA|<sup>B</sup> (<sup>a</sup> <sup>|</sup> <sup>b</sup>) = <sup>p</sup>A,B(a,b) pB(b) .
- Random variables A and B are independent iff pA,B(a, b) = p<sup>A</sup> (a) × p<sup>B</sup> (b).

# <span id="page-498-0"></span>**A.5 Expectations**

Sometimes we want the **expectation** of a function, such as E[g(x)] = P <sup>x</sup>∈X g(x)p(x). Expectations are easiest to think about in terms of probability distributions over discrete events:

- If it is sunny, Lucia will eat three ice creams.
- If it is rainy, she will eat only one ice cream.
- There's a 80% chance it will be sunny.
- The expected number of ice creams she will eat is 0.8 × 3 + 0.2 × 1 = 2.6.

If the random variable X is continuous, the expectation is an integral:

$$E[g(x)] = \int_{\mathcal{X}} g(x)\mathbf{p}(x)dx \qquad [A.30]$$

For example, a fast food restaurant in Quebec has a special offer for cold days: they give a 1% discount on poutine for every degree below zero. Assuming a thermometer with infinite precision, the expected price would be an integral over all possible temperatures,

$$E[\operatorname{price}(x)] = \int_{\mathcal{X}} \min(1, 1+x) \times \operatorname{original-price} \times p(x) dx.$$
 [A.31]

# <span id="page-499-0"></span>A.6 Modeling and estimation

**Probabilistic models** provide a principled way to reason about random events and random variables. Let's consider the coin toss example. Each toss can be modeled as a random event, with probability  $\theta$  of the event H, and probability  $1 - \theta$  of the complementary event T. If we write a random variable X as the total number of heads on three coin flips, then the distribution of X depends on  $\theta$ . In this case, X is distributed as a **binomial random variable**, meaning that it is drawn from a binomial distribution, with **parameters**  $(\theta, N = 3)$ . This is written,

$$X \sim \text{Binomial}(\theta, N = 3).$$
 [A.32]

The properties of the binomial distribution enable us to make statements about the X, such as its expected value and the likelihood that its value will fall within some interval.

Now suppose that  $\theta$  is unknown, but we have run an experiment, in which we executed *N* trials, and obtained *x* heads. We can **estimate**  $\theta$  by the principle of **maximum likelihood**:

$$\hat{\theta} = \underset{\theta}{\operatorname{argmax}} p_X(x; \theta, N).$$
 [A.33]

This says that the estimate  $\hat{\theta}$  should be the value that maximizes the likelihood of the data. The semicolon indicates that  $\theta$  and N are parameters of the probability function. The likelihood  $p_X(x; \theta, N)$  can be computed from the binomial distribution,

$$p_X(x;\theta,N) = \frac{N!}{x!(N-x)!} \theta^x (1-\theta)^{N-x}.$$
 [A.34]

This likelihood is proportional to the product of the probability of individual outcomes: for example, the sequence T, H, H, T, H would have probability  $\theta^3(1-\theta)^2$ . The term  $\frac{N!}{x!(N-x)!}$  arises from the many possible orderings by which we could obtain x heads on N trials. This term does not depend on  $\theta$ , so it can be ignored during estimation.

In practice, we maximize the log-likelihood, which is a monotonic function of the likelihood. Under the binomial distribution, the log-likelihood is a **convex** function of  $\theta$  (see

Jacob Eisenstein. Draft of November 13, 2018.

<span id="page-499-1"></span>482

<span id="page-500-0"></span>§ 2.4), so it can be maximized by taking the derivative and setting it equal to zero.

$$\ell(\theta) = x \log \theta + (N - x) \log(1 - \theta)$$
[A.35]

$$\frac{\partial \ell(\theta)}{\partial \theta} = \frac{x}{\theta} - \frac{N - x}{1 - \theta}$$
[A.36]

$$\frac{N-x}{1-\theta} = \frac{x}{\theta}$$
[A.37]

$$\frac{N-x}{x} = \frac{1-\theta}{\theta}$$
[A.38]

$$\frac{N}{x} - 1 = \frac{1}{\theta} - 1 \tag{A.39}$$

$$\hat{\theta} = \frac{x}{N}.$$
 [A.40]

In this case, the maximum likelihood estimate is equal to  $\frac{x}{N}$ , the fraction of trials that came up heads. This intuitive solution is also known as the **relative frequency estimate**, since it is equal to the relative frequency of the outcome.

Is maximum likelihood estimation always the right choice? Suppose you conduct one trial, and get heads. Would you conclude that  $\theta = 1$ , meaning that the coin is guaranteed to come up heads? If not, then you must have some **prior expectation** about  $\theta$ . To incorporate this prior information, we can treat  $\theta$  as a random variable, and use Bayes' rule:

$$\mathbf{p}(\theta \mid x; N) = \frac{\mathbf{p}(x \mid \theta) \times \mathbf{p}(\theta)}{\mathbf{p}(x)}$$
[A.41]

$$\propto \mathbf{p}(x \mid \theta) \times \mathbf{p}(\theta)$$
 [A.42]

$$\hat{\theta} = \underset{\theta}{\operatorname{argmax}} p(x \mid \theta) \times p(\theta).$$
[A.43]

This it the **maximum a posteriori** (MAP) estimate. Given a form for  $p(\theta)$ , you can derive the MAP estimate using the same approach that was used to derive the maximum likelihood estimate.

### Additional resources

A good introduction to probability theory is offered by Manning and Schütze (1999), which helped to motivate this section. For more detail, Sharon Goldwater provides another useful reference, http://homepages.inf.ed.ac.uk/sgwater/teaching/general/ probability.pdf. A historical and philosophical perspective on probability is offered by Diaconis and Skyrms (2017).

# <span id="page-502-2"></span><span id="page-502-0"></span>Appendix B Numerical optimization

Unconstrained numerical optimization involves solving problems of the form,

$$\min_{\boldsymbol{x} \in \mathbb{R}^D} f(\boldsymbol{x}), \qquad [B.1]$$

where  $x \in \mathbb{R}^D$  is a vector of D real numbers.

Differentiation is fundamental to numerical optimization. Suppose that at some  $x^*$ , every partial derivative of f is equal to 0: formally,  $\frac{\partial f}{\partial x_i}\Big|_{x^*} = 0$ . Then  $x^*$  is said to be a **critical point** of f. If f is a **convex** function (defined in § 2.4), then the value of  $f(x^*)$  is equal to the global minimum of f iff  $x^*$  is a critical point of f.

As an example, consider the convex function  $f(x) = (x-2)^2 + 3$ , shown in Figure B.1a. The derivative is  $\frac{\partial f}{\partial x} = 2x - 4$ . A unique minimum can be obtained by setting the derivative equal to zero and solving for x, obtaining  $x^* = 2$ . Now consider the multivariate convex function  $f(x) = \frac{1}{2} ||x - [2, 1]^\top ||^2$ , where  $||x||^2$  is the squared Euclidean norm. The partial

<span id="page-502-1"></span>![](_page_502_Figure_6.jpeg)

(a) The function  $f(x) = (x - 2)^2 + 3$ 

![](_page_502_Figure_8.jpeg)

Figure B.1: Two functions with unique global minima

derivatives are,

$$\frac{\partial d}{\partial x_1} = x_1 - 2 \tag{B.2}$$

$$\frac{\partial d}{\partial x_2} = x_2 - 1 \tag{B.3}$$

The unique minimum is  $\boldsymbol{x}^* = [2, 1]^\top$ .

For non-convex functions, critical points are not necessarily global minima. A **local minimum**  $x^*$  is a point at which the function takes a smaller value than at all nearby neighbors: formally,  $x^*$  is a local minimum if there is some positive  $\epsilon$  such that  $f(x^*) \leq f(x)$  for all x within distance  $\epsilon$  of  $x^*$ . Figure B.1b shows the function  $f(x) = |x| - 2\cos(x)$ , which has many local minima, as well as a unique global minimum at x = 0. A critical point may also be the local or global maximum of the function; it may be a **saddle point**, which is a minimum with respect to at least one coordinate, and a maximum with respect at least one other coordinate; it may be an **inflection point**, which is neither or a minimum nor maximum. When available, the second derivative of f can help to distinguish these cases.

### <span id="page-503-0"></span>**B.1** Gradient descent

For many convex functions, it is not possible to solve for  $x^*$  in closed form. In gradient descent, we compute a series of solutions,  $x^{(0)}, x^{(1)}, \ldots$  by taking steps along the local gradient  $\nabla_{x^{(t)}} f$ , which is the vector of partial derivatives of the function f, evaluated at the point  $x^{(t)}$ . Each solution  $x^{(t+1)}$  is computed,

$$\boldsymbol{x}^{(t+1)} \leftarrow \boldsymbol{x}^{(t)} - \eta^{(t)} \nabla_{\boldsymbol{x}^{(t)}} f.$$
[B.4]

where  $\eta^{(t)} > 0$  is a **step size**. If the step size is chosen appropriately, this procedure will find the global minimum of a differentiable convex function. For non-convex functions, gradient descent will find a local minimum. The extension to non-differentiable convex functions is discussed in § 2.4.

### <span id="page-503-1"></span>**B.2** Constrained optimization

Optimization must often be performed under constraints: for example, when optimizing the parameters of a probability distribution, the probabilities of all events must sum to one. Constrained optimization problems can be written,

$$\min f(\boldsymbol{x})$$
[B.5]

s.t. 
$$g_c(x) \le 0, \quad \forall c = 1, 2, ..., C$$
 [B.6]

Jacob Eisenstein. Draft of November 13, 2018.

<span id="page-503-2"></span>486

<span id="page-504-2"></span>where each  $g_c(x)$  is a scalar function of x. For example, suppose that x must be non-negative, and that its sum cannot exceed a budget b. Then there are D + 1 inequality constraints,

$$g_i(x) = -x_i, \quad \forall i = 1, 2, \dots, D$$
 [B.7]

$$g_{D+1}(\boldsymbol{x}) = -b + \sum_{i=1}^{D} x_i.$$
 [B.8]

Inequality constraints can be combined with the original objective function *f* by forming a **Lagrangian**,

$$L(\boldsymbol{x}, \boldsymbol{\lambda}) = f(\boldsymbol{x}) + \sum_{c=1}^{C} \lambda_c g_c(\boldsymbol{x}),$$
[B.9]

where  $\lambda_c$  is a **Lagrange multiplier**. For any Lagrangian, there is a corresponding dual form, which is a function of  $\lambda$ :

$$D(\boldsymbol{\lambda}) = \min_{\boldsymbol{x}} L(\boldsymbol{x}, \boldsymbol{\lambda}).$$
 [B.10]

The Lagrangian *L* can be referred to as the **primal form**.

# <span id="page-504-0"></span>**B.3** Example: Passive-aggressive online learning

Sometimes it is possible to solve a constrained optimization problem by manipulating the Lagrangian. One example is maximum-likelihood estimation of a Naïve Bayes probability model, as described in § 2.2.3. In that case, it is unnecessary to explicitly compute the Lagrange multiplier. Another example is illustrated by the **passive-aggressive** algorithm for online learning (Crammer et al., 2006). This algorithm is similar to the perceptron, but the goal at each step is to make the most conservative update that gives zero margin loss on the current example.<sup>1</sup> Each update can be formulated as a constrained optimization over the weights  $\theta$ :

$$\min_{\boldsymbol{\theta}} \frac{1}{2} ||\boldsymbol{\theta} - \boldsymbol{\theta}^{(i-1)}||^2$$
[B.11]

s.t. 
$$\ell^{(i)}(\boldsymbol{\theta}) = 0$$
 [B.12]

where  $\theta^{(i-1)}$  is the previous set of weights, and  $\ell^{(i)}(\theta)$  is the margin loss on instance *i*. As in § 2.4.1, this loss is defined as,

$$\ell^{(i)}(\boldsymbol{\theta}) = 1 - \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) + \max_{y \neq y^{(i)}} \boldsymbol{\theta} \cdot \boldsymbol{f}(\boldsymbol{x}^{(i)}, y).$$
[B.13]

<span id="page-504-1"></span><sup>&</sup>lt;sup>1</sup>This is the basis for the name of the algorithm: it is passive when the loss is zero, but it aggressively moves to make the loss zero when necessary.

When the margin loss is zero for θ (i−1), the optimal solution is θ <sup>∗</sup> = θ (i−1), so we will focus on the case where ` (i) (θ (i−1)) > 0. The Lagrangian for this problem is,

$$L(\boldsymbol{\theta}, \lambda) = \frac{1}{2} ||\boldsymbol{\theta} - \boldsymbol{\theta}^{(i-1)}||^2 + \lambda \ell^{(i)}(\boldsymbol{\theta}), \qquad [B.14]$$

Holding λ constant, we can solve for θ by differentiating,

$$\nabla_{\boldsymbol{\theta}} L = \boldsymbol{\theta} - \boldsymbol{\theta}^{(i-1)} + \lambda \frac{\partial}{\partial \boldsymbol{\theta}} \ell^{(i)}(\boldsymbol{\theta})$$
[B.15]

$$\boldsymbol{\theta}^* = \boldsymbol{\theta}^{(i-1)} + \lambda \boldsymbol{\delta}, \qquad [B.16]$$

where δ = f(x (i) , y(i) ) − f(x (i) , yˆ) and yˆ = argmaxy6=<sup>y</sup> (i) θ · f(x (i) , y).

The Lagrange multiplier λ acts as the learning rate in a perceptron-style update to θ. We can solve for λ by plugging θ <sup>∗</sup> back into the Lagrangian, obtaining the dual function,

$$D(\lambda) = \frac{1}{2} ||\boldsymbol{\theta}^{(i-1)} + \lambda \boldsymbol{\delta} - \boldsymbol{\theta}^{(i-1)}||^2 + \lambda (1 - (\boldsymbol{\theta}^{(i-1)} + \lambda \boldsymbol{\delta}) \cdot \boldsymbol{\delta})$$
[B.17]

$$=\frac{\lambda^2}{2}||\boldsymbol{\delta}||^2 - \lambda^2||\boldsymbol{\delta}||^2 + \lambda(1 - \boldsymbol{\theta}^{(i-1)} \cdot \boldsymbol{\delta})$$
[B.18]

$$= -\frac{\lambda^2}{2} ||\boldsymbol{\delta}||^2 + \lambda \ell^{(i)}(\boldsymbol{\theta}^{(i-1)}).$$
[B.19]

Differentiating and solving for λ,

$$\frac{\partial D}{\partial \lambda} = -\lambda ||\boldsymbol{\delta}||^2 + \ell^{(i)}(\boldsymbol{\theta}^{(i-1)})$$
[B.20]

$$\lambda^* = \frac{\ell^{(i)}(\boldsymbol{\theta}^{(i-1)})}{||\boldsymbol{\delta}||^2}.$$
[B.21]

The complete update equation is therefore:

$$\boldsymbol{\theta}^* = \boldsymbol{\theta}^{(i-1)} + \frac{\ell^{(i)}(\boldsymbol{\theta}^{(i-1)})}{||\boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) - \boldsymbol{f}(\boldsymbol{x}^{(i)}, \hat{y})||^2} (\boldsymbol{f}(\boldsymbol{x}^{(i)}, y^{(i)}) - \boldsymbol{f}(\boldsymbol{x}^{(i)}, \hat{y})).$$
[B.22]

This learning rate makes intuitive sense. The numerator grows with the loss; the denominator grows with the norm of the difference between the feature vectors associated with the correct and predicted label. If this norm is large, then the step with respect to each feature should be small, and vice versa.

# **Bibliography**

<span id="page-506-16"></span><span id="page-506-0"></span>(1992).

<span id="page-506-13"></span>(1996).

<span id="page-506-9"></span>(1997).

<span id="page-506-4"></span>(1998).

<span id="page-506-1"></span>(1999).

<span id="page-506-8"></span>(2000).

- <span id="page-506-19"></span>(2001).
- <span id="page-506-10"></span>(2002).

<span id="page-506-12"></span>(2002).

<span id="page-506-15"></span>(2003).

- <span id="page-506-17"></span>(2003).
- <span id="page-506-11"></span>(2004).

<span id="page-506-14"></span>(2004).

- <span id="page-506-2"></span>(2005).
- <span id="page-506-7"></span>(2005).
- <span id="page-506-3"></span>(2006).
- <span id="page-506-6"></span>(2006).
- <span id="page-506-18"></span>(2006).

<span id="page-506-5"></span>(2007).

#### 490 BIBLIOGRAPHY

<span id="page-507-6"></span>(2007).

<span id="page-507-12"></span>(2007).

<span id="page-507-13"></span>(2007).

<span id="page-507-1"></span>(2008).

<span id="page-507-4"></span>(2008).

<span id="page-507-9"></span>(2008).

<span id="page-507-8"></span>(2009).

<span id="page-507-14"></span>(2009).

<span id="page-507-19"></span>(2009).

<span id="page-507-24"></span>(2009).

<span id="page-507-0"></span>(2010).

<span id="page-507-2"></span>(2010).

<span id="page-507-7"></span>(2010).

<span id="page-507-11"></span>(2010).

<span id="page-507-22"></span>(2010).

<span id="page-507-5"></span>(2011).

<span id="page-507-15"></span>(2011).

<span id="page-507-16"></span>(2011).

<span id="page-507-17"></span>(2011).

<span id="page-507-20"></span>(2011).

<span id="page-507-3"></span>(2012).

<span id="page-507-10"></span>(2012).

<span id="page-507-18"></span>(2012).

<span id="page-507-21"></span>(2012).

<span id="page-507-23"></span>(2012).

<span id="page-508-24"></span>(2012).

<span id="page-508-3"></span>(2013).

<span id="page-508-6"></span>(2013).

<span id="page-508-10"></span>(2013).

- <span id="page-508-12"></span>(2013).
- <span id="page-508-15"></span>(2013).

<span id="page-508-4"></span>(2014).

<span id="page-508-7"></span>(2014).

<span id="page-508-11"></span>(2014).

<span id="page-508-18"></span>(2014).

<span id="page-508-19"></span>(2014).

<span id="page-508-20"></span>(2014).

- <span id="page-508-2"></span>(2015).
- <span id="page-508-5"></span>(2015).
- <span id="page-508-9"></span>(2015).

<span id="page-508-13"></span>(2015).

- <span id="page-508-14"></span>(2015).
- <span id="page-508-17"></span>(2015).
- <span id="page-508-1"></span>(2016).
- <span id="page-508-8"></span>(2016).
- <span id="page-508-16"></span>(2016).
- <span id="page-508-21"></span>(2016).
- <span id="page-508-22"></span>(2016).

<span id="page-508-23"></span>(2016).

<span id="page-508-0"></span>(2017).

- <span id="page-509-9"></span>(2017).
- <span id="page-509-13"></span>(2017).
- <span id="page-509-14"></span>(2017).
- <span id="page-509-15"></span>(2017).
- <span id="page-509-10"></span>(2018).
- <span id="page-509-11"></span>(2018).

<span id="page-509-12"></span>(2018).

- <span id="page-509-1"></span>Abadi, M., A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. J. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man ´ e, R. Monga, S. Moore, ´ D. G. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. A. Tucker, V. Vanhoucke, V. Vasudevan, F. B. Viegas, O. Vinyals, P. Warden, M. Watten- ´ berg, M. Wicke, Y. Yu, and X. Zheng (2016). Tensorflow: Large-scale machine learning on heterogeneous distributed systems. *CoRR abs/1603.04467*.
- <span id="page-509-5"></span>Abend, O. and A. Rappoport (2017). The state of the art in semantic representation. See [acl](#page-508-0) [\(2017\)](#page-508-0).
- <span id="page-509-0"></span>Abney, S., R. E. Schapire, and Y. Singer (1999). Boosting applied to tagging and PP attachment. See [emn](#page-506-1) [\(1999\)](#page-506-1), pp. 132–134.
- <span id="page-509-2"></span>Abney, S. P. (1987). *The English noun phrase in its sentential aspect*. Ph. D. thesis, Massachusetts Institute of Technology.
- <span id="page-509-3"></span>Abney, S. P. and M. Johnson (1991). Memory requirements and local ambiguities of parsing strategies. *Journal of Psycholinguistic Research 20*(3), 233–250.
- <span id="page-509-8"></span>Adafre, S. F. and M. De Rijke (2006). Finding similar sentences across multiple languages in wikipedia. In *Proceedings of the Workshop on NEW TEXT Wikis and blogs and other dynamic text sources*.
- <span id="page-509-6"></span>Ahn, D. (2006). The stages of event extraction. In *Proceedings of the Workshop on Annotating and Reasoning about Time and Events*, pp. 1–8. Association for Computational Linguistics.
- <span id="page-509-4"></span>Aho, A. V., M. S. Lam, R. Sethi, and J. D. Ullman (2006). *Compilers: Principles, Techniques, & Tools* (2nd ed.). Addison-Wesley Publishing Company.

<span id="page-509-7"></span>Aikhenvald, A. Y. (2004). *Evidentiality*. Oxford University Press.

- <span id="page-510-6"></span>Akaike, H. (1974). A new look at the statistical model identification. *IEEE Transactions on Automatic Control 19*(6), 716–723.
- <span id="page-510-0"></span>Akmajian, A., R. A. Demers, A. K. Farmer, and R. M. Harnish (2010). *Linguistics: An introduction to language and communication* (Sixth ed.). Cambridge, MA: MIT press.
- <span id="page-510-1"></span>Alfano, M., D. Hovy, M. Mitchell, and M. Strube (2018). Proceedings of the second acl workshop on ethics in natural language processing. In *Proceedings of the Second ACL Workshop on Ethics in Natural Language Processing*. Association for Computational Linguistics.
- <span id="page-510-8"></span>Alfau, F. (1999). *Chromos*. Dalkey Archive Press.
- <span id="page-510-7"></span>Allauzen, C., M. Riley, J. Schalkwyk, W. Skut, and M. Mohri (2007). OpenFst: A general and efficient weighted finite-state transducer library. In *International Conference on Implementation and Application of Automata*, pp. 11–23. Springer.
- <span id="page-510-9"></span>Allen, J. F. (1984). Towards a general theory of action and time. *Artificial intelligence 23*(2), 123–154.
- <span id="page-510-11"></span>Allen, J. F., B. W. Miller, E. K. Ringger, and T. Sikorski (1996). A robust system for natural spoken dialogue. In *Proceedings of the Association for Computational Linguistics (ACL)*, pp. 62–70.
- <span id="page-510-10"></span>Allen, J. F., L. K. Schubert, G. Ferguson, P. Heeman, C. H. Hwang, T. Kato, M. Light, N. Martin, B. Miller, M. Poesio, and D. Traum (1995). The TRAINS project: A case study in building a conversational planning agent. *Journal of Experimental & Theoretical Artificial Intelligence 7*(1), 7–48.
- <span id="page-510-4"></span>Alm, C. O., D. Roth, and R. Sproat (2005). Emotions from text: machine learning for text-based emotion prediction. See [emn](#page-506-2) [\(2005\)](#page-506-2), pp. 579–586.
- <span id="page-510-5"></span>Alu´ısio, S., J. Pelizzoni, A. Marchi, L. de Oliveira, R. Manenti, and V. Marquiafavel (2003). ´ An account of the challenge of tagging a reference corpus for Brazilian Portuguese. *Computational Processing of the Portuguese Language*, 194–194.
- <span id="page-510-3"></span>Anand, P., M. Walker, R. Abbott, J. E. Fox Tree, R. Bowmani, and M. Minor (2011). Cats rule and dogs drool!: Classifying stance in online debate. In *Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis*, pp. 1–9. Association for Computational Linguistics.
- <span id="page-510-2"></span>Anandkumar, A. and R. Ge (2016). Efficient approaches for escaping higher order saddle points in non-convex optimization. In *Proceedings of the Conference On Learning Theory (COLT)*, pp. 81–102.

- <span id="page-511-5"></span>Anandkumar, A., R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky (2014). Tensor decompositions for learning latent variable models. *The Journal of Machine Learning Research 15*(1), 2773–2832.
- <span id="page-511-3"></span>Ando, R. K. and T. Zhang (2005). A framework for learning predictive structures from multiple tasks and unlabeled data. *The Journal of Machine Learning Research 6*, 1817– 1853.
- <span id="page-511-9"></span>Andor, D., C. Alberti, D. Weiss, A. Severyn, A. Presta, K. Ganchev, S. Petrov, and M. Collins (2016). Globally normalized transition-based neural networks. See [acl](#page-508-1) [\(2016\)](#page-508-1), pp. 2442–2452.
- <span id="page-511-13"></span>Angeli, G., P. Liang, and D. Klein (2010). A simple domain-independent probabilistic approach to generation. See [emn](#page-507-0) [\(2010\)](#page-507-0), pp. 502–512.
- <span id="page-511-1"></span>Antol, S., A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick, and D. Parikh (2015). Vqa: Visual question answering. See [icc](#page-508-2) [\(2015\)](#page-508-2), pp. 2425–2433.
- <span id="page-511-7"></span>Aronoff, M. (1976). *Word formation in generative grammar*. MIT Press.
- <span id="page-511-0"></span>Arora, S. and B. Barak (2009). *Computational complexity: a modern approach*. Cambridge University Press.
- <span id="page-511-4"></span>Arora, S., R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu, and M. Zhu (2013). A practical algorithm for topic modeling with provable guarantees. See [icm](#page-508-3) [\(2013\)](#page-508-3), pp. 280–288.
- <span id="page-511-11"></span>Arora, S., Y. Li, Y. Liang, T. Ma, and A. Risteski (2018). Linear algebraic structure of word senses, with applications to polysemy. *Transactions of the Association of Computational Linguistics 6*, 483–495.
- <span id="page-511-2"></span>Artstein, R. and M. Poesio (2008). Inter-coder agreement for computational linguistics. *Computational Linguistics 34*(4), 555–596.
- <span id="page-511-10"></span>Artzi, Y. and L. Zettlemoyer (2013). Weakly supervised learning of semantic parsers for mapping instructions to actions. *Transactions of the Association for Computational Linguistics 1*, 49–62.
- <span id="page-511-8"></span>Attardi, G. (2006). Experiments with a multilanguage non-projective dependency parser. In *Proceedings of the Conference on Natural Language Learning (CoNLL)*, pp. 166–170.
- <span id="page-511-6"></span>Auer, P. (2013). *Code-switching in conversation: Language, interaction and identity*. Routledge.
- <span id="page-511-12"></span>Auer, S., C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives (2007). Dbpedia: A nucleus for a web of open data. *The semantic web*, 722–735.

- <span id="page-512-5"></span>Austin, J. L. (1962). *How to do things with words*. Oxford University Press.
- <span id="page-512-1"></span>Aw, A., M. Zhang, J. Xiao, and J. Su (2006). A phrase-based statistical model for SMS text normalization. See [acl](#page-506-3) [\(2006\)](#page-506-3), pp. 33–40.
- <span id="page-512-0"></span>Ba, J. L., J. R. Kiros, and G. E. Hinton (2016). Layer normalization. *arXiv preprint arXiv:1607.06450*.
- <span id="page-512-11"></span>Bagga, A. and B. Baldwin (1998a). Algorithms for scoring coreference chains. In *Proceedings of the Language Resources and Evaluation Conference*, pp. 563–566.
- <span id="page-512-12"></span>Bagga, A. and B. Baldwin (1998b). Entity-based cross-document coreferencing using the vector space model. See [col](#page-506-4) [\(1998\)](#page-506-4), pp. 79–85.
- <span id="page-512-10"></span>Bahdanau, D., K. Cho, and Y. Bengio (2014). Neural machine translation by jointly learning to align and translate. See [nip](#page-508-4) [\(2014\)](#page-508-4).
- <span id="page-512-8"></span>Baldwin, T. and S. N. Kim (2010). Multiword expressions. In *Handbook of natural language processing*, Volume 2, pp. 267–292. Boca Raton, USA: CRC Press.
- <span id="page-512-6"></span>Balle, B., A. Quattoni, and X. Carreras (2011). A spectral learning algorithm for finite state transducers. In *Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML)*, pp. 156–171.
- <span id="page-512-7"></span>Banarescu, L., C. Bonial, S. Cai, M. Georgescu, K. Griffitt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer, and N. Schneider (2013, August). Abstract meaning representation for sembanking. In *Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse*, Sofia, Bulgaria, pp. 178–186. Association for Computational Linguistics.
- <span id="page-512-13"></span>Banko, M., M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni (2007). Open information extraction from the web. See [ijc](#page-506-5) [\(2007\)](#page-506-5), pp. 2670–2676.
- <span id="page-512-9"></span>Bansal, N., A. Blum, and S. Chawla (2004). Correlation clustering. *Machine Learning 56*(1- 3), 89–113.
- <span id="page-512-3"></span>Barber, D. (2012). *Bayesian reasoning and machine learning*. Cambridge University Press.
- <span id="page-512-4"></span>Barman, U., A. Das, J. Wagner, and J. Foster (2014). Code mixing: A challenge for language identification in the language of social media. In *Proceedings of the First Workshop on Computational Approaches to Code Switching*, pp. 13–23. Association for Computational Linguistics.
- <span id="page-512-2"></span>Baron, A. and P. Rayson (2008). Vard2: A tool for dealing with spelling variation in historical corpora. In *Postgraduate conference in corpus linguistics*.

- <span id="page-513-6"></span>Baroni, M., R. Bernardi, and R. Zamparelli (2014). Frege in space: A program for compositional distributional semantics. *Linguistic Issues in Language Technologies*.
- <span id="page-513-9"></span>Barzilay, R. and M. Lapata (2008). Modeling local coherence: An Entity-Based approach. *Computational Linguistics 34*(1), 1–34.
- <span id="page-513-14"></span>Barzilay, R. and K. R. McKeown (2005). Sentence fusion for multidocument news summarization. *Computational Linguistics 31*(3), 297–328.
- <span id="page-513-5"></span>Beesley, K. R. and L. Karttunen (2003). *Finite-state morphology*. Stanford, CA: Center for the Study of Language and Information.
- <span id="page-513-10"></span>Bejan, C. A. and S. Harabagiu (2014). Unsupervised event coreference resolution. *Computational Linguistics 40*(2), 311–347.
- <span id="page-513-8"></span>Bell, E. T. (1934). Exponential numbers. *The American Mathematical Monthly 41*(7), 411–419.
- <span id="page-513-0"></span>Bender, E. M. (2013). *Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax*, Volume 6 of *Synthesis Lectures on Human Language Technologies*. Morgan & Claypool Publishers.
- <span id="page-513-13"></span>Bengio, S., O. Vinyals, N. Jaitly, and N. Shazeer (2015). Scheduled sampling for sequence prediction with recurrent neural networks. See [nip](#page-508-5) [\(2015\)](#page-508-5), pp. 1171–1179.
- <span id="page-513-4"></span>Bengio, Y., R. Ducharme, P. Vincent, and C. Janvin (2003). A neural probabilistic language model. *The Journal of Machine Learning Research 3*, 1137–1155.
- <span id="page-513-3"></span>Bengio, Y., P. Simard, and P. Frasconi (1994). Learning long-term dependencies with gradient descent is difficult. *IEEE Transactions on Neural Networks 5*(2), 157–166.
- <span id="page-513-7"></span>Bengtson, E. and D. Roth (2008). Understanding the value of features for coreference resolution. See [emn](#page-507-1) [\(2008\)](#page-507-1), pp. 294–303.
- <span id="page-513-1"></span>Benjamini, Y. and Y. Hochberg (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. *Journal of the Royal Statistical Society. Series B (Methodological)*, 289–300.
- <span id="page-513-12"></span>Berant, J., A. Chou, R. Frostig, and P. Liang (2013). Semantic parsing on freebase from question-answer pairs. See [emn](#page-508-6) [\(2013\)](#page-508-6), pp. 1533–1544.
- <span id="page-513-11"></span>Berant, J., V. Srikumar, P.-C. Chen, A. Vander Linden, B. Harding, B. Huang, P. Clark, and C. D. Manning (2014). Modeling biological processes for reading comprehension. See [emn](#page-508-7) [\(2014\)](#page-508-7).
- <span id="page-513-2"></span>Berg-Kirkpatrick, T., A. Bouchard-Cotˆ e, J. DeNero, and D. Klein (2010). Painless unsuper- ´ vised learning with features. See [naa](#page-507-2) [\(2010\)](#page-507-2), pp. 582–590.

- <span id="page-514-6"></span>Berg-Kirkpatrick, T., D. Burkett, and D. Klein (2012). An empirical investigation of statistical significance in NLP. See [emn](#page-507-3) [\(2012\)](#page-507-3), pp. 995–1005.
- <span id="page-514-3"></span>Berger, A. L., V. J. D. Pietra, and S. A. D. Pietra (1996). A maximum entropy approach to natural language processing. *Computational linguistics 22*(1), 39–71.
- <span id="page-514-11"></span>Bergsma, S., D. Lin, and R. Goebel (2008). Distributional identification of non-referential pronouns. See [acl](#page-507-4) [\(2008\)](#page-507-4), pp. 10–18.
- <span id="page-514-15"></span>Bernardi, R., R. Cakici, D. Elliott, A. Erdem, E. Erdem, N. Ikizler-Cinbis, F. Keller, A. Muscat, and B. Plank (2016). Automatic description generation from images: A survey of models, datasets, and evaluation measures. *Journal of Artificial Intelligence Research 55*, 409–442.
- <span id="page-514-2"></span>Bertsekas, D. P. (2012). Incremental gradient, subgradient, and proximal methods for convex optimization: A survey. In S. Sra, S. Nowozin, and S. J. Wright (Eds.), *Optimization for machine learning*. MIT Press.
- <span id="page-514-8"></span>Bhatia, P., R. Guthrie, and J. Eisenstein (2016). Morphological priors for probabilistic neural word embeddings. See [emn](#page-508-8) [\(2016\)](#page-508-8).
- <span id="page-514-14"></span>Bhatia, P., Y. Ji, and J. Eisenstein (2015). Better document-level sentiment analysis from rst discourse parsing. See [emn](#page-508-9) [\(2015\)](#page-508-9).
- <span id="page-514-5"></span>Biber, D. (1991). *Variation across speech and writing*. Cambridge University Press.
- <span id="page-514-13"></span>Bird, S., E. Klein, and E. Loper (2009). *Natural language processing with Python*. California: O'Reilly Media.
- <span id="page-514-9"></span>Bishop, C. M. (2006). *Pattern recognition and machine learning*. springer.
- <span id="page-514-12"></span>Bjorkelund, A. and P. Nugues (2011). Exploring lexicalized features for coreference reso- ¨ lution. See [con](#page-507-5) [\(2011\)](#page-507-5), pp. 45–50.
- <span id="page-514-10"></span>Blackburn, P. and J. Bos (2005). *Representation and inference for natural language: A first course in computational semantics*. CSLI.
- <span id="page-514-0"></span>Blei, D. M. (2012). Probabilistic topic models. *Communications of the ACM 55*(4), 77–84.
- <span id="page-514-1"></span>Blei, D. M. (2014). Build, compute, critique, repeat: Data analysis with latent variable models. *Annual Review of Statistics and Its Application 1*, 203–232.
- <span id="page-514-4"></span>Blei, D. M., A. Y. Ng, and M. I. Jordan (2003). Latent dirichlet allocation. *The Journal of Machine Learning Research 3*, 993–1022.
- <span id="page-514-7"></span>Blitzer, J., M. Dredze, and F. Pereira (2007). Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. See [acl](#page-507-6) [\(2007\)](#page-507-6), pp. 440–447.

- <span id="page-515-3"></span>Blum, A. and T. Mitchell (1998). Combining labeled and unlabeled data with co-training. In *Proceedings of the Conference On Learning Theory (COLT)*, pp. 92–100.
- <span id="page-515-13"></span>Bobrow, D. G., R. M. Kaplan, M. Kay, D. A. Norman, H. Thompson, and T. Winograd (1977). Gus, a frame-driven dialog system. *Artificial intelligence 8*(2), 155–173.
- <span id="page-515-5"></span>Bohnet, B. (2010). Very high accuracy and fast dependency parsing is not a contradiction. See [col](#page-507-7) [\(2010\)](#page-507-7), pp. 89–97.
- <span id="page-515-12"></span>Boitet, C. (1988). Pros and cons of the pivot and transfer approaches in multilingual machine translation. *Readings in machine translation*, 273–279.
- <span id="page-515-8"></span>Bojanowski, P., E. Grave, A. Joulin, and T. Mikolov (2017). Enriching word vectors with subword information. *Transactions of the Association for Computational Linguistics 5*, 135– 146.
- <span id="page-515-6"></span>Bollacker, K., C. Evans, P. Paritosh, T. Sturge, and J. Taylor (2008). Freebase: a collaboratively created graph database for structuring human knowledge. In *Proceedings of the ACM International Conference on Management of Data (SIGMOD)*, pp. 1247–1250. AcM.
- <span id="page-515-7"></span>Bolukbasi, T., K.-W. Chang, J. Y. Zou, V. Saligrama, and A. T. Kalai (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In *Neural Information Processing Systems (NIPS)*, pp. 4349–4357.
- <span id="page-515-11"></span>Bordes, A., N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko (2013). Translating embeddings for modeling multi-relational data. See [nip](#page-508-10) [\(2013\)](#page-508-10), pp. 2787–2795.
- <span id="page-515-10"></span>Bordes, A., J. Weston, R. Collobert, Y. Bengio, et al. (2011). Learning structured embeddings of knowledge bases. In *Proceedings of the National Conference on Artificial Intelligence (AAAI)*, pp. 301–306.
- <span id="page-515-0"></span>Borges, J. L. (1993). *Other Inquisitions 1937–1952*. University of Texas Press. Translated by Ruth L. C. Simms.
- <span id="page-515-4"></span>Botha, J. A. and P. Blunsom (2014). Compositional morphology for word representations and language modelling. See [icm](#page-508-11) [\(2014\)](#page-508-11).
- <span id="page-515-2"></span>Bottou, L. (2012). Stochastic gradient descent tricks. In *Neural networks: Tricks of the trade*, pp. 421–436. Springer.
- <span id="page-515-1"></span>Bottou, L., F. E. Curtis, and J. Nocedal (2016). Optimization methods for large-scale machine learning. *arXiv preprint arXiv:1606.04838*.
- <span id="page-515-9"></span>Bowman, S. R., L. Vilnis, O. Vinyals, A. Dai, R. Jozefowicz, and S. Bengio (2016). Generating sentences from a continuous space. In *Proceedings of the Conference on Natural Language Learning (CoNLL)*, pp. 10–21.

- <span id="page-516-1"></span>boyd, d. and K. Crawford (2012). Critical questions for big data. *Information, Communication & Society 15*(5), 662–679.
- <span id="page-516-2"></span>Boyd, S. and L. Vandenberghe (2004). *Convex Optimization*. New York: Cambridge University Press.
- <span id="page-516-5"></span>Boydstun, A. E. (2013). *Making the news: Politics, the media, and agenda setting*. University of Chicago Press.
- <span id="page-516-3"></span>Branavan, S., H. Chen, J. Eisenstein, and R. Barzilay (2009). Learning document-level semantic properties from free-text annotations. *Journal of Artificial Intelligence Research 34*(2), 569–603.
- <span id="page-516-0"></span>Branavan, S. R., H. Chen, L. S. Zettlemoyer, and R. Barzilay (2009). Reinforcement learning for mapping instructions to actions. See [acl](#page-507-8) [\(2009\)](#page-507-8), pp. 82–90.
- <span id="page-516-9"></span>Brants, T. and A. Franz (2006). The Google 1T 5-gram corpus. LDC2006T13.
- <span id="page-516-11"></span>Braud, C., O. Lacroix, and A. Søgaard (2017). Does syntax help discourse segmentation? not so much. See [emn](#page-509-9) [\(2017\)](#page-509-9), pp. 2432–2442.
- <span id="page-516-10"></span>Brennan, S. E., M. W. Friedman, and C. J. Pollard (1987). A centering approach to pronouns. In *Proceedings of the Association for Computational Linguistics (ACL)*, pp. 155–162.
- <span id="page-516-6"></span>Briscoe, T. (2011). Introduction to formal semantics for natural language. https://www.cl.cam.ac.uk/teaching/1011/L107/semantics.pdf.
- <span id="page-516-12"></span>Brown, P. F., J. Cocke, S. A. D. Pietra, V. J. D. Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin (1990). A statistical approach to machine translation. *Computational linguistics 16*(2), 79–85.
- <span id="page-516-7"></span>Brown, P. F., P. V. Desouza, R. L. Mercer, V. J. D. Pietra, and J. C. Lai (1992). Class-based n-gram models of natural language. *Computational linguistics 18*(4), 467–479.
- <span id="page-516-13"></span>Brown, P. F., V. J. D. Pietra, S. A. D. Pietra, and R. L. Mercer (1993). The mathematics of statistical machine translation: Parameter estimation. *Computational linguistics 19*(2), 263–311.
- <span id="page-516-4"></span>Brun, C. and C. Roux (2014). Decomposition des "hash tags" pour l'am ´ elioration de la ´ classification en polarite des "tweets". ´ *Proceedings of Traitement Automatique des Langues Naturelles*, 473–478.
- <span id="page-516-8"></span>Bruni, E., N.-K. Tran, and M. Baroni (2014). Multimodal distributional semantics. *Journal of Artificial Intelligence Research 49*(2014), 1–47.

- <span id="page-517-0"></span>Brutzkus, A., A. Globerson, E. Malach, and S. Shalev-Shwartz (2018). Sgd learns overparameterized networks that provably generalize on linearly separable data. See [icl](#page-509-10) [\(2018\)](#page-509-10).
- <span id="page-517-4"></span>Bullinaria, J. A. and J. P. Levy (2007). Extracting semantic representations from word cooccurrence statistics: A computational study. *Behavior research methods 39*(3), 510–526.
- <span id="page-517-11"></span>Bunescu, R. C. and R. J. Mooney (2005). A shortest path dependency kernel for relation extraction. See [emn](#page-506-2) [\(2005\)](#page-506-2), pp. 724–731.
- <span id="page-517-10"></span>Bunescu, R. C. and M. Pasca (2006). Using encyclopedic knowledge for named entity disambiguation. See [eac](#page-506-6) [\(2006\)](#page-506-6), pp. 9–16.
- <span id="page-517-8"></span>Burstein, J., D. Marcu, and K. Knight (2003). Finding the WRITE stuff: Automatic identification of discourse structure in student essays. *IEEE Intelligent Systems 18*(1), 32–39.
- <span id="page-517-9"></span>Burstein, J., J. Tetreault, and S. Andreyev (2010). Using entity-based features to model coherence in student essays. In *Human language technologies: The 2010 annual conference of the North American chapter of the Association for Computational Linguistics*, pp. 681–684. Association for Computational Linguistics.
- <span id="page-517-7"></span>Burstein, J., J. Tetreault, and M. Chodorow (2013). Holistic discourse coherence annotation for noisy essay writing. *Dialogue & Discourse 4*(2), 34–52.
- <span id="page-517-3"></span>Cai, Q. and A. Yates (2013). Large-scale semantic parsing via schema matching and lexicon extension. See [acl](#page-508-12) [\(2013\)](#page-508-12), pp. 423–433.
- <span id="page-517-5"></span>Caliskan, A., J. J. Bryson, and A. Narayanan (2017). Semantics derived automatically from language corpora contain human-like biases. *Science 356*(6334), 183–186.
- <span id="page-517-1"></span>Canny, J. (1987). A computational approach to edge detection. In *Readings in Computer Vision*, pp. 184–203. Elsevier.
- <span id="page-517-2"></span>Cappe, O. and E. Moulines (2009). On-line expectation–maximization algorithm for latent ´ data models. *Journal of the Royal Statistical Society: Series B (Statistical Methodology) 71*(3), 593–613.
- <span id="page-517-12"></span>Carbonell, J. and J. Goldstein (1998). The use of MMR, diversity-based reranking for reordering documents and producing summaries. In *Proceedings of ACM SIGIR conference on Research and development in information retrieval*.
- <span id="page-517-13"></span>Carbonell, J. R. (1970). Mixed-initiative man-computer instructional dialogues. Technical report, Bolt Beranek and Newman.
- <span id="page-517-6"></span>Cardie, C. and K. Wagstaff (1999). Noun phrase coreference as clustering. See [emn](#page-506-1) [\(1999\)](#page-506-1), pp. 82–89.

- <span id="page-518-0"></span>Carletta, J. (1996). Assessing agreement on classification tasks: the kappa statistic. *Computational linguistics 22*(2), 249–254.
- <span id="page-518-10"></span>Carletta, J. (2007). Unleashing the killer corpus: experiences in creating the multieverything ami meeting corpus. *Language Resources and Evaluation 41*(2), 181–190.
- <span id="page-518-11"></span>Carlson, L. and D. Marcu (2001). Discourse tagging reference manual. Technical Report ISI-TR-545, Information Sciences Institute.
- <span id="page-518-12"></span>Carlson, L., M. E. Okurowski, and D. Marcu (2002). RST discourse treebank. Linguistic Data Consortium, University of Pennsylvania.
- <span id="page-518-6"></span>Carpenter, B. (1997). *Type-logical semantics*. Cambridge, MA: MIT Press.
- <span id="page-518-4"></span>Carreras, X., M. Collins, and T. Koo (2008). Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing. In *Proceedings of the Conference on Natural Language Learning (CoNLL)*, pp. 9–16.
- <span id="page-518-7"></span>Carreras, X. and L. Marquez (2005). Introduction to the conll-2005 shared task: Semantic ` role labeling. In *Proceedings of the Ninth Conference on Computational Natural Language Learning*, pp. 152–164. Association for Computational Linguistics.
- <span id="page-518-3"></span>Carroll, L. (1865). *Alice's Adventures in Wonderland*. London: Macmillan.
- <span id="page-518-2"></span>Carroll, L. (1917). *Through the looking glass: And what Alice found there*. Chicago: Rand, McNally.
- <span id="page-518-13"></span>Chambers, N. and D. Jurafsky (2008). Jointly combining implicit constraints improves temporal ordering. See [emn](#page-507-1) [\(2008\)](#page-507-1), pp. 698–706.
- <span id="page-518-9"></span>Chang, K.-W., A. Krishnamurthy, A. Agarwal, H. Daume III, and J. Langford (2015). Learning to search better than your teacher. See [icm](#page-508-13) [\(2015\)](#page-508-13).
- <span id="page-518-1"></span>Chang, M.-W., L. Ratinov, and D. Roth (2007). Guiding semi-supervision with constraintdriven learning. See [acl](#page-507-6) [\(2007\)](#page-507-6), pp. 280–287.
- <span id="page-518-8"></span>Chang, M.-W., L.-A. Ratinov, N. Rizzolo, and D. Roth (2008). Learning and inference with constraints. In *Proceedings of the National Conference on Artificial Intelligence (AAAI)*, pp. 1513–1518.
- <span id="page-518-14"></span>Chapman, W. W., W. Bridewell, P. Hanbury, G. F. Cooper, and B. G. Buchanan (2001). A simple algorithm for identifying negated findings and diseases in discharge summaries. *Journal of biomedical informatics 34*(5), 301–310.
- <span id="page-518-5"></span>Charniak, E. (1997). Statistical techniques for natural language parsing. *AI magazine 18*(4), 33–43.

- <span id="page-519-6"></span>Charniak, E. and M. Johnson (2005). Coarse-to-fine n-best parsing and maxent discriminative reranking. See [acl](#page-506-7) [\(2005\)](#page-506-7), pp. 173–180.
- <span id="page-519-1"></span>Chelba, C. and A. Acero (2006). Adaptation of maximum entropy capitalizer: Little data can help a lot. *Computer Speech & Language 20*(4), 382–399.
- <span id="page-519-4"></span>Chelba, C., T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and T. Robinson (2013). One billion word benchmark for measuring progress in statistical language modeling. *arXiv preprint arXiv:1312.3005*.
- <span id="page-519-10"></span>Chen, D., J. Bolton, and C. D. Manning (2016). A thorough examination of the CNN/Daily Mail reading comprehension task. See [acl](#page-508-1) [\(2016\)](#page-508-1).
- <span id="page-519-7"></span>Chen, D. and C. D. Manning (2014). A fast and accurate dependency parser using neural networks. See [emn](#page-508-7) [\(2014\)](#page-508-7), pp. 740–750.
- <span id="page-519-13"></span>Chen, D. L. and R. J. Mooney (2008). Learning to sportscast: a test of grounded language acquisition. See [icm](#page-507-9) [\(2008\)](#page-507-9), pp. 128–135.
- <span id="page-519-8"></span>Chen, H., S. Branavan, R. Barzilay, and D. R. Karger (2009). Content modeling using latent permutations. *Journal of Artificial Intelligence Research 36*(1), 129–163.
- <span id="page-519-2"></span>Chen, M., Z. Xu, K. Weinberger, and F. Sha (2012). Marginalized denoising autoencoders for domain adaptation. See [icm](#page-507-10) [\(2012\)](#page-507-10).
- <span id="page-519-11"></span>Chen, M. X., O. Firat, A. Bapna, M. Johnson, W. Macherey, G. Foster, L. Jones, N. Parmar, M. Schuster, Z. Chen, Y. Wu, and M. Hughes (2018). The best of both worlds: Combining recent advances in neural machine translation. See [acl](#page-509-11) [\(2018\)](#page-509-11).
- <span id="page-519-3"></span>Chen, S. F. and J. Goodman (1999). An empirical study of smoothing techniques for language modeling. *Computer Speech & Language 13*(4), 359–393.
- <span id="page-519-0"></span>Chen, T. and C. Guestrin (2016). Xgboost: A scalable tree boosting system. In *Proceedings of Knowledge Discovery and Data Mining (KDD)*, pp. 785–794.
- <span id="page-519-5"></span>Chen, X., X. Qiu, C. Zhu, P. Liu, and X. Huang (2015). Long short-term memory neural networks for chinese word segmentation. See [emn](#page-508-9) [\(2015\)](#page-508-9), pp. 1197–1206.
- <span id="page-519-12"></span>Chen, Y., S. Gilroy, A. Malletti, K. Knight, and J. May (2018). Recurrent neural networks as weighted language recognizers. See [naa](#page-509-12) [\(2018\)](#page-509-12).
- <span id="page-519-9"></span>Chen, Z. and H. Ji (2009). Graph-based event coreference resolution. In *Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing*, pp. 54–57. Association for Computational Linguistics.

- <span id="page-520-13"></span>Cheng, X. and D. Roth (2013). Relational inference for wikification. See [emn](#page-508-6) [\(2013\)](#page-508-6), pp. 1787–1796.
- <span id="page-520-14"></span>Chiang, D. (2007). Hierarchical phrase-based translation. *Computational Linguistics 33*(2), 201–228.
- <span id="page-520-8"></span>Chiang, D., J. Graehl, K. Knight, A. Pauls, and S. Ravi (2010). Bayesian inference for finite-state transducers. See [naa](#page-507-2) [\(2010\)](#page-507-2), pp. 447–455.
- <span id="page-520-7"></span>Chinchor, N. and P. Robinson (1997). MUC-7 named entity task definition. In *Proceedings of the 7th Conference on Message Understanding*, Volume 29.
- <span id="page-520-4"></span>Cho, K. (2015). Natural language understanding with distributed representation. *CoRR abs/1511.07916*.
- <span id="page-520-5"></span>Cho, K., B. Van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and ¨ Y. Bengio (2014). Learning phrase representations using rnn encoder-decoder for statistical machine translation. See [emn](#page-508-7) [\(2014\)](#page-508-7).
- <span id="page-520-9"></span>Chomsky, N. (1957). *Syntactic structures*. The Hague: Mouton & Co.
- <span id="page-520-12"></span>Chomsky, N. (1982). *Some concepts and consequences of the theory of government and binding*, Volume 6. MIT press.
- <span id="page-520-1"></span>Choromanska, A., M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun (2015). The loss surfaces of multilayer networks. In *Proceedings of Artificial Intelligence and Statistics (AIS-TATS)*, pp. 192–204.
- <span id="page-520-6"></span>Christodoulopoulos, C., S. Goldwater, and M. Steedman (2010). Two decades of unsupervised pos induction: How far have we come? See [emn](#page-507-0) [\(2010\)](#page-507-0), pp. 575–584.
- <span id="page-520-10"></span>Chu, Y.-J. and T.-H. Liu (1965). On shortest arborescence of a directed graph. *Scientia Sinica 14*(10), 1396–1400.
- <span id="page-520-2"></span>Chung, C. and J. W. Pennebaker (2007). The psychological functions of function words. In K. Fiedler (Ed.), *Social communication*, pp. 343–359. New York and Hove: Psychology Press.
- <span id="page-520-0"></span>Church, K. (2011). A pendulum swung too far. *Linguistic Issues in Language Technology 6*(5), 1–27.
- <span id="page-520-3"></span>Church, K. W. (2000). Empirical estimates of adaptation: the chance of two Noriegas is closer to p/2 than p 2 . See [col](#page-506-8) [\(2000\)](#page-506-8), pp. 180–186.
- <span id="page-520-11"></span>Church, K. W. and P. Hanks (1990). Word association norms, mutual information, and lexicography. *Computational linguistics 16*(1), 22–29.

- <span id="page-521-10"></span>Ciaramita, M. and M. Johnson (2003). Supersense tagging of unknown nouns in wordnet. In *Proceedings of Empirical Methods for Natural Language Processing (EMNLP)*, pp. 168– 175.
- <span id="page-521-12"></span>Clark, K. and C. D. Manning (2015). Entity-centric coreference resolution with model stacking. See [acl](#page-508-14) [\(2015\)](#page-508-14), pp. 1405–1415.
- <span id="page-521-11"></span>Clark, K. and C. D. Manning (2016). Improving coreference resolution by learning entitylevel distributed representations. See [acl](#page-508-1) [\(2016\)](#page-508-1).
- <span id="page-521-13"></span>Clark, P. (2015). Elementary school science and math tests as a driver for AI: take the aristo challenge! In *Proceedings of the National Conference on Artificial Intelligence (AAAI)*, pp. 4019–4021.
- <span id="page-521-9"></span>Clarke, J., D. Goldwasser, M.-W. Chang, and D. Roth (2010). Driving semantic parsing from the world's response. See [con](#page-507-11) [\(2010\)](#page-507-11), pp. 18–27.
- <span id="page-521-4"></span>Clarke, J. and M. Lapata (2008). Global inference for sentence compression: An integer linear programming approach. *Journal of Artificial Intelligence Research 31*, 399–429.
- <span id="page-521-1"></span>Cohen, J. (1960). A coefficient of agreement for nominal scales. *Educational and psychological measurement 20*(1), 37–46.
- <span id="page-521-2"></span>Cohen, S. (2016). *Bayesian analysis in natural language processing*. Synthesis Lectures on Human Language Technologies. San Rafael, CA: Morgan & Claypool Publishers.
- <span id="page-521-6"></span>Cohen, S. B., K. Stratos, M. Collins, D. P. Foster, and L. Ungar (2014). Spectral learning of latent-variable PCFGs: Algorithms and sample complexity. *Journal of Machine Learning Research 15*, 2399–2449.
- <span id="page-521-3"></span>Collier, N., C. Nobata, and J.-i. Tsujii (2000). Extracting the names of genes and gene products with a hidden markov model. See [col](#page-506-8) [\(2000\)](#page-506-8), pp. 201–207.
- <span id="page-521-5"></span>Collins, M. (1997). Three generative, lexicalised models for statistical parsing. See [acl](#page-506-9) [\(1997\)](#page-506-9), pp. 16–23.
- <span id="page-521-0"></span>Collins, M. (2002). Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. See [emn](#page-506-10) [\(2002\)](#page-506-10), pp. 1–8.
- <span id="page-521-7"></span>Collins, M. and T. Koo (2005). Discriminative reranking for natural language parsing. *Computational Linguistics 31*(1), 25–70.
- <span id="page-521-8"></span>Collins, M. and B. Roark (2004). Incremental parsing with the perceptron algorithm. See [acl](#page-506-11) [\(2004\)](#page-506-11).

- <span id="page-522-3"></span>Collobert, R., K. Kavukcuoglu, and C. Farabet (2011). Torch7: A matlab-like environment for machine learning. Technical Report EPFL-CONF-192376, EPFL.
- <span id="page-522-9"></span>Collobert, R. and J. Weston (2008). A unified architecture for natural language processing: Deep neural networks with multitask learning. See [icm](#page-507-9) [\(2008\)](#page-507-9), pp. 160–167.
- <span id="page-522-1"></span>Collobert, R., J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa (2011). Natural language processing (almost) from scratch. *Journal of Machine Learning Research 12*, 2493–2537.
- <span id="page-522-12"></span>Colton, S., J. Goodwin, and T. Veale (2012). Full-face poetry generation. In *Proceedings of the International Conference on Computational Creativity*, pp. 95–102.
- <span id="page-522-11"></span>Conneau, A., D. Kiela, H. Schwenk, L. Barrault, and A. Bordes (2017). Supervised learning of universal sentence representations from natural language inference data. See [emn](#page-509-9) [\(2017\)](#page-509-9), pp. 681–691.
- <span id="page-522-0"></span>Cormen, T. H., C. E. Leiserson, R. L. Rivest, and C. Stein (2009). *Introduction to algorithms* (third ed.). MIT press.
- <span id="page-522-10"></span>Cotterell, R., H. Schutze, and J. Eisner (2016). Morphological smoothing and extrapolation ¨ of word embeddings. See [acl](#page-508-1) [\(2016\)](#page-508-1), pp. 1651–1660.
- <span id="page-522-4"></span>Coviello, L., Y. Sohn, A. D. Kramer, C. Marlow, M. Franceschetti, N. A. Christakis, and J. H. Fowler (2014). Detecting emotional contagion in massive social networks. *PloS one 9*(3), e90315.
- <span id="page-522-8"></span>Covington, M. A. (2001). A fundamental algorithm for dependency parsing. In *Proceedings of the 39th annual ACM southeast conference*, pp. 95–102.
- <span id="page-522-13"></span>Crammer, K., O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer (2006, December). Online passive-aggressive algorithms. *The Journal of Machine Learning Research 7*, 551– 585.
- <span id="page-522-5"></span>Crammer, K. and Y. Singer (2001). Pranking with ranking. In *Neural Information Processing Systems (NIPS)*, pp. 641–647.
- <span id="page-522-2"></span>Crammer, K. and Y. Singer (2003). Ultraconservative online algorithms for multiclass problems. *The Journal of Machine Learning Research 3*, 951–991.
- <span id="page-522-6"></span>Creutz, M. and K. Lagus (2007). Unsupervised models for morpheme segmentation and morphology learning. *ACM Transactions on Speech and Language Processing (TSLP) 4*(1), 3.
- <span id="page-522-7"></span>Cross, J. and L. Huang (2016). Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles. See [emn](#page-508-8) [\(2016\)](#page-508-8), pp. 1–11.

- <span id="page-523-11"></span>Cucerzan, S. (2007). Large-scale named entity disambiguation based on wikipedia data. See [emn](#page-507-12) [\(2007\)](#page-507-12).
- <span id="page-523-7"></span>Cui, H., R. Sun, K. Li, M.-Y. Kan, and T.-S. Chua (2005). Question answering passage retrieval using dependency relations. In *Proceedings of ACM SIGIR conference on Research and development in information retrieval*, pp. 400–407.
- <span id="page-523-12"></span>Cui, Y., Z. Chen, S. Wei, S. Wang, T. Liu, and G. Hu (2017). Attention-over-attention neural networks for reading comprehension. See [acl](#page-508-0) [\(2017\)](#page-508-0).
- <span id="page-523-6"></span>Culotta, A. and J. Sorensen (2004). Dependency tree kernels for relation extraction. See [acl](#page-506-11) [\(2004\)](#page-506-11).
- <span id="page-523-10"></span>Culotta, A., M. Wick, and A. McCallum (2007). First-order probabilistic models for coreference resolution. See [naa](#page-507-13) [\(2007\)](#page-507-13), pp. 81–88.
- <span id="page-523-3"></span>Curry, H. B. and R. Feys (1958). *Combinatory Logic*, Volume I. Amsterdam: North Holland.
- <span id="page-523-1"></span>Danescu-Niculescu-Mizil, C., M. Sudhof, D. Jurafsky, J. Leskovec, and C. Potts (2013). A computational approach to politeness with application to social factors. See [acl](#page-508-12) [\(2013\)](#page-508-12), pp. 250–259.
- <span id="page-523-9"></span>Das, D., D. Chen, A. F. Martins, N. Schneider, and N. A. Smith (2014). Frame-semantic parsing. *Computational Linguistics 40*(1), 9–56.
- <span id="page-523-2"></span>Daume III, H. (2007). Frustratingly easy domain adaptation. See [acl](#page-507-6) [\(2007\)](#page-507-6). ´
- <span id="page-523-5"></span>Daume III, H., J. Langford, and D. Marcu (2009). Search-based structured prediction. ´ *Machine learning 75*(3), 297–325.
- <span id="page-523-4"></span>Daume III, H. and D. Marcu (2005). A large-scale exploration of effective global features ´ for a joint entity detection and tracking model. See [emn](#page-506-2) [\(2005\)](#page-506-2), pp. 97–104.
- <span id="page-523-0"></span>Dauphin, Y. N., R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio (2014). Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. See [nip](#page-508-4) [\(2014\)](#page-508-4), pp. 2933–2941.
- <span id="page-523-8"></span>Davidson, D. (1967). The logical form of action sentences. In N. Rescher (Ed.), *The Logic of Decision and Action*. Pittsburgh: University of Pittsburgh Press.
- <span id="page-523-13"></span>De Gispert, A. and J. B. Marino (2006). Catalan-english statistical machine translation without parallel corpus: bridging through spanish. In *Proc. of 5th International Conference on Language Resources and Evaluation (LREC)*, pp. 65–68. Citeseer.

- <span id="page-524-6"></span>De Marneffe, M.-C. and C. D. Manning (2008). The stanford typed dependencies representation. In *Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation*, pp. 1–8. Association for Computational Linguistics.
- <span id="page-524-1"></span>Dean, J. and S. Ghemawat (2008). Mapreduce: simplified data processing on large clusters. *Communications of the ACM 51*(1), 107–113.
- <span id="page-524-7"></span>Deerwester, S. C., S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman (1990). Indexing by latent semantic analysis. *Journal of the American society for information science 41*(6), 391–407.
- <span id="page-524-2"></span>Dehdari, J. (2014). *A Neurophysiologically-Inspired Statistical Language Model*. Ph. D. thesis, The Ohio State University.
- <span id="page-524-0"></span>Deisenroth, M. P., A. A. Faisal, and C. S. Ong (2018). *Mathematics For Machine Learning*. Cambridge University Press.
- <span id="page-524-5"></span>Dempster, A. P., N. M. Laird, and D. B. Rubin (1977). Maximum likelihood from incomplete data via the EM algorithm. *Journal of the Royal Statistical Society. Series B (Methodological)*, 1–38.
- <span id="page-524-8"></span>Denis, P. and J. Baldridge (2007). A ranking approach to pronoun resolution. See [ijc](#page-506-5) [\(2007\)](#page-506-5).
- <span id="page-524-9"></span>Denis, P. and J. Baldridge (2008). Specialized models and ranking for coreference resolution. See [emn](#page-507-1) [\(2008\)](#page-507-1), pp. 660–669.
- <span id="page-524-10"></span>Denis, P. and J. Baldridge (2009). Global joint models for coreference resolution and named entity classification. *Procesamiento del Lenguaje Natural 42*.
- <span id="page-524-12"></span>Derrida, J. (1985). Des tours de babel. In J. Graham (Ed.), *Difference in translation*. Ithaca, NY: Cornell University Press.
- <span id="page-524-11"></span>Dhingra, B., H. Liu, Z. Yang, W. W. Cohen, and R. Salakhutdinov (2017). Gated-attention readers for text comprehension. See [acl](#page-508-0) [\(2017\)](#page-508-0).
- <span id="page-524-13"></span>Diaconis, P. and B. Skyrms (2017). *Ten Great Ideas About Chance*. Princeton University Press.
- <span id="page-524-3"></span>Dietterich, T. G. (1998). Approximate statistical tests for comparing supervised classification learning algorithms. *Neural computation 10*(7), 1895–1923.
- <span id="page-524-4"></span>Dietterich, T. G., R. H. Lathrop, and T. Lozano-Perez (1997). Solving the multiple instance ´ problem with axis-parallel rectangles. *Artificial intelligence 89*(1), 31–71.

- <span id="page-525-5"></span>Dimitrova, L., N. Ide, V. Petkevic, T. Erjavec, H. J. Kaalep, and D. Tufis (1998). Multexteast: Parallel and comparable corpora and lexicons for six central and eastern european languages. See [col](#page-506-4) [\(1998\)](#page-506-4), pp. 315–319.
- <span id="page-525-12"></span>Doddington, G. R., A. Mitchell, M. A. Przybocki, L. A. Ramshaw, S. Strassel, and R. M. Weischedel (2004). The automatic content extraction (ACE) program-tasks, data, and evaluation. In *Proceedings of the Language Resources and Evaluation Conference*, pp. 837– 840.
- <span id="page-525-11"></span>dos Santos, C., B. Xiang, and B. Zhou (2015). Classifying relations by ranking with convolutional neural networks. See [acl](#page-508-14) [\(2015\)](#page-508-14), pp. 626–634.
- <span id="page-525-6"></span>Dowty, D. (1991). Thematic proto-roles and argument selection. *Language*, 547–619.
- <span id="page-525-10"></span>Dredze, M., P. McNamee, D. Rao, A. Gerber, and T. Finin (2010). Entity disambiguation for knowledge base population. See [col](#page-507-7) [\(2010\)](#page-507-7), pp. 277–285.
- <span id="page-525-2"></span>Dredze, M., M. J. Paul, S. Bergsma, and H. Tran (2013). Carmen: A Twitter geolocation system with applications to public health. In *AAAI workshop on expanding the boundaries of health informatics using AI (HIAI)*, pp. 20–24.
- <span id="page-525-0"></span>Dreyfus, H. L. (1992). *What computers still can't do: a critique of artificial reason*. MIT press.
- <span id="page-525-3"></span>Dror, R., G. Baumer, M. Bogomolov, and R. Reichart (2017). Replicability analysis for natural language processing: Testing significance with multiple datasets. *Transactions of the Association for Computational Linguistics 5*, 471–486.
- <span id="page-525-4"></span>Dror, R., G. Baumer, S. Shlomov, and R. Reichart (2018). The hitchhiker's guide to testing statistical significance in natural language processing. See [acl](#page-509-11) [\(2018\)](#page-509-11), pp. 1383–1392.
- <span id="page-525-8"></span>Du, L., W. Buntine, and M. Johnson (2013). Topic segmentation with a structured topic model. See [naa](#page-508-15) [\(2013\)](#page-508-15), pp. 190–200.
- <span id="page-525-1"></span>Duchi, J., E. Hazan, and Y. Singer (2011). Adaptive subgradient methods for online learning and stochastic optimization. *The Journal of Machine Learning Research 12*, 2121–2159.
- <span id="page-525-13"></span>Dunietz, J., L. Levin, and J. Carbonell (2017). The because corpus 2.0: Annotating causality and overlapping relations. In *Proceedings of the Linguistic Annotation Workshop*.
- <span id="page-525-9"></span>Durrett, G., T. Berg-Kirkpatrick, and D. Klein (2016). Learning-based single-document summarization with compression and anaphoricity constraints. See [acl](#page-508-1) [\(2016\)](#page-508-1), pp. 1998–2008.
- <span id="page-525-7"></span>Durrett, G. and D. Klein (2013). Easy victories and uphill battles in coreference resolution. See [emn](#page-508-6) [\(2013\)](#page-508-6).

- <span id="page-526-7"></span>Durrett, G. and D. Klein (2015). Neural CRF parsing. See [acl](#page-508-14) [\(2015\)](#page-508-14).
- <span id="page-526-12"></span>Dyer, C., M. Ballesteros, W. Ling, A. Matthews, and N. A. Smith (2015). Transition-based dependency parsing with stack long short-term memory. See [acl](#page-508-14) [\(2015\)](#page-508-14), pp. 334–343.
- <span id="page-526-1"></span>Dyer, C., A. Kuncoro, M. Ballesteros, and N. A. Smith (2016). Recurrent neural network grammars. See [naa](#page-508-16) [\(2016\)](#page-508-16), pp. 199–209.
- <span id="page-526-10"></span>Edmonds, J. (1967). Optimum branchings. *Journal of Research of the National Bureau of Standards B 71*(4), 233–240.
- <span id="page-526-3"></span>Efron, B. and R. J. Tibshirani (1993). An introduction to the bootstrap: Monographs on statistics and applied probability, vol. 57. *New York and London: Chapman and Hall/CRC*.
- <span id="page-526-14"></span>Eisenstein, J. (2009). Hierarchical text segmentation from multi-scale lexical cohesion. See [naa](#page-507-14) [\(2009\)](#page-507-14).
- <span id="page-526-13"></span>Eisenstein, J. and R. Barzilay (2008). Bayesian unsupervised topic segmentation. See [emn](#page-507-1) [\(2008\)](#page-507-1).
- <span id="page-526-11"></span>Eisner, J. (1997). State-of-the-art algorithms for minimum spanning trees: A tutorial discussion. [https://www.cs.jhu.edu/˜jason/papers/eisner.mst-tutorial.](https://www.cs.jhu.edu/~jason/papers/eisner.mst-tutorial.pdf) [pdf](https://www.cs.jhu.edu/~jason/papers/eisner.mst-tutorial.pdf).
- <span id="page-526-8"></span>Eisner, J. (2000). Bilexical grammars and their cubic-time parsing algorithms. In *Advances in probabilistic and other parsing technologies*, pp. 29–61. Springer.
- <span id="page-526-6"></span>Eisner, J. (2002). Parameter estimation for probabilistic finite-state transducers. See [acl](#page-506-12) [\(2002\)](#page-506-12), pp. 1–8.
- <span id="page-526-5"></span>Eisner, J. (2016). Inside-outside and forward-backward algorithms are just backprop. In *Proceedings of the Workshop on Structured Prediction for NLP*, pp. 1–17.
- <span id="page-526-9"></span>Eisner, J. M. (1996). Three new probabilistic models for dependency parsing: An exploration. See [col](#page-506-13) [\(1996\)](#page-506-13), pp. 340–345.
- <span id="page-526-2"></span>Ekman, P. (1992). Are there basic emotions? *Psychological Review 99*(3), 550–553.
- <span id="page-526-4"></span>Elman, J. L. (1990). Finding structure in time. *Cognitive science 14*(2), 179–211.
- <span id="page-526-0"></span>Elman, J. L., E. A. Bates, M. H. Johnson, A. Karmiloff-Smith, D. Parisi, and K. Plunkett (1998). *Rethinking innateness: A connectionist perspective on development*, Volume 10. MIT press.
- <span id="page-526-15"></span>Elsner, M. and E. Charniak (2010). Disentangling chat. *Computational Linguistics 36*(3), 389–409.

- <span id="page-527-1"></span>Esuli, A. and F. Sebastiani (2006). SentiWordNet: A publicly available lexical resource for opinion mining. In *Proceedings of the Language Resources and Evaluation Conference*, pp. 417–422.
- <span id="page-527-9"></span>Etzioni, O., A. Fader, J. Christensen, S. Soderland, and M. Mausam (2011). Open information extraction: The second generation. See [ijc](#page-507-15) [\(2011\)](#page-507-15), pp. 3–10.
- <span id="page-527-5"></span>Faruqui, M., J. Dodge, S. K. Jauhar, C. Dyer, E. Hovy, and N. A. Smith (2015). Retrofitting word vectors to semantic lexicons. See [naa](#page-508-17) [\(2015\)](#page-508-17).
- <span id="page-527-6"></span>Faruqui, M. and C. Dyer (2014). Improving vector space word representations using multilingual correlation. See [eac](#page-508-18) [\(2014\)](#page-508-18), pp. 462–471.
- <span id="page-527-2"></span>Faruqui, M., R. McDonald, and R. Soricut (2016). Morpho-syntactic lexicon generation using graph-based semi-supervised learning. *Transactions of the Association for Computational Linguistics 4*, 1–16.
- <span id="page-527-4"></span>Faruqui, M., Y. Tsvetkov, P. Rastogi, and C. Dyer (2016). Problems with evaluation of word embeddings using word similarity tasks. In *Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP*, pp. 30–35. Association for Computational Linguistics.
- <span id="page-527-0"></span>Fellbaum, C. (2010). *WordNet*. Springer.
- <span id="page-527-8"></span>Feng, V. W., Z. Lin, and G. Hirst (2014). The impact of deep hierarchical discourse structures in the evaluation of text coherence. See [col](#page-508-19) [\(2014\)](#page-508-19), pp. 940–949.
- <span id="page-527-10"></span>Feng, X., L. Huang, D. Tang, H. Ji, B. Qin, and T. Liu (2016). A language-independent neural network for event detection. See [acl](#page-508-1) [\(2016\)](#page-508-1), pp. 66–71.
- <span id="page-527-7"></span>Fernandes, E. R., C. N. dos Santos, and R. L. Milidiu (2014). Latent trees for coreference ´ resolution. *Computational Linguistics*.
- <span id="page-527-11"></span>Ferrucci, D., E. Brown, J. Chu-Carroll, J. Fan, D. Gondek, A. A. Kalyanpur, A. Lally, J. W. Murdock, E. Nyberg, J. Prager, et al. (2010). Building Watson: An overview of the DeepQA project. *AI magazine 31*(3), 59–79.
- <span id="page-527-13"></span>Ficler, J. and Y. Goldberg (2017). Controlling linguistic style aspects in neural language generation. In *Proceedings of the Workshop on Stylistic Variation*, pp. 94–104. Association for Computational Linguistics.
- <span id="page-527-12"></span>Filippova, K. and M. Strube (2008). Sentence fusion via dependency graph compression. See [emn](#page-507-1) [\(2008\)](#page-507-1), pp. 177–185.
- <span id="page-527-3"></span>Fillmore, C. J. (1968). The case for case. In E. Bach and R. Harms (Eds.), *Universals in linguistic theory*. Holt, Rinehart, and Winston.

- <span id="page-528-5"></span>Fillmore, C. J. (1976). Frame semantics and the nature of language. *Annals of the New York Academy of Sciences 280*(1), 20–32.
- <span id="page-528-6"></span>Fillmore, C. J. and C. Baker (2009). A frames approach to semantic analysis. In *The Oxford Handbook of Linguistic Analysis*. Oxford University Press.
- <span id="page-528-2"></span>Finkel, J. R., T. Grenager, and C. Manning (2005). Incorporating non-local information into information extraction systems by gibbs sampling. See [acl](#page-506-7) [\(2005\)](#page-506-7), pp. 363–370.
- <span id="page-528-4"></span>Finkel, J. R., T. Grenager, and C. D. Manning (2007). The infinite tree. See [acl](#page-507-6) [\(2007\)](#page-507-6), pp. 272–279.
- <span id="page-528-3"></span>Finkel, J. R., A. Kleeman, and C. D. Manning (2008). Efficient, feature-based, conditional random field parsing. See [acl](#page-507-4) [\(2008\)](#page-507-4), pp. 959–967.
- <span id="page-528-1"></span>Finkel, J. R. and C. Manning (2009). Hierarchical bayesian domain adaptation. See [naa](#page-507-14) [\(2009\)](#page-507-14), pp. 602–610.
- <span id="page-528-10"></span>Finkel, J. R. and C. D. Manning (2008). Enforcing transitivity in coreference resolution. See [acl](#page-507-4) [\(2008\)](#page-507-4), pp. 45–48.
- <span id="page-528-9"></span>Finkelstein, L., E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin (2002). Placing search in context: The concept revisited. *ACM Transactions on Information Systems 20*(1), 116–131.
- <span id="page-528-8"></span>Firth, J. R. (1957). *Papers in Linguistics 1934-1951*. Oxford University Press.
- <span id="page-528-7"></span>Flanigan, J., S. Thomson, J. Carbonell, C. Dyer, and N. A. Smith (2014). A discriminative graph-based parser for the abstract meaning representation. See [acl](#page-508-20) [\(2014\)](#page-508-20), pp. 1426– 1436.
- <span id="page-528-11"></span>Foltz, P. W., W. Kintsch, and T. K. Landauer (1998). The measurement of textual coherence with latent semantic analysis. *Discourse processes 25*(2-3), 285–307.
- <span id="page-528-12"></span>Fordyce, C. (2007). Overview of the IWSLT 2007 evaluation campaign. In *International Workshop on Spoken Language Translation (IWSLT) 2007*.
- <span id="page-528-14"></span>Forsyth, E. N. and C. H. Martell (2007). Lexical and discourse analysis of online chat dialog. In *International Conference on Semantic Computing*, pp. 19–26. IEEE.
- <span id="page-528-0"></span>Fort, K., G. Adda, and K. B. Cohen (2011). Amazon mechanical turk: Gold mine or coal mine? *Computational Linguistics 37*(2), 413–420.
- <span id="page-528-13"></span>Fox, H. (2002). Phrasal cohesion and statistical machine translation. See [emn](#page-506-10) [\(2002\)](#page-506-10), pp. 304–3111.

- <span id="page-529-2"></span>Francis, W. and H. Kucera (1982). *Frequency analysis of English usage*. Houghton Mifflin Company.
- <span id="page-529-5"></span>Francis, W. N. (1964). A standard sample of present-day English for use with digital computers. Report to the U.S Office of Education on Cooperative Research Project No. E-007.
- <span id="page-529-1"></span>Freund, Y. and R. E. Schapire (1999). Large margin classification using the perceptron algorithm. *Machine learning 37*(3), 277–296.
- <span id="page-529-0"></span>Fromkin, V., R. Rodman, and N. Hyams (2013). *An introduction to language*. Cengage Learning.
- <span id="page-529-8"></span>Fundel, K., R. Kuffner, and R. Zimmer (2007). Relex – relation extraction using depen- ¨ dency parse trees. *Bioinformatics 23*(3), 365–371.
- <span id="page-529-7"></span>Gabow, H. N., Z. Galil, T. Spencer, and R. E. Tarjan (1986). Efficient algorithms for finding minimum spanning trees in undirected and directed graphs. *Combinatorica 6*(2), 109– 122.
- <span id="page-529-9"></span>Gabrilovich, E. and S. Markovitch (2007). Computing semantic relatedness using wikipedia-based explicit semantic analysis. In *Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)*, Volume 7, pp. 1606–1611.
- <span id="page-529-10"></span>Gage, P. (1994). A new algorithm for data compression. *The C Users Journal 12*(2), 23–38.
- <span id="page-529-3"></span>Gale, W. A., K. W. Church, and D. Yarowsky (1992). One sense per discourse. In *Proceedings of the workshop on Speech and Natural Language*, pp. 233–237. Association for Computational Linguistics.
- <span id="page-529-12"></span>Galley, M., M. Hopkins, K. Knight, and D. Marcu (2004). What's in a translation rule? See [naa](#page-506-14) [\(2004\)](#page-506-14), pp. 273–280.
- <span id="page-529-11"></span>Galley, M., K. R. McKeown, E. Fosler-Lussier, and H. Jing (2003). Discourse segmentation of multi-party conversation. See [acl](#page-506-15) [\(2003\)](#page-506-15).
- <span id="page-529-4"></span>Ganchev, K. and M. Dredze (2008). Small statistical models by random feature mixing. In *Proceedings of Workshop on Mobile Language Processing*, pp. 19–20. Association of Computational Linguistics.
- <span id="page-529-6"></span>Ganchev, K., J. Grac¸a, J. Gillenwater, and B. Taskar (2010). Posterior regularization for structured latent variable models. *The Journal of Machine Learning Research 11*, 2001– 2049.

- <span id="page-530-3"></span>Ganin, Y., E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky (2016). Domain-adversarial training of neural networks. *The Journal of Machine Learning Research 17*(59), 1–35.
- <span id="page-530-1"></span>Gao, J., G. Andrew, M. Johnson, and K. Toutanova (2007). A comparative study of parameter estimation methods for statistical natural language processing. See [acl](#page-507-6) [\(2007\)](#page-507-6), pp. 824–831.
- <span id="page-530-7"></span>Garg, N., L. Schiebinger, D. Jurafsky, and J. Zou (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes. *Proceedings of the National Academy of Sciences 115*(16), E3635–E3644.
- <span id="page-530-12"></span>Gatt, A. and E. Krahmer (2018). Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. *Journal of Artificial Intelligence Research 61*, 65–170.
- <span id="page-530-0"></span>Ge, D., X. Jiang, and Y. Ye (2011). A note on the complexity of l<sup>p</sup> minimization. *Mathematical programming 129*(2), 285–299.
- <span id="page-530-9"></span>Ge, N., J. Hale, and E. Charniak (1998). A statistical approach to anaphora resolution. In *Proceedings of the sixth workshop on very large corpora*, Volume 71, pp. 76.
- <span id="page-530-2"></span>Ge, R., F. Huang, C. Jin, and Y. Yuan (2015). Escaping from saddle points — online stochastic gradient for tensor decomposition. In P. Grunwald, E. Hazan, and S. Kale (Eds.), ¨ *Proceedings of the Conference On Learning Theory (COLT)*.
- <span id="page-530-5"></span>Ge, R. and R. J. Mooney (2005). A statistical semantic parser that integrates syntax and semantics. In *Proceedings of the Conference on Natural Language Learning (CoNLL)*, pp. 9–16.
- <span id="page-530-8"></span>Geach, P. T. (1962). *Reference and generality: An examination of some medieval and modern theories*. Cornell University Press.
- <span id="page-530-11"></span>Gehring, J., M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin (2017). Convolutional sequence to sequence learning. See [icm](#page-509-13) [\(2017\)](#page-509-13), pp. 1243–1252.
- <span id="page-530-6"></span>Gildea, D. and D. Jurafsky (2002). Automatic labeling of semantic roles. *Computational linguistics 28*(3), 245–288.
- <span id="page-530-4"></span>Gimpel, K., N. Schneider, B. O'Connor, D. Das, D. Mills, J. Eisenstein, M. Heilman, D. Yogatama, J. Flanigan, and N. A. Smith (2011). Part-of-speech tagging for Twitter: annotation, features, and experiments. See [acl](#page-507-16) [\(2011\)](#page-507-16), pp. 42–47.
- <span id="page-530-10"></span>Glass, J., T. J. Hazen, S. Cyphers, I. Malioutov, D. Huynh, and R. Barzilay (2007). Recent progress in the mit spoken lecture processing project. In *Eighth Annual Conference of the International Speech Communication Association*.

- <span id="page-531-3"></span>Glorot, X. and Y. Bengio (2010). Understanding the difficulty of training deep feedforward neural networks. In *Proceedings of Artificial Intelligence and Statistics (AISTATS)*, pp. 249– 256.
- <span id="page-531-2"></span>Glorot, X., A. Bordes, and Y. Bengio (2011). Deep sparse rectifier networks. See [ais](#page-507-17) [\(2011\)](#page-507-17), pp. 315–323.
- <span id="page-531-9"></span>Godfrey, J. J., E. C. Holliman, and J. McDaniel (1992). Switchboard: Telephone speech corpus for research and development. In *Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP)*, pp. 517–520. IEEE.
- <span id="page-531-5"></span>Goldberg, Y. (2017a, June). An adversarial review of "adversarial generation of natural language". https://medium.com/@yoav.goldberg/an-adversarial-review-ofadversarial-generation-of-natural-language-409ac3378bd7.
- <span id="page-531-4"></span>Goldberg, Y. (2017b). *Neural Network Methods for Natural Language Processing*. Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers.
- <span id="page-531-11"></span>Goldberg, Y. and M. Elhadad (2010). An efficient algorithm for easy-first non-directional dependency parsing. See [naa](#page-507-2) [\(2010\)](#page-507-2), pp. 742–750.
- <span id="page-531-10"></span>Goldberg, Y. and J. Nivre (2012). A dynamic oracle for arc-eager dependency parsing. See [col](#page-507-18) [\(2012\)](#page-507-18), pp. 959–976.
- <span id="page-531-12"></span>Goldberg, Y., K. Zhao, and L. Huang (2013). Efficient implementation of beam-search incremental parsers. See [acl](#page-508-12) [\(2013\)](#page-508-12), pp. 628–633.
- <span id="page-531-8"></span>Goldwater, S. and T. Griffiths (2007). A fully bayesian approach to unsupervised part-ofspeech tagging. See [acl](#page-507-6) [\(2007\)](#page-507-6).
- <span id="page-531-13"></span>Gonc¸alo Oliveira, H. R., F. A. Cardoso, and F. C. Pereira (2007). Tra-la-lyrics: An approach to generate text based on rhythm. In *Proceedings of the 4th. International Joint Workshop on Computational Creativity*. A. Cardoso and G. Wiggins.
- <span id="page-531-1"></span>Goodfellow, I., Y. Bengio, and A. Courville (2016). *Deep learning*. MIT Press.
- <span id="page-531-7"></span>Goodman, J. T. (2001). A bit of progress in language modeling. *Computer Speech & Language 15*(4), 403–434.
- <span id="page-531-6"></span>Gouws, S., D. Metzler, C. Cai, and E. Hovy (2011). Contextual bearing on linguistic variation in social media. In *Proceedings of the Workshop on Language and Social Media*. Association for Computational Linguistics.
- <span id="page-531-0"></span>Goyal, A., H. Daume III, and S. Venkatasubramanian (2009). Streaming for large scale ´ NLP: Language modeling. See [naa](#page-507-14) [\(2009\)](#page-507-14), pp. 512–520.

- <span id="page-532-11"></span>Graves, A. (2012). Sequence transduction with recurrent neural networks. See [icm](#page-507-10) [\(2012\)](#page-507-10).
- <span id="page-532-1"></span>Graves, A. and N. Jaitly (2014). Towards end-to-end speech recognition with recurrent neural networks. See [icm](#page-508-11) [\(2014\)](#page-508-11), pp. 1764–1772.
- <span id="page-532-3"></span>Graves, A. and J. Schmidhuber (2005). Framewise phoneme classification with bidirectional lstm and other neural network architectures. *Neural Networks 18*(5), 602–610.
- <span id="page-532-6"></span>Grice, H. P. (1975). Logic and conversation. In P. Cole and J. L. Morgan (Eds.), *Syntax and Semantics Volume 3: Speech Acts*, pp. 41–58. Academic Press.
- <span id="page-532-0"></span>Grishman, R. (2012). Information extraction: Capabilities and challenges. Notes prepared for the 2012 International Winter School in Language and Speech Technologies, Rovira i Virgili University, Tarragona, Spain.
- <span id="page-532-10"></span>Grishman, R. (2015). Information extraction. *IEEE Intelligent Systems 30*(5), 8–15.
- <span id="page-532-4"></span>Grishman, R., C. Macleod, and J. Sterling (1992). Evaluating parsing strategies using standardized parse files. In *Proceedings of the third conference on Applied natural language processing*, pp. 156–161. Association for Computational Linguistics.
- <span id="page-532-9"></span>Grishman, R. and B. Sundheim (1996). Message understanding conference-6: A brief history. See [col](#page-506-13) [\(1996\)](#page-506-13), pp. 466–471.
- <span id="page-532-5"></span>Groenendijk, J. and M. Stokhof (1991). Dynamic predicate logic. *Linguistics and philosophy 14*(1), 39–100.
- <span id="page-532-14"></span>Grosz, B. J. (1979). Focusing and description in natural language dialogues. Technical report, SRI International.
- <span id="page-532-7"></span>Grosz, B. J., S. Weinstein, and A. K. Joshi (1995). Centering: A framework for modeling the local coherence of discourse. *Computational linguistics 21*(2), 203–225.
- <span id="page-532-13"></span>Gu, J., Z. Lu, H. Li, and V. O. Li (2016). Incorporating copying mechanism in sequence-tosequence learning. See [acl](#page-508-1) [\(2016\)](#page-508-1), pp. 1631–1640.
- <span id="page-532-12"></span>Gulcehre, C., S. Ahn, R. Nallapati, B. Zhou, and Y. Bengio (2016). Pointing the unknown words. See [acl](#page-508-1) [\(2016\)](#page-508-1), pp. 140–149.
- <span id="page-532-2"></span>Gutmann, M. U. and A. Hyvarinen (2012). Noise-contrastive estimation of unnormalized ¨ statistical models, with applications to natural image statistics. *The Journal of Machine Learning Research 13*(1), 307–361.
- <span id="page-532-8"></span>Haghighi, A. and D. Klein (2007). Unsupervised coreference resolution in a nonparametric bayesian model. See [acl](#page-507-6) [\(2007\)](#page-507-6).

- <span id="page-533-9"></span>Haghighi, A. and D. Klein (2009). Simple coreference resolution with rich syntactic and semantic features. See [emn](#page-507-19) [\(2009\)](#page-507-19), pp. 1152–1161.
- <span id="page-533-8"></span>Haghighi, A. and D. Klein (2010). Coreference resolution in a modular, entity-centered model. See [naa](#page-507-2) [\(2010\)](#page-507-2), pp. 385–393.
- <span id="page-533-6"></span>Hajic, J. and B. Hladk ˇ a (1998). Tagging inflective languages: Prediction of morphological ´ categories for a rich, structured tagset. In *Proceedings of the Association for Computational Linguistics (ACL)*, pp. 483–490.
- <span id="page-533-10"></span>Halliday, M. and R. Hasan (1976). *Cohesion in English*. London: Longman.
- <span id="page-533-7"></span>Hammerton, J. (2003). Named entity recognition with long short-term memory. In *Proceedings of the Conference on Natural Language Learning (CoNLL)*, pp. 172–175.
- <span id="page-533-11"></span>Han, X. and L. Sun (2012). An entity-topic model for entity linking. See [emn](#page-507-3) [\(2012\)](#page-507-3), pp. 105–115.
- <span id="page-533-12"></span>Han, X., L. Sun, and J. Zhao (2011). Collective entity linking in web text: a graph-based method. In *Proceedings of ACM SIGIR conference on Research and development in information retrieval*, pp. 765–774.
- <span id="page-533-2"></span>Hannak, A., E. Anderson, L. F. Barrett, S. Lehmann, A. Mislove, and M. Riedewald (2012). Tweetin'in the rain: Exploring societal-scale effects of weather on mood. In *Proceedings of the International Conference on Web and Social Media (ICWSM)*.
- <span id="page-533-13"></span>Hardmeier, C. (2012). Discourse in statistical machine translation. a survey and a case study. *Discours* (11).
- <span id="page-533-5"></span>Haspelmath, M. and A. Sims (2013). *Understanding morphology*. Routledge.
- <span id="page-533-0"></span>Hastie, T., R. Tibshirani, and J. Friedman (2009). *The elements of statistical learning* (Second ed.). New York: Springer.
- <span id="page-533-3"></span>Hatzivassiloglou, V. and K. R. McKeown (1997). Predicting the semantic orientation of adjectives. See [acl](#page-506-9) [\(1997\)](#page-506-9), pp. 174–181.
- <span id="page-533-4"></span>Hayes, A. F. and K. Krippendorff (2007). Answering the call for a standard reliability measure for coding data. *Communication methods and measures 1*(1), 77–89.
- <span id="page-533-14"></span>He, H., A. Balakrishnan, M. Eric, and P. Liang (2017). Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings. See [acl](#page-508-0) [\(2017\)](#page-508-0), pp. 1766– 1776.
- <span id="page-533-1"></span>He, K., X. Zhang, S. Ren, and J. Sun (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. See [icc](#page-508-2) [\(2015\)](#page-508-2), pp. 1026–1034.

- <span id="page-534-0"></span>He, K., X. Zhang, S. Ren, and J. Sun (2016). Deep residual learning for image recognition. In *Proceedings of the International Conference on Computer Vision (ICCV)*, pp. 770–778.
- <span id="page-534-4"></span>He, L., K. Lee, M. Lewis, and L. Zettlemoyer (2017). Deep semantic role labeling: What works and what's next. See [acl](#page-508-0) [\(2017\)](#page-508-0).
- <span id="page-534-10"></span>He, Z., S. Liu, M. Li, M. Zhou, L. Zhang, and H. Wang (2013). Learning entity representation for entity disambiguation. See [acl](#page-508-12) [\(2013\)](#page-508-12), pp. 30–34.
- <span id="page-534-2"></span>Hearst, M. A. (1992). Automatic acquisition of hyponyms from large text corpora. See [col](#page-506-16) [\(1992\)](#page-506-16), pp. 539–545.
- <span id="page-534-7"></span>Hearst, M. A. (1997). Texttiling: Segmenting text into multi-paragraph subtopic passages. *Computational linguistics 23*(1), 33–64.
- <span id="page-534-9"></span>Heerschop, B., F. Goossen, A. Hogenboom, F. Frasincar, U. Kaymak, and F. de Jong (2011). Polarity analysis of texts using discourse structure. In *Proceedings of the 20th ACM international conference on Information and knowledge management*, pp. 1061–1070. ACM.
- <span id="page-534-1"></span>Henderson, J. (2004). Discriminative training of a neural network statistical parser. See [acl](#page-506-11) [\(2004\)](#page-506-11), pp. 95–102.
- <span id="page-534-11"></span>Hendrickx, I., S. N. Kim, Z. Kozareva, P. Nakov, D. O S ´ eaghdha, S. Pad ´ o, M. Pennacchiotti, ´ L. Romano, and S. Szpakowicz (2009). Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In *Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions*, pp. 94–99. Association for Computational Linguistics.
- <span id="page-534-12"></span>Hermann, K. M., T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom (2015). Teaching machines to read and comprehend. In *Advances in Neural Information Processing Systems*, pp. 1693–1701.
- <span id="page-534-8"></span>Hernault, H., H. Prendinger, D. A. duVerle, and M. Ishizuka (2010). HILDA: A discourse parser using support vector machine classification. *Dialogue and Discourse 1*(3), 1–33.
- <span id="page-534-13"></span>Hill, F., A. Bordes, S. Chopra, and J. Weston (2016). The goldilocks principle: Reading children's books with explicit memory representations. See [icl](#page-508-21) [\(2016\)](#page-508-21).
- <span id="page-534-6"></span>Hill, F., K. Cho, and A. Korhonen (2016). Learning distributed representations of sentences from unlabelled data. See [naa](#page-508-16) [\(2016\)](#page-508-16).
- <span id="page-534-5"></span>Hill, F., R. Reichart, and A. Korhonen (2015). Simlex-999: Evaluating semantic models with (genuine) similarity estimation. *Computational Linguistics 41*(4), 665–695.
- <span id="page-534-3"></span>Hindle, D. and M. Rooth (1993). Structural ambiguity and lexical relations. *Computational linguistics 19*(1), 103–120.

- <span id="page-535-8"></span>Hirao, T., Y. Yoshida, M. Nishino, N. Yasuda, and M. Nagata (2013). Single-document summarization as a tree knapsack problem. See [emn](#page-508-6) [\(2013\)](#page-508-6), pp. 1515–1520.
- <span id="page-535-11"></span>Hirschman, L. and R. Gaizauskas (2001). Natural language question answering: the view from here. *natural language engineering 7*(4), 275–300.
- <span id="page-535-12"></span>Hirschman, L., M. Light, E. Breck, and J. D. Burger (1999). Deep read: A reading comprehension system. In *Proceedings of the Association for Computational Linguistics (ACL)*, pp. 325–332.
- <span id="page-535-7"></span>Hobbs, J. R. (1978). Resolving pronoun references. *Lingua 44*(4), 311–338.
- <span id="page-535-2"></span>Hobbs, J. R., D. Appelt, J. Bear, D. Israel, M. Kameyama, M. Stickel, and M. Tyson (1997). Fastus: A cascaded finite-state transducer for extracting information from naturallanguage text. *Finite-state language processing*, 383–406.
- <span id="page-535-4"></span>Hochreiter, S. and J. Schmidhuber (1997). Long short-term memory. *Neural computation 9*(8), 1735–1780.
- <span id="page-535-6"></span>Hockenmaier, J. and M. Steedman (2007). CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank. *Computational Linguistics 33*(3), 355–396.
- <span id="page-535-9"></span>Hoffart, J., M. A. Yosef, I. Bordino, H. Furstenau, M. Pinkal, M. Spaniol, B. Taneva, ¨ S. Thater, and G. Weikum (2011). Robust disambiguation of named entities in text. See [emn](#page-507-20) [\(2011\)](#page-507-20), pp. 782–792.
- <span id="page-535-10"></span>Hoffmann, R., C. Zhang, X. Ling, L. Zettlemoyer, and D. S. Weld (2011). Knowledge-based weak supervision for information extraction of overlapping relations. See [acl](#page-507-16) [\(2011\)](#page-507-16), pp. 541–550.
- <span id="page-535-1"></span>Holmstrom, L. and P. Koistinen (1992). Using additive noise in back-propagation training. *IEEE Transactions on Neural Networks 3*(1), 24–38.
- <span id="page-535-0"></span>Hovy, D., S. Spruit, M. Mitchell, E. M. Bender, M. Strube, and H. Wallach (2017). Proceedings of the first acl workshop on ethics in natural language processing. In *Proceedings of the First ACL Workshop on Ethics in Natural Language Processing*. Association for Computational Linguistics.
- <span id="page-535-3"></span>Hovy, E. and J. Lavid (2010). Towards a 'science' of corpus annotation: a new methodological challenge for corpus linguistics. *International journal of translation 22*(1), 13–36.
- <span id="page-535-5"></span>Hsu, D., S. M. Kakade, and T. Zhang (2012). A spectral algorithm for learning hidden markov models. *Journal of Computer and System Sciences 78*(5), 1460–1480.

- <span id="page-536-1"></span>Hu, M. and B. Liu (2004). Mining and summarizing customer reviews. In *Proceedings of Knowledge Discovery and Data Mining (KDD)*, pp. 168–177.
- <span id="page-536-14"></span>Hu, Z., Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing (2017). Toward controlled generation of text. See [icm](#page-509-13) [\(2017\)](#page-509-13), pp. 1587–1596.
- <span id="page-536-6"></span>Huang, F. and A. Yates (2012). Biased representation learning for domain adaptation. See [emn](#page-507-3) [\(2012\)](#page-507-3), pp. 1313–1323.
- <span id="page-536-13"></span>Huang, L. and D. Chiang (2007). Forest rescoring: Faster decoding with integrated language models. See [acl](#page-507-6) [\(2007\)](#page-507-6), pp. 144–151.
- <span id="page-536-4"></span>Huang, L., S. Fayong, and Y. Guo (2012). Structured perceptron with inexact search. See [naa](#page-507-21) [\(2012\)](#page-507-21), pp. 142–151.
- <span id="page-536-9"></span>Huang, Y. (2015). *Pragmatics* (Second ed.). Oxford Textbooks in Linguistics. Oxford University Press.
- <span id="page-536-3"></span>Huang, Z., W. Xu, and K. Yu (2015). Bidirectional LSTM-CRF models for sequence tagging. *arXiv preprint arXiv:1508.01991*.
- <span id="page-536-11"></span>Huddleston, R. and G. K. Pullum (2005). *A student's introduction to English grammar*. Cambridge University Press.
- <span id="page-536-7"></span>Huffman, D. A. (1952). A method for the construction of minimum-redundancy codes. *Proceedings of the IRE 40*(9), 1098–1101.
- <span id="page-536-10"></span>Humphreys, K., R. Gaizauskas, and S. Azzam (1997). Event coreference for information extraction. In *Proceedings of a Workshop on Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts*, pp. 75–81. Association for Computational Linguistics.
- <span id="page-536-2"></span>Ide, N. and Y. Wilks (2006). Making sense about sense. In *Word sense disambiguation*, pp. 47–73. Springer.
- <span id="page-536-0"></span>Ioffe, S. and C. Szegedy (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. See [icm](#page-508-13) [\(2015\)](#page-508-13), pp. 448–456.
- <span id="page-536-12"></span>Isozaki, H., T. Hirao, K. Duh, K. Sudoh, and H. Tsukada (2010). Automatic evaluation of translation quality for distant language pairs. See [emn](#page-507-0) [\(2010\)](#page-507-0), pp. 944–952.
- <span id="page-536-5"></span>Ivanova, A., S. Oepen, L. Øvrelid, and D. Flickinger (2012). Who did what to whom? a contrastive study of syntacto-semantic dependencies. In *Proceedings of the Sixth Linguistic Annotation Workshop*, pp. 2–11. Association for Computational Linguistics.
- <span id="page-536-8"></span>Iyyer, M., V. Manjunatha, J. Boyd-Graber, and H. Daume III (2015). Deep unordered com- ´ position rivals syntactic methods for text classification. See [acl](#page-508-14) [\(2015\)](#page-508-14), pp. 1681–1691.

- <span id="page-537-0"></span>James, G., D. Witten, T. Hastie, and R. Tibshirani (2013). *An introduction to statistical learning*, Volume 112. Springer.
- <span id="page-537-7"></span>Janin, A., D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, et al. (2003). The ICSI meeting corpus. In *Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP)*.
- <span id="page-537-13"></span>Jean, S., K. Cho, R. Memisevic, and Y. Bengio (2015). On using very large target vocabulary for neural machine translation. See [acl](#page-508-14) [\(2015\)](#page-508-14), pp. 1–10.
- <span id="page-537-2"></span>Jeong, M., C.-Y. Lin, and G. G. Lee (2009). Semi-supervised speech act recognition in emails and forums. See [emn](#page-507-19) [\(2009\)](#page-507-19), pp. 1250–1259.
- <span id="page-537-12"></span>Ji, H. and R. Grishman (2011). Knowledge base population: Successful approaches and challenges. See [acl](#page-507-16) [\(2011\)](#page-507-16), pp. 1148–1158.
- <span id="page-537-10"></span>Ji, Y., T. Cohn, L. Kong, C. Dyer, and J. Eisenstein (2015). Document context language models. In *International Conference on Learning Representations, Workshop Track*, Volume abs/1511.03962.
- <span id="page-537-8"></span>Ji, Y. and J. Eisenstein (2014). Representation learning for text-level discourse parsing. See [acl](#page-508-20) [\(2014\)](#page-508-20).
- <span id="page-537-5"></span>Ji, Y. and J. Eisenstein (2015). One vector is not enough: Entity-augmented distributional semantics for discourse relations. *Transactions of the Association for Computational Linguistics (TACL)*.
- <span id="page-537-3"></span>Ji, Y., G. Haffari, and J. Eisenstein (2016). A latent variable recurrent neural network for discourse relation language models. See [naa](#page-508-16) [\(2016\)](#page-508-16).
- <span id="page-537-9"></span>Ji, Y. and N. A. Smith (2017). Neural discourse structure for text categorization. See [acl](#page-508-0) [\(2017\)](#page-508-0), pp. 996–1005.
- <span id="page-537-6"></span>Ji, Y., C. Tan, S. Martschat, Y. Choi, and N. A. Smith (2017). Dynamic entity representations in neural language models. See [emn](#page-509-9) [\(2017\)](#page-509-9), pp. 1831–1840.
- <span id="page-537-1"></span>Jiang, L., M. Yu, M. Zhou, X. Liu, and T. Zhao (2011). Target-dependent twitter sentiment classification. See [acl](#page-507-16) [\(2011\)](#page-507-16), pp. 151–160.
- <span id="page-537-4"></span>Jing, H. (2000). Sentence reduction for automatic text summarization. In *Proceedings of the sixth conference on Applied natural language processing*, pp. 310–315. Association for Computational Linguistics.
- <span id="page-537-11"></span>Joachims, T. (2002). Optimizing search engines using clickthrough data. In *Proceedings of Knowledge Discovery and Data Mining (KDD)*, pp. 133–142.

- <span id="page-538-1"></span>Jockers, M. L. (2015). Revealing sentiment and plot arcs with the syuzhet package. [http:](http://www.matthewjockers.net/2015/02/02/syuzhet/) [//www.matthewjockers.net/2015/02/02/syuzhet/](http://www.matthewjockers.net/2015/02/02/syuzhet/).
- <span id="page-538-13"></span>Johnson, A. E., T. J. Pollard, L. Shen, H. L. Li-wei, M. Feng, M. Ghassemi, B. Moody, P. Szolovits, L. A. Celi, and R. G. Mark (2016). MIMIC-III, a freely accessible critical care database. *Scientific data 3*, 160035.
- <span id="page-538-8"></span>Johnson, M. (1998). PCFG models of linguistic tree representations. *Computational Linguistics 24*(4), 613–632.
- <span id="page-538-0"></span>Johnson, R. and T. Zhang (2017). Deep pyramid convolutional neural networks for text categorization. See [acl](#page-508-0) [\(2017\)](#page-508-0), pp. 562–570.
- <span id="page-538-5"></span>Joshi, A. K. (1985). Tree adjoining grammars: How much context-sensitivity is required to provide reasonable structural descriptions? In *Natural Language Processing – Theoretical, Computational and Psychological Perspectives*. New York, NY: Cambridge University Press.
- <span id="page-538-7"></span>Joshi, A. K. and Y. Schabes (1997). Tree-adjoining grammars. In *Handbook of formal languages*, pp. 69–123. Springer.
- <span id="page-538-6"></span>Joshi, A. K., K. V. Shanker, and D. Weir (1991). The convergence of mildly context-sensitive grammar formalisms. In *Foundational Issues in Natural Language Processing*. Cambridge MA: MIT Press.
- <span id="page-538-3"></span>Jozefowicz, R., O. Vinyals, M. Schuster, N. Shazeer, and Y. Wu (2016). Exploring the limits of language modeling. *arXiv preprint arXiv:1602.02410*.
- <span id="page-538-2"></span>Jozefowicz, R., W. Zaremba, and I. Sutskever (2015). An empirical exploration of recurrent network architectures. See [icm](#page-508-13) [\(2015\)](#page-508-13), pp. 2342–2350.
- <span id="page-538-9"></span>Jurafsky, D. (1996). A probabilistic model of lexical and syntactic access and disambiguation. *Cognitive Science 20*(2), 137–194.
- <span id="page-538-11"></span>Jurafsky, D. and J. H. Martin (2009). *Speech and Language Processing* (Second ed.). Prentice Hall.
- <span id="page-538-10"></span>Jurafsky, D. and J. H. Martin (2019). *Speech and Language Processing* (Third ed.). Prentice Hall.
- <span id="page-538-12"></span>Kadlec, R., M. Schmid, O. Bajgar, and J. Kleindienst (2016). Text understanding with the attention sum reader network. See [acl](#page-508-1) [\(2016\)](#page-508-1), pp. 908–918.
- <span id="page-538-4"></span>Kalchbrenner, N. and P. Blunsom (2013). Recurrent convolutional neural networks for discourse compositionality. In *Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality*, pp. 119–126. Association for Computational Linguistics.

- <span id="page-539-5"></span>Kalchbrenner, N., L. Espeholt, K. Simonyan, A. v. d. Oord, A. Graves, and K. Kavukcuoglu (2016). Neural machine translation in linear time. *arXiv preprint arXiv:1610.10099*.
- <span id="page-539-4"></span>Kalchbrenner, N., E. Grefenstette, and P. Blunsom (2014). A convolutional neural network for modelling sentences. See [acl](#page-508-20) [\(2014\)](#page-508-20), pp. 655–665.
- <span id="page-539-9"></span>Karlsson, F. (2007). Constraints on multiple center-embedding of clauses. *Journal of Linguistics 43*(2), 365–392.
- <span id="page-539-10"></span>Kate, R. J., Y. W. Wong, and R. J. Mooney (2005). Learning to transform natural to formal languages. In *Proceedings of the National Conference on Artificial Intelligence (AAAI)*.
- <span id="page-539-2"></span>Kawaguchi, K., L. P. Kaelbling, and Y. Bengio (2017). Generalization in deep learning. *arXiv preprint arXiv:1710.05468*.
- <span id="page-539-11"></span>Kehler, A. (2007). Rethinking the SMASH approach to pronoun interpretation. In *Interdisciplinary perspectives on reference processing*, New Directions in Cognitive Science Series, pp. 95–122. Oxford University Press.
- <span id="page-539-12"></span>Kibble, R. and R. Power (2004). Optimizing referential coherence in text generation. *Computational Linguistics 30*(4), 401–416.
- <span id="page-539-0"></span>Kilgarriff, A. (1997). I don't believe in word senses. *Computers and the Humanities 31*(2), 91–113.
- <span id="page-539-13"></span>Kilgarriff, A. and G. Grefenstette (2003). Introduction to the special issue on the web as corpus. *Computational linguistics 29*(3), 333–347.
- <span id="page-539-8"></span>Kim, M.-J. (2002). Does Korean have adjectives? *MIT Working Papers in Linguistics 43*, 71–89.
- <span id="page-539-7"></span>Kim, S.-M. and E. Hovy (2006, July). Extracting opinions, opinion holders, and topics expressed in online news media text. In *Proceedings of the Workshop on Sentiment and Subjectivity in Text*, pp. 1–8. Association for Computational Linguistics.
- <span id="page-539-6"></span>Kim, Y. (2014). Convolutional neural networks for sentence classification. See [emn](#page-508-7) [\(2014\)](#page-508-7), pp. 1746–1751.
- <span id="page-539-14"></span>Kim, Y., C. Denton, L. Hoang, and A. M. Rush (2017). Structured attention networks. See [icl](#page-509-14) [\(2017\)](#page-509-14).
- <span id="page-539-1"></span>Kim, Y., Y. Jernite, D. Sontag, and A. M. Rush (2016). Character-aware neural language models. See [aaa](#page-508-22) [\(2016\)](#page-508-22).
- <span id="page-539-3"></span>Kingma, D. and J. Ba (2014). Adam: A method for stochastic optimization. *arXiv preprint arXiv:1412.6980*.

- <span id="page-540-2"></span>Kiperwasser, E. and Y. Goldberg (2016). Simple and accurate dependency parsing using bidirectional lstm feature representations. *Transactions of the Association for Computational Linguistics 4*, 313–327.
- <span id="page-540-4"></span>Kipper-Schuler, K. (2005). *VerbNet: A broad-coverage, comprehensive verb lexicon*. Ph. D. thesis, Computer and Information Science, University of Pennsylvania.
- <span id="page-540-7"></span>Kiros, R., R. Salakhutdinov, and R. Zemel (2014). Multimodal neural language models. See [icm](#page-508-11) [\(2014\)](#page-508-11), pp. 595–603.
- <span id="page-540-5"></span>Kiros, R., Y. Zhu, R. Salakhudinov, R. S. Zemel, A. Torralba, R. Urtasun, and S. Fidler (2015). Skip-thought vectors. See [nip](#page-508-5) [\(2015\)](#page-508-5).
- <span id="page-540-1"></span>Klein, D. and C. D. Manning (2003). Accurate unlexicalized parsing. See [acl](#page-506-15) [\(2003\)](#page-506-15), pp. 423–430.
- <span id="page-540-3"></span>Klein, D. and C. D. Manning (2004). Corpus-based induction of syntactic structure: Models of dependency and constituency. See [acl](#page-506-11) [\(2004\)](#page-506-11).
- <span id="page-540-13"></span>Klein, G., Y. Kim, Y. Deng, J. Senellart, and A. M. Rush (2017). OpenNMT: Open-source toolkit for neural machine translation. *arXiv preprint arXiv:1701.02810*.
- <span id="page-540-6"></span>Klementiev, A., I. Titov, and B. Bhattarai (2012). Inducing crosslingual distributed representations of words. See [col](#page-507-18) [\(2012\)](#page-507-18), pp. 1459–1474.
- <span id="page-540-8"></span>Klenner, M. (2007). Enforcing consistency on coreference sets. In *Recent Advances in Natural Language Processing (RANLP)*, pp. 323–328.
- <span id="page-540-10"></span>Knight, K. (1999). Decoding complexity in word-replacement translation models. *Computational Linguistics 25*(4), 607–615.
- <span id="page-540-12"></span>Knight, K. and J. Graehl (1998). Machine transliteration. *Computational Linguistics 24*(4), 599–612.
- <span id="page-540-14"></span>Knight, K. and D. Marcu (2000). Statistics-based summarization-step one: Sentence compression. In *Proceedings of the National Conference on Artificial Intelligence (AAAI)*, pp. 703–710.
- <span id="page-540-0"></span>Knight, K. and J. May (2009). Applications of weighted automata in natural language processing. In *Handbook of Weighted Automata*, pp. 571–596. Springer.
- <span id="page-540-9"></span>Knott, A. (1996). *A data-driven methodology for motivating a set of coherence relations*. Ph. D. thesis, The University of Edinburgh.
- <span id="page-540-11"></span>Koehn, P. (2005). Europarl: A parallel corpus for statistical machine translation. In *MT summit*, Volume 5, pp. 79–86.

- <span id="page-541-0"></span>Koehn, P. (2009). *Statistical machine translation*. Cambridge University Press.
- <span id="page-541-14"></span>Koehn, P. (2017). Neural machine translation. *arXiv preprint arXiv:1709.07809*.
- <span id="page-541-15"></span>Konstas, I. and M. Lapata (2013). A global model for concept-to-text generation. *Journal of Artificial Intelligence Research 48*, 305–346.
- <span id="page-541-12"></span>Koo, T., X. Carreras, and M. Collins (2008). Simple semi-supervised dependency parsing. See [acl](#page-507-4) [\(2008\)](#page-507-4), pp. 595–603.
- <span id="page-541-11"></span>Koo, T. and M. Collins (2005). Hidden-variable models for discriminative reranking. See [emn](#page-506-2) [\(2005\)](#page-506-2), pp. 507–514.
- <span id="page-541-6"></span>Koo, T. and M. Collins (2010). Efficient third-order dependency parsers. See [acl](#page-507-22) [\(2010\)](#page-507-22).
- <span id="page-541-7"></span>Koo, T., A. Globerson, X. Carreras, and M. Collins (2007). Structured prediction models via the matrix-tree theorem. See [emn](#page-507-12) [\(2007\)](#page-507-12), pp. 141–150.
- <span id="page-541-13"></span>Kovach, B. and T. Rosenstiel (2014). *The elements of journalism: What newspeople should know and the public should expect*. Three Rivers Press.
- <span id="page-541-10"></span>Krishnamurthy, J. (2016). Probabilistic models for learning a semantic parser lexicon. See [naa](#page-508-16) [\(2016\)](#page-508-16), pp. 606–616.
- <span id="page-541-8"></span>Krishnamurthy, J. and T. M. Mitchell (2012). Weakly supervised training of semantic parsers. See [emn](#page-507-3) [\(2012\)](#page-507-3), pp. 754–765.
- <span id="page-541-1"></span>Krizhevsky, A., I. Sutskever, and G. E. Hinton (2012). Imagenet classification with deep convolutional neural networks. In *Neural Information Processing Systems (NIPS)*, pp. 1097–1105.
- <span id="page-541-4"></span>Kubler, S., R. McDonald, and J. Nivre (2009). Dependency parsing. ¨ *Synthesis Lectures on Human Language Technologies 1*(1), 1–127.
- <span id="page-541-5"></span>Kuhlmann, M. and J. Nivre (2010). Transition-based techniques for non-projective dependency parsing. *Northern European Journal of Language Technology (NEJLT) 2*(1), 1–19.
- <span id="page-541-2"></span>Kummerfeld, J. K., T. Berg-Kirkpatrick, and D. Klein (2015). An empirical analysis of optimization for max-margin NLP. See [emn](#page-508-9) [\(2015\)](#page-508-9).
- <span id="page-541-9"></span>Kwiatkowski, T., S. Goldwater, L. Zettlemoyer, and M. Steedman (2012). A probabilistic model of syntactic and semantic acquisition from child-directed utterances and their meanings. See [eac](#page-507-23) [\(2012\)](#page-507-23), pp. 234–244.
- <span id="page-541-3"></span>Lafferty, J., A. McCallum, and F. Pereira (2001). Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In *Proceedings of the International Conference on Machine Learning (ICML)*.

- <span id="page-542-11"></span>Lakoff, G. (1973). Hedges: A study in meaning criteria and the logic of fuzzy concepts. *Journal of philosophical logic 2*(4), 458–508.
- <span id="page-542-3"></span>Lample, G., M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer (2016). Neural architectures for named entity recognition. See [naa](#page-508-16) [\(2016\)](#page-508-16), pp. 260–270.
- <span id="page-542-12"></span>Langkilde, I. and K. Knight (1998). Generation that exploits corpus-based statistical knowledge. In *Proceedings of the Association for Computational Linguistics (ACL)*, pp. 704– 710.
- <span id="page-542-9"></span>Lapata, M. (2003). Probabilistic text structuring: Experiments with sentence ordering. See [acl](#page-506-15) [\(2003\)](#page-506-15), pp. 545–552.
- <span id="page-542-6"></span>Lappin, S. and H. J. Leass (1994). An algorithm for pronominal anaphora resolution. *Computational linguistics 20*(4), 535–561.
- <span id="page-542-4"></span>Lari, K. and S. J. Young (1990). The estimation of stochastic context-free grammars using the inside-outside algorithm. *Computer speech & language 4*(1), 35–56.
- <span id="page-542-10"></span>Lascarides, A. and N. Asher (2007). Segmented discourse representation theory: Dynamic semantics with discourse structure. In *Computing meaning*, pp. 87–124. Springer.
- <span id="page-542-2"></span>Law, E. and L. v. Ahn (2011). Human computation. *Synthesis Lectures on Artificial Intelligence and Machine Learning 5*(3), 1–121.
- <span id="page-542-13"></span>Lebret, R., D. Grangier, and M. Auli (2016). Neural text generation from structured data with application to the biography domain. See [emn](#page-508-8) [\(2016\)](#page-508-8), pp. 1203–1213.
- <span id="page-542-0"></span>LeCun, Y., L. Bottou, G. B. Orr, and K.-R. Muller (1998). Efficient backprop. In ¨ *Neural networks: Tricks of the trade*, pp. 9–50. Springer.
- <span id="page-542-1"></span>Lee, C. M. and S. S. Narayanan (2005). Toward detecting emotions in spoken dialogs. *IEEE Transactions on speech and audio processing 13*(2), 293–303.
- <span id="page-542-5"></span>Lee, H., A. Chang, Y. Peirsman, N. Chambers, M. Surdeanu, and D. Jurafsky (2013). Deterministic coreference resolution based on entity-centric, precision-ranked rules. *Computational Linguistics 39*(4), 885–916.
- <span id="page-542-7"></span>Lee, H., Y. Peirsman, A. Chang, N. Chambers, M. Surdeanu, and D. Jurafsky (2011). Stanford's multi-pass sieve coreference resolution system at the conll-2011 shared task. See [con](#page-507-5) [\(2011\)](#page-507-5), pp. 28–34.
- <span id="page-542-8"></span>Lee, K., L. He, M. Lewis, and L. Zettlemoyer (2017). End-to-end neural coreference resolution. See [emn](#page-509-9) [\(2017\)](#page-509-9).

- <span id="page-543-8"></span>Lee, K., L. He, and L. Zettlemoyer (2018). Higher-order coreference resolution with coarseto-fine inference. See [naa](#page-509-12) [\(2018\)](#page-509-12), pp. 687–692.
- <span id="page-543-11"></span>Lenat, D. B., R. V. Guha, K. Pittman, D. Pratt, and M. Shepherd (1990). Cyc: toward programs with common sense. *Communications of the ACM 33*(8), 30–49.
- <span id="page-543-1"></span>Lesk, M. (1986). Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In *Proceedings of the 5th annual international conference on Systems documentation*, pp. 24–26. ACM.
- <span id="page-543-0"></span>Levesque, H. J., E. Davis, and L. Morgenstern (2011). The Winograd schema challenge. In *AAAI spring symposium: Logical formalizations of commonsense reasoning*.
- <span id="page-543-13"></span>Levin, E., R. Pieraccini, and W. Eckert (1998). Using markov decision process for learning dialogue strategies. In *Proceedings of the International Conference on Acoustics, Speech and Signal Processing*, Volume 1, pp. 201–204. IEEE.
- <span id="page-543-4"></span>Levy, O. and Y. Goldberg (2014). Dependency-based word embeddings. See [acl](#page-508-20) [\(2014\)](#page-508-20), pp. 302–308.
- <span id="page-543-5"></span>Levy, O., Y. Goldberg, and I. Dagan (2015). Improving distributional similarity with lessons learned from word embeddings. *Transactions of the Association for Computational Linguistics 3*, 211–225.
- <span id="page-543-2"></span>Levy, R. and C. Manning (2009). An informal introduction to computational semantics. [http://idiom.ucsd.edu/˜rlevy/teaching/winter2009/ligncse256/](http://idiom.ucsd.edu/~rlevy/teaching/winter2009/ligncse256/lectures/lecture_14_compositional_semantics.pdf) [lectures/lecture\\_14\\_compositional\\_semantics.pdf](http://idiom.ucsd.edu/~rlevy/teaching/winter2009/ligncse256/lectures/lecture_14_compositional_semantics.pdf).
- <span id="page-543-7"></span>Lewis, M. and M. Steedman (2013). Combined distributional and logical semantics. *Transactions of the Association for Computational Linguistics 1*, 179–192.
- <span id="page-543-12"></span>Lewis II, P. M. and R. E. Stearns (1968). Syntax-directed transduction. *Journal of the ACM 15*(3), 465–488.
- <span id="page-543-3"></span>Li, J. and D. Jurafsky (2015). Do multi-sense embeddings improve natural language understanding? See [emn](#page-508-9) [\(2015\)](#page-508-9), pp. 1722–1732.
- <span id="page-543-10"></span>Li, J. and D. Jurafsky (2017). Neural net models of open-domain discourse coherence. See [emn](#page-509-9) [\(2017\)](#page-509-9), pp. 198–209.
- <span id="page-543-9"></span>Li, J., R. Li, and E. Hovy (2014). Recursive deep models for discourse parsing. See [emn](#page-508-7) [\(2014\)](#page-508-7).
- <span id="page-543-6"></span>Li, J., M.-T. Luong, and D. Jurafsky (2015). A hierarchical neural autoencoder for paragraphs and documents. See [emn](#page-508-9) [\(2015\)](#page-508-9).

- <span id="page-544-9"></span>Li, J., T. Luong, D. Jurafsky, and E. Hovy (2015). When are tree structures necessary for deep learning of representations? See [emn](#page-508-9) [\(2015\)](#page-508-9), pp. 2304–2314.
- <span id="page-544-15"></span>Li, J., W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and J. Gao (2016). Deep reinforcement learning for dialogue generation. See [emn](#page-508-8) [\(2016\)](#page-508-8), pp. 1192–1202.
- <span id="page-544-11"></span>Li, Q., S. Anzaroot, W.-P. Lin, X. Li, and H. Ji (2011). Joint inference for cross-document information extraction. In *Proceedings of the International Conference on Information and Knowledge Management (CIKM)*, pp. 2225–2228.
- <span id="page-544-12"></span>Li, Q., H. Ji, and L. Huang (2013). Joint event extraction via structured prediction with global features. See [acl](#page-508-12) [\(2013\)](#page-508-12), pp. 73–82.
- <span id="page-544-8"></span>Liang, P. (2005). Semi-supervised learning for natural language. Master's thesis, Massachusetts Institute of Technology.
- <span id="page-544-5"></span>Liang, P., A. Bouchard-Cotˆ e, D. Klein, and B. Taskar (2006). An end-to-end discriminative ´ approach to machine translation. See [acl](#page-506-3) [\(2006\)](#page-506-3), pp. 761–768.
- <span id="page-544-14"></span>Liang, P., M. Jordan, and D. Klein (2009). Learning semantic correspondences with less supervision. See [acl](#page-507-8) [\(2009\)](#page-507-8), pp. 91–99.
- <span id="page-544-4"></span>Liang, P., M. I. Jordan, and D. Klein (2013). Learning dependency-based compositional semantics. *Computational Linguistics 39*(2), 389–446.
- <span id="page-544-1"></span>Liang, P. and D. Klein (2009). Online EM for unsupervised models. See [naa](#page-507-14) [\(2009\)](#page-507-14), pp. 611–619.
- <span id="page-544-3"></span>Liang, P., S. Petrov, M. I. Jordan, and D. Klein (2007). The infinite PCFG using hierarchical Dirichlet processes. See [emn](#page-507-12) [\(2007\)](#page-507-12), pp. 688–697.
- <span id="page-544-6"></span>Liang, P. and C. Potts (2015). Bringing machine learning and compositional semantics together. *Annual Review of Linguistics 1*(1), 355–376.
- <span id="page-544-2"></span>Lieber, R. (2015). *Introducing morphology*. Cambridge University Press.
- <span id="page-544-7"></span>Lin, D. (1998). Automatic retrieval and clustering of similar words. See [col](#page-506-4) [\(1998\)](#page-506-4), pp. 768–774.
- <span id="page-544-0"></span>Lin, J. and C. Dyer (2010). Data-intensive text processing with mapreduce. *Synthesis Lectures on Human Language Technologies 3*(1), 1–177.
- <span id="page-544-13"></span>Lin, Z., M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou, and Y. Bengio (2017). A structured self-attentive sentence embedding. See [icl](#page-509-14) [\(2017\)](#page-509-14).
- <span id="page-544-10"></span>Lin, Z., M.-Y. Kan, and H. T. Ng (2009). Recognizing implicit discourse relations in the penn discourse treebank. See [emn](#page-507-19) [\(2009\)](#page-507-19), pp. 343–351.

- <span id="page-545-8"></span>Lin, Z., H. T. Ng, and M.-Y. Kan (2011). Automatically evaluating text coherence using discourse relations. See [acl](#page-507-16) [\(2011\)](#page-507-16), pp. 997–1006.
- <span id="page-545-6"></span>Lin, Z., H. T. Ng, and M.-Y. Kan (2014). A pdtb-styled end-to-end discourse parser. *Natural Language Engineering 20*(2), 151–184.
- <span id="page-545-5"></span>Ling, W., C. Dyer, A. Black, and I. Trancoso (2015). Two/too simple adaptations of word2vec for syntax problems. See [naa](#page-508-17) [\(2015\)](#page-508-17).
- <span id="page-545-3"></span>Ling, W., T. Lu´ıs, L. Marujo, R. F. Astudillo, S. Amir, C. Dyer, A. W. Black, and I. Trancoso (2015). Finding function in form: Compositional character models for open vocabulary word representation. See [emn](#page-508-9) [\(2015\)](#page-508-9).
- <span id="page-545-12"></span>Ling, W., G. Xiang, C. Dyer, A. Black, and I. Trancoso (2013). Microblogs as parallel corpora. See [acl](#page-508-12) [\(2013\)](#page-508-12).
- <span id="page-545-10"></span>Ling, X., S. Singh, and D. S. Weld (2015). Design challenges for entity linking. *Transactions of the Association for Computational Linguistics 3*, 315–328.
- <span id="page-545-11"></span>Linguistic Data Consortium (2005). ACE (automatic content extraction) English annotation guidelines for relations. Technical Report Version 5.8.3, Linguistic Data Consortium.
- <span id="page-545-1"></span>Liu, B. (2015). *Sentiment Analysis: Mining Opinions, Sentiments, and Emotions*. Cambridge University Press.
- <span id="page-545-0"></span>Liu, D. C. and J. Nocedal (1989). On the limited memory BFGS method for large scale optimization. *Mathematical programming 45*(1-3), 503–528.
- <span id="page-545-13"></span>Liu, Y., Q. Liu, and S. Lin (2006). Tree-to-string alignment template for statistical machine translation. See [acl](#page-506-3) [\(2006\)](#page-506-3), pp. 609–616.
- <span id="page-545-2"></span>Loper, E. and S. Bird (2002). NLTK: The natural language toolkit. In *Proceedings of the Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics*, pp. 63–70. Association for Computational Linguistics.
- <span id="page-545-7"></span>Louis, A., A. Joshi, and A. Nenkova (2010). Discourse indicators for content selection in summarization. In *Proceedings of the Special Interest Group on Discourse and Dialogue (SIGDIAL)*, pp. 147–156.
- <span id="page-545-9"></span>Louis, A. and A. Nenkova (2013). What makes writing great? first experiments on article quality prediction in the science journalism domain. *Transactions of the Association for Computational Linguistics 1*, 341–352.

<span id="page-545-4"></span>Loveland, D. W. (2016). *Automated Theorem Proving: a logical basis*. Elsevier.

- <span id="page-546-14"></span>Lowe, R., N. Pow, I. V. Serban, and J. Pineau (2015). The Ubuntu Dialogue Corpus: A large dataset for research in unstructured multi-turn dialogue systems. In *Proceedings of the Special Interest Group on Discourse and Dialogue (SIGDIAL)*.
- <span id="page-546-7"></span>Luo, X. (2005). On coreference resolution performance metrics. See [emn](#page-506-2) [\(2005\)](#page-506-2), pp. 25–32.
- <span id="page-546-6"></span>Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos (2004). A mentionsynchronous coreference resolution algorithm based on the bell tree. See [acl](#page-506-11) [\(2004\)](#page-506-11).
- <span id="page-546-5"></span>Luong, M.-T., R. Socher, and C. D. Manning (2013). Better word representations with recursive neural networks for morphology.
- <span id="page-546-12"></span>Luong, T., H. Pham, and C. D. Manning (2015). Effective approaches to attention-based neural machine translation. See [emn](#page-508-9) [\(2015\)](#page-508-9), pp. 1412–1421.
- <span id="page-546-13"></span>Luong, T., I. Sutskever, Q. Le, O. Vinyals, and W. Zaremba (2015). Addressing the rare word problem in neural machine translation. See [acl](#page-508-14) [\(2015\)](#page-508-14), pp. 11–19.
- <span id="page-546-2"></span>Maas, A. L., A. Y. Hannun, and A. Y. Ng (2013). Rectifier nonlinearities improve neural network acoustic models. See [icm](#page-508-3) [\(2013\)](#page-508-3).
- <span id="page-546-4"></span>Magerman, D. M. (1995). Statistical decision-tree models for parsing. In *Proceedings of the Association for Computational Linguistics (ACL)*, pp. 276–283.
- <span id="page-546-15"></span>Mairesse, F. and M. A. Walker (2011). Controlling user perceptions of linguistic style: Trainable generation of personality traits. *Computational Linguistics 37*(3), 455–488.
- <span id="page-546-11"></span>Mani, I., M. Verhagen, B. Wellner, C. M. Lee, and J. Pustejovsky (2006). Machine learning of temporal relations. See [acl](#page-506-3) [\(2006\)](#page-506-3), pp. 753–760.
- <span id="page-546-8"></span>Mann, W. C. and S. A. Thompson (1988). Rhetorical structure theory: Toward a functional theory of text organization. *Text 8*(3), 243–281.
- <span id="page-546-1"></span>Manning, C. D. (2015). Last words: Computational linguistics and deep learning. *Computational Linguistics 41*(4), 701–707.
- <span id="page-546-0"></span>Manning, C. D., P. Raghavan, H. Schutze, et al. (2008). ¨ *Introduction to information retrieval*, Volume 1. Cambridge university press.
- <span id="page-546-3"></span>Manning, C. D. and H. Schutze (1999). ¨ *Foundations of Statistical Natural Language Processing*. Cambridge, Massachusetts: MIT press.
- <span id="page-546-9"></span>Marcu, D. (1996). Building up rhetorical structure trees. In *Proceedings of the National Conference on Artificial Intelligence*, pp. 1069–1074.
- <span id="page-546-10"></span>Marcu, D. (1997a). From discourse structures to text summaries. In *Proceedings of the workshop on Intelligent Scalable Text Summarization*.

- <span id="page-547-12"></span>Marcu, D. (1997b). From local to global coherence: A bottom-up approach to text planning. In *Proceedings of the National Conference on Artificial Intelligence (AAAI)*, pp. 629–635.
- <span id="page-547-0"></span>Marcus, M. P., M. A. Marcinkiewicz, and B. Santorini (1993). Building a large annotated corpus of English: The Penn Treebank. *Computational Linguistics 19*(2), 313–330.
- <span id="page-547-1"></span>Maron, O. and T. Lozano-Perez (1998). A framework for multiple-instance learning. In ´ *Neural Information Processing Systems (NIPS)*, pp. 570–576.
- <span id="page-547-10"></span>Marquez, G. G. (1970). ´ *One Hundred Years of Solitude*. Harper & Row. English translation by Gregory Rabassa.
- <span id="page-547-5"></span>Martins, A. F. T., N. A. Smith, and E. P. Xing (2009). Concise integer linear programming formulations for dependency parsing. See [acl](#page-507-8) [\(2009\)](#page-507-8), pp. 342–350.
- <span id="page-547-6"></span>Martins, A. F. T., N. A. Smith, E. P. Xing, P. M. Q. Aguiar, and M. A. T. Figueiredo (2010). Turbo parsers: Dependency parsing by approximate variational inference. See [emn](#page-507-0) [\(2010\)](#page-507-0), pp. 34–44.
- <span id="page-547-3"></span>Matsuzaki, T., Y. Miyao, and J. Tsujii (2005). Probabilistic cfg with latent annotations. See [acl](#page-506-7) [\(2005\)](#page-506-7), pp. 75–82.
- <span id="page-547-9"></span>Matthiessen, C. and J. A. Bateman (1991). *Text generation and systemic-functional linguistics: experiences from English and Japanese*. Pinter Publishers.
- <span id="page-547-2"></span>McCallum, A. and W. Li (2003). Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. See [naa](#page-506-17) [\(2003\)](#page-506-17), pp. 188–191.
- <span id="page-547-11"></span>McCallum, A. and B. Wellner (2004). Conditional models of identity uncertainty with application to noun coreference. In *Neural Information Processing Systems (NIPS)*, pp. 905–912.
- <span id="page-547-7"></span>McDonald, R., K. Crammer, and F. Pereira (2005). Online large-margin training of dependency parsers. See [acl](#page-506-7) [\(2005\)](#page-506-7), pp. 91–98.
- <span id="page-547-8"></span>McDonald, R., K. Hannan, T. Neylon, M. Wells, and J. Reynar (2007). Structured models for fine-to-coarse sentiment analysis. See [acl](#page-507-6) [\(2007\)](#page-507-6).
- <span id="page-547-4"></span>McDonald, R. and F. Pereira (2006). Online learning of approximate dependency parsing algorithms. See [eac](#page-506-6) [\(2006\)](#page-506-6).
- <span id="page-547-13"></span>McKeown, K. (1992). *Text generation*. Cambridge University Press.
- <span id="page-547-14"></span>McKeown, K., S. Rosenthal, K. Thadani, and C. Moore (2010). Time-efficient creation of an accurate sentence fusion corpus. See [naa](#page-507-2) [\(2010\)](#page-507-2), pp. 317–320.

- <span id="page-548-13"></span>McKeown, K. R., R. Barzilay, D. Evans, V. Hatzivassiloglou, J. L. Klavans, A. Nenkova, C. Sable, B. Schiffman, and S. Sigelman (2002). Tracking and summarizing news on a daily basis with Columbia's Newsblaster. In *Proceedings of the second international conference on Human Language Technology Research*, pp. 280–285.
- <span id="page-548-6"></span>McNamee, P. and H. T. Dang (2009). Overview of the tac 2009 knowledge base population track. In *Text Analysis Conference (TAC)*, Volume 17, pp. 111–113.
- <span id="page-548-8"></span>Medlock, B. and T. Briscoe (2007). Weakly supervised learning for hedge classification in scientific literature. See [acl](#page-507-6) [\(2007\)](#page-507-6), pp. 992–999.
- <span id="page-548-10"></span>Mei, H., M. Bansal, and M. R. Walter (2016). What to talk about and how? selective generation using lstms with coarse-to-fine alignment. See [naa](#page-508-16) [\(2016\)](#page-508-16), pp. 720–730.
- <span id="page-548-3"></span>Merity, S., N. S. Keskar, and R. Socher (2018). Regularizing and optimizing LSTM language models. See [icl](#page-509-10) [\(2018\)](#page-509-10).
- <span id="page-548-11"></span>Merity, S., C. Xiong, J. Bradbury, and R. Socher (2017). Pointer sentinel mixture models. See [icl](#page-509-14) [\(2017\)](#page-509-14).
- <span id="page-548-9"></span>Messud, C. (2014, June). A new 'l'etranger'. ´ *New York Review of Books*.
- <span id="page-548-12"></span>Miao, Y. and P. Blunsom (2016). Language as a latent variable: Discrete generative models for sentence compression. See [emn](#page-508-8) [\(2016\)](#page-508-8), pp. 319–328.
- <span id="page-548-5"></span>Miao, Y., L. Yu, and P. Blunsom (2016). Neural variational inference for text processing. In *Proceedings of the International Conference on Machine Learning (ICML)*.
- <span id="page-548-0"></span>Mihalcea, R., T. A. Chklovski, and A. Kilgarriff (2004). The SENSEVAL-3 English lexical sample task. In *Proceedings of SENSEVAL-3*, Barcelona, Spain, pp. 25–28. Association for Computational Linguistics.
- <span id="page-548-7"></span>Mihalcea, R. and D. Radev (2011). *Graph-based natural language processing and information retrieval*. Cambridge University Press.
- <span id="page-548-4"></span>Mikolov, T., K. Chen, G. Corrado, and J. Dean (2013). Efficient estimation of word representations in vector space. In *Proceedings of International Conference on Learning Representations*.
- <span id="page-548-2"></span>Mikolov, T., A. Deoras, D. Povey, L. Burget, and J. Cernocky (2011). Strategies for training large scale neural network language models. In *Proceedings of the Workshop on Automatic Speech Recognition and Understanding (ASRU)*, pp. 196–201.
- <span id="page-548-1"></span>Mikolov, T., M. Karafiat, L. Burget, J. Cernock ´ y, and S. Khudanpur (2010). Recurrent ` neural network based language model. In *INTERSPEECH*, pp. 1045–1048.

- <span id="page-549-9"></span>Mikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and J. Dean (2013). Distributed representations of words and phrases and their compositionality. In *Advances in Neural Information Processing Systems*, pp. 3111–3119.
- <span id="page-549-10"></span>Mikolov, T., W.-t. Yih, and G. Zweig (2013). Linguistic regularities in continuous space word representations. See [naa](#page-508-15) [\(2013\)](#page-508-15), pp. 746–751.
- <span id="page-549-4"></span>Mikolov, T. and G. Zweig. Context dependent recurrent neural network language model. In *Proceedings of Spoken Language Technology (SLT)*, pp. 234–239.
- <span id="page-549-0"></span>Miller, G. A., G. A. Heise, and W. Lichten (1951). The intelligibility of speech as a function of the context of the test materials. *Journal of experimental psychology 41*(5), 329.
- <span id="page-549-2"></span>Miller, M., C. Sathi, D. Wiesenthal, J. Leskovec, and C. Potts (2011). Sentiment flow through hyperlink networks. In *Proceedings of the International Conference on Web and Social Media (ICWSM)*.
- <span id="page-549-8"></span>Miller, S., J. Guinness, and A. Zamanian (2004). Name tagging with word clusters and discriminative training. See [naa](#page-506-14) [\(2004\)](#page-506-14), pp. 337–342.
- <span id="page-549-13"></span>Milne, D. and I. H. Witten (2008). Learning to link with wikipedia. In *Proceedings of the International Conference on Information and Knowledge Management (CIKM)*, pp. 509–518.
- <span id="page-549-12"></span>Miltsakaki, E. and K. Kukich (2004). Evaluation of text coherence for electronic essay scoring systems. *Natural Language Engineering 10*(1), 25–55.
- <span id="page-549-5"></span>Minka, T. P. (1999). From hidden markov models to linear dynamical systems. Tech. Rep. 531, Vision and Modeling Group of Media Lab, MIT.
- <span id="page-549-7"></span>Minsky, M. (1974). A framework for representing knowledge. Technical Report 306, MIT AI Laboratory.
- <span id="page-549-1"></span>Minsky, M. and S. Papert (1969). *Perceptrons*. MIT press.
- <span id="page-549-3"></span>Mintz, M., S. Bills, R. Snow, and D. Jurafsky (2009). Distant supervision for relation extraction without labeled data. See [acl](#page-507-8) [\(2009\)](#page-507-8), pp. 1003–1011.
- <span id="page-549-14"></span>Mirza, P., R. Sprugnoli, S. Tonelli, and M. Speranza (2014). Annotating causality in the TempEval-3 corpus. In *Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language (CAtoCL)*, pp. 10–19.
- <span id="page-549-6"></span>Misra, D. K. and Y. Artzi (2016). Neural shift-reduce ccg semantic parsing. See [emn](#page-508-8) [\(2016\)](#page-508-8).
- <span id="page-549-11"></span>Mitchell, J. and M. Lapata (2010). Composition in distributional models of semantics. *Cognitive Science 34*(8), 1388–1429.

- <span id="page-550-8"></span>Miwa, M. and M. Bansal (2016). End-to-end relation extraction using lstms on sequences and tree structures. See [acl](#page-508-1) [\(2016\)](#page-508-1), pp. 1105–1116.
- <span id="page-550-3"></span>Mnih, A. and G. Hinton (2007). Three new graphical models for statistical language modelling. In *Proceedings of the International Conference on Machine Learning (ICML)*, pp. 641– 648.
- <span id="page-550-7"></span>Mnih, A. and G. E. Hinton (2008). A scalable hierarchical distributed language model. In *Neural Information Processing Systems (NIPS)*, pp. 1081–1088.
- <span id="page-550-2"></span>Mnih, A. and Y. W. Teh (2012). A fast and simple algorithm for training neural probabilistic language models. See [icm](#page-507-10) [\(2012\)](#page-507-10).
- <span id="page-550-1"></span>Mohammad, S. M. and P. D. Turney (2013). Crowdsourcing a word–emotion association lexicon. *Computational Intelligence 29*(3), 436–465.
- <span id="page-550-5"></span>Mohri, M., F. Pereira, and M. Riley (2002). Weighted finite-state transducers in speech recognition. *Computer Speech & Language 16*(1), 69–88.
- <span id="page-550-0"></span>Mohri, M., A. Rostamizadeh, and A. Talwalkar (2012). *Foundations of machine learning*. MIT press.
- <span id="page-550-6"></span>Montague, R. (1973). The proper treatment of quantification in ordinary english. In *Approaches to natural language*, pp. 221–242. Springer.
- <span id="page-550-13"></span>Moore, J. D. and C. L. Paris (1993). Planning text for advisory dialogues: Capturing intentional and rhetorical information. *Computational Linguistics 19*(4), 651–694.
- <span id="page-550-12"></span>Morante, R. and E. Blanco (2012). \*SEM 2012 shared task: Resolving the scope and focus of negation. In *Proceedings of the First Joint Conference on Lexical and Computational Semantics*, pp. 265–274. Association for Computational Linguistics.
- <span id="page-550-11"></span>Morante, R. and W. Daelemans (2009). Learning the scope of hedge cues in biomedical texts. In *Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing*, pp. 28–36. Association for Computational Linguistics.
- <span id="page-550-10"></span>Morante, R. and C. Sporleder (2012). Modality and negation: An introduction to the special issue. *Computational linguistics 38*(2), 223–260.
- <span id="page-550-9"></span>Mostafazadeh, N., A. Grealish, N. Chambers, J. Allen, and L. Vanderwende (2016). CaTeRS: Causal and temporal relation scheme for semantic annotation of event structures. In *Proceedings of the Fourth Workshop on Events*, pp. 51–61. Association for Computational Linguistics.
- <span id="page-550-4"></span>Mueller, T., H. Schmid, and H. Schutze (2013). Efficient higher-order CRFs for morpho- ¨ logical tagging. See [emn](#page-508-6) [\(2013\)](#page-508-6), pp. 322–332.

- <span id="page-551-4"></span>Muller, C. and M. Strube (2006). Multi-level annotation of linguistic data with mmax2. ¨ *Corpus technology and language pedagogy: New resources, new tools, new methods 3*, 197– 214.
- <span id="page-551-7"></span>Muralidharan, A. and M. A. Hearst (2013). Supporting exploratory text analysis in literature study. *Literary and linguistic computing 28*(2), 283–295.
- <span id="page-551-1"></span>Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. The MIT Press.
- <span id="page-551-8"></span>Nakagawa, T., K. Inui, and S. Kurohashi (2010). Dependency tree-based sentiment classification using CRFs with hidden variables. See [naa](#page-507-2) [\(2010\)](#page-507-2), pp. 786–794.
- <span id="page-551-10"></span>Nakazawa, T., M. Yaguchi, K. Uchimoto, M. Utiyama, E. Sumita, S. Kurohashi, and H. Isahara (2016). ASPEC: Asian scientific paper excerpt corpus. See [lre](#page-508-23) [\(2016\)](#page-508-23), pp. 2204–2208.
- <span id="page-551-3"></span>Navigli, R. (2009). Word sense disambiguation: A survey. *ACM Computing Surveys 41*(2), 10.
- <span id="page-551-5"></span>Neal, R. M. and G. E. Hinton (1998). A view of the em algorithm that justifies incremental, sparse, and other variants. In *Learning in graphical models*, pp. 355–368. Springer.
- <span id="page-551-9"></span>Nenkova, A. and K. McKeown (2012). A survey of text summarization techniques. In *Mining text data*, pp. 43–76. Springer.
- <span id="page-551-11"></span>Neubig, G. (2017). Neural machine translation and sequence-to-sequence models: A tutorial. *arXiv preprint arXiv:1703.01619*.
- <span id="page-551-2"></span>Neubig, G., C. Dyer, Y. Goldberg, A. Matthews, W. Ammar, A. Anastasopoulos, M. Ballesteros, D. Chiang, D. Clothiaux, T. Cohn, K. Duh, M. Faruqui, C. Gan, D. Garrette, Y. Ji, L. Kong, A. Kuncoro, G. Kumar, C. Malaviya, P. Michel, Y. Oda, M. Richardson, N. Saphra, S. Swayamdipta, and P. Yin (2017). Dynet: The dynamic neural network toolkit.
- <span id="page-551-0"></span>Neubig, G., Y. Goldberg, and C. Dyer (2017). On-the-fly operation batching in dynamic computation graphs. See [nip](#page-509-15) [\(2017\)](#page-509-15).
- <span id="page-551-12"></span>Neubig, G., M. Sperber, X. Wang, M. Felix, A. Matthews, S. Padmanabhan, Y. Qi, D. S. Sachan, P. Arthur, P. Godard, J. Hewitt, R. Riad, and L. Wang (2018). XNMT: The extensible neural machine translation toolkit. In *Conference of the Association for Machine Translation in the Americas (AMTA)*.
- <span id="page-551-6"></span>Neuhaus, P. and N. Broker (1997). The complexity of recognition of linguistically ade- ¨ quate dependency grammars. In *Proceedings of the European Chapter of the Association for Computational Linguistics (EACL)*, pp. 337–343.

- <span id="page-552-11"></span>Newman, D., C. Chemudugunta, and P. Smyth (2006). Statistical entity-topic models. In *Proceedings of Knowledge Discovery and Data Mining (KDD)*, pp. 680–686.
- <span id="page-552-9"></span>Ng, V. (2010). Supervised noun phrase coreference research: The first fifteen years. See [acl](#page-507-22) [\(2010\)](#page-507-22), pp. 1396–1411.
- <span id="page-552-5"></span>Nguyen, D. and A. S. Dogruoz (2013). Word level language identification in online multi- ¨ lingual communication. See [emn](#page-508-6) [\(2013\)](#page-508-6).
- <span id="page-552-10"></span>Nguyen, D. T. and S. Joty (2017). A neural local coherence model. See [acl](#page-508-0) [\(2017\)](#page-508-0), pp. 1320–1330.
- <span id="page-552-2"></span>Nigam, K., A. K. McCallum, S. Thrun, and T. Mitchell (2000). Text classification from labeled and unlabeled documents using em. *Machine learning 39*(2-3), 103–134.
- <span id="page-552-12"></span>Nirenburg, S. and Y. Wilks (2001). What's in a symbol: ontology, representation and language. *Journal of Experimental & Theoretical Artificial Intelligence 13*(1), 9–23.
- <span id="page-552-7"></span>Nivre, J. (2008). Algorithms for deterministic incremental dependency parsing. *Computational Linguistics 34*(4), 513–553.
- <span id="page-552-3"></span>Nivre, J., M.-C. de Marneffe, F. Ginter, Y. Goldberg, J. Hajic, C. D. Manning, R. McDonald, ˇ S. Petrov, S. Pyysalo, N. Silveira, R. Tsarfaty, and D. Zeman (2016). Universal dependencies v1: A multilingual treebank collection. See [lre](#page-508-23) [\(2016\)](#page-508-23).
- <span id="page-552-6"></span>Nivre, J. and J. Nilsson (2005). Pseudo-projective dependency parsing. See [acl](#page-506-7) [\(2005\)](#page-506-7), pp. 99–106.
- <span id="page-552-0"></span>Novikoff, A. B. (1962). On convergence proofs on perceptrons. In *Proceedings of the Symposium on the Mathematical Theory of Automata*, Volume 12, pp. 615––622.
- <span id="page-552-13"></span>Och, F. J. and H. Ney (2003). A systematic comparison of various statistical alignment models. *Computational linguistics 29*(1), 19–51.
- <span id="page-552-1"></span>O'Connor, B., M. Krieger, and D. Ahn (2010). Tweetmotif: Exploratory search and topic summarization for twitter. In *Proceedings of the International Conference on Web and Social Media (ICWSM)*, pp. 384–385.
- <span id="page-552-8"></span>Oepen, S., M. Kuhlmann, Y. Miyao, D. Zeman, D. Flickinger, J. Hajic, A. Ivanova, and Y. Zhang (2014). Semeval 2014 task 8: Broad-coverage semantic dependency parsing. In *Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014)*, pp. 63–72.
- <span id="page-552-4"></span>Oflazer, K. and `I. Kuruoz (1994). Tagging and morphological disambiguation of turkish ¨ text. In *Proceedings of the fourth conference on Applied natural language processing*, pp. 144– 149.

- <span id="page-553-7"></span>Ohta, T., Y. Tateisi, and J.-D. Kim (2002). The genia corpus: An annotated research abstract corpus in molecular biology domain. In *Proceedings of the second international conference on Human Language Technology Research*, pp. 82–86. Morgan Kaufmann Publishers Inc.
- <span id="page-553-13"></span>Onishi, T., H. Wang, M. Bansal, K. Gimpel, and D. McAllester (2016). Who did what: A large-scale person-centered cloze dataset. See [emn](#page-508-8) [\(2016\)](#page-508-8), pp. 2230–2235.
- <span id="page-553-9"></span>Owoputi, O., B. O'Connor, C. Dyer, K. Gimpel, N. Schneider, and N. A. Smith (2013). Improved part-of-speech tagging for online conversational text with word clusters. See [naa](#page-508-15) [\(2013\)](#page-508-15), pp. 380–390.
- <span id="page-553-11"></span>Packard, W., E. M. Bender, J. Read, S. Oepen, and R. Dridan (2014). Simple negation scope resolution through deep parsing: A semantic solution to a semantic problem. See [acl](#page-508-20) [\(2014\)](#page-508-20), pp. 69–78.
- <span id="page-553-5"></span>Paice, C. D. (1990). Another stemmer. In *ACM SIGIR Forum*, Volume 24, pp. 56–61.
- <span id="page-553-0"></span>Pak, A. and P. Paroubek (2010). Twitter as a corpus for sentiment analysis and opinion mining. In *Proceedings of the Language Resources and Evaluation Conference*, pp. 1320–1326.
- <span id="page-553-12"></span>Palmer, F. R. (2001). *Mood and modality*. Cambridge University Press.
- <span id="page-553-8"></span>Palmer, M., D. Gildea, and P. Kingsbury (2005). The proposition bank: An annotated corpus of semantic roles. *Computational linguistics 31*(1), 71–106.
- <span id="page-553-6"></span>Pan, S. J. and Q. Yang (2010). A survey on transfer learning. *IEEE Transactions on knowledge and data engineering 22*(10), 1345–1359.
- <span id="page-553-3"></span>Pang, B. and L. Lee (2004). A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. See [acl](#page-506-11) [\(2004\)](#page-506-11), pp. 271–278.
- <span id="page-553-4"></span>Pang, B. and L. Lee (2005). Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. See [acl](#page-506-7) [\(2005\)](#page-506-7), pp. 115–124.
- <span id="page-553-1"></span>Pang, B. and L. Lee (2008). Opinion mining and sentiment analysis. *Foundations and trends in information retrieval 2*(1-2), 1–135.
- <span id="page-553-2"></span>Pang, B., L. Lee, and S. Vaithyanathan (2002). Thumbs up?: sentiment classification using machine learning techniques. See [emn](#page-506-10) [\(2002\)](#page-506-10), pp. 79–86.
- <span id="page-553-14"></span>Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu (2002). Bleu: a method for automatic evaluation of machine translation. See [acl](#page-506-12) [\(2002\)](#page-506-12), pp. 311–318.
- <span id="page-553-10"></span>Park, J. and C. Cardie (2012). Improving implicit discourse relation recognition through feature set optimization. See [sig](#page-508-24) [\(2012\)](#page-508-24), pp. 108–112.

<span id="page-554-7"></span>Parsons, T. (1990). *Events in the Semantics of English*, Volume 5. MIT Press.

- <span id="page-554-1"></span>Pascanu, R., T. Mikolov, and Y. Bengio (2013). On the difficulty of training recurrent neural networks. See [icm](#page-508-3) [\(2013\)](#page-508-3), pp. 1310–1318.
- <span id="page-554-13"></span>Paul, M., M. Federico, and S. Stuker (2010). Overview of the iwslt 2010 evaluation cam- ¨ paign. In *International Workshop on Spoken Language Translation (IWSLT) 2010*.
- <span id="page-554-10"></span>Pedersen, T., S. Patwardhan, and J. Michelizzi (2004). Wordnet::similarity - measuring the relatedness of concepts. See [naa](#page-506-14) [\(2004\)](#page-506-14), pp. 38–41.
- <span id="page-554-0"></span>Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay (2011). Scikit-learn: Machine learning in Python. *Journal of Machine Learning Research 12*, 2825–2830.
- <span id="page-554-5"></span>Pei, W., T. Ge, and B. Chang (2015). An effective neural network model for graph-based dependency parsing. See [acl](#page-508-14) [\(2015\)](#page-508-14), pp. 313–322.
- <span id="page-554-11"></span>Peldszus, A. and M. Stede (2013). From argument diagrams to argumentation mining in texts: A survey. *International Journal of Cognitive Informatics and Natural Intelligence (IJCINI) 7*(1), 1–31.
- <span id="page-554-12"></span>Peldszus, A. and M. Stede (2015). An annotated corpus of argumentative microtexts. In *Proceedings of the First Conference on Argumentation*.
- <span id="page-554-3"></span>Peng, F., F. Feng, and A. McCallum (2004). Chinese segmentation and new word detection using conditional random fields. In *Proceedings of the International Conference on Computational Linguistics (COLING)*, pp. 562–568.
- <span id="page-554-8"></span>Pennington, J., R. Socher, and C. Manning (2014). Glove: Global vectors for word representation. See [emn](#page-508-7) [\(2014\)](#page-508-7), pp. 1532–1543.
- <span id="page-554-4"></span>Pereira, F. and Y. Schabes (1992). Inside-outside reestimation from partially bracketed corpora. In *Proceedings of the Association for Computational Linguistics (ACL)*, pp. 128– 135.
- <span id="page-554-6"></span>Pereira, F. C. N. and S. M. Shieber (2002). *Prolog and natural-language analysis*. Microtome Publishing.
- <span id="page-554-9"></span>Peters, M. E., M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer (2018). Deep contextualized word representations. See [naa](#page-509-12) [\(2018\)](#page-509-12).
- <span id="page-554-2"></span>Peterson, W. W., T. G. Birdsall, and W. C. Fox (1954). The theory of signal detectability. *Transactions of the IRE professional group on information theory 4*(4), 171–212.

- <span id="page-555-6"></span>Petrov, S., L. Barrett, R. Thibaux, and D. Klein (2006). Learning accurate, compact, and interpretable tree annotation. See [acl](#page-506-3) [\(2006\)](#page-506-3).
- <span id="page-555-3"></span>Petrov, S., D. Das, and R. McDonald (2012). A universal part-of-speech tagset. In *Proceedings of the Language Resources and Evaluation Conference*.
- <span id="page-555-2"></span>Petrov, S. and R. McDonald (2012). Overview of the 2012 shared task on parsing the web. In *Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL)*.
- <span id="page-555-0"></span>Pinker, S. (2003). *The language instinct: How the mind creates language*. Penguin UK.
- <span id="page-555-5"></span>Pinter, Y., R. Guthrie, and J. Eisenstein (2017). Mimicking word embeddings using subword RNNs. See [emn](#page-509-9) [\(2017\)](#page-509-9).
- <span id="page-555-14"></span>Pitler, E., A. Louis, and A. Nenkova (2009). Automatic sense prediction for implicit discourse relations in text. See [acl](#page-507-8) [\(2009\)](#page-507-8).
- <span id="page-555-13"></span>Pitler, E. and A. Nenkova (2009). Using syntax to disambiguate explicit discourse connectives in text. See [acl](#page-507-8) [\(2009\)](#page-507-8), pp. 13–16.
- <span id="page-555-15"></span>Pitler, E., M. Raghupathy, H. Mehta, A. Nenkova, A. Lee, and A. Joshi (2008). Easily identifiable discourse relations. In *Proceedings of the International Conference on Computational Linguistics (COLING)*, pp. 87–90.
- <span id="page-555-4"></span>Plank, B., A. Søgaard, and Y. Goldberg (2016). Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss. See [acl](#page-508-1) [\(2016\)](#page-508-1).
- <span id="page-555-12"></span>Poesio, M., R. Stevenson, B. Di Eugenio, and J. Hitzeman (2004). Centering: A parametric theory and its instantiations. *Computational linguistics 30*(3), 309–363.
- <span id="page-555-1"></span>Polanyi, L. and A. Zaenen (2006). Contextual valence shifters. In *Computing attitude and affect in text: Theory and applications*. Springer.
- <span id="page-555-10"></span>Ponzetto, S. P. and M. Strube (2006). Exploiting semantic role labeling, wordnet and wikipedia for coreference resolution. See [naa](#page-506-18) [\(2006\)](#page-506-18), pp. 192–199.
- <span id="page-555-11"></span>Ponzetto, S. P. and M. Strube (2007). Knowledge derived from wikipedia for computing semantic relatedness. *Journal of Artificial Intelligence Research 30*, 181–212.
- <span id="page-555-9"></span>Poon, H. and P. Domingos (2008). Joint unsupervised coreference resolution with markov logic. See [emn](#page-507-1) [\(2008\)](#page-507-1), pp. 650–659.
- <span id="page-555-8"></span>Poon, H. and P. Domingos (2009). Unsupervised semantic parsing. See [emn](#page-507-19) [\(2009\)](#page-507-19), pp. 1–10.
- <span id="page-555-7"></span>Popel, M., D. Marecek, J. Stepanek, D. Zeman, and Z. Zabokrtsk ´ y (2013). Coordination ` structures in dependency treebanks. See [acl](#page-508-12) [\(2013\)](#page-508-12), pp. 517–527.

- <span id="page-556-3"></span>Popescu, A.-M., O. Etzioni, and H. Kautz (2003). Towards a theory of natural language interfaces to databases. In *Proceedings of Intelligent User Interfaces (IUI)*, pp. 149–157.
- <span id="page-556-2"></span>Poplack, S. (1980). Sometimes I'll start a sentence in Spanish y termino en Espanol: toward ˜ a typology of code-switching. *Linguistics 18*(7-8), 581–618.
- <span id="page-556-1"></span>Porter, M. F. (1980). An algorithm for suffix stripping. *Program 14*(3), 130–137.
- <span id="page-556-13"></span>Prabhakaran, V., O. Rambow, and M. Diab (2010). Automatic committed belief tagging. See [col](#page-507-7) [\(2010\)](#page-507-7), pp. 1014–1022.
- <span id="page-556-8"></span>Pradhan, S., X. Luo, M. Recasens, E. Hovy, V. Ng, and M. Strube (2014). Scoring coreference partitions of predicted mentions: A reference implementation. See [acl](#page-508-20) [\(2014\)](#page-508-20), pp. 30–35.
- <span id="page-556-7"></span>Pradhan, S., L. Ramshaw, M. Marcus, M. Palmer, R. Weischedel, and N. Xue (2011). CoNLL-2011 shared task: Modeling unrestricted coreference in OntoNotes. See [con](#page-507-5) [\(2011\)](#page-507-5), pp. 1–27.
- <span id="page-556-5"></span>Pradhan, S., W. Ward, K. Hacioglu, J. H. Martin, and D. Jurafsky (2005). Semantic role labeling using different syntactic views. See [acl](#page-506-7) [\(2005\)](#page-506-7), pp. 581–588.
- <span id="page-556-9"></span>Prasad, R., N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. Joshi, and B. Webber (2008). The Penn Discourse Treebank 2.0. In *Proceedings of the Language Resources and Evaluation Conference*.
- <span id="page-556-6"></span>Punyakanok, V., D. Roth, and W.-t. Yih (2008). The importance of syntactic parsing and inference in semantic role labeling. *Computational Linguistics 34*(2), 257–287.
- <span id="page-556-12"></span>Pustejovsky, J., P. Hanks, R. Saur´ı, A. See, R. Gaizauskas, A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro, et al. (2003). The timebank corpus. In *Proceedings of Corpus linguistics*, pp. 647–656.
- <span id="page-556-11"></span>Pustejovsky, J., B. Ingria, R. Sauri, J. Castano, J. Littman, R. Gaizauskas, A. Setzer, G. Katz, and I. Mani (2005). The specification language TimeML. In *The language of time: A reader*, pp. 545–557. Oxford University Press.
- <span id="page-556-10"></span>Qin, L., Z. Zhang, H. Zhao, Z. Hu, and E. Xing (2017). Adversarial connective-exploiting networks for implicit discourse relation classification. See [acl](#page-508-0) [\(2017\)](#page-508-0), pp. 1006–1017.
- <span id="page-556-0"></span>Qiu, G., B. Liu, J. Bu, and C. Chen (2011). Opinion word expansion and target extraction through double propagation. *Computational linguistics 37*(1), 9–27.
- <span id="page-556-4"></span>Quattoni, A., S. Wang, L.-P. Morency, M. Collins, and T. Darrell (2007). Hidden conditional random fields. *IEEE Transactions on Pattern Analysis and Machine Intelligence 29*(10).

- <span id="page-557-9"></span>Rahman, A. and V. Ng (2011). Narrowing the modeling gap: a cluster-ranking approach to coreference resolution. *Journal of Artificial Intelligence Research 40*, 469–521.
- <span id="page-557-11"></span>Rajpurkar, P., J. Zhang, K. Lopyrev, and P. Liang (2016). Squad: 100,000+ questions for machine comprehension of text. See [emn](#page-508-8) [\(2016\)](#page-508-8), pp. 2383–2392.
- <span id="page-557-13"></span>Ranzato, M., S. Chopra, M. Auli, and W. Zaremba (2016). Sequence level training with recurrent neural networks. See [icl](#page-508-21) [\(2016\)](#page-508-21).
- <span id="page-557-3"></span>Rao, D., D. Yarowsky, A. Shreevats, and M. Gupta (2010). Classifying latent user attributes in twitter. In *Proceedings of Workshop on Search and mining user-generated contents*.
- <span id="page-557-6"></span>Ratinov, L. and D. Roth (2009). Design challenges and misconceptions in named entity recognition. In *Proceedings of the Conference on Natural Language Learning (CoNLL)*, pp. 147–155.
- <span id="page-557-10"></span>Ratinov, L., D. Roth, D. Downey, and M. Anderson (2011). Local and global algorithms for disambiguation to wikipedia. See [acl](#page-507-16) [\(2011\)](#page-507-16), pp. 1375–1384.
- <span id="page-557-1"></span>Ratliff, N. D., J. A. Bagnell, and M. Zinkevich (2007). (approximate) subgradient methods for structured prediction. In *Proceedings of Artificial Intelligence and Statistics (AISTATS)*, pp. 380–387.
- <span id="page-557-5"></span>Ratnaparkhi, A. (1996). A maximum entropy model for part-of-speech tagging. In *Proceedings of Empirical Methods for Natural Language Processing (EMNLP)*, pp. 133–142.
- <span id="page-557-7"></span>Ratnaparkhi, A., J. Reynar, and S. Roukos (1994). A maximum entropy model for prepositional phrase attachment. In *Proceedings of the workshop on Human Language Technology*, pp. 250–255.
- <span id="page-557-2"></span>Read, J. (2005). Using emoticons to reduce dependency in machine learning techniques for sentiment classification. In *Proceedings of the ACL student research workshop*, pp. 43–48.
- <span id="page-557-8"></span>Reisinger, D., R. Rudinger, F. Ferraro, C. Harman, K. Rawlins, and B. V. Durme (2015). Semantic proto-roles. *Transactions of the Association for Computational Linguistics 3*, 475– 488.
- <span id="page-557-4"></span>Reisinger, J. and R. J. Mooney (2010). Multi-prototype vector-space models of word meaning. See [naa](#page-507-2) [\(2010\)](#page-507-2), pp. 109–117.
- <span id="page-557-0"></span>Reiter, E. and R. Dale (2000). *Building natural language generation systems*. Cambridge University Press.
- <span id="page-557-12"></span>Resnik, P., M. B. Olsen, and M. Diab (1999). The bible as a parallel corpus: Annotating the 'Book of 2000 Tongues'. *Computers and the Humanities 33*(1-2), 129–153.

- <span id="page-558-9"></span>Resnik, P. and N. A. Smith (2003). The web as a parallel corpus. *Computational Linguistics 29*(3), 349–380.
- <span id="page-558-1"></span>Ribeiro, F. N., M. Araujo, P. Gonc¸alves, M. A. Gonc¸alves, and F. Benevenuto (2016). ´ Sentibench-a benchmark comparison of state-of-the-practice sentiment analysis methods. *EPJ Data Science 5*(1), 1–29.
- <span id="page-558-8"></span>Richardson, M., C. J. Burges, and E. Renshaw (2013). MCTest: A challenge dataset for the open-domain machine comprehension of text. See [emn](#page-508-6) [\(2013\)](#page-508-6), pp. 193–203.
- <span id="page-558-6"></span>Riedel, S., L. Yao, and A. McCallum (2010). Modeling relations and their mentions without labeled text. In *Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML)*, pp. 148–163.
- <span id="page-558-12"></span>Riedl, M. O. and R. M. Young (2010). Narrative planning: Balancing plot and character. *Journal of Artificial Intelligence Research 39*, 217–268.
- <span id="page-558-11"></span>Rieser, V. and O. Lemon (2011). *Reinforcement learning for adaptive dialogue systems: a datadriven methodology for dialogue management and natural language generation*. Springer Science & Business Media.
- <span id="page-558-7"></span>Riloff, E. (1996). Automatically generating extraction patterns from untagged text. In *Proceedings of the National Conference on Artificial Intelligence (AAAI)*, pp. 1044–1049.
- <span id="page-558-0"></span>Riloff, E. and J. Wiebe (2003). Learning extraction patterns for subjective expressions. In *Proceedings of the 2003 conference on Empirical methods in natural language processing*, pp. 105–112. Association for Computational Linguistics.
- <span id="page-558-13"></span>Ritchie, G. (2001). Current directions in computational humour. *Artificial Intelligence Review 16*(2), 119–135.
- <span id="page-558-10"></span>Ritter, A., C. Cherry, and W. B. Dolan (2011). Data-driven response generation in social media. See [emn](#page-507-20) [\(2011\)](#page-507-20), pp. 583–593.
- <span id="page-558-5"></span>Ritter, A., S. Clark, Mausam, and O. Etzioni (2011). Named entity recognition in tweets: an experimental study. See [emn](#page-507-20) [\(2011\)](#page-507-20).
- <span id="page-558-4"></span>Roark, B., M. Saraclar, and M. Collins (2007). Discriminative n-gram language modeling. *Computer Speech & Language 21*(2), 373–392.
- <span id="page-558-2"></span>Robert, C. and G. Casella (2013). *Monte Carlo statistical methods*. Springer Science & Business Media.
- <span id="page-558-3"></span>Rosenfeld, R. (1996). A maximum entropy approach to adaptive statistical language modelling. *Computer Speech & Language 10*(3), 187–228.

- <span id="page-559-6"></span>Ross, S., G. Gordon, and D. Bagnell (2011). A reduction of imitation learning and structured prediction to no-regret online learning. See [ais](#page-507-17) [\(2011\)](#page-507-17), pp. 627–635.
- <span id="page-559-12"></span>Roy, N., J. Pineau, and S. Thrun (2000). Spoken dialogue management using probabilistic reasoning. In *Proceedings of the Association for Computational Linguistics (ACL)*, pp. 93– 100.
- <span id="page-559-5"></span>Rudinger, R., J. Naradowsky, B. Leonard, and B. Van Durme (2018). Gender bias in coreference resolution. See [naa](#page-509-12) [\(2018\)](#page-509-12).
- <span id="page-559-11"></span>Rudnicky, A. and W. Xu (1999). An agenda-based dialog management architecture for spoken language systems. In *IEEE Automatic Speech Recognition and Understanding Workshop*, Volume 13.
- <span id="page-559-10"></span>Rush, A. M., S. Chopra, and J. Weston (2015). A neural attention model for abstractive sentence summarization. See [emn](#page-508-9) [\(2015\)](#page-508-9), pp. 379–389.
- <span id="page-559-4"></span>Rush, A. M., D. Sontag, M. Collins, and T. Jaakkola (2010). On dual decomposition and linear programming relaxations for natural language processing. See [emn](#page-507-0) [\(2010\)](#page-507-0), pp. 1–11.
- <span id="page-559-0"></span>Russell, S. J. and P. Norvig (2009). *Artificial intelligence: a modern approach* (3rd ed.). Prentice Hall.
- <span id="page-559-8"></span>Rutherford, A., V. Demberg, and N. Xue (2017). A systematic study of neural discourse models for implicit discourse relation. In *Proceedings of the European Chapter of the Association for Computational Linguistics (EACL)*, pp. 281–291.
- <span id="page-559-7"></span>Rutherford, A. T. and N. Xue (2014). Discovering implicit discourse relations through brown cluster pair representation and coreference patterns. See [eac](#page-508-18) [\(2014\)](#page-508-18).
- <span id="page-559-1"></span>Sag, I. A., T. Baldwin, F. Bond, A. Copestake, and D. Flickinger (2002). Multiword expressions: A pain in the neck for nlp. In *International Conference on Intelligent Text Processing and Computational Linguistics*, pp. 1–15. Springer.
- <span id="page-559-9"></span>Sagae, K. (2009). Analysis of discourse structure with syntactic dependencies and datadriven shift-reduce parsing. In *Proceedings of the 11th International Conference on Parsing Technologies*, pp. 81–84.
- <span id="page-559-3"></span>Santos, C. D. and B. Zadrozny (2014). Learning character-level representations for partof-speech tagging. See [icm](#page-508-11) [\(2014\)](#page-508-11), pp. 1818–1826.
- <span id="page-559-2"></span>Sato, M.-A. and S. Ishii (2000). On-line em algorithm for the normalized gaussian network. *Neural computation 12*(2), 407–432.

- <span id="page-560-11"></span>Saur´ı, R. and J. Pustejovsky (2009). Factbank: a corpus annotated with event factuality. *Language resources and evaluation 43*(3), 227.
- <span id="page-560-2"></span>Saxe, A. M., J. L. McClelland, and S. Ganguli (2014). Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In *Proceedings of the International Conference on Learning Representations (ICLR)*.
- <span id="page-560-5"></span>Schank, R. C. and R. Abelson (1977). *Scripts, goals, plans, and understanding*. Hillsdale, NJ: Erlbaum.
- <span id="page-560-1"></span>Schapire, R. E. and Y. Singer (2000). BoosTexter: A boosting-based system for text categorization. *Machine learning 39*(2-3), 135–168.
- <span id="page-560-0"></span>Schaul, T., S. Zhang, and Y. LeCun (2013). No more pesky learning rates. See [icm](#page-508-3) [\(2013\)](#page-508-3), pp. 343–351.
- <span id="page-560-8"></span>Schnabel, T., I. Labutov, D. Mimno, and T. Joachims (2015). Evaluation methods for unsupervised word embeddings. See [emn](#page-508-9) [\(2015\)](#page-508-9), pp. 298–307.
- <span id="page-560-6"></span>Schneider, N., J. Flanigan, and T. O'Gorman (2015). The logic of AMR: Practical, unified, graph-based sentence semantics for NLP. See [naa](#page-508-17) [\(2015\)](#page-508-17), pp. 4–5.
- <span id="page-560-3"></span>Schutze, H. (1998). Automatic word sense discrimination. ¨ *Computational linguistics 24*(1), 97–123.
- <span id="page-560-10"></span>Schwarm, S. E. and M. Ostendorf (2005). Reading level assessment using support vector machines and statistical language models. See [acl](#page-506-7) [\(2005\)](#page-506-7), pp. 523–530.
- <span id="page-560-12"></span>See, A., P. J. Liu, and C. D. Manning (2017). Get to the point: Summarization with pointergenerator networks. See [acl](#page-508-0) [\(2017\)](#page-508-0), pp. 1073–1083.
- <span id="page-560-9"></span>Sennrich, R., B. Haddow, and A. Birch (2016). Neural machine translation of rare words with subword units. See [acl](#page-508-1) [\(2016\)](#page-508-1), pp. 1715–1725.
- <span id="page-560-14"></span>Serban, I. V., A. Sordoni, Y. Bengio, A. C. Courville, and J. Pineau (2016). Building endto-end dialogue systems using generative hierarchical neural network models. See [aaa](#page-508-22) [\(2016\)](#page-508-22), pp. 3776–3784.
- <span id="page-560-4"></span>Settles, B. (2012). Active learning. *Synthesis Lectures on Artificial Intelligence and Machine Learning 6*(1), 1–114.
- <span id="page-560-13"></span>Shang, L., Z. Lu, and H. Li (2015). Neural responding machine for short-text conversation. See [acl](#page-508-14) [\(2015\)](#page-508-14), pp. 1577–1586.
- <span id="page-560-7"></span>Shen, D. and M. Lapata (2007). Using semantic roles to improve question answering. See [emn](#page-507-12) [\(2007\)](#page-507-12), pp. 12–21.

- <span id="page-561-12"></span>Shen, S., Y. Cheng, Z. He, W. He, H. Wu, M. Sun, and Y. Liu (2016). Minimum risk training for neural machine translation. See [acl](#page-508-1) [\(2016\)](#page-508-1), pp. 1683–1692.
- <span id="page-561-8"></span>Shen, W., J. Wang, and J. Han (2015). Entity linking with a knowledge base: Issues, techniques, and solutions. *IEEE Transactions on Knowledge and Data Engineering 27*(2), 443– 460.
- <span id="page-561-3"></span>Shieber, S. M. (1985). Evidence against the context-freeness of natural language. *Linguistics and Philosophy 8*(3), 333–343.
- <span id="page-561-10"></span>Siegelmann, H. T. and E. D. Sontag (1995). On the computational power of neural nets. *Journal of computer and system sciences 50*(1), 132–150.
- <span id="page-561-7"></span>Singh, S., A. Subramanya, F. Pereira, and A. McCallum (2011). Large-scale crossdocument coreference using distributed inference and hierarchical models. See [acl](#page-507-16) [\(2011\)](#page-507-16), pp. 793–803.
- <span id="page-561-0"></span>Sipser, M. (2012). *Introduction to the Theory of Computation*. Cengage Learning.
- <span id="page-561-11"></span>Smith, D. A. and J. Eisner (2006). Minimum risk annealing for training log-linear models. See [acl](#page-506-3) [\(2006\)](#page-506-3), pp. 787–794.
- <span id="page-561-5"></span>Smith, D. A. and J. Eisner (2008). Dependency parsing by belief propagation. See [emn](#page-507-1) [\(2008\)](#page-507-1), pp. 145–156.
- <span id="page-561-6"></span>Smith, D. A. and N. A. Smith (2007). Probabilistic models of nonprojective dependency trees. See [emn](#page-507-12) [\(2007\)](#page-507-12), pp. 132–140.
- <span id="page-561-1"></span>Smith, N. A. (2011). Linguistic structure prediction. *Synthesis Lectures on Human Language Technologies 4*(2), 1–274.
- <span id="page-561-9"></span>Snover, M., B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul (2006). A study of translation edit rate with targeted human annotation. In *Proceedings of Association for Machine Translation in the Americas (AMTA)*.
- <span id="page-561-2"></span>Snow, R., B. O'Connor, D. Jurafsky, and A. Y. Ng (2008). Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. See [emn](#page-507-1) [\(2008\)](#page-507-1), pp. 254– 263.
- <span id="page-561-13"></span>Snyder, B. and R. Barzilay (2007). Database-text alignment via structured multilabel classification. See [ijc](#page-506-5) [\(2007\)](#page-506-5), pp. 1713–1718.
- <span id="page-561-4"></span>Socher, R., J. Bauer, C. D. Manning, and A. Y. Ng (2013). Parsing with compositional vector grammars. See [acl](#page-508-12) [\(2013\)](#page-508-12).

- <span id="page-562-8"></span>Socher, R., B. Huval, C. D. Manning, and A. Y. Ng (2012). Semantic compositionality through recursive matrix-vector spaces. See [emn](#page-507-3) [\(2012\)](#page-507-3), pp. 1201–1211.
- <span id="page-562-1"></span>Socher, R., A. Perelygin, J. Y. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts (2013). Recursive deep models for semantic compositionality over a sentiment treebank. See [emn](#page-508-6) [\(2013\)](#page-508-6).
- <span id="page-562-4"></span>Søgaard, A. (2013). Semi-supervised learning and domain adaptation in natural language processing. *Synthesis Lectures on Human Language Technologies 6*(2), 1–103.
- <span id="page-562-6"></span>Solorio, T. and Y. Liu (2008). Learning to predict code-switching points. See [emn](#page-507-1) [\(2008\)](#page-507-1), pp. 973–981.
- <span id="page-562-10"></span>Somasundaran, S., G. Namata, J. Wiebe, and L. Getoor (2009). Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification. See [emn](#page-507-19) [\(2009\)](#page-507-19).
- <span id="page-562-2"></span>Somasundaran, S. and J. Wiebe (2009). Recognizing stances in online debates. See [acl](#page-507-8) [\(2009\)](#page-507-8), pp. 226–234.
- <span id="page-562-5"></span>Song, L., B. Boots, S. M. Siddiqi, G. J. Gordon, and A. J. Smola (2010). Hilbert space embeddings of hidden markov models. In *Proceedings of the International Conference on Machine Learning (ICML)*, pp. 991–998.
- <span id="page-562-12"></span>Song, L., Y. Zhang, X. Peng, Z. Wang, and D. Gildea (2016). AMR-to-text generation as a traveling salesman problem. See [emn](#page-508-8) [\(2016\)](#page-508-8), pp. 2084–2089.
- <span id="page-562-0"></span>Soon, W. M., H. T. Ng, and D. C. Y. Lim (2001). A machine learning approach to coreference resolution of noun phrases. *Computational linguistics 27*(4), 521–544.
- <span id="page-562-13"></span>Sordoni, A., M. Galley, M. Auli, C. Brockett, Y. Ji, M. Mitchell, J.-Y. Nie, J. Gao, and B. Dolan (2015). A neural network approach to context-sensitive generation of conversational responses. See [naa](#page-508-17) [\(2015\)](#page-508-17).
- <span id="page-562-9"></span>Soricut, R. and D. Marcu (2003). Sentence level discourse parsing using syntactic and lexical information. See [naa](#page-506-17) [\(2003\)](#page-506-17), pp. 149–156.
- <span id="page-562-7"></span>Sowa, J. F. (2000). *Knowledge representation: logical, philosophical, and computational foundations*. Pacific Grove, CA: Brooks/Cole.
- <span id="page-562-11"></span>Sparck Jones, K. (1972). A statistical interpretation of term specificity and its application ¨ in retrieval. *Journal of documentation 28*(1), 11–21.
- <span id="page-562-3"></span>Spitkovsky, V. I., H. Alshawi, D. Jurafsky, and C. D. Manning (2010). Viterbi training improves unsupervised dependency parsing. See [con](#page-507-11) [\(2010\)](#page-507-11), pp. 9–17.

- <span id="page-563-11"></span>Sporleder, C. and M. Lapata (2005). Discourse chunking and its application to sentence compression. See [emn](#page-506-2) [\(2005\)](#page-506-2), pp. 257–264.
- <span id="page-563-4"></span>Sproat, R., A. Black, S. Chen, S. Kumar, M. Ostendorf, and C. Richards (2001). Normalization of non-standard words. *Computer Speech & Language 15*(3), 287–333.
- <span id="page-563-7"></span>Sproat, R., W. Gale, C. Shih, and N. Chang (1996). A stochastic finite-state wordsegmentation algorithm for chinese. *Computational linguistics 22*(3), 377–404.
- <span id="page-563-0"></span>Sra, S., S. Nowozin, and S. J. Wright (2012). *Optimization for machine learning*. MIT Press.
- <span id="page-563-2"></span>Srivastava, N., G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov (2014). Dropout: A simple way to prevent neural networks from overfitting. *The Journal of Machine Learning Research 15*(1), 1929–1958.
- <span id="page-563-1"></span>Srivastava, R. K., K. Greff, and J. Schmidhuber (2015). Training very deep networks. See [nip](#page-508-5) [\(2015\)](#page-508-5), pp. 2377–2385.
- <span id="page-563-13"></span>Stab, C. and I. Gurevych (2014a). Annotating argument components and relations in persuasive essays. See [col](#page-508-19) [\(2014\)](#page-508-19), pp. 1501–1510.
- <span id="page-563-12"></span>Stab, C. and I. Gurevych (2014b). Identifying argumentative discourse structures in persuasive essays. See [emn](#page-508-7) [\(2014\)](#page-508-7), pp. 46–56.
- <span id="page-563-14"></span>Stede, M. (2011, nov). *Discourse Processing*, Volume 4 of *Synthesis Lectures on Human Language Technologies*. Morgan & Claypool Publishers.
- <span id="page-563-8"></span>Steedman, M. and J. Baldridge (2011). Combinatory categorial grammar. In *Non-Transformational Syntax: Formal and Explicit Models of Grammar*. Wiley-Blackwell.
- <span id="page-563-5"></span>Stenetorp, P., S. Pyysalo, G. Topic, T. Ohta, S. Ananiadou, and J. Tsujii (2012). Brat: a ´ web-based tool for nlp-assisted text annotation. See [eac](#page-507-23) [\(2012\)](#page-507-23), pp. 102–107.
- <span id="page-563-9"></span>Stern, M., J. Andreas, and D. Klein (2017). A minimal span-based neural constituency parser. See [acl](#page-508-0) [\(2017\)](#page-508-0).
- <span id="page-563-6"></span>Stolcke, A., K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema, and M. Meteer (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. *Computational linguistics 26*(3), 339–373.
- <span id="page-563-3"></span>Stone, P. J. (1966). *The General Inquirer: A Computer Approach to Content Analysis*. The MIT Press.
- <span id="page-563-10"></span>Stoyanov, V., N. Gilbert, C. Cardie, and E. Riloff (2009). Conundrums in noun phrase coreference resolution: Making sense of the state-of-the-art. See [acl](#page-507-8) [\(2009\)](#page-507-8), pp. 656–664.

- <span id="page-564-0"></span>Strang, G. (2016). *Introduction to linear algebra* (Fifth ed.). Wellesley, MA: Wellesley-Cambridge Press.
- <span id="page-564-1"></span>Strubell, E., P. Verga, D. Belanger, and A. McCallum (2017). Fast and accurate entity recognition with iterated dilated convolutions. See [emn](#page-509-9) [\(2017\)](#page-509-9).
- <span id="page-564-10"></span>Suchanek, F. M., G. Kasneci, and G. Weikum (2007). Yago: a core of semantic knowledge. In *Proceedings of the Conference on World-Wide Web (WWW)*, pp. 697–706.
- <span id="page-564-5"></span>Sun, X., T. Matsuzaki, D. Okanohara, and J. Tsujii (2009). Latent variable perceptron algorithm for structured classification. In *Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)*, pp. 1236–1242.
- <span id="page-564-11"></span>Sun, Y., L. Lin, D. Tang, N. Yang, Z. Ji, and X. Wang (2015). Modeling mention, context and entity with neural networks for entity disambiguation. In *Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)*, pp. 1333–1339.
- <span id="page-564-3"></span>Sundermeyer, M., R. Schluter, and H. Ney (2012). LSTM neural networks for language ¨ modeling. In *Proceedings of the International Speech Communication Association (INTER-SPEECH)*.
- <span id="page-564-12"></span>Surdeanu, M., J. Tibshirani, R. Nallapati, and C. D. Manning (2012). Multi-instance multilabel learning for relation extraction. See [emn](#page-507-3) [\(2012\)](#page-507-3), pp. 455–465.
- <span id="page-564-13"></span>Sutskever, I., O. Vinyals, and Q. V. Le (2014). Sequence to sequence learning with neural networks. See [nip](#page-508-4) [\(2014\)](#page-508-4), pp. 3104–3112.
- <span id="page-564-7"></span>Sutton, R. S. and A. G. Barto (1998). *Reinforcement learning: An introduction*, Volume 1. MIT press Cambridge.
- <span id="page-564-8"></span>Sutton, R. S., D. A. McAllester, S. P. Singh, and Y. Mansour (2000). Policy gradient methods for reinforcement learning with function approximation. In *Neural Information Processing Systems (NIPS)*, pp. 1057–1063.
- <span id="page-564-4"></span>Suzuki, J., S. Takase, H. Kamigaito, M. Morishita, and M. Nagata (2018). An empirical study of building a strong baseline for constituency parsing. See [acl](#page-509-11) [\(2018\)](#page-509-11), pp. 612– 618.
- <span id="page-564-6"></span>Sweeney, L. (2013). Discrimination in online ad delivery. *Queue 11*(3), 10.
- <span id="page-564-2"></span>Taboada, M., J. Brooke, M. Tofiloski, K. Voll, and M. Stede (2011). Lexicon-based methods for sentiment analysis. *Computational linguistics 37*(2), 267–307.
- <span id="page-564-9"></span>Taboada, M. and W. C. Mann (2006). Rhetorical structure theory: Looking back and moving ahead. *Discourse studies 8*(3), 423–459.

- <span id="page-565-8"></span>Tackstr ¨ om, O., K. Ganchev, and D. Das (2015). Efficient inference and structured learning ¨ for semantic role labeling. *Transactions of the Association for Computational Linguistics 3*, 29–41.
- <span id="page-565-10"></span>Tackstr ¨ om, O., R. McDonald, and J. Uszkoreit (2012). Cross-lingual word clusters for ¨ direct transfer of linguistic structure. See [naa](#page-507-21) [\(2012\)](#page-507-21), pp. 477–487.
- <span id="page-565-9"></span>Tang, D., B. Qin, and T. Liu (2015). Document modeling with gated recurrent neural network for sentiment classification. See [emn](#page-508-9) [\(2015\)](#page-508-9), pp. 1422–1432.
- <span id="page-565-4"></span>Taskar, B., C. Guestrin, and D. Koller (2003). Max-margin markov networks. In *Neural Information Processing Systems (NIPS)*.
- <span id="page-565-0"></span>Tausczik, Y. R. and J. W. Pennebaker (2010). The psychological meaning of words: LIWC and computerized text analysis methods. *Journal of Language and Social Psychology 29*(1), 24–54.
- <span id="page-565-3"></span>Teh, Y. W. (2006). A hierarchical bayesian language model based on pitman-yor processes. See [acl](#page-506-3) [\(2006\)](#page-506-3), pp. 985–992.
- <span id="page-565-7"></span>Tesniere, L. (1966). ` *El´ements de syntaxe structurale ´* (second ed.). Paris: Klincksieck.
- <span id="page-565-11"></span>Teufel, S., J. Carletta, and M. Moens (1999). An annotation scheme for discourse-level argumentation in research articles. In *Proceedings of the European Chapter of the Association for Computational Linguistics (EACL)*, pp. 110–117.
- <span id="page-565-12"></span>Teufel, S. and M. Moens (2002). Summarizing scientific articles: experiments with relevance and rhetorical status. *Computational linguistics 28*(4), 409–445.
- <span id="page-565-2"></span>Thomas, M., B. Pang, and L. Lee (2006). Get out the vote: Determining support or opposition from Congressional floor-debate transcripts. In *Proceedings of Empirical Methods for Natural Language Processing (EMNLP)*, pp. 327–335.
- <span id="page-565-1"></span>Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. *Journal of the Royal Statistical Society. Series B (Methodological)*, 267–288.
- <span id="page-565-6"></span>Titov, I. and J. Henderson (2007). Constituent parsing with incremental sigmoid belief networks. See [acl](#page-507-6) [\(2007\)](#page-507-6), pp. 632–639.
- <span id="page-565-5"></span>Toutanova, K., D. Klein, C. D. Manning, and Y. Singer (2003). Feature-rich part-of-speech tagging with a cyclic dependency network. See [naa](#page-506-17) [\(2003\)](#page-506-17).
- <span id="page-565-13"></span>Trivedi, R. and J. Eisenstein (2013). Discourse connectors for latent subjectivity in sentiment analysis. See [naa](#page-508-15) [\(2013\)](#page-508-15), pp. 808–813.

- <span id="page-566-4"></span>Tromble, R. W. and J. Eisner (2006). A fast finite-state relaxation method for enforcing global constraints on sequence decoding. See [naa](#page-506-18) [\(2006\)](#page-506-18).
- <span id="page-566-1"></span>Tsochantaridis, I., T. Hofmann, T. Joachims, and Y. Altun (2004). Support vector machine learning for interdependent and structured output spaces. In *Proceedings of the International Conference on Machine Learning (ICML)*.
- <span id="page-566-6"></span>Tsvetkov, Y., M. Faruqui, W. Ling, G. Lample, and C. Dyer (2015). Evaluation of word vector representations by subspace alignment. See [emn](#page-508-9) [\(2015\)](#page-508-9), pp. 2049–2054.
- <span id="page-566-13"></span>Tu, Z., Z. Lu, Y. Liu, X. Liu, and H. Li (2016). Modeling coverage for neural machine translation. See [acl](#page-508-1) [\(2016\)](#page-508-1), pp. 76–85.
- <span id="page-566-5"></span>Turian, J., L. Ratinov, and Y. Bengio (2010). Word representations: a simple and general method for semi-supervised learning. See [acl](#page-507-22) [\(2010\)](#page-507-22), pp. 384–394.
- <span id="page-566-0"></span>Turing, A. M. (2009). Computing machinery and intelligence. In R. Epstein, G. Roberts, and G. Beber (Eds.), *Parsing the Turing Test*, pp. 23–65. Springer.
- <span id="page-566-7"></span>Turney, P. D. and P. Pantel (2010). From frequency to meaning: Vector space models of semantics. *Journal of Artificial Intelligence Research 37*, 141–188.
- <span id="page-566-12"></span>Tutin, A. and R. Kittredge (1992). Lexical choice in context: generating procedural texts. See [col](#page-506-16) [\(1992\)](#page-506-16), pp. 763–769.
- <span id="page-566-2"></span>Twain, M. (1997). *A Tramp Abroad*. New York: Penguin.
- <span id="page-566-3"></span>Tzeng, E., J. Hoffman, T. Darrell, and K. Saenko (2015). Simultaneous deep transfer across domains and tasks. See [icc](#page-508-2) [\(2015\)](#page-508-2), pp. 4068–4076.
- <span id="page-566-9"></span>Usunier, N., D. Buffoni, and P. Gallinari (2009). Ranking with ordered weighted pairwise classification. See [icm](#page-507-24) [\(2009\)](#page-507-24), pp. 1057–1064.
- <span id="page-566-14"></span>Uthus, D. C. and D. W. Aha (2013). The ubuntu chat corpus for multiparticipant chat analysis. In *AAAI Spring Symposium: Analyzing Microtext*, Volume 13, pp. 01.
- <span id="page-566-8"></span>Utiyama, M. and H. Isahara (2001). A statistical model for domain-independent text segmentation. See [acl](#page-506-19) [\(2001\)](#page-506-19), pp. 499–506.
- <span id="page-566-11"></span>Utiyama, M. and H. Isahara (2007). A comparison of pivot methods for phrase-based statistical machine translation. See [naa](#page-507-13) [\(2007\)](#page-507-13), pp. 484–491.
- <span id="page-566-10"></span>Uzuner, O., X. Zhang, and T. Sibanda (2009). Machine learning and rule-based approaches ¨ to assertion classification. *Journal of the American Medical Informatics Association 16*(1), 109–115.

- <span id="page-567-2"></span>Vadas, D. and J. R. Curran (2011). Parsing noun phrases in the penn treebank. *Computational Linguistics 37*(4), 753–809.
- <span id="page-567-3"></span>Van Eynde, F. (2006). NP-internal agreement and the structure of the noun phrase. *Journal of Linguistics 42*(1), 139–186.
- <span id="page-567-1"></span>Van Gael, J., A. Vlachos, and Z. Ghahramani (2009). The infinite hmm for unsupervised pos tagging. See [emn](#page-507-19) [\(2009\)](#page-507-19), pp. 678–687.
- <span id="page-567-12"></span>Vaswani, A., S. Bengio, E. Brevdo, F. Chollet, A. N. Gomez, S. Gouws, L. Jones, L. Kaiser, N. Kalchbrenner, N. Parmar, R. Sepassi, N. Shazeer, and J. Uszkoreit (2018). Tensor2tensor for neural machine translation. *CoRR abs/1803.07416*.
- <span id="page-567-10"></span>Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin (2017). Attention is all you need. See [nip](#page-509-15) [\(2017\)](#page-509-15), pp. 6000–6010.
- <span id="page-567-8"></span>Velldal, E., L. Øvrelid, J. Read, and S. Oepen (2012). Speculation and negation: Rules, rankers, and the role of syntax. *Computational linguistics 38*(2), 369–410.
- <span id="page-567-6"></span>Versley, Y. (2011). Towards finer-grained tagging of discourse connectives. In *Proceedings of the Workshop Beyound Semantics: Corpus-based Investigations of Pragmatic and Discourse Phenomena*, pp. 2–63.
- <span id="page-567-5"></span>Vilain, M., J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman (1995). A modeltheoretic coreference scoring scheme. In *Proceedings of the 6th conference on Message understanding*, pp. 45–52. Association for Computational Linguistics.
- <span id="page-567-4"></span>Vincent, P., H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. *The Journal of Machine Learning Research 11*, 3371–3408.
- <span id="page-567-9"></span>Vincze, V., G. Szarvas, R. Farkas, G. Mora, and J. Csirik (2008). The bioscope corpus: ´ biomedical texts annotated for uncertainty, negation and their scopes. *BMC bioinformatics 9*(11), S9.
- <span id="page-567-11"></span>Vinyals, O., A. Toshev, S. Bengio, and D. Erhan (2015). Show and tell: A neural image caption generator. In *Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on*, pp. 3156–3164. IEEE.
- <span id="page-567-0"></span>Viterbi, A. (1967). Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. *IEEE transactions on Information Theory 13*(2), 260–269.
- <span id="page-567-7"></span>Voll, K. and M. Taboada (2007). Not all words are created equal: Extracting semantic orientation as a function of adjective relevance. In *Proceedings of Australian Conference on Artificial Intelligence*.

- <span id="page-568-0"></span>Wager, S., S. Wang, and P. S. Liang (2013). Dropout training as adaptive regularization. See [nip](#page-508-10) [\(2013\)](#page-508-10), pp. 351–359.
- <span id="page-568-4"></span>Wainwright, M. J. and M. I. Jordan (2008). Graphical models, exponential families, and variational inference. *Foundations and Trends* <sup>R</sup> *in Machine Learning 1*(1-2), 1–305.
- <span id="page-568-6"></span>Walker, M. A. (2000). An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email. *Journal of Artificial Intelligence Research 12*, 387–416.
- <span id="page-568-13"></span>Walker, M. A., J. E. Cahn, and S. J. Whittaker (1997). Improvising linguistic style: Social and affective bases for agent personality. In *Proceedings of the first international conference on Autonomous agents*, pp. 96–105. ACM.
- <span id="page-568-5"></span>Wang, C., N. Xue, and S. Pradhan (2015). A Transition-based Algorithm for AMR Parsing. See [naa](#page-508-17) [\(2015\)](#page-508-17), pp. 366–375.
- <span id="page-568-11"></span>Wang, H., T. Onishi, K. Gimpel, and D. McAllester (2017). Emergent predication structure in hidden state vectors of neural readers. In *Proceedings of the 2nd Workshop on Representation Learning for NLP*, pp. 26–36.
- <span id="page-568-3"></span>Weaver, W. (1955). Translation. *Machine translation of languages 14*, 15–23.
- <span id="page-568-7"></span>Webber, B. (2004). D-LTAG: extending lexicalized TAG to discourse. *Cognitive Science 28*(5), 751–779.
- <span id="page-568-9"></span>Webber, B., M. Egg, and V. Kordoni (2012). Discourse structure and language technology. *Journal of Natural Language Engineering 18*(4), 437–490.
- <span id="page-568-10"></span>Webber, B. and A. Joshi (2012). Discourse structure and computation: past, present and future. In *Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries*, pp. 42–54. Association for Computational Linguistics.
- <span id="page-568-2"></span>Wei, G. C. and M. A. Tanner (1990). A Monte Carlo implementation of the EM algorithm and the poor man's data augmentation algorithms. *Journal of the American Statistical Association 85*(411), 699–704.
- <span id="page-568-1"></span>Weinberger, K., A. Dasgupta, J. Langford, A. Smola, and J. Attenberg (2009). Feature hashing for large scale multitask learning. See [icm](#page-507-24) [\(2009\)](#page-507-24), pp. 1113–1120.
- <span id="page-568-12"></span>Weizenbaum, J. (1966). Eliza—a computer program for the study of natural language communication between man and machine. *Communications of the ACM 9*(1), 36–45.
- <span id="page-568-8"></span>Wellner, B. and J. Pustejovsky (2007). Automatically identifying the arguments of discourse connectives. See [emn](#page-507-12) [\(2007\)](#page-507-12), pp. 92–101.

- <span id="page-569-15"></span>Wen, T.-H., M. Gasic, N. Mrksiˇ c, P.-H. Su, D. Vandyke, and S. Young (2015). Semantically ´ conditioned lstm-based natural language generation for spoken dialogue systems. See [emn](#page-508-9) [\(2015\)](#page-508-9), pp. 1711–1721.
- <span id="page-569-9"></span>Weston, J., S. Bengio, and N. Usunier (2011). Wsabie: Scaling up to large vocabulary image annotation. See [ijc](#page-507-15) [\(2011\)](#page-507-15), pp. 2764–2770.
- <span id="page-569-1"></span>Wiebe, J., T. Wilson, and C. Cardie (2005). Annotating expressions of opinions and emotions in language. *Language resources and evaluation 39*(2), 165–210.
- <span id="page-569-3"></span>Wieting, J., M. Bansal, K. Gimpel, and K. Livescu (2016a). CHARAGRAM: Embedding words and sentences via character n-grams. See [emn](#page-508-8) [\(2016\)](#page-508-8), pp. 1504–1515.
- <span id="page-569-4"></span>Wieting, J., M. Bansal, K. Gimpel, and K. Livescu (2016b). Towards universal paraphrastic sentence embeddings. See [icl](#page-508-21) [\(2016\)](#page-508-21).
- <span id="page-569-14"></span>Williams, J. D. and S. Young (2007). Partially observable markov decision processes for spoken dialog systems. *Computer Speech & Language 21*(2), 393–422.
- <span id="page-569-11"></span>Williams, P., R. Sennrich, M. Post, and P. Koehn (2016). Syntax-based statistical machine translation. *Synthesis Lectures on Human Language Technologies 9*(4), 1–208.
- <span id="page-569-0"></span>Wilson, T., J. Wiebe, and P. Hoffmann (2005). Recognizing contextual polarity in phraselevel sentiment analysis. See [emn](#page-506-2) [\(2005\)](#page-506-2), pp. 347–354.
- <span id="page-569-6"></span>Winograd, T. (1972). Understanding natural language. *Cognitive psychology 3*(1), 1–191.
- <span id="page-569-5"></span>Wiseman, S., A. M. Rush, and S. M. Shieber (2016). Learning global features for coreference resolution. See [naa](#page-508-16) [\(2016\)](#page-508-16), pp. 994–1004.
- <span id="page-569-12"></span>Wiseman, S., S. Shieber, and A. Rush (2017). Challenges in data-to-document generation. See [emn](#page-509-9) [\(2017\)](#page-509-9), pp. 2253–2263.
- <span id="page-569-7"></span>Wiseman, S. J., A. M. Rush, S. M. Shieber, and J. Weston (2015). Learning anaphoricity and antecedent ranking features for coreference resolution. See [acl](#page-508-14) [\(2015\)](#page-508-14).
- <span id="page-569-8"></span>Wolf, F. and E. Gibson (2005). Representing discourse coherence: A corpus-based study. *Computational Linguistics 31*(2), 249–287.
- <span id="page-569-10"></span>Wolfe, T., M. Dredze, and B. Van Durme (2017). Pocket knowledge base population. See [acl](#page-508-0) [\(2017\)](#page-508-0), pp. 305–310.
- <span id="page-569-13"></span>Wong, Y. W. and R. Mooney (2007). Generation by inverting a semantic parser that uses statistical machine translation. See [naa](#page-507-13) [\(2007\)](#page-507-13), pp. 172–179.
- <span id="page-569-2"></span>Wong, Y. W. and R. J. Mooney (2006). Learning for semantic parsing with statistical machine translation. See [naa](#page-506-18) [\(2006\)](#page-506-18), pp. 439–446.

- <span id="page-570-5"></span>Wu, B. Y. and K.-M. Chao (2004). *Spanning trees and optimization problems*. CRC Press.
- <span id="page-570-11"></span>Wu, D. (1997). Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. *Computational linguistics 23*(3), 377–403.
- <span id="page-570-10"></span>Wu, F. and D. S. Weld (2010). Open information extraction using wikipedia. See [acl](#page-507-22) [\(2010\)](#page-507-22), pp. 118–127.
- <span id="page-570-1"></span>Wu, X., R. Ward, and L. Bottou (2018). WNGrad: Learn the learning rate in gradient descent. *arXiv preprint arXiv:1803.02865*.
- <span id="page-570-0"></span>Wu, Y., M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, Łukasz Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes, and J. Dean (2016). Google's neural machine translation system: Bridging the gap between human and machine translation. *CoRR abs/1609.08144*.
- <span id="page-570-4"></span>Xia, F. (2000). The part-of-speech tagging guidelines for the Penn Chinese Treebank (3.0). Technical report, University of Pennsylvania Institute for Research in Cognitive Science.
- <span id="page-570-12"></span>Xu, K., J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, and Y. Bengio (2015). Show, attend and tell: Neural image caption generation with visual attention. See [icm](#page-508-13) [\(2015\)](#page-508-13), pp. 2048–2057.
- <span id="page-570-3"></span>Xu, W., X. Liu, and Y. Gong (2003). Document clustering based on non-negative matrix factorization. In *Proceedings of ACM SIGIR conference on Research and development in information retrieval*, pp. 267–273.
- <span id="page-570-9"></span>Xu, Y., L. Mou, G. Li, Y. Chen, H. Peng, and Z. Jin (2015). Classifying relations via long short term memory networks along shortest dependency paths. See [emn](#page-508-9) [\(2015\)](#page-508-9), pp. 1785–1794.
- <span id="page-570-8"></span>Xuan Bach, N., N. L. Minh, and A. Shimazu (2012). A reranking model for discourse segmentation using subtree features. See [sig](#page-508-24) [\(2012\)](#page-508-24).
- <span id="page-570-2"></span>Xue, N. et al. (2003). Chinese word segmentation as character tagging. *Computational Linguistics and Chinese Language Processing 8*(1), 29–48.
- <span id="page-570-7"></span>Xue, N., H. T. Ng, S. Pradhan, R. Prasad, C. Bryant, and A. T. Rutherford (2015). The CoNLL-2015 shared task on shallow discourse parsing. In *Proceedings of the Conference on Natural Language Learning (CoNLL)*.
- <span id="page-570-6"></span>Yamada, H. and Y. Matsumoto (2003). Statistical dependency analysis with support vector machines. In *Proceedings of the 8th International Workshop on Parsing Technologies (IWPT)*, pp. 195–206.

- <span id="page-571-14"></span>Yamada, K. and K. Knight (2001). A syntax-based statistical translation model. See [acl](#page-506-19) [\(2001\)](#page-506-19), pp. 523–530.
- <span id="page-571-9"></span>Yang, B. and C. Cardie (2014). Context-aware learning for sentence-level sentiment analysis with posterior regularization. See [acl](#page-508-20) [\(2014\)](#page-508-20).
- <span id="page-571-11"></span>Yang, Y., M.-W. Chang, and J. Eisenstein (2016). Toward socially-infused information extraction: Embedding authors, mentions, and entities. See [emn](#page-508-8) [\(2016\)](#page-508-8).
- <span id="page-571-4"></span>Yang, Y. and J. Eisenstein (2013). A log-linear model for unsupervised text normalization. See [emn](#page-508-6) [\(2013\)](#page-508-6).
- <span id="page-571-5"></span>Yang, Y. and J. Eisenstein (2015). Unsupervised multi-domain adaptation with feature embeddings. See [naa](#page-508-17) [\(2015\)](#page-508-17).
- <span id="page-571-12"></span>Yang, Y., W.-t. Yih, and C. Meek (2015). WikiQA: A challenge dataset for open-domain question answering. See [emn](#page-508-9) [\(2015\)](#page-508-9), pp. 2013–2018.
- <span id="page-571-10"></span>Yannakoudakis, H., T. Briscoe, and B. Medlock (2011). A new dataset and method for automatically grading esol texts. See [acl](#page-507-16) [\(2011\)](#page-507-16), pp. 180–189.
- <span id="page-571-3"></span>Yarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. In *Proceedings of the Association for Computational Linguistics (ACL)*, pp. 189–196. Association for Computational Linguistics.
- <span id="page-571-8"></span>Yee, L. C. and T. Y. Jones (2012, March). Apple CEO in China mission to clear up problems. *Reuters*. retrieved on March 26, 2017.
- <span id="page-571-0"></span>Yi, Y., C.-Y. Lai, S. Petrov, and K. Keutzer (2011). Efficient parallel cky parsing on gpus. In *Proceedings of the 12th International Conference on Parsing Technologies*, pp. 175–185.
- <span id="page-571-7"></span>Yu, C.-N. J. and T. Joachims (2009). Learning structural svms with latent variables. See [icm](#page-507-24) [\(2009\)](#page-507-24), pp. 1169–1176.
- <span id="page-571-2"></span>Yu, F. and V. Koltun (2016). Multi-scale context aggregation by dilated convolutions. See [icl](#page-508-21) [\(2016\)](#page-508-21).
- <span id="page-571-13"></span>Zaidan, O. F. and C. Callison-Burch (2011). Crowdsourcing translation: Professional quality from non-professionals. See [acl](#page-507-16) [\(2011\)](#page-507-16), pp. 1220–1229.
- <span id="page-571-6"></span>Zaremba, W., I. Sutskever, and O. Vinyals. Recurrent neural network regularization. *arXiv preprint arXiv:1409.2329*.
- <span id="page-571-1"></span>Zeiler, M. D. (2012). ADADELTA: an adaptive learning rate method. *arXiv preprint arXiv:1212.5701*.

- <span id="page-572-11"></span>Zelenko, D., C. Aone, and A. Richardella (2003). Kernel methods for relation extraction. *The Journal of Machine Learning Research 3*, 1083–1106.
- <span id="page-572-8"></span>Zelle, J. M. and R. J. Mooney (1996). Learning to parse database queries using inductive logic programming. In *Proceedings of the National Conference on Artificial Intelligence (AAAI)*, pp. 1050–1055.
- <span id="page-572-12"></span>Zeng, D., K. Liu, S. Lai, G. Zhou, and J. Zhao (2014). Relation classification via convolutional deep neural network. See [col](#page-508-19) [\(2014\)](#page-508-19), pp. 2335–2344.
- <span id="page-572-3"></span>Zettlemoyer, L. S. and M. Collins (2005). Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In *Proceedings of Uncertainty in Artificial Intelligence (UAI)*.
- <span id="page-572-0"></span>Zhang, C., S. Bengio, M. Hardt, B. Recht, and O. Vinyals (2017). Understanding deep learning requires rethinking generalization. See [icl](#page-509-14) [\(2017\)](#page-509-14).
- <span id="page-572-1"></span>Zhang, X., J. Zhao, and Y. LeCun (2015). Character-level convolutional networks for text classification. See [nip](#page-508-5) [\(2015\)](#page-508-5), pp. 649–657.
- <span id="page-572-5"></span>Zhang, Y. and S. Clark (2008). A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search. See [emn](#page-507-1) [\(2008\)](#page-507-1), pp. 562–571.
- <span id="page-572-4"></span>Zhang, Y., T. Lei, R. Barzilay, T. Jaakkola, and A. Globerson (2014). Steps to excellence: Simple inference with refined scoring of dependency trees. See [acl](#page-508-20) [\(2014\)](#page-508-20), pp. 197–207.
- <span id="page-572-6"></span>Zhang, Y. and J. Nivre (2011). Transition-based dependency parsing with rich non-local features. See [acl](#page-507-16) [\(2011\)](#page-507-16), pp. 188–193.
- <span id="page-572-7"></span>Zhang, Z. (2017). A note on counting dependency trees. *arXiv preprint arXiv:1708.08789*.
- <span id="page-572-10"></span>Zhao, J., T. Wang, M. Yatskar, e. V. Ordonez, and K.-W. Chang (2018). Gender bias in coreference resolution: Evaluation and debiasing methods. See [naa](#page-509-12) [\(2018\)](#page-509-12).
- <span id="page-572-9"></span>Zhou, J. and W. Xu (2015). End-to-end learning of semantic role labeling using recurrent neural networks. See [acl](#page-508-14) [\(2015\)](#page-508-14), pp. 1127–1137.
- <span id="page-572-13"></span>Zhu, J., Z. Nie, X. Liu, B. Zhang, and J.-R. Wen (2009). Statsnowball: a statistical approach to extracting entity relationships. In *Proceedings of the Conference on World-Wide Web (WWW)*, pp. 101–110.
- <span id="page-572-2"></span>Zhu, X., Z. Ghahramani, and J. D. Lafferty (2003). Semi-supervised learning using gaussian fields and harmonic functions. In *Proceedings of the International Conference on Machine Learning (ICML)*, pp. 912–919.

- <span id="page-573-1"></span>Zhu, X. and A. B. Goldberg (2009). Introduction to semi-supervised learning. *Synthesis lectures on artificial intelligence and machine learning 3*(1), 1–130.
- <span id="page-573-0"></span>Zipf, G. K. (1949). *Human behavior and the principle of least effort*. Addison-Wesley.
- <span id="page-573-3"></span>Zirn, C., M. Niepert, H. Stuckenschmidt, and M. Strube (2011). Fine-grained sentiment analysis with structural features. See [ijc](#page-507-15) [\(2011\)](#page-507-15), pp. 336–344.
- <span id="page-573-2"></span>Zou, W. Y., R. Socher, D. Cer, and C. D. Manning (2013). Bilingual word embeddings for phrase-based machine translation. See [emn](#page-508-6) [\(2013\)](#page-508-6), pp. 1393–1398.

# **Index**

α-conversion, [295](#page-312-3) β-reduction, β-conversion, [293](#page-310-3) n-gram, [24](#page-41-2) language model, [198](#page-215-1) p-value, [84](#page-101-4) one-tailed, [85](#page-102-1) F -MEASURE, [82](#page-99-1) balanced, [83](#page-100-3) macro, [82](#page-99-1) micro, [83](#page-100-3) WORDNET, [9,](#page-10-0) [74](#page-91-3) BLEU, [433](#page-450-3) METEOR, [435](#page-452-3) RIBES, [435](#page-452-3) semantics extra-propositional, [422](#page-439-3) ablation test, [84](#page-101-4) Abstract Meaning Representation (AMR), [307,](#page-324-4) [319](#page-336-2) accuracy, [23,](#page-40-1) [81](#page-98-1) action, in reinforcement learning, [365](#page-382-2) AdaGrad, [40,](#page-57-1) [61](#page-78-0) adequacy, in translation, [433](#page-450-3) adjectives, [177](#page-194-1) adjuncts, [306](#page-323-2) adpositions, [178](#page-195-0) adverbs, [177](#page-194-1) affix, [195](#page-212-0) inflectional, [78](#page-95-2) agent (thematic role), [308](#page-325-3)

alignment in machine translation, [432,](#page-449-1) [437](#page-454-2) in semantic parsing, [321](#page-338-1) in text generation, [459](#page-476-2) Amazon Mechanical Turk, [91](#page-108-0) ambiguity, [210,](#page-227-3) [218](#page-235-2) attachment, [257](#page-274-3) derivational, [222](#page-239-1) spurious, [222,](#page-239-1) [269,](#page-286-2) [298](#page-315-1) syntactic, [229](#page-246-3) anaphoricity, [360](#page-377-1) anchored productions, [232](#page-249-2) animacy, [307](#page-324-4) annealing, [452](#page-469-1) antecedent mention, [351,](#page-368-2) [359](#page-376-1) antonymy, [74,](#page-91-3) [281](#page-298-0) apophony, [194](#page-211-2) area under the curve (AUC), [83](#page-100-3) argumentation, [392](#page-409-1) arguments, [403](#page-420-1) article, [182](#page-199-4) aspect, [177](#page-194-1) aspect-based opinion mining, [71](#page-88-1) autoencoder, [345](#page-362-2) denoising, [345,](#page-362-2) [408](#page-425-2) variational, [465](#page-482-1) automated theorem provers, [288](#page-305-1) automatic differentiation, [56](#page-73-1) auxiliary verbs, [178](#page-195-0) average mutual information, [333](#page-350-2) averaged perceptron, [27](#page-44-2)

backchannel, [187](#page-204-3) backoff, [130](#page-147-2) Katz, [130](#page-147-2) backpropagation, [55](#page-72-1) through time, [136](#page-153-1) backward recurrence, [165,](#page-182-3) [166](#page-183-2) backward-looking center, [382](#page-399-4) bag of words, [13](#page-30-2) balanced test set, [81](#page-98-1) batch normalization, [61](#page-78-0) Baum-Welch algorithm, [170](#page-187-2) Bayes' rule, [478](#page-495-0) Bayesian nonparametrics, [103,](#page-120-1) [249](#page-266-2) beam sampling, [172](#page-189-4) beam search, [272](#page-289-3) in coreference resolution, [363](#page-380-2) in machine translation, [450,](#page-467-3) [451](#page-468-2) Bell number, [362](#page-379-2) bias, [23](#page-40-1) bias-variance tradeoff, [23,](#page-40-1) [127,](#page-144-1) [129](#page-146-2) bigrams, [24,](#page-41-2) [70](#page-87-3) bilinear product, [330](#page-347-1) binarization, [211,](#page-228-2) [228](#page-245-1) binomial distribution, [85](#page-102-1) random variable, [482](#page-499-1) test, [84](#page-101-4) BIO notation, [184,](#page-201-3) [317](#page-334-4) biomedical natural language processing, [183](#page-200-1) Bonferroni correction, [87](#page-104-2) boolean semiring, [199](#page-216-0) boosting, [48](#page-65-2) bootstrap samples, [86](#page-103-0) brevity penalty, [434](#page-451-1) Brown clusters, [327,](#page-344-2) [332](#page-349-1) byte-pair encoding, [343,](#page-360-1) [449](#page-466-3) c-command, [353](#page-370-1) case marking, [182,](#page-199-4) [219](#page-236-4) Catalan number, [225](#page-242-1)

cataphora, [352](#page-369-4) center embedding, [207](#page-224-2) centering theory, [355,](#page-372-1) [382](#page-399-4) character-level language models, [141](#page-158-3) chatbots, [470](#page-487-2) Chomsky Normal Form (CNF), [211](#page-228-2) Chu-Liu-Edmonds algorithm, [264](#page-281-1) CKY algorithm, [226](#page-243-3) class imbalance, [81](#page-98-1) classification, [13](#page-30-2) large margin, [30](#page-47-1) lexicon-based, [72](#page-89-1) weights, [14](#page-31-4) closed-vocabulary, [141](#page-158-3) closure, of classes of formal languages, [192](#page-209-1) cluster ranking, [362](#page-379-2) clustering, [96](#page-113-2) K-means, [96](#page-113-2) exchange, [333](#page-350-2) hierarchical, [332](#page-349-1) soft, [97](#page-114-1) co-hypernymy, [281](#page-298-0) co-training, [108](#page-125-1) code switching, [180,](#page-197-1) [186](#page-203-2) Cohen's Kappa, [90](#page-107-2) coherence, [396](#page-413-0) cohesion, [379,](#page-396-2) [400](#page-417-0) collective entity linking, [408](#page-425-2) collocation extraction, [344](#page-361-2) combinatory categorial grammar, [220](#page-237-2) complement clause, [213](#page-230-1) complement event (probability), [476](#page-493-2) composition (CCG), [221](#page-238-2) compositional vector grammars, [391](#page-408-2) compositionality, [2,](#page-1-0) [7,](#page-8-0) [10,](#page-11-0) [291,](#page-308-1) [341](#page-358-4) computation graph, [48,](#page-65-2) [55](#page-72-1) dynamic, [57](#page-74-2) computational linguistics (versus natural language processing), [1](#page-0-1) computational social science, [5](#page-6-0)

#### BIBLIOGRAPHY 559

concept (in Abstract Meaning Representation), [319](#page-336-2) conditional independence, [154,](#page-171-2) [480](#page-497-2) conditional random field, [162](#page-179-3) confidence interval, [86](#page-103-0) configuration (transition-based parsing), [269](#page-286-2) consistency, in logic, [291](#page-308-1) constants, in logic, [286](#page-303-1) constituents, [212](#page-229-0) split, [312](#page-329-3) constrained optimization, [315](#page-332-2) content selection, [457](#page-474-2) content words, [178](#page-195-0) context-free grammars, [208](#page-225-3) probabilistic (PCFGs), [235](#page-252-2) synchronous, [441](#page-458-3) weighted, [218,](#page-235-2) [227,](#page-244-2) [233](#page-250-1) context-free languages, [207,](#page-224-2) [208](#page-225-3) context-free production, [209](#page-226-1) recursive, [209](#page-226-1) unary, [210](#page-227-3) context-sensitive languages, [218](#page-235-2) continuous bag-of-words (CBOW), [334](#page-351-2) contradiction, [346](#page-363-2) conversational turns, [187](#page-204-3) convexity, [29,](#page-46-3) [58,](#page-75-1) [300,](#page-317-2) [482,](#page-499-1) [485](#page-502-2) biconvexity, [102](#page-119-3) convolution dilated, [64,](#page-81-2) [185](#page-202-2) narrow, [63](#page-80-1) one-dimensional, [63](#page-80-1) wide, [63](#page-80-1) convolutional neural networks, [62](#page-79-1) in machine translation, [447](#page-464-2) in relation extraction, [415](#page-432-0) in semantic role labeling, [318](#page-335-2) in sequence labeling, [170](#page-187-2) cooperative principle, [351](#page-368-2) coordinate ascent, [102](#page-119-3) coordinating conjunctions, [178](#page-195-0)

copula, [177,](#page-194-1) [217,](#page-234-0) [260](#page-277-4) coreference resolution, [347,](#page-364-3) [351](#page-368-2) cross-document, [406](#page-423-2) coreferent, [351](#page-368-2) cosine similarity, [339,](#page-356-2) [380](#page-397-4) cost function, [31](#page-48-2) cost-augmented decoding, [32](#page-49-1) coverage loss, [465](#page-482-1) critical point, [58,](#page-75-1) [485](#page-502-2) cross-entropy, [53,](#page-70-2) [416](#page-433-1) cross-serial dependencies, [219](#page-236-4) cross-validation, [24](#page-41-2) crowdsourcing, [91](#page-108-0) cumulative probability distribution, [86](#page-103-0) decidability, [291](#page-308-1) decision trees, [48](#page-65-2) decoding cost-augmented, [161](#page-178-2) in conditional random fields, [163](#page-180-2) definiteness, [183](#page-200-1) delta function, [22](#page-39-3) denotation, [286](#page-303-1) dependency, [258](#page-275-2) grammar, [257](#page-274-3) graph, [258](#page-275-2) labels, [259](#page-276-2) path, [75,](#page-92-2) [314,](#page-331-1) [413](#page-430-1) syntactic, [258](#page-275-2) dependency parsing, [257](#page-274-3) arc-eager, [269,](#page-286-2) [270](#page-287-0) arc-factored, [263](#page-280-1) arc-standard, [269](#page-286-2) pseudo-projective, [272](#page-289-3) second-order, [263](#page-280-1) third-order, [264](#page-281-1) derivation in context-free languages, [209](#page-226-1) in dependency parsing, [269](#page-286-2) in semantic parsing, [293](#page-310-3) determiner, [179](#page-196-1)

phrase, [215](#page-232-1) development set, [23,](#page-40-1) [81](#page-98-1) dialogue acts, [90,](#page-107-2) [187,](#page-204-3) [471](#page-488-0) dialogue management, [468](#page-485-2) dialogue systems, [125,](#page-142-1) [466](#page-483-1) agenda-based, [468](#page-485-2) mixed-initiative, [468](#page-485-2) digital humanities, [5,](#page-6-0) [69](#page-86-3) Dirichlet Compound Multinomial, [398](#page-415-0) Dirichlet distribution, [115](#page-132-2) discounting, [130](#page-147-2) absolute, [130](#page-147-2) discourse, [379](#page-396-2) connectives, [385](#page-402-2) depth, [394](#page-411-2) parsing, [385](#page-402-2) segment, [379](#page-396-2) unit, [389](#page-406-5) discourse relations, [347,](#page-364-3) [384](#page-401-7) coordinating, [389](#page-406-5) implicit, [385](#page-402-2) sense classification, [386](#page-403-1) subordinating, [389](#page-406-5) distant supervision, [118,](#page-135-0) [418,](#page-435-1) [419](#page-436-2) distributed semantics, [327](#page-344-2) distributional hypothesis, [325,](#page-342-6) [326](#page-343-2) semantics, [10,](#page-11-0) [327](#page-344-2) statistics, [75,](#page-92-2) [248,](#page-265-2) [326](#page-343-2) document frequency, [407](#page-424-1) domain adaptation, [95,](#page-112-2) [111](#page-128-1) by projection, [112](#page-129-3) dropout, [57,](#page-74-2) [137](#page-154-3) dual decomposition, [317](#page-334-4) dynamic programming, [149](#page-166-2) E-step, [99](#page-116-2) early stopping, [27,](#page-44-2) [61](#page-78-0) early update, [276](#page-293-1)

edit distance, [201,](#page-218-2) [435](#page-452-3) effective counts, [129](#page-146-2)

elementwise nonlinearity, [50](#page-67-5) Elman unit, [135](#page-152-2) ELMo (embeddings from language models), [340](#page-357-1) embedding, [167](#page-184-3) emotion, [71](#page-88-1) empirical Bayes, [116](#page-133-0) empty string, [192](#page-209-1) encoder-decoder model, [345,](#page-362-2) [442,](#page-459-1) [460](#page-477-3) ensemble learning, [48,](#page-65-2) [318,](#page-335-2) [444](#page-461-2) entailment, [290,](#page-307-0) [346](#page-363-2) entity, [403](#page-420-1) embeddings, [407](#page-424-1) grid, [383](#page-400-2) linking, [351,](#page-368-2) [403,](#page-420-1) [405,](#page-422-3) [416](#page-433-1) linking, collective, [409](#page-426-1) nil, [405](#page-422-3) entropy, [42,](#page-59-1) [99](#page-116-2) estimation, [482](#page-499-1) EuroParl corpus, [435](#page-452-3) evaluation extrinsic, [139,](#page-156-2) [338](#page-355-2) intrinsic, [139,](#page-156-2) [338](#page-355-2) event, [403](#page-420-1) coreference, [421](#page-438-0) detection, [421](#page-438-0) event semantics, [305](#page-322-3) events, in probability, [475](#page-492-2) disjoint, [476](#page-493-2) evidentiality, [182,](#page-199-4) [423](#page-440-1) expectation, [481](#page-498-1) expectation-maximization, [97](#page-114-1) hard, [102](#page-119-3) in language modeling, [131](#page-148-1) in machine translation, [439](#page-456-5) incremental, [102](#page-119-3) online, [102](#page-119-3) explicit semantic analysis, [328](#page-345-3) factoid question, [424](#page-441-3) factor graph, [163](#page-180-2)

factuality, [423](#page-440-1) fairness and bias, [5](#page-6-0) in machine translation, [434](#page-451-1) in word embeddings, [340](#page-357-1) false discovery rate, [87](#page-104-2) false negative, [81](#page-98-1) rate, [478](#page-495-0) false positive, [81,](#page-98-1) [479](#page-496-1) rate, [83,](#page-100-3) [478](#page-495-0) feature co-adaptation, [58](#page-75-1) function, [14](#page-31-4) hashing, [80](#page-97-2) noising, [58](#page-75-1) selection, [41](#page-58-3) features, [6](#page-7-0) bilexical, [266](#page-283-0) collocation, [75](#page-92-2) emission, [148](#page-165-0) lexical, [47](#page-64-1) offset, [15](#page-32-1) pivot, [113](#page-130-0) transition, [148](#page-165-0) finite state acceptor, [193](#page-210-2) acceptor, chain, [205](#page-222-0) acceptors, weighted, [197](#page-214-3) automata, [193](#page-210-2) automaton, deterministic, [194](#page-211-2) composition, [204](#page-221-2) transducers, [196,](#page-213-2) [201](#page-218-2) finite-state transduction, [192](#page-209-1) fluency, [125,](#page-142-1) [433](#page-450-3) formal language theory, [191](#page-208-1) forward recurrence, [164](#page-181-3) variable, [164,](#page-181-3) [166](#page-183-2) forward-backward algorithm, [165,](#page-182-3) [206,](#page-223-3) [240](#page-257-3) forward-looking centers, [382](#page-399-4)

frame, [310](#page-327-2) element, [310](#page-327-2) in dialogue systems, [467](#page-484-2) FrameNet, [310](#page-327-2) Frobenius norm, [57,](#page-74-2) [105](#page-122-2) function words, [178](#page-195-0) function, in logic, [289](#page-306-1) functional segmentation, [379,](#page-396-2) [381](#page-398-2) garden path sentence, [146](#page-163-1) gazetteer, [184,](#page-201-3) [357,](#page-374-2) [413](#page-430-1) generalization, [27](#page-44-2) of neural networks, [59](#page-76-1) generalized linear models, [42](#page-59-1) generative model for classification, [17](#page-34-5) for coreference, [363](#page-380-2) for interpolated language modeling, [131](#page-148-1) for parsing, [235](#page-252-2) for sequence labeling, [154](#page-171-2) Gibbs sampling, [115,](#page-132-2) [409](#page-426-1) collapsed, [116](#page-133-0) gloss, [125,](#page-142-1) [179,](#page-196-1) [433](#page-450-3) government and binding theory, [353](#page-370-1) gradient, [29](#page-46-3) clipping, [60](#page-77-0) descent, [38](#page-55-2) exploding, [137](#page-154-3) vanishing, [51,](#page-68-2) [137](#page-154-3) Gram matrix, [414](#page-431-3) grammar equivalence, [210](#page-227-3) grammar induction, [241](#page-258-1) grammaticality, [396](#page-413-0) graphical model, [154](#page-171-2) graphics processing units (GPUs), [170,](#page-187-2) [185](#page-202-2) grid search, [23](#page-40-1) Hamming cost, [161](#page-178-2) Hansards corpus, [435](#page-452-3)

hanzi, [77](#page-94-1) head rules, for lexicalized CFGs, [246,](#page-263-3) [257](#page-274-3) head word, [212,](#page-229-0) [244,](#page-261-2) [257,](#page-274-3) [353](#page-370-1) of a dependency edge, [258](#page-275-2) hedging, [423](#page-440-1) held-out data, [139](#page-156-2) Hessian matrix, [39](#page-56-1) hidden Markov models, [154](#page-171-2) hierarchical recurrent network, [470](#page-487-2) highway network, [52](#page-69-1) holonymy, [75](#page-92-2) homonymy, [73](#page-90-2) human computation, [91](#page-108-0) hypergraph, [392](#page-409-1) hypernymy, [75,](#page-92-2) [281](#page-298-0) hyperparameter, [22](#page-39-3) hyponymy, [75](#page-92-2) illocutionary force, [187](#page-204-3) importance sampling, [453](#page-470-0) importance score, [453](#page-470-0) independent and identically distributed (IID), [17](#page-34-5) inference in structured prediction, [147](#page-164-3) logical, [285](#page-302-1) rules for propositional logic, [288](#page-305-1) inflection point, [486](#page-503-2) information extraction, [403](#page-420-1) open, [419](#page-436-2) information retrieval, [5,](#page-6-0) [416](#page-433-1) inside recurrence, [235–](#page-252-2)[237](#page-254-1) inside-outside algorithm, [240,](#page-257-3) [249](#page-266-2) instance labels, [16](#page-33-2) instance, in Abstract Meaning Representation, [319](#page-336-2) integer linear programming, [315](#page-332-2) in coreference resolution, [362](#page-379-2) in entity linking, [409](#page-426-1) in extractive summarization, [395](#page-412-2) in sentence compression, [466](#page-483-1)

inter-annotator agreement, [90](#page-107-2) interjections, [177](#page-194-1) interlingua, [432](#page-449-1) interpolation, [131,](#page-148-1) [199](#page-216-0) interval algebra, [421](#page-438-0) inverse document frequency, [407](#page-424-1) inversion (of finite state automata), [203](#page-220-2) irrealis, [70](#page-87-3) Jensen's inequality, [99](#page-116-2) Kalman smoother, [172](#page-189-4) kernel, [48,](#page-65-2) [414](#page-431-3) Kleene star, [192](#page-209-1) knapsack problem, [395](#page-412-2) knowledge base, [403](#page-420-1) population, [416](#page-433-1) label bias problem, [275,](#page-292-2) [280](#page-297-0) label propagation, [110,](#page-127-3) [120](#page-137-0) Lagrangian, [487](#page-504-2) lambda calculus, [292](#page-309-5) lambda expressions, [292](#page-309-5) language model, [4,](#page-5-0) [126](#page-143-1) latent Dirichlet allocation, [380](#page-397-4) latent semantic analysis, [328,](#page-345-3) [329](#page-346-2) latent variable, [98,](#page-115-2) [206,](#page-223-3) [299,](#page-316-4) [419,](#page-436-2) [432](#page-449-1) conditional random field, [301](#page-318-2) in parsing, [249](#page-266-2) perceptron, [207,](#page-224-2) [300,](#page-317-2) [360](#page-377-1) layer normalization, [61,](#page-78-0) [447](#page-464-2) learning active, [118](#page-135-0) batch, [25](#page-42-1) constraint-driven, [118](#page-135-0) deep, [47](#page-64-1) discriminative, [25](#page-42-1) multiple instance, [118,](#page-135-0) [418](#page-435-1) multitask, [118](#page-135-0) online, [25,](#page-42-1) [39](#page-56-1) reinforcement, [365,](#page-382-2) [451](#page-468-2) semi-supervised, [76,](#page-93-2) [95,](#page-112-2) [339](#page-356-2)

to search, [257,](#page-274-3) [277,](#page-294-1) [366](#page-383-0) transfer, [118](#page-135-0) unsupervised, [71,](#page-88-1) [95](#page-112-2) learning rate, [38](#page-55-2) least squares, [72](#page-89-1) leave-one-out cross-validation, [24](#page-41-2) lemma, [73,](#page-90-2) [202](#page-219-2) lemmatization, [79](#page-96-2) lexical entry, [293](#page-310-3) lexical unit, [310](#page-327-2) lexicalization in parsing, [245](#page-262-1) in text generation, [457](#page-474-2) lexicalized tree-adjoining grammar for discourse (D-LTAG), [385](#page-402-2) lexicon, [293](#page-310-3) in combinatory categorial grammar, [221](#page-238-2) lexicon-based classification, [72](#page-89-1) seed, [73](#page-90-2) light verb, [321](#page-338-1) linear separability, [25](#page-42-1) linearization, [466,](#page-483-1) [472](#page-489-0) link function, [42](#page-59-1) literal character, [192](#page-209-1) local optimum, [102,](#page-119-3) [486](#page-503-2) locally-normalized objective, [275](#page-292-2) log-bilinear language model, [342](#page-359-1) log-likelihood conditional, [52](#page-69-1) log-linear models, [42](#page-59-1) logic, [287](#page-304-3) first-order, [288](#page-305-1) higher-order, [289](#page-306-1) propositional, [287](#page-304-3) logistic function, [42](#page-59-1) long short-term memory (LSTM), [52,](#page-69-1) [135,](#page-152-2) [137,](#page-154-3) [181,](#page-198-0) [442](#page-459-1) bidirectional, [169,](#page-186-2) [445](#page-462-1) deep, [443](#page-460-3) LSTM-CRF, [169,](#page-186-2) [318](#page-335-2)

memory cell, [137](#page-154-3) lookup layer, [53,](#page-70-2) [135](#page-152-2) loss function, [27](#page-44-2) hinge, [29](#page-46-3) logistic, [36](#page-53-2) WARP, [411](#page-428-2) zero-one, [28](#page-45-1) machine learning, [2](#page-1-0) supervised, [16](#page-33-2) theory, [26](#page-43-3) machine reading, [425](#page-442-2) macro, [404](#page-421-1) micro, [404](#page-421-1) machine translation, [125](#page-142-1) neural, [432](#page-449-1) statistical, [432](#page-449-1) margin, [26,](#page-43-3) [30](#page-47-1) functional, [32](#page-49-1) geometric, [32](#page-49-1) marginal relevance, [465](#page-482-1) marginalization, [477](#page-494-4) markable, [358](#page-375-2) Markov assumption, [154](#page-171-2) Markov blanket, [154](#page-171-2) Markov Chain Monte Carlo (MCMC), [103,](#page-120-1) [115,](#page-132-2) [172](#page-189-4) Markov decision process, [468](#page-485-2) partially-observable (POMDP), [470](#page-487-2) Markov random fields, [162](#page-179-3) Markovization vertical, [244](#page-261-2) matrix-tree theorem, [268](#page-285-1) max-margin Markov network, [161](#page-178-2) max-product algorithm, [157](#page-174-2) maximum a posteriori, [22,](#page-39-3) [483](#page-500-0) maximum conditional likelihood, [36](#page-53-2) maximum directed spanning tree, [264](#page-281-1) maximum entropy, [42](#page-59-1) maximum likelihood, [17,](#page-34-5) [21,](#page-38-2) [482](#page-499-1)

McNemar's test, [84](#page-101-4) meaning representation, [285](#page-302-1) membership problem, [191](#page-208-1) mention in coreference resolution, [351](#page-368-2) in entity linking, [403](#page-420-1) in information extraction, [405](#page-422-3) mention ranking, [360](#page-377-1) mention-pair model, [359](#page-376-1) meronymy, [75](#page-92-2) method of moments, [117](#page-134-1) mildly context-sensitive languages, [219](#page-236-4) minibatch, [39](#page-56-1) minimization of finite-state automata, [196](#page-213-2) minimum error-rate training (MERT), [452](#page-469-1) minimum risk training, [452](#page-469-1) modality, [422](#page-439-3) model, [8,](#page-9-0) [38](#page-55-2) modifier, [258](#page-275-2) modus ponens, [288,](#page-305-1) [303](#page-320-3) moment-matching, [42](#page-59-1) monomorphemic, [196](#page-213-2) morpheme, [6,](#page-7-0) [141,](#page-158-3) [195](#page-212-0) morphological segmentation, [159](#page-176-1) morphology, [79,](#page-96-2) [158,](#page-175-1) [194,](#page-211-2) [342,](#page-359-1) [448](#page-465-3) derivational, [194](#page-211-2) inflectional, [177,](#page-194-1) [194,](#page-211-2) [202](#page-219-2) morphosyntactic, [176](#page-193-3) attributes, [180](#page-197-1) morphotactics, [195](#page-212-0) multi-view learning, [108](#page-125-1) multiclass classification one-versus-all, [414](#page-431-3) one-versus-one, [414](#page-431-3) multinomial distribution, [18](#page-35-3) Na¨ıve Bayes, [19](#page-36-3) Na¨ıve Bayes, [17](#page-34-5)

name dictionary, [406](#page-423-2) named entity, [183](#page-200-1) linking, [405](#page-422-3) recognition, [169,](#page-186-2) [403,](#page-420-1) [405](#page-422-3) types, [405](#page-422-3) nearest-neighbor, [48,](#page-65-2) [414](#page-431-3) negation, [70,](#page-87-3) [422](#page-439-3) scope, [423](#page-440-1) negative sampling, [336,](#page-353-1) [337,](#page-354-2) [411](#page-428-2) Neo-Davidsonian event semantics, [306](#page-323-2) neural attention, [371,](#page-388-2) [426,](#page-443-1) [443,](#page-460-3) [444,](#page-461-2) [460](#page-477-3) coarse-to-fine, [463](#page-480-1) structured, [463](#page-480-1) neural gate, [52,](#page-69-1) [445](#page-462-1) neural network, [48](#page-65-2) adversarial, [113](#page-130-0) bidirectional recurrent, [168](#page-185-0) convolutional, [53,](#page-70-2) [62,](#page-79-1) [185,](#page-202-2) [415](#page-432-0) feedforward, [50](#page-67-5) recurrent, [134,](#page-151-2) [415](#page-432-0) recursive, [250,](#page-267-2) [343,](#page-360-1) [346,](#page-363-2) [395](#page-412-2) noise-contrastive estimation, [135](#page-152-2) noisy channel model, [126,](#page-143-1) [436](#page-453-2) nominal modifier, [215](#page-232-1) nominals, [351,](#page-368-2) [357](#page-374-2) non-terminals, [209](#page-226-1) normalization, [78](#page-95-2) noun phrase, [2,](#page-1-0) [212](#page-229-0) nouns, [176](#page-193-3) NP-hard, [41,](#page-58-3) [409](#page-426-1) nucleus, in RST, [389](#page-406-5) null hypothesis, [84](#page-101-4) null subjects, [375](#page-392-0) numerals, [179](#page-196-1) one-hot vector, [53](#page-70-2) online support vector machine, [31](#page-48-2) ontology, [9](#page-10-0) open word classes, [176](#page-193-3)

Jacob Eisenstein. Draft of November 13, 2018.

opinion polarity, [69](#page-86-3)

optimization

batch, [38](#page-55-2) combinatorial, [8](#page-9-0) constrained, [33](#page-50-1) convex, [38](#page-55-2) numerical, [8](#page-9-0) quasi-Newton, [39](#page-56-1) oracle, [275](#page-292-2) dynamic, [277](#page-294-1) in learning to search, [366](#page-383-0) orthography, [196,](#page-213-2) [204](#page-221-2) orthonormal matrix, [60](#page-77-0) out-of-vocabulary words, [181](#page-198-0) outside recurrence, [237,](#page-254-1) [240](#page-257-3) overfitting, [23,](#page-40-1) [27](#page-44-2) in neural networks, [59](#page-76-1) overgeneration, [203,](#page-220-2) [213](#page-230-1) parallel corpus, [435](#page-452-3) parameters, [482](#page-499-1) paraphase, [346](#page-363-2) parent annotation, [244](#page-261-2) parsing, [209](#page-226-1) agenda-based, [279](#page-296-0) chart, [226](#page-243-3) easy-first, [279](#page-296-0) graph-based, [263](#page-280-1) transition-based, [225](#page-242-1) part-of-speech, [6,](#page-7-0) [175](#page-192-2) tagging, [145](#page-162-3) particle, [179,](#page-196-1) [217](#page-234-0) partition, [477](#page-494-4) partition function, [164](#page-181-3) passive-aggressive, [43,](#page-60-1) [487](#page-504-2) path, in an FSA, [193](#page-210-2) Penn Discourse Treebank (PDTB), [385](#page-402-2) Penn Treebank, [140,](#page-157-1) [159,](#page-176-1) [180,](#page-197-1) [212,](#page-229-0) [238](#page-255-2) perceptron, [25](#page-42-1) incremental, [277,](#page-294-1) [364](#page-381-1) multilayer, [50](#page-67-5) structured, [160](#page-177-2) perplexity, [140](#page-157-1)

phonology, [196](#page-213-2) phrase, [212](#page-229-0) phrase-structure grammar, [212](#page-229-0) pointwise mutual information, [281,](#page-298-0) [329,](#page-346-2) [330](#page-347-1) in collocation identification, [344](#page-361-2) positive, [331](#page-348-2) shifted positive, [337](#page-354-2) policy, [276,](#page-293-1) [365,](#page-382-2) [469](#page-486-2) policy gradient, [366](#page-383-0) polysemy, [74](#page-91-3) pooling, [63,](#page-80-1) [65,](#page-82-0) [372,](#page-389-0) [460](#page-477-3) positional encodings in machine translation, [447](#page-464-2) in relation extraction, [415](#page-432-0) power law, [2](#page-1-0) pragmatics, [351](#page-368-2) precision, [82,](#page-99-1) [478](#page-495-0) at-k, [83,](#page-100-3) [398](#page-415-0) labeled, [230](#page-247-2) unlabeled, [230](#page-247-2) precision-recall curve, [83,](#page-100-3) [418](#page-435-1) predicate, [403](#page-420-1) predicative adjectives, [217](#page-234-0) predictive likelihood, [103](#page-120-1) prepositional phrase, [2,](#page-1-0) [217](#page-234-0) primal form, [487](#page-504-2) prior expectation, [483](#page-500-0) probabilistic models, [482](#page-499-1) probabilistic topic model, [5,](#page-6-0) [409](#page-426-1) probability chain rule, [477](#page-494-4) conditional, [35,](#page-52-3) [477,](#page-494-4) [481](#page-498-1) density function, [481](#page-498-1) distribution, [480](#page-497-2) joint, [17,](#page-34-5) [35,](#page-52-3) [481](#page-498-1) likelihood, [478](#page-495-0) marginal, [481](#page-498-1) mass function, [85,](#page-102-1) [481](#page-498-1) posterior, [478](#page-495-0) prior, [22,](#page-39-3) [36,](#page-53-2) [478](#page-495-0)

simplex, [18](#page-35-3) processes, [422](#page-439-3) productivity, [195](#page-212-0) projectivity, [261](#page-278-1) pronominal anaphora resolution, [351](#page-368-2) pronoun, [178](#page-195-0) reflexive, [353](#page-370-1) PropBank, [310](#page-327-2) proper nouns, [177](#page-194-1) proposal distribution, [453](#page-470-0) propositions, [286,](#page-303-1) [287,](#page-304-3) [422](#page-439-3) prosody, [187](#page-204-3) proto-roles, [309](#page-326-4) pumping lemma, [207](#page-224-2) pushdown automata, [209,](#page-226-1) [251](#page-268-1) quadratic program, [34](#page-51-3) quantifier, [289](#page-306-1) existential, [289](#page-306-1) universal, [289](#page-306-1) question answering, [346,](#page-363-2) [405](#page-422-3) cloze, [425](#page-442-2) extractive, [426](#page-443-1) multiple-choice, [425](#page-442-2) random outcomes, [475](#page-492-2) random variable, [480](#page-497-2) discrete, [480](#page-497-2) indicator, [480](#page-497-2) ranking, [406](#page-423-2) loss, [406](#page-423-2) recall, [82,](#page-99-1) [478](#page-495-0) at-k, [398](#page-415-0) labeled, [230](#page-247-2) unlabeled, [230](#page-247-2) receiver operating characteristic (ROC), [83](#page-100-3) rectified linear unit (ReLU), [51](#page-68-2) leaky, [51](#page-68-2) reference arguments, [322](#page-339-3) reference resolution, [351](#page-368-2)

reference translations, [433](#page-450-3) referent, [351](#page-368-2) generic, [356](#page-373-1) referring expressions, [351,](#page-368-2) [381,](#page-398-2) [458](#page-475-1) regression, [72](#page-89-1) linear, [72](#page-89-1) logistic, [35](#page-52-3) ridge, [72](#page-89-1) regular expression, [192](#page-209-1) regular language, [192](#page-209-1) regularization, [23,](#page-40-1) [32](#page-49-1) reification (events), [305](#page-322-3) relation extraction, [277](#page-294-1) logical, [286,](#page-303-1) [295](#page-312-3) relation extraction, [411](#page-428-2) relations in information extraction, [403](#page-420-1) relative frequency estimate, [21,](#page-38-2) [126,](#page-143-1) [483](#page-500-0) reranking, [250](#page-267-2) residual networks, [52](#page-69-1) retrofitting, [343](#page-360-1) Rhetorical Structure Theory (RST), [389](#page-406-5) rhetorical zones, [381](#page-398-2) risk, [452](#page-469-1) roll-in, roll-out, [366](#page-383-0) saddle point, [58,](#page-75-1) [486](#page-503-2) sample space, [475](#page-492-2) satellite, in RST, [389](#page-406-5) satisfaction, [290](#page-307-0) scheduled sampling, [451](#page-468-2) schema, [403,](#page-420-1) [404,](#page-421-1) [419](#page-436-2) search error, [252,](#page-269-0) [363](#page-380-2) segmented discourse representation theory (SDRT), [384](#page-401-7) self-attention, [446](#page-463-2) self-training, [108](#page-125-1) semantic concordance, [76](#page-93-2) semantic parsing, [291](#page-308-1) semantic role, [306](#page-323-2)

semantic role labeling, [306,](#page-323-2) [412,](#page-429-2) [420](#page-437-1) semantics, [285](#page-302-1) dynamic, [302,](#page-319-0) [384](#page-401-7) in parsing, [242](#page-259-3) lexical, [73](#page-90-2) model-theoretic, [286](#page-303-1) underspecification, [302](#page-319-0) semi-supervised learning, [105](#page-122-2) semiring, [199](#page-216-0) algebra, [172](#page-189-4) expectation, [207](#page-224-2) tropical, [173](#page-190-1) sentence compression, [465](#page-482-1) sentence fusion, [465](#page-482-1) sentence, in logic, [289](#page-306-1) sentiment analysis, [69](#page-86-3) lexicon-based, [70](#page-87-3) targeted, [71](#page-88-1) sentiment lexicon, [16](#page-33-2) sequence-to-sequence model, [442](#page-459-1) shift-reduce parsing, [251](#page-268-1) shortest-path algorithm, [197](#page-214-3) sigmoid, [49](#page-66-2) simplex, [115](#page-132-2) singular value decomposition, [60,](#page-77-0) [104](#page-121-3) truncated, [104,](#page-121-3) [330](#page-347-1) singular vectors, [60](#page-77-0) skipgram word embeddings, [335](#page-352-2) slack variables, [34](#page-51-3) slot filling, [416](#page-433-1) slots, in dialogue systems, [467](#page-484-2) smooth functions, [29](#page-46-3) smoothing, [22,](#page-39-3) [129](#page-146-2) Jeffreys-Perks, [129](#page-146-2) Kneser-Ney, [133](#page-150-2) Laplace, [22,](#page-39-3) [129](#page-146-2) Lidstone, [129](#page-146-2) softmax, [49,](#page-66-2) [134,](#page-151-2) [416](#page-433-1) hierarchical, [135,](#page-152-2) [336](#page-353-1) source language, [431](#page-448-2) spanning tree, [258](#page-275-2)

sparse matrix, [330](#page-347-1) sparsity, [41](#page-58-3) spectral learning, [117](#page-134-1) speech acts, [187](#page-204-3) speech recognition, [125](#page-142-1) squashing function, [51,](#page-68-2) [135](#page-152-2) stand-off annotations, [90](#page-107-2) Stanford Natural Language Inference corpus, [346](#page-363-2) statistical significance, [84](#page-101-4) stemming, [7,](#page-8-0) [78,](#page-95-2) [195](#page-212-0) step size, [486](#page-503-2) stochastic gradient descent, [29,](#page-46-3) [35,](#page-52-3) [39](#page-56-1) stopwords, [80](#page-97-2) string, in formal language theory, [191](#page-208-1) string-to-tree translation, [441](#page-458-3) strong compositionality criterion, [391](#page-408-2) structure induction, [170](#page-187-2) structure prediction, [15](#page-32-1) subgradient, [29,](#page-46-3) [41](#page-58-3) subjectivity detection, [70](#page-87-3) subordinating conjunctions, [178](#page-195-0) sum-product algorithm, [164](#page-181-3) summarization, [125,](#page-142-1) [393](#page-410-2) abstractive, [393,](#page-410-2) [464](#page-481-4) extractive, [393](#page-410-2) multi-document, [465](#page-482-1) of sentences, [464](#page-481-4) supersenses, [339](#page-356-2) support vector machine, [34](#page-51-3) kernel, [48,](#page-65-2) [414](#page-431-3) structured, [161](#page-178-2) support vectors, [34](#page-51-3) surface form, [203](#page-220-2) surface realization, [457](#page-474-2) synonymy, [74,](#page-91-3) [281,](#page-298-0) [325](#page-342-6) synset, [74,](#page-91-3) [369](#page-386-1) syntactic path, [313](#page-330-1) syntactic-semantic grammar, [293](#page-310-3) syntax, [175,](#page-192-2) [211,](#page-228-2) [285](#page-302-1)

tagset, [176](#page-193-3) tanh activation function, [51](#page-68-2) target language, [431](#page-448-2) tense, [177](#page-194-1) terminal symbols, [209](#page-226-1) test set, [23,](#page-40-1) [95](#page-112-2) test statistic, [84](#page-101-4) text mining, [5](#page-6-0) text planning, [457](#page-474-2) thematic roles, [307](#page-324-4) third axiom of probability, [476](#page-493-2) TimeML, [421](#page-438-0) tokenization, [77,](#page-94-1) [185](#page-202-2) tokens and types, [19](#page-36-3) topic segmentation, [379](#page-396-2) hierarchical, [381](#page-398-2) trace, [222](#page-239-1) training set, [17,](#page-34-5) [95](#page-112-2) transformer architecture, [446](#page-463-2) transition system, [269](#page-286-2) for context-free grammars, [251](#page-268-1) transitive closure, [361](#page-378-1) translation error rate (TER), [435](#page-452-3) translation model, [126](#page-143-1) transliteration, [449](#page-466-3) tree-adjoining grammar, [220](#page-237-2) tree-to-string translation, [442](#page-459-1) tree-to-tree translation, [441](#page-458-3) treebank, [238](#page-255-2) trellis, [150,](#page-167-3) [205](#page-222-0) trigrams, [24](#page-41-2) trilexical dependencies, [248](#page-265-2) tropical semiring, [199](#page-216-0) true negative, [81](#page-98-1) true positive, [81,](#page-98-1) [479](#page-496-1) rate, [83](#page-100-3) truth conditions, [290](#page-307-0) tuning set, *see* development set, [81](#page-98-1) Turing test, [3](#page-4-0) two-tailed test, [86,](#page-103-0) [92](#page-109-2) type systems, [295](#page-312-3)

type-raising, [221,](#page-238-2) [295](#page-312-3) unary closure, [228](#page-245-1) undercutting, in argumentation, [393,](#page-410-2) [399](#page-416-0) underfitting, [23](#page-40-1) underflow, [17](#page-34-5) undergeneration, [203,](#page-220-2) [213](#page-230-1) Universal Dependencies, [176,](#page-193-3) [257](#page-274-3) unseen word, [169](#page-186-2) utterances, [187](#page-204-3) validation function, [301](#page-318-2) validity, in logic, [290](#page-307-0) value iteration, [469](#page-486-2) variable, [319](#page-336-2) bound, [289](#page-306-1) free, [289](#page-306-1) variance, [22,](#page-39-3) [87](#page-104-2) Vauquois Pyramid, [432](#page-449-1) verb phrase, [213](#page-230-1) VerbNet, [308](#page-325-3) verbs, [177](#page-194-1) Viterbi algorithm, [148](#page-165-0) variable, [149](#page-166-2) volition, [307](#page-324-4) weight decay, [57](#page-74-2) Wikification, [405](#page-422-3) Winograd schemas, [3](#page-4-0) word embeddings, [47,](#page-64-1) [53,](#page-70-2) [135,](#page-152-2) [136,](#page-153-1) [327,](#page-344-2) [334](#page-351-2) embeddings, fine-tuned, [340](#page-357-1) embeddings, pre-trained, [339](#page-356-2) representations, [326](#page-343-2) sense disambiguation, [73](#page-90-2) senses, [73,](#page-90-2) [309](#page-326-4) tokens, [77](#page-94-1) world model, [286](#page-303-1) builder, [291](#page-308-1) checker, [291](#page-308-1)

#### BIBLIOGRAPHY 569

yield, [209](#page-226-1) Zipf's law, [143](#page-160-2)