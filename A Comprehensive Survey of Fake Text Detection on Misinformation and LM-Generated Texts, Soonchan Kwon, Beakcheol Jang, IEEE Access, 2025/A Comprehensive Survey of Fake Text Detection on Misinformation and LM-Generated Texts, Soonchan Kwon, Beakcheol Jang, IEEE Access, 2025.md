![](_page_0_Picture_0.jpeg)

<span id="page-0-10"></span>Received 2 January 2025, accepted 27 January 2025, date of publication 4 February 2025, date of current version 10 February 2025. *Digital Object Identifier 10.1109/ACCESS.2025.3538805*

# A Comprehensive Survey of Fake Text Detection on Misinformation and LM-Generated Texts

SOONCHAN KWO[N](https://orcid.org/0009-0007-8600-7092) , (Member, IEEE), AND BEAKCHEOL JAN[G](https://orcid.org/0000-0002-3911-5935) , (Member, IEEE)

Graduate School of Information, Yonsei University, Seoul 03722, South Korea

Corresponding author: Beakcheol Jang (bjang@yonsei.ac.kr)

This work was supported by the National Research Foundation of Korea (NRF) funded by Korean Government under Grant RS-2023-00273751.

**ABSTRACT** This paper presents a pioneering and comprehensive analysis of fake text, a pressing issue in the digital age, by categorizing it into two main types: Misinformation and LM-generated texts. It is the first study to systematically dissect and examine the intricate challenges and nuances in distinguishing between genuine and artificial text. Through a meticulous review of various methodologies and technologies in fake text detection, the paper provides an in-depth evaluation of their effectiveness across diverse scenarios. Furthermore, this research delves into the significant societal impacts of both misinformation and LM-generated texts, underlining the urgent need for precise and effective detection mechanisms in our increasingly information-saturated world. This extensive survey not only offers a unique perspective on the current landscape of fake text detection, but also paves the way for future research, highlighting critical areas where further innovation and exploration are essential.

**INDEX TERMS** Nature language processing, large language model, fake text detection, deep learning.

#### **I. INTRODUCTION**

With the development of large language model (LLM) technology, which has significantly improved the performance of various natural language processing tasks [\[1\],](#page-19-0) the amount of text-based information has grown at an unprecedented rate. The vast amount of text generated by LLMs offers significant benefits in various fields [\[2\], dr](#page-19-1)iving innovation in numerous industries[\[3\]. Ho](#page-19-2)wever, as people are now exposed to this massive influx of textual information, selecting content that contains the desired information has become increasingly challenging. In particular, distinguishing between texts that provide accurate and reliable information and synthetic texts is difficult. Consequently, one of the key challenges in the era of big data is the detection of fake text to quickly identify meaningful and accurate information.

Identifying meaningful and accurate information is not an easy task, but becomes more difficult when fake information is spread for multiple purposes or created with the help

The associate editor coordinating the revie[w of](https://orcid.org/0000-0002-7565-5963) this manuscript and approving it for publication was Arianna D'Ulizia .

<span id="page-0-4"></span><span id="page-0-3"></span><span id="page-0-1"></span><span id="page-0-0"></span>of artificial intelligence. This can lead to various social challenges and disruptions, as well as increased difficulties in detection. Furthermore, techniques such as reinforcement learning from human feedback [\[4\]](#page-19-3) continue to refine and enhance the capabilities of LLMs, making it even more challenging to detect synthetic texts [\[5\]. Th](#page-19-4)erefore, advances in techniques for detecting fake text play an important role in improving the quality of information and preventing the spread of misinformation. This study provides a general analysis of current issues caused by fake text and a comprehensive analysis of fake text detection techniques, focusing on two main aspects of fake text: misinformation and LM-generated text.

<span id="page-0-9"></span><span id="page-0-8"></span><span id="page-0-7"></span><span id="page-0-6"></span><span id="page-0-5"></span><span id="page-0-2"></span>Fake text encompasses two concepts: text that contains misinformation [\[6\], an](#page-19-5)d text generated by a machine or artificial intelligence rather than a human [\[7\]. M](#page-19-6)isinformation detection is an essential component to maintain the trustworthiness of digital communications such as false news, misleading information, and rumors, which have long been emphasized for their ability to spread social unrest [\[8\],](#page-19-7) [\[9\].](#page-19-8) With the development of AI, the detection of generated text is also becoming increasingly important [\[10\]. I](#page-19-9)n particular,

<span id="page-1-0"></span>![](_page_1_Figure_2.jpeg)

**FIGURE 1.** A framework for categorizing fake text detection techniques.

the development of natural language processing with the emergence of llms makes it difficult to distinguish between LM-generated text and human-written texts, and at the same time, it presents new challenges in modern societies in the form of various social challenges [\[11\].](#page-19-10)

This paper comprehensively investigates various aspects of text detection technology to address these challenges and other issues. In particular, we analyze various techniques based on the latest machine learning and artificial intelligence techniques to detect misinformation and LM-generated text. Figure [1.](#page-1-0) shows a framework that classifies fake text detection techniques into two categories: misinformation and LM-generated text. It also highlights the past and present status of text detection technology and the challenges faced, explores possible solutions to overcome them, and discusses the future development of text detection techniques and their potential impacts on society and industry.

This study analyzes the challenges arising from fake text that cause social disruption and damage to humanity considering two aspects: misinformation and LM-generated text. In addition, we introduce text detection technology from two perspectives and improve the overall understanding. In this way, this study should help researchers, developers, and other people acquire knowledge of related concepts and provides a wide range of related information considering the growing interest in text detection. In addition, it provides guidelines for future research to facilitate the more effective use of this technology.

The main contributions of this study are as follows.

• It provides an overall survey of various textual challenges to be considered in big data studies, recognizing their importance and explaining the role of fake text detection in social trust and digital transparency.

- <span id="page-1-1"></span>• Based on a broad survey of the current areas of fake text detection, this study provides researchers with a broad update on state-of-the-art detection technologies through a comparative survey of trends and comparisons. This suggests solutions for future development challenges.
- This is the first study to systematically describe a wide range of text detection techniques by categorizing them into misinformation and text generation.

The remainder of this paper is organized as follows. In Section [II,](#page-2-0) we review existing studies using related surveys and analyze the differences between these studies and the characteristics of each study. Section [III](#page-4-0) introduces the challenges caused by fake text in modern society, which are divided into two aspects: misinformation and text generation. This section defines each type of challenge and its social impact. Section [IV](#page-6-0) describes misinformation detection techniques. Explores the developmental trends of modern misinformation detection models, including various machine learning and artificial intelligence techniques, and analyzes their strengths, weaknesses, and effectiveness. Section [V](#page-14-0) describes LM-generated text detection techniques. This section analyzes the current state-of-the-art and developments in the detection of LM-generated text, including text formed using state-of-the-art large-language models. In Section [VI,](#page-16-0) we reiterate the importance of text detection challenges and discuss future research directions and limitations. This section provides a detailed discussion of the impact and potential role of text detection technology in society, industry, and science. Finally, Section [VII](#page-18-0)

#### <span id="page-2-1"></span>**TABLE 1.** Previous survey research on fake text detection.

| Authors                           | М            | G | Major Contribution                                                                                                                                                                                                  |
|-----------------------------------|--------------|---|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Fortuna & Nunes [12]<br>(2018)    | $\checkmark$ |   | Provides a structured overview of previous approaches to hate speech detection, including core algorithms, methods, and main features used                                                                          |
| Zubiaga et al. [13]<br>(2018)     | √            |   | Introduces two types of rumors that circulate on social media and explores an approach to the four components of a rumor                                                                                            |
| Zhou et al. [14]<br>(2018)        | ~            |   | Reviews, summarizes, and evaluates ongoing fake news research from four perspectives: (1) misinformation, (2) writing style, (3) propagation patterns, and (4) credibility of sources                               |
| Guo et al. [15]<br>(2019)         | √            |   | Investigates the extraction and usage of crowd intelligence in misinformation detection, which paves a promising way to tackle misinformation detection challenges                                                  |
| Wu et al. [6]<br>(2019)           | ~            |   | Examines the difference between misinformation detection and classic super-<br>vised learning and explains the characteristics of individual methods of misin-<br>formation detection                               |
| Bondielli et al. [16]<br>(2019)   | √            |   | Provides a comprehensive analysis on the various techniques used to perform<br>rumor and fake news detection                                                                                                        |
| Zhang and Ghorbani [17]<br>(2020) | √            |   | Summarizes information on fake news to date, exploring its negative effects and the latest detection methods                                                                                                        |
| Jawahar et al. [18]<br>(2020)     |              | ~ | Conducts an in-depth error analysis of the state-of-the-art detectors and discusses research directions to guide future work in this exciting area                                                                  |
| Hu et al. [19]<br>(2022)          | √            |   | Offers a comprehensive examination and critique of current deep learning approaches to fake news detection, emphasizing their focus on diverse features                                                             |
| Aïmeur et al. [20]<br>(2023)      |              | ~ | Conducts an extensive and methodical analysis of existing fake news research, highlighting the most advanced techniques currently in use for detecting and discerning fake news                                     |
| Ray [21]<br>(2023)                | ~            | ~ | Explores the different ways that ChatGPT has revolutionized scientific research, from data processing and hypothesis generation to collaboration and public outreach                                                |
| Crothers et al. [7]<br>(2023)     |              | V | Provides an extensive analysis of the threat model posed by modern natural language generation systems and the most complete review of machine-generated text detection methods to date                             |
| Wu et al. [22]<br>(2023)          |              | ~ | Analyzes various LLM generated text detection paradigms, shedding light on challenges such as out-of-distribution challenges, potential attacks, and data ambiguity                                                 |
| Dhaini et al. [23]<br>(2023)      |              | ~ | Provides an overview of the current approaches employed to differentiate be-<br>tween texts generated by humans and ChatGPT                                                                                         |
| Tang et al. [24]<br>(2024)        |              | ~ | Provides an overview of existing LLM-generated text detection techniques and emphasizes crucial considerations for future research                                                                                  |
| Abdali et al. [25]<br>(2024)      | ✓            |   | Explores multimoda misinformation detection, focusing on cross-modal relation-<br>ships like image-text inconsistencies, and demonstrates how these approaches<br>enhance adaptability in social media environments |
| Lin et al. [26]<br>(2024)         |              | ~ | Categorizes detection methods for large AI models into 'pure detection' and 'be-<br>yond detection,' emphasizing benchmarking datasets like HC3 and TuringBench<br>for evaluating multimoda AI-generated content    |
| Liu et al. [27]<br>(2024)         |              | ~ | Surveys recent advancements in text watermarking, categorizing methods into lexical, syntactic, and generation-based approaches, while highlighting their role in mitigating misinformation risks                   |

summarizes the main findings and conclusions of this study. This section summarizes the important findings of this study and presents conclusions on how advances in text detection technology can improve the quality of information and prevent the spread of misinformation in the future.

# <span id="page-2-0"></span>**II. RELATED WORKS**

In this section, we describe recent studies on fake text detection in terms of disinformation and LM-generated text detection, clarify the methodological approach, key themes, and differences. Table [1](#page-2-1) summarizes recent survey research on fake text detection, where M denotes research related to misinformation, and G denotes research related to LM-generated text.

#### A. MISINFORMATION DETECTION

<span id="page-2-2"></span>Research in the field of misinformation has made steady progress, and regulation of language generation models. Ferrara [\[28\]](#page-19-11) reviewed the state of the art in the detection of social bots and the most prominent real-world applications to date to identify gaps and emerging trends in the field and address the unique challenges posed by AI-generated conversations. In addition to detecting generated information, various perspectives and approaches have been proposed. Guo et al. [\[15\]](#page-19-12) conducted an in-depth investigation of misinformation detection methods. Unlike other studies, this study investigates the field of misinformation detection from a challenge-oriented perspective. In particular, it emphasizes the investigation of crowd intelligence-based detection models in addition to various other detection models. Wu et al. [\[6\]](#page-19-5) discussed misinformation in the context of social media. They integrated different definitions of misinformation propagation and investigated optimal misinformation detection techniques under specific conditions and contexts. Deep learning-based detection methods have also been investigated with the development of deep learning techniques [\[29\]. T](#page-19-13)hey systematically reviewed existing misinformation challenges and solutions based on various deep learning techniques.

Research has also been conducted on similar concepts of misinformation. Bondielli and Marcelloni [\[16\]](#page-19-14) provided a fundamental analysis of rumor detection techniques and provided a comprehensive analysis of rumor definitions, data extraction and selection, analysis and detection techniques, and future research directions. Abdali et al. [\[25\]](#page-19-15) explored multimodal misinformation detection techniques, focusing on cross-modal relationships such as imagetext inconsistencies. These methods have demonstrated improved adaptability and accuracy compared to unimodal approaches, particularly in dynamic social media environments. Aïmeur et al. [\[20\]](#page-19-16) addressed fake news, disinformation, and misinformation. In particular, researchers have analyzed the factors that hinder effective automatic detection solutions on social media platforms and categorized and highlighted representative studies that address techniques for detection and mitigation. Among the subcategories of misinformation, there have been many studies on fake news. Zhou and Zafarani [\[14\]](#page-19-17) examined the detection of fake news from four perspectives: knowledge, style, propagation, and credibility. They proposed a comprehensive framework for studying fake news. Zhou and Zafarani [\[30\]](#page-19-18) compared various concepts related to fake news and provided a theoretical background to study fake news through an extensive literature survey conducted in various disciplines. They also explored methodologies for detecting fake news by categorizing fake news from four perspectives: knowledge, style, propagation, and source. Similarly, considering recent advances in deep learning, Hu et al. [\[19\]](#page-19-19) presented an analysis and review of the performance of deep-learning-based fake news detection methods in three sections: supervised, semi-supervised, and unsupervised learning on various datasets. Other studies have summarized practical and research-based approaches for online fake news detection [\[17\]](#page-19-20) or provided reviews of NLP approaches for automatic fake news detection [\[31\].](#page-19-21) Although it did not directly address detection techniques, another study reviewed the challenge of misinformation in the health sector and the methodologies to address it [\[32\].](#page-20-0)

<span id="page-3-3"></span>In this study, we build on previous misinformation research by providing a more comprehensive analysis that synthesizes existing methodologies and addresses unique challenges. We also analyze the main strengths and limitations of a variety of techniques to identify current and future trends in misinformation detection research.

# B. LM-GENERATED TEXT DETECTION

Text generation has become increasingly important in recent years, due to the rapid development of artificial intelligence technology. llms such as ChatGPT and Gemini can generate texts that are similar to those written by humans [\[21\]. H](#page-19-22)ence, the recent spate in research on LM-generated text detection.

<span id="page-3-0"></span>Of course, text detection research has been conducted even before the recent advances in AI technology. Jawahar et al. [\[18\]](#page-19-23) addressed the classification challenge of distinguishing text generated by text generation models from human-authored text and discussed how to automatically remove LM-generated text from various online platforms. They also comprehensively reviewed several aspects of large-scale text generation models, including decoding techniques, model architecture, training costs, and controllability. Crothers et al. [\[7\]](#page-19-6) comprehensively analyzed the risks posed by machine-generated text and detection techniques. They presented various threat models, including natural language generation. They also critically reviewed existing machine-generated text detection methods and evaluated them in terms of fairness, robustness, accountability, and reliability, as well as from a practical perspective based on the principles of trusted AI.

<span id="page-3-1"></span>Survey studies on LM-generated text have been relatively scarce compared to those on misinformation, but the number has recently started to increase, reflecting a surge in the popularity of generative AI techniques. Wu et al. [\[22\]](#page-19-24) identified whether a text was generated by an LLM and summarized recent detection achievements. They also emphasized the urgency and need to strengthen research on detectors. Research has also been conducted on the detection of ChatGPT [\[23\]. I](#page-19-25)n this study, we reviewed various methods for detecting LM-generated text and explained how they relate to the detection of ChatGPT-generated text. Lin et al. [\[26\]](#page-19-26) classified detection methods for large AI models into 'pure detection' and 'beyond detection,' emphasizing the role of benchmarking datasets like HC3 and TuringBench in improving multimodal content detection. Tang et al. [\[24\]](#page-19-27) provided an overview of LLM-generated text detection techniques and proposed an increase in control text, Ji et al. [\[33\]](#page-20-1) addressed another challenge arising from LM-generated text: hallucinations. They highlighted trends in the generation of hallucinations in AI models, reviewed advances in task-specific research related to hallucinations, and explored methods to measure and mitigate hallucinations. Liu et al. [\[27\]](#page-19-28) discussed recent advances in text watermarking, categorizing methods into lexical, syntactic, and generation-based approaches, and highlighting their role in tracking and mitigating risks of misinformation propagation.

<span id="page-3-4"></span><span id="page-3-2"></span>In this study, we build on existing studies by identifying the unique challenges posed by recent advances in llms, such as hallucinations, and comprehensively exploring and

<span id="page-4-1"></span>![](_page_4_Figure_2.jpeg)

**FIGURE 2.** An overview of the challenges categorized into misinformation and LM-generated texts.

comparing the latest detection techniques. Reflecting on the latest trends and emerging issues in AI-generated text, we also aim to provide a detailed understanding of the current landscape and identify critical areas for future research.

# <span id="page-4-0"></span>**III. CHALLENGES**

Text detection is a challenge with multiple social, political, technological, and ethical implications and presents significant challenges. Understanding these issues is crucial for the safe and beneficial use of text generation technologies and the exploration of text detection techniques. To emphasize the importance of research on text detection techniques, this section reviews the main issues that need to be addressed, which are categorized into two main areas: misinformation and LM-generated text. Within each category, we explore the nature of the challenges posed and their impact on society and individuals. To this end, our objective was to gain a deeper understanding of strategies to minimize harm and maximize benefits. Figure [2.](#page-4-1) illustrates the challenges posed by fake text in two dimensions.

Among existing studies, survey papers define this challenge. Wu et al. [\[6\]](#page-19-5) characterized misinformation using various words. Crothers et al. [\[7\]](#page-19-6) presented a threat model that incorporated the generation of natural languages to describe the challenge. Jawahar et al. [\[18\]](#page-19-23) defined various challenges as social influences. We divided existing studies into 10 categories, five for each aspect, to define the challenges posed by fake text.

# A. CHALLENGES IN MISINFORMATION

There are several definitions of misinformation and there have been many studies on this topic. We consider the different definitions of misinformation before diving into the social challenges it causes. Islam et al. [\[29\]](#page-19-13) defined misinformation as a false statement that conceals the correct facts to mislead people. They recognize misinformation as a negative influence that leads to a breakdown in trust and weakening relationships. Wu et al. [6] [defi](#page-19-5)ne misinformation as false or inaccurate information that is intentionally created and intentionally or unintentionally disseminated. Guo et al. [\[15\]](#page-19-12) define misinformation as information or stories distributed on social networks that are ultimately confirmed to be false or inaccurate. A common definition of misinformation, including other studies, is the concept of 'false information.'

<span id="page-4-2"></span>Misinformation is recognized as a major threat to society [\[34\]. S](#page-20-2)ince the word ''misinformation'' does not describe a wide range of related challenges, this part explores the challenges caused by misinformation, as defined by various studies.

# <span id="page-4-3"></span>1) ONLINE REVIEWS

Online reviews significantly influence consumers [\[35\]. F](#page-20-3)ake reviews that overly praise a product or content, whether factual or experiential, are misleading and misleading to consumers. Furthermore, fake informational reviews that include negative ratings of other people's products or content can harm both the product or content seller and potential buyer. Furthermore, due to advances in LLM, fake reviews can be intentionally generated and used to attack online review systems and influence online shoppers' purchase decisions [\[36\].](#page-20-4)

#### <span id="page-4-4"></span>2) FAKE NEWS

<span id="page-4-5"></span>Fake news has had a significant impact on presidential elections [\[37\], a](#page-20-5)nd among the challenges caused by misinformation, detection has received the most attention. Definitions vary, with Gelfert et al. [\[38\]](#page-20-6) defining fake news as ''intentionally false or misleading claims as news.'' Research on fake news detection also defines fake news as ''content on social media created with suspicious or misleading intent that has the potential to cause serious challenges for society'' [\[17\]](#page-19-20) or ''false news and intentionally false news published by a news outlet'' [\[14\]. F](#page-19-17)ake news is misleading by design and reflects the systematic nature of the sources and channels through which it is disseminated, thus manipulating the cognitive processes of the audience [\[38\]. T](#page-20-6)hus, fake news has both large and small negative impacts on society.

# 3) RUMORS

<span id="page-5-1"></span>This is a long-standing concept. Rumors have existed for several years. Difonzo et al. [\[39\]](#page-20-7) define rumors as ''unverified, instrumentally relevant information statements that serve a function in helping people understand and manage risk.''In the existing literature on rumor detection [\[29\], d](#page-19-13)efine rumors as information with questionable authenticity that circulates among people. Wu et al. [\[6\]](#page-19-5) define rumors as unverified information that may be true or false. The confusing thing about rumors is that they are stories that have questionable credibility but are difficult to verify. As such, they often cause doubt or anxiety. Rumors can be partially true, entirely false, or unverified. Therefore, rumors are a serious social challenge that must be addressed.

# 4) DISINFORMATION

<span id="page-5-2"></span>Hernon and Peter [\[40\]](#page-20-8) define disinformation as false information that is intentionally spread with the intent of being misleading. Kumar and Arun HS [\[41\]](#page-20-9) referred to disinformation as false facts created to deliberately deceive or betray. Although both misinformation and disinformation refer to false or inaccurate information, the key difference is intent. Disinformation is spread with intent [\[6\]; wh](#page-19-5)at makes disinformation a serious social challenge is not just that it is false, but that it is created with the intent to exploit. False information and the intent to exploit it are both challengeatic, but disinformation is a major social challenge because it has both.

#### 5) HATE SPEECH

Fortuna et al. [\[12\]](#page-19-29) defined hate speech as language that attacks or demeans a group of people based on certain characteristics such as appearance, religion, ancestry, national or ethnic origin, sexual orientation, or gender identity, and incites violence or hatred. It also states that hate speech can be found in many different language styles, even in subtle forms or when humor is used. In this study, we recognize hate speech as a social challenge that causes conflict among various stakeholders in society, because it is information that is not true or information whose truth depends on an idea or point of view that can be used aggressively.

# <span id="page-5-0"></span>B. CHALLENGES IN LM-GENERATED TEXT

LM-generated text is artificial text generated by a large-scale language model or generation model, such as ChatGPT, which can appear to be written by a human [\[23\].](#page-19-25) Such authentic-looking text can be automatically generated to deceive people [\[18\].](#page-19-23) Crothers et al. [\[7\]](#page-19-6) noted that machine-generated text can enable a variety of attacks, creating threats with a specific purpose, such as compromising a computer system, exploiting a specific individual for financial gain, or harassing a specific community on a large scale.

## 1) COPYRIGHT INFRINGEMENT

Machine-generated text has the potential to generate copyrighted content. This phenomenon can create distrust in the society and adversely affect creators in the academic, employment, and media sectors [\[7\]. In](#page-19-6) the modern world, there are endless categories of creative work. From the most basic humanities, such as novels and poetry; to entertainment, such as song lyrics, text content, and drama scripts; social media, such as blogs; and marketing copies for various proposals and resumes. It is no exaggeration to state that they can be found in almost every sector of modern society. Of course, there are positive aspects of increased efficiency, but when it enters the realm of evaluation where it has to be compared to other people's work, it can be damaging. As a result, it can cause challenges that lead to a widespread distrust in all creative work.

#### 2) SPAM AND PHISHING

<span id="page-5-3"></span>Islam et al. [\[29\]](#page-19-13) defined spam as unsolicited messages sent over the Internet and used spread malicious software and advertisements. Phishing is the aggressive use of LM-generated text on social media and various online platforms to threaten the security of personal information, according to Crothers et al. [\[7\]. Bo](#page-19-6)th spam and phishing are wrongful behaviors in their own right that socially harass individuals or groups. When generated by LLMs, they gain power in two ways. First, they can be mass-produced much faster than by human authors. Second, it is generated at a level of quality comparable to that of human authors. These two aspects create spam and phish, which are larger social challenges.

#### 3) MULTIPLE BIASES

Generative models reflect the content and patterns learned from training data. These can include social, cultural, gender, racial, and other biases; therefore, the model may learn these biases and generate biased texts that reflect them. This can lead to challenges communicating the correct information and reinforce social inequalities [\[18\]. A](#page-19-23)n even more serious challenge is that biases are unintentionally embedded in text. Unintentionally, these biases adversely affect various groups and individuals [\[7\].](#page-19-6)

<span id="page-6-1"></span>

|  |  |  |  |  | TABLE 2. A brief comparison of types of misinformation detection techniques based on traditional approach. |
|--|--|--|--|--|------------------------------------------------------------------------------------------------------------|
|--|--|--|--|--|------------------------------------------------------------------------------------------------------------|

| Author                          | Algorithm           | Use                     |
|---------------------------------|---------------------|-------------------------|
| Qazvinian et al. [42]<br>(2011) | Bayes classifiers   | Contents based Features |
| Yang et al. [43]<br>(2012)      | SVM                 | Many Features           |
| Zhang et al. [44]<br>(2012)     | SVM                 | CHI method              |
| Kwon et al. [45]<br>(2013)      | DT, RF, SVM         | Three aspects           |
| Zhang et al. [46]<br>(2015)     | SVM, TF-IDF         | Many Features           |
| Zeng et al. [47]<br>(2016)      | LR, Gaussian NB, RF | Sentiment Analysis      |
| Wu et al. [48]<br>(2017)        | CERT                | Historical data         |
| Ruchansky et al. [49]<br>(2017) | CSI                 | RNN, LSTM               |
| Jin et al. [50]<br>(2017)       | TF-IDF              | Text matching           |
| Tacchini et al. [51]<br>(2017)  | LR, harmonic BLC    | Whether to 'like'       |
| Zhang et al. [52]<br>(2020)     | Deep Diffusive Unit | RNN                     |
| Silva et al. [53]<br>(2021)     | PDRCNN              | LSTM                    |

#### 4) ACADEMIC DISRUPTION

<span id="page-6-3"></span><span id="page-6-2"></span>LM-generated text submitted as student work in educational or academic settings can undermine educational morals and confuse assessors. The emergence of generative AI platforms such as ChatGPT has significantly impacted the educational environment [\[54\]. W](#page-20-10)hile they offer the superficial appeal of replacing human effort, they also have the potential to undermine the integrity of academic assessments, adversely affect the learning process, and give rise to various forms of academic dishonesty [\[55\]. I](#page-20-11)n fact, educators have expressed concerns about misuse or abuse of AI for unintended purposes [\[56\].](#page-20-12) This is also true for academic research. In fact, Scigen raised awareness that automatic paper generation is a significant challenge in academia. If these concerns are effectively addressed, they can provide many benefits [\[55\]](#page-20-11) however, if not, they can cause major social challenges.

#### <span id="page-6-4"></span>5) HALLUCINATIONS

<span id="page-6-5"></span>Hallucinations are unrealistic perceptions that feel real [\[33\].](#page-20-1) Although texts produced by LLMs are often convincing and trustworthy, there are cases in which they produce texts with errors or misleading information, such as hallucinations [\[23\].](#page-19-25) The challenge with misleading texts is that they give the impression of being natural and unobtrusive even though they are incorrect and meaningless. This can lead people to believe false information as if it were true. Although hallucinations can occur in texts in a variety of fields, they can be especially challengeatic if not overcome and mitigated in fields where the truthfulness of information is important, such as law and medicine [\[57\].](#page-20-13)

## <span id="page-6-0"></span>**IV. MISINFORMATION DETECTION TECHNIQUES**

Misinformation detection aims to identify misinformation that can cause social disruption and has a significant impact on human judgment and belief formation. In this section, we categorize and introduce existing misinformation-detection techniques into four main groups and provide an overview of their overall development and the challenges.

#### A. TRADITIONAL APPROACHES

<span id="page-6-7"></span><span id="page-6-6"></span>Traditional approaches refer to the methods used in the early days of research on misinformation detection. Misinformation detection has been an important and well-researched field [\[58\],](#page-20-14) [\[59\]. T](#page-20-15)he relevant approaches can be categorized as feature-and machine learning-based. Various misinformation detection techniques based on traditional approaches are briefly compared in Table [2.](#page-6-1)

#### 1) FEATURE-BASED APPROACHES

Feature-based approaches analyze specific data features to identify misinformation. This method extracts various attributes or characteristics of the data and learns or classifies patterns based on these characteristics [\[60\].](#page-20-16)

<span id="page-6-9"></span><span id="page-6-8"></span>Hamidian and Diab [\[61\]](#page-20-17) used various linguistic features to classify information as trustworthy or untrustworthy. In addition to traditional features, Kwon et al. [\[45\]](#page-20-18) developed more practical and network-related features and improved the accuracy of classification by examining three aspects of rumor spread: temporal, structural, and linguistic. Wu et al. [\[48\]](#page-20-19) utilized learning-based feature selection

<span id="page-7-0"></span>![](_page_7_Figure_2.jpeg)

**FIGURE 3.** The CSI model specification [\[49\].](#page-20-20)

in a novel framework called CERT(Cross-topic Emerging Rumor deTection) to study whether knowledge learned from past data can help identify newly emerging rumors. Other studies have also utilized existing and new features [\[43\].](#page-20-21) Ruchansky et al. [\[49\]](#page-20-20) developed a CSI(Capture, Score, and Integrate) model that utilizes all three features to detect fake news: the text of an article, the user reactions it receives, and the source user spreading it.

Figure [3.](#page-7-0) shows how the three features of the CSI model were combined and utilized. Zhang et al. [\[52\]](#page-20-22) built FAKEDETECTOR, a diffusion network model based on explicit and latent features extracted from textual information, to find deceptive and socially harmful words in fake news. Other studies have evaluated the performance of each feature separately using the Athens model, which has the best performance and is sufficiently computationally fast to perform a large number of experiments [\[62\].](#page-20-23)

<span id="page-7-2"></span>Feature-based approaches often employ statistical techniques to support claims. To distinguishing rumors, various Bayesian classifiers from high-level features were proposed [\[42\], a](#page-20-24)nd the linear functions of these classifiers were trained to perform a search in the first task and classification in the second. TF-IDF [\[63\]](#page-20-25) is a popular model for statistical text classification. It scores text by representing it as a numerical vector, where TF stands for term frequency and IDF stands for inverse document frequency. TF-IDF compares the score with a predetermined threshold to classify whether it is a rumor [\[50\]. Z](#page-20-26)hang et al. [\[46\]](#page-20-27) also modify the existing TF-IDF in the process of classifying the emotional polarity of a message and propose TF-FW, which calculates the weights of the elements according to the type of dictionary. Feature-based approaches can be combined with machine learning-based approaches to yield better results.

Feature-based methods, such as TF-IDF and n-grams, rely on extracting linguistic and statistical properties from text, making them a popular choice for detecting misinformation in static datasets. Their interpretability is a key advantage, as these methods allow for a clear understanding of the features influencing predictions. Additionally, they are computationally efficient and suitable for scenarios with limited resources or well-defined linguistic markers. However, feature-based approaches face significant limitations when applied to dynamic environments like social media, where misinformation evolves rapidly. The reliance on predefined features often limits their ability to adapt to unseen patterns or capture semantic nuances, reducing their effectiveness in complex or high-variance contexts.

# <span id="page-7-1"></span>2) MACHINE LEARNING-BASED APPROACH

Machine learning-based approaches learn patterns from data and use them to classify and predict future data. It extracts rules from the data and uses them to determine the authenticity of new information. Machine learning can be applied in many forms, including supervised, unsupervised, and reinforcement learning, and various algorithms have been employed. In the field of misinformation detection, machine learning-based research has focussed on automatically determining the credibility of texts, such as those in Twitter [\[60\]. B](#page-20-16)ecause the main goal of misinformation detection in machine learning techniques is classification, most studies have compared the performance of classification models such as logistic regression [\[47\],](#page-20-28) [\[51\],](#page-20-29) [\[64\],](#page-20-30) [\[65\].](#page-20-31)

Support vector machines (SVMs) are also popular supervised models forclassification [\[66\]. Z](#page-20-32)hang et al. [\[44\]](#page-20-33) utilized SVM-based approaches for fake news detection. Other studies, such as Giasemidis et al. [\[67\],](#page-20-34) utilized SVM models to generate confidence scores by considering various features. Silva et al. [\[53\]](#page-20-35) used SVMs to determine whether a topic was a rumor based on temporal, structural, and linguistic features. In addition to SVM, these studies utilized various classifiers, such as decision trees [\[68\]](#page-20-36) and random forests [\[69\]. T](#page-20-37)here are also a number of studies that use machine learning-based models, including decision trees and random forests [\[53\],](#page-20-35) [\[61\],](#page-20-17) [\[67\],](#page-20-34) [\[70\],](#page-20-38) [\[71\].](#page-20-39)

<span id="page-8-6"></span><span id="page-8-5"></span>While traditional approaches are advantageous in terms of their simplicity and interpretability and are still relevant, they can have limitations when considering complex and diverse forms of misinformation. More advanced techniques are needed, especially in today's digital environment, where data are fast-paced and diverse, such as in social media. Nevertheless, these traditional methods have laid the foundation for modern deep learning and other advanced analytical techniques, and still play an important role in several analytical tasks.

# B. DEEP LEARNING-BASED APPROACH

In recent years, deep learning-based approaches have emerged significantly in the field of misinformation detection. Deep learning is an innovative method that uses multiple layers of artificial neural networks to learn complex data patterns and use them to make sophisticated and accurate predictions. This technique is particularly useful for processing unstructured data, such as natural language text or images. Therefore, even in the existing detection surveys, studies have often described detection techniques using deep learning as the main topic [\[19\],](#page-19-19) [\[29\]. A](#page-19-13)mong various neural networks for deep learning, graph neural networks [\[72\]](#page-20-40) are used to analyze the graph structure of data, and recently, models that utilize the attention mechanism [\[73\]](#page-20-41) have been used for misinformation detection.

#### 1) NEURAL NETWORK

<span id="page-8-11"></span><span id="page-8-10"></span>Deep-learning models use different types of neural network structures. Neural networks are key components of deep learning and are effective in solving a wide range of challenges. Among them, convolutional neural networks (CNNs) [\[74\]](#page-20-42) are primarily used to extract features from visual data such as images, whereas recurrent neural networks (RNNs) [\[75\]](#page-20-43) are advantageous in processing sequential <span id="page-8-13"></span><span id="page-8-12"></span>data, especially natural language processing. Additionally, LSTMs [\[76\]](#page-20-44) and GRUs [\[77\]en](#page-20-45)able these models to learn the complex nature of data, which in turn enables sophisticated pattern recognition and classification. Almost all deep learning-based models incorporate the concept of a neural network; however, in this section, we narrow our focus to models with a neural network structure.

<span id="page-8-15"></span><span id="page-8-14"></span><span id="page-8-3"></span><span id="page-8-2"></span><span id="page-8-1"></span><span id="page-8-0"></span>Several techniques utilize CNNs for detection. Chen et al. [\[78\]](#page-20-46) used a CNN-based classification method with single and multiword embeddings to detect rumors. Yu et al. [\[79\]](#page-21-0) utilized a model called CAMI to seamlessly extract features scattered in an input sequence and form interactions to effectively detect misinformation. Additionally, there are other CNN-related studies, especially those that combine CNN with other deep learning models instead of using a single model [\[80\],](#page-21-1) [\[81\],](#page-21-2) [\[82\],](#page-21-3) [\[83\],](#page-21-4) [\[84\]](#page-21-5) and those that develop new CNN-based models [\[85\],](#page-21-6) [\[86\].](#page-21-7)

<span id="page-8-24"></span><span id="page-8-23"></span><span id="page-8-22"></span><span id="page-8-21"></span><span id="page-8-20"></span><span id="page-8-19"></span><span id="page-8-18"></span><span id="page-8-17"></span><span id="page-8-16"></span><span id="page-8-7"></span><span id="page-8-4"></span>In addition to CNNs, RNNs are frequently used as detection techniques. For example, Ma et al. [\[87\]](#page-21-8) were the first to use RNN models to learn continuous representations for rumor detection, and Ma et al. [\[88\]](#page-21-9) used RNNs to model propagation trees for news dissemination. Similar to CNNs, RNNs are often combined with other RNN-based model [\[89\],](#page-21-10) [\[90\]. S](#page-21-11)pecifically, Xu et al. [\[91\]](#page-21-12) proposed a novel deep recurrent neural model with a symmetric network structure and Afroz et al. [\[59\]](#page-20-15) used RNNs to extract the temporal representation of articles.

<span id="page-8-27"></span><span id="page-8-26"></span><span id="page-8-25"></span>LSTMs are models with long-term memory and are the basis of many detection techniques. Jin et al. [\[50\]](#page-20-26) integrated the features obtained from a network of LSTMs to produce a reliable fusion classification. Ruchansky et al. [\[49\]](#page-20-20) designed a fake news detection framework by feeding the temporal data of various attributes, including the engagement level of news, into an LSTM.

<span id="page-8-30"></span><span id="page-8-29"></span><span id="page-8-28"></span>Kudugunta and Ferrara [\[92\]](#page-21-13) proposed a deep neural network based on LSTM architecture, and Long et al. [\[93\]](#page-21-14) proposed a novel method to incorporate speaker profiles into an attention-based LSTM model. There are studies that combine different deep learning models, such as [\[94\],](#page-21-15) which combines CNNs and models to propose a mixed deep learning model to automatically identify inappropriate language [\[13\],](#page-19-30) [\[95\].](#page-21-16)

#### <span id="page-8-31"></span><span id="page-8-9"></span><span id="page-8-8"></span>2) ATTENTION-BASED

<span id="page-8-34"></span><span id="page-8-33"></span><span id="page-8-32"></span>Recently, models that use attention mechanisms have gained considerable attention in deep learning. This technique allows the model to process information more effectively by paying ''attention'' to important parts of the input data. The detection of misinformation has also gained attention [\[104\].](#page-21-17) Attention techniques are often used in combination with other neural networks [\[83\].](#page-21-4) Chen et al. [\[105\]](#page-21-18) used an RNN with an attention mechanism to automatically censor rumors during the spreading stage for early detection. Liu et al. [\[106\]](#page-21-19) introduced a technique called AIM (Attentionbased approach for Identification of Misinformation) to

| Year | Author                          | Algorithm  | Use                           |
|------|---------------------------------|------------|-------------------------------|
| 2014 | Friggeri et al. [96]<br>(2014)  | -          | Propagation path              |
| 2015 | Zhao et al. [97]<br>(2015)      | SVM, DT    | Signal tweets                 |
| 2015 | Ma et al. [98]<br>(2015)        | DSTS       | Context features              |
| 2017 | Ma et al. [99]<br>(2017)        | SVM        | РТК                           |
| 2018 | Kudugunta et al. [92]<br>(2018) | LSTM       | User metadata                 |
| 2018 | Della et al. [100]<br>(2018)    | TF-IDF, LR | Content and social engagement |
| 2018 | Shu et al. [101]<br>(2018)      | -          | User trust and User profile   |
| 2018 | Ma et al. [88]<br>(2018)        | RNN        | GRU units                     |
| 2019 | Shu et al. [102]<br>(2019)      | TriFN      | Social context                |
| 2020 | Nguyen et al. [103]<br>(2020)   | FANG       | Social actors' interactions   |

<span id="page-9-0"></span>

<span id="page-9-1"></span>detect misinformation based on text with the maximum attention value. Guo et al. [\[94\]](#page-21-15) proposed a novel hierarchical neural network combined with social information. Chen et al. [\[107\]](#page-21-20) proposed an attention-residual network called ARC, which includes long-range dependencies via an attention mechanism.

# 3) GNN BASED

GNNs are effective models when data is represented in a graph structure. They are suitable for graphical data, such as social networks, knowledge graphs, and molecular structures, and are used to analyze the relationships and connections between nodes. GNNs are useful for identifying important patterns and relationships in complex network structures

<span id="page-9-2"></span>Lu et al. [\[108\]](#page-21-21) structured users engaged in social interactions as a complete graph and utilized Graph Neural Network (GNN) techniques for identifying fake news. Their approach also incorporated user modeling, merging news content with user data and comments for more effective fake news detection. Meanwhile, Song et al. [\[109\]](#page-21-22) introduced an innovative framework for detecting fake news, which integrates structural, semantic, and temporal aspects using a temporal propagation approach. Particularly, it models the temporal evolutionary patterns of real news by utilizing evolving graphs in a continuous-time dynamic diffusion network setting. Hu et al. [\[110\]](#page-21-23) propose CompareNet, an endto-end graph neural model that uses entities to compare news to a knowledge base. Additionally, Bian et al. [\[111\]](#page-21-24) utilized a dual approach in rumor detection: a top-down graph convolutional network to analyze the causal aspects of rumor dissemination, and a bottom-up graph convolutional network designed to assess the structural characteristics of how rumors spread. Sun et al. [\[112\]](#page-21-25) developed a double dynamic graph convolutional network (DDGCN) to model the news

<span id="page-9-6"></span><span id="page-9-4"></span>

<span id="page-9-7"></span>propagation process using a dynamic graph neural network for social media rumor detection, achieving excellent results. Furthermore, Qian et al. [\[113\]](#page-21-26) modeled posts as graphs to obtain long-range semantic representation and further developed it to propose a knowledge-aware multimodal adaptive graph convolutional network that integrates text, knowledge concepts, and images, and fuses them into a framework that uses multimoda techniques. This method considers not only text, but also images to detect misinformation.

#### 4) MULTIMODAL

<span id="page-9-10"></span><span id="page-9-9"></span><span id="page-9-8"></span><span id="page-9-5"></span><span id="page-9-3"></span>A multimodal approach uses a combination of different types of data, such as text, images, and videos, to detect misinformation. Although the focus of our research is on detection through text, this approach can also incorporate information from non-text data types, enabling a richer and more multidimensional analysis. Multimodal models are particularly effective in environments such as social media where information arises in several forms. A common approach is to use an encoder to extract text features and an image encoder to extract features from images and then concatenate these features into news features. Wang et al. [\[114\]](#page-21-27) proposed an end-to-end framework called an event-adversarial neural network to help detect fake news from emerging events. The framework comprises three main components: extracting features from text and images in a bimodal feature extractor that learns a discriminative representation for fake news detection, and maintaining features shared between events. Jin et al. [\[115\]](#page-21-28) integrated features extracted from images into joint features of text and social context, and incorporated features obtained from a deep learning model, an LSTM network, to produce a reliable fusion classification. Wang et al. [\[116\]](#page-21-29) presented a process that integrates meta-learning and neural network processing

<span id="page-10-0"></span>![](_page_10_Figure_2.jpeg)

**FIGURE 4.** Tri-relationship embedding [\[102\].](#page-21-30)

methods. Particularly, they proposed a label-embedding module and hard attachment mechanism. Khattar et al. [\[117\]](#page-21-31) utilized a representation obtained by utilizing a multimodal variational autoencoder for fake news detection. They also proposed a multimodal variational autoencoder, which is an end-to-end network that combines a bimodal variable autoencoder and binary classifier.

Research has also been conducted on the conversion of images into text to measure the similarity. Zhou et al. [\[118\]](#page-21-32) used an image captioning model to convert images to text and then a multimodal dissimilarity was calculated by evaluating the similarity between the original news text and LM-generated text. Ghorbanpour et al. [\[119\]](#page-21-33) presented a fake news rebuilder method called transform learning, which utilizes contextual and semantic features and contrast loss extraction techniques to determine the similarity between images and text.

<span id="page-10-5"></span>Other studies include Zhang et al. [\[127\],](#page-22-0) a multimodal study that fuses multimodal information using a multi-channel CNN and attention mechanism, and Song et al. [\[128\],](#page-22-1) which models the bidirectional enhancement between images and text using a joint attention transducer.

<span id="page-10-10"></span><span id="page-10-7"></span>In addition to these four approaches, some studies used language models based on transformers [\[129\].](#page-22-2) Gundapu et al. [\[130\]](#page-22-3) used an ensemble of three transformerbased language models, BERT [\[131\],](#page-22-4) ALBERT [\[132\],](#page-22-5) and XLNET [\[133\],](#page-22-6) to detect fake news based on misinformation generated during the COVID-19 pandemic. <span id="page-10-12"></span><span id="page-10-11"></span><span id="page-10-1"></span>Singhal et al. [\[134\]](#page-22-7) introduced pre-trained language models such as BERT and XLNet to encode text features. Zhang et al. [\[135\]](#page-22-8) proposed a model called BERT-EMO to represent the emotions and social sentiments of the the posters.

<span id="page-10-2"></span>Recent advances in deep learning have combined multiple neural network layers, such as LSTM, CNN, and Transformer blocks, to improve the detection of AI-generated content. For example, a study proposed an ensemble of Transformer-based architectures, including BERT, ALBERT, and XLNet, which achieved an accuracy of 99% in identifying AI-generated fake news during the COVID-19 pandemic [\[136\].](#page-22-9)

<span id="page-10-13"></span><span id="page-10-4"></span><span id="page-10-3"></span>Deep learning-based approaches are applicable to a wide variety of challenges in misinformation detection and have helped solve several challenges due to their remarkable performance. Despite these strengths, deep learning approaches are not without their challenges. They require large, highquality datasets for training, which may not always be available for misinformation-specific tasks. Moreover, the black-box nature of these models raises concerns about interpretability and transparency, especially in sensitive applications such as healthcare and legal contexts. The high computational cost associated with training and deploying these models further limits their accessibility in resourceconstrained environments.

#### <span id="page-10-9"></span><span id="page-10-8"></span><span id="page-10-6"></span>C. INFORMATION-BASED APPROACH

In misinformation detection, an information-based approach focuses on analyzing the content of the data and the additional

| Author                               | Algorithm                     | Use                        |
|--------------------------------------|-------------------------------|----------------------------|
| Bhattacharjee et al. [120]<br>(2017) | Active Learning Framework     | Deep-Shallow Fusion        |
| Tacchini et al. [51]<br>(2017)       | LR, harmonic BLC              | Whether to 'like'          |
| Nguyen et al. [121]<br>(2018)        | Probabilistic graphical model | User interaction           |
| Vo et al. [122]<br>(2018)            | Factual URL recommendation    | User behavior data         |
| Kim et al. [123]<br>(2018)           | Curb                          | Temporal point processes   |
| Tschiatschek et al. [124]<br>(2018)  | Detective                     | Crowd signals              |
| Della et al. [100]<br>(2018)         | TF-IDF, LR                    | User engagement            |
| Pennycook et al. [125]<br>(2019)     | -                             | Crowdsourced trust ratings |
| Clayton et al. [126]<br>(2020)       | Ordinary Least Squares        | General warnings           |

<span id="page-11-0"></span>**TABLE 4.** A brief comparison of types of misinformation detection techniques based on human aid-based approach.

information that it provides. This approach extends beyond the surface characteristics of textual or visual data and focuses on understanding the deeper information contained in the data. For example, [\[42\]](#page-20-24) analyzed the linguistic patterns and structure of tweets and used statistical models to distinguish between tweets related to rumors and non-related to rumors. This method was one of the first to contribute to the effective identification of misinformation in social media. Information-based approaches often identify misinformation based on the content and context of the information and are divided into two categories: content-based and context-based approaches. Various misinformation detection techniques based on information-based approaches are briefly compared in Table [3.](#page-9-0)

# 1) CONTENT-BASED

Content-based approaches utilize the content of the data, such as text, images, and videos, for analysis. For texts, this includes linguistic characteristics, context, and keyword analysis. Content-based analysis plays an important role in gaining a deeper understanding of the information in the data.

Kudugunta and Ferrara [\[92\]](#page-21-13) proposed an approach that integrates textual content and metadata for bot detection. The method achieves very high accuracy (AUC over 0.96) in distinguishing between bots and humans in a single tweet. And Liu et al. [\[106\]](#page-21-19) present a new methodological framework for the detection of misinformation on social networks that utilize AIM to detect misinformation by leveraging the attention to content associated with each characteristic of the text. Volkova and Jang [\[137\]](#page-22-10) proposed extracting reader attitudes and mental states from news comments and combining them with news content to identify fake news. Additionally, Zhang et al. [\[46\]](#page-20-27) proposed an automatic rumor detection method that combines the implicit and shallow features of messages to detect misinformation. Sun et al. [\[112\]](#page-21-25) leveraged external knowledge on news and comments through a DDGCN model to improve the comprehensibility of content and comments and used a dynamic graph neural network to model the news propagation process. The proposed implicit features include popularity orientation, internal and external consistency, comment polarity and opinions, social influence, comment retweet influence, and message congruence. Zhao et al. [\[97\]](#page-21-34) leveraged user viewing behavior to capture rumors signals and clustered tweets based on these signals to identify suspected rumor topics.

# 2) CONTEXT-BASED

<span id="page-11-2"></span>A context-based approach considers not only the content of the text or data itself, but also the context in which it exists. It detects misinformation by analyzing contextual factors, such as the location, time, and behavioral patterns of the user. Previous studies used context-based approaches [\[96\],](#page-21-35) [\[138\].](#page-22-11) Context-based approaches include analyzing social media posting patterns, interactions between users, and temporal trends, allowing for a more sophisticated analysis by going beyond single data points and understanding how users interact with each other and their environment.

<span id="page-11-3"></span><span id="page-11-1"></span>For example, Della et al. [\[100\]](#page-21-36) used RNNs to learn hidden representations that capture changes in contextual information of related posts over time. Ma et al. [\[98\]](#page-21-37) focused on capturing the changes in social contextual information over time. They proposed a new time-series model called DSTS(the Dynamic Series-Time Structure) to capture changes in social context features during the spread of event messages, which contributes to distinguishing rumors from nonrumors. Several researchers have proposed new models and frameworks for this purpose. Wu et al. [\[139\]](#page-22-12) rationally selects three information evaluation metrics to distinguish false information and proposes a hybrid deep model to represent the textual meaning of information together with context and capture semantic features of sentiment for the detection of false information. Nguyen et al. [\[103\]](#page-21-38) proposed a novel graphical-based learning framework, FANG(Factual News Graph), to improve the detection of fake news by capturing the social context in a high-resolution representation. Unlike previous context models, FANG focused on learning representations rather than performance. For fake news detection, Shu et al. [\[102\]](#page-21-30) proposed TriFN, a tripartite relationship embedding framework that takes advantage of the social context to simultaneously model publisher-news relationships and user-news interactions. Figure [4.](#page-10-0) depicts the tri-relationship embedding process.

Research has also been conducted using a combination of content-based approaches. For example, Della et al. [\[100\]](#page-21-36) employed a strategy that combines news content and user engagement, specifically likes, to identify fake news, based on the premise that news items receiving a higher number of likes are likely to be more credible. In a different approach, Volkova et al. [\[137\]](#page-22-10) suggested a method that involves extracting the sentiments and psychological conditions of readers from their comments on news articles and then integrating this information with the news content for more effective detection of fake news. Shu et al. [\[101\]](#page-21-39) incorporated social engagement in auxiliary information to correlate user profiles with fake news. Furthermore, Shu et al. [\[140\]](#page-22-13) presented FakenewsNET, a fake news data repository with various features such as news content, social context, and spatio-temporal information to aid in detection.

A large body of research also utilizes propagationbased methods. Propagation-based approaches analyze how information is spread through social networks or other digital platforms to detect misinformation. They focused on determining whether a particular message was true or misleading considering the pattern, speed, and influence of the spread of the information. Liu et al. [\[141\]](#page-22-14) modeled the propagation path of a news story as a multivariate time series.

They constructed a time-series classifier by integrating both recurrent and convolutional networks that captured global and local variations in user characteristics along the propagation path. Silva et al. [\[53\]](#page-20-35) proposed a method for training an autoencoder to learn the embedding of an entire propaganda network based on partial networks to detect fake news as early as possible. Liu et al. [\[142\]](#page-22-15) explored user-specific features for rumor detection in social media environments. Research has also been conducted on the utilization of propagation trees. Ma et al. [\[99\]](#page-21-40) introduced a kernel-based technique called PTK(the Propagation Tree Kernel). This method is designed to identify higher-order patterns unique to various types of rumors by assessing the similarities in their propagation tree structures. Ma et al. [\[88\]](#page-21-9) exploited the inherently recursive nature of propagation trees to accelerate the learning of post-representations by inserting hidden indicator signals into the structure to recognize rumors more accurately.

However, these approaches are not without limitations. Content-based methods often struggle with rapidly evolving misinformation that deviates from established patterns, making them less adaptable to novel or unseen scenarios. Similarly, context-based approaches rely heavily on the availability and accuracy of external data, such as metadata or social context, which may be incomplete or outdated in fast-moving environments like social media. Moreover, both methods can face scalability challenges, particularly when applied to large-scale datasets or real-time detection systems. Addressing these limitations will require integrating information-based techniques with adaptive machine learning frameworks, which can enhance their flexibility and effectiveness in combating misinformation across diverse domains.

#### D. HUMAN AID-BASED APPROACH

Human-assisted approaches utilize human intuition and expertise to detect misinformation. Combining technical analysis with human judgment to obtain more accurate conclusions, even in complex and ambiguous situations. Human emotions, cultural understanding, and deep awareness of the social context are key to this approach. Leveraging the human aspects of how people understand, interact with, and build trust in AI fact-checking systems, along with several automated fact-checking systems that have recently been proposed, can improve the performance of misinformation detection. Various misinformation detection techniques based on information-based approaches are briefly compared in Table [4.](#page-11-0)

<span id="page-12-4"></span><span id="page-12-3"></span><span id="page-12-1"></span><span id="page-12-0"></span>Das et al. [\[143\]](#page-22-16) review key aspects of NLP-based fact checking including human-centered strategies and examine the effectiveness of applying NLP-based fact-checking tools to support human fact checkers. Chung et al. [\[144\]](#page-22-17) showed that people exposed to fake news with fact-checking information evaluate the news more negatively than those without exposure; as this perception increases, their intention to share fake news on social media decreases. Clayton et al. [\[126\]](#page-22-18) show that human engagement has an important impact on detection. They found that people perceive the accuracy of false headlines to be lower when they are labeled with tags such as ''Disputed'' or ''Rated false,'' indicating that they have questioned the news story. Other studies include one that proposes a human-machine collaborative learning system that can assess the truthfulness of news content with a limited number of annotated data samples [\[120\],](#page-21-41) one that envisions a mixed-initiative approach to fact checking that combines human knowledge and experience with the efficiency and scalability of automated information retrieval and machine learning [\[121\].](#page-21-42) Another study leverages guardians (online users who correct misinformation and fake news by referring to URLs to check online facts) to propose a URL recommendation model that allows guardians to participate more in fact-checking activities [\[122\].](#page-21-43)

<span id="page-12-2"></span>Among human-aided approaches, there are studies that utilize the concept of crowd-based. This approach identifies misinformation based on the knowledge and judgments of a large group of users. In Pennycook et al. [\[125\],](#page-22-19) they analyze a potential approach to prioritize the display of content from

<span id="page-13-0"></span>![](_page_13_Figure_2.jpeg)

**FIGURE 5.** Generative approach.

news sources rated as trustworthy by users. They determined whether crowd-sourced credibility ratings can discriminate between the credibility of sources. Kim et al. [\[123\]](#page-22-20) argue that the spread of misinformation can be efficiently reduced through verifiable guarantees, such as users flagging stories as false and having them evaluated by trusted people as they accumulate. Tschiatschek et al. [\[124\]](#page-22-21) developed Detective, which curates a small number of news stories each day, forwards them to expert third-party fact checkers, and aims to stop the spread of news judged to be fake by experts by co-learning users' trust accuracy over time.

Using likes to make judgments has also been proposed. Tacchini et al. [\[51\]](#page-20-29) shows that fake or non-fake posts can be classified with high accuracy based on who has liked a Facebook post, and Della et al. [\[100\]](#page-21-36) detects fake news by fusing news content with users' liking behavior. Their work was based on the intuitive assumption that news with a high number of likes tends to be more true.

<span id="page-13-1"></span>In addition to this, recent advances in large-scale language models, such as ChatGPT, offer promising new approaches to addressing the problem of misinformation. Chen and Shu [\[145\]](#page-22-22) highlight techniques such as external knowledge enhancement and multimodal reasoning that enable LLMs to cross-validate claims across domains, enhancing the detection of complex and domain-specific misinformation. These advances represent a significant step forward in misinformation detection and open new possibilities to improve accuracy and scalability.

However, human-aided approaches also face notable limitations. Human involvement in fact-checking can be resource-intensive and require significant time and effort, which limits the scalability of real-time or large-scale detection tasks. Furthermore, reliance on subjective judgments introduces variability, as individuals' cultural biases, personal beliefs, and differing levels of expertise may affect the consistency and accuracy of evaluations. Crowdsourcing, while scalable, can also suffer from coordination challenges and the potential for manipulation if bad actors exploit the system.

# <span id="page-14-0"></span>**V. LM-GENERATED TEXT TECHNIQUES**

<span id="page-14-3"></span>The detection of LM-generated text-based techniques has been explored using various approaches, including statistical, machine learning, and deep learning, with the recent trend of integrating GNNs with recently pre-trained and generative language models. Several datasets have been developed to benchmark these detection techniques, each contributing uniquely to the field. The M4 data set [\[146\]](#page-22-23) provides a comprehensive benchmark for detecting LM-generated text in multiple languages, domains, and generators, demonstrating the shortcomings of LLM for unseen domains and highlighting the challenge and need for robust detection methods. The MULTITuDE dataset [\[147\]](#page-22-24) provides a largescale, multilingual benchmark for LM-generated text detection, facilitating research in a variety of linguistic contexts. WaterBench [\[148\]](#page-22-25) also provides the first comprehensive evaluation of watermarking algorithms on llms. Benchmarking frameworks such as MGTBench [\[149\]](#page-22-26) systematically assess the robustness of machine-generated text detectors across diverse datasets and adversarial scenarios, offering valuable insight into the strengths and weaknesses of current detection techniques. Additionally, the GPT Reddit Dataset (GRiD) [\[150\]](#page-22-27) offers a novel collection of prompt context pairs from Reddit, distinguishing between human-generated and GPT-generated responses, and serves as a valuable data set for assessing the effectiveness of various detection models. Various representative models that use the generation approach can be seen in Figure [5.](#page-13-0)

# <span id="page-14-5"></span>A. STATISTICAL APPROACH

Although several machine learning and deep learning-based models and techniques have shown promising results, statistical methods still exhibit the best performance. As statistical methods are based on probabilistic models that assess the likelihood of observed data, given certain assumptions or hypotheses, they can help identify anomalies that may indicate deception or signal potential synthetic text to assess the likelihood of an event (or series of events) with an unusual pattern. Feature-based approaches also generate feature vectors from input sequences and use classification techniques through various machine learning models.

<span id="page-14-9"></span><span id="page-14-7"></span><span id="page-14-6"></span>Gehrmann et al. [\[151\]](#page-22-28) developed a detection tool, giant language model test room (GLTR), which applies a collection of basic statistical methods to detect generation artifacts in common sampling schemes. GLTR helps to test the probability of words, absolute rank, entropy of the predicted distribution, etc. for detecting LM-generated texts. The texts used were extracted through rule-based template approaches, such as k-max sampling [\[152\]](#page-22-29) and beam search [\[153\].](#page-22-30) Nguyen-Son et al. [\[154\]](#page-22-31) extracts frequency features by comparing the word distribution frequencies with their <span id="page-14-1"></span>corresponding Zipfian distributions. Because humangenerated text contains more complex syntax than that in computer-generated text, they extracted complex syntactic features, and showed that the higher coherence of human-generated text is quantified at the sentence level using phrasal verbs and at the paragraph level using cross-reference resolution relations incorporated into coherence features. Although statistical methods outperform deep neural networks, they provide additional adversarial robustness, which can be exploited by ensemble detection models [\[7\]. In](#page-19-6) this process, they found that complex syntactic features previously effective for machine-generated text detection have insignificant predictive power compared to that of modern generative models and identified promising statistical features to use instead. The democratization of language-generation models makes it easier to generate human-like text on a large scale for nefarious activities, from spreading misinformation to targeting specific groups with hate speech. Therefore, it is important to understand how people interact with bots and develop methods for detecting bot-generated text. They showed that their method for detecting bot-generated text is more robust across datasets and models using information on how people react directly to the bot's text [\[155\].](#page-22-32) Fröhling et al. [\[156\]](#page-22-33) proposed a simple feature-based classifier for the detection challenge using a model that attempts to model the intrinsic differences between human-generated and machine text. The main advantage of this classifier is its efficiency, which does not depend on the availability of expensive language models and has a comparable level of detection competitiveness.

<span id="page-14-11"></span><span id="page-14-10"></span><span id="page-14-4"></span><span id="page-14-2"></span>Recently, DetectGPT [\[54\]](#page-20-10) has shown promising detection performance, but suffers from significant inefficiencies because it requires scoring hundreds of perturbations with the source LLM to detect a single candidate. To bridge this gap, we propose incorporating a Bayesian surrogate model that can improve query efficiency by selecting typical samples based on Bayesian uncertainty and interpolating the scores of typical samples to other samples.

#### B. GENERATIVE APPROACH

Although several studies have attempted to use machine learning approaches, others have adopted different

<span id="page-14-8"></span>approaches that use deep learning methods. It is particularly powerful for detecting LM-generated text because of its ability to automatically learn hierarchical representations and complex patterns from the data. The sequence modeling of RNNs (that is, RNN and LSTM) and the use of dominant transformer-based models, such as OpenAI's families, have shown promising results in distinguishing between genuine and generated content information that can significantly enforce the model to capture dependencies and relationships between words and phrases in a manner that traditional models cannot. However, the most recent approaches in this area detect LLM-generated text by obtaining data from the LM-generated text or by measuring the similarity, that

<span id="page-15-0"></span>![](_page_15_Figure_2.jpeg)

**FIGURE 6.** A framework of the DetectGPT model [\[54\].](#page-20-10)

is, identifying LLM-generated text by generating text with LLM.

The earliest and most representative model of the generative approach was the fake news conditional generation model of fake news called Grover [\[157\]](#page-22-34) which was created to prevent fake news. Experiments on detecting fake news generated by Grover showed that the best defense against Grover was Grover itself, with 0.92 accuracy. This shows that both exposure bias and sampling strategies that mitigate its effects leave artifacts that can be replicated by similar identifiers and highlights the importance of publicly disclosing robust generators. Building on the success of generative models like Grover, recent advances in Transformer-based architectures have further enhanced the detection of LMgenerated text [\[136\].](#page-22-9)

<span id="page-15-2"></span>Rodriguez et al. [\[158\]](#page-22-35) considered the challenge of detecting technical research texts generated by GPT-2. They first considered a realistic scenario in which the defender does not have full information about the adversary's text generation pipeline but can label a small amount of in-domain genuine and synthetic text to adapt to the target distribution. Even in the extreme scenario of applying a physical domain detector to a biomedical detector, they found that only a few hundred labels were sufficient to achieve good performance. Finally, they demonstrated that paragraph-level detectors can be used to detect tampering with full-length documents under different threat models. Salminen et al. [\[159\]](#page-22-36) experimented with two language models, ULMFiT and GPT-2, to generate fake product reviews based on an Amazon e-Commerce data set. The best model, GPT-2, was used to create a dataset for fake review detection classification. With this dataset, they created a model called fakeRoBERTa, a fine-tuned version of RoBERTa, to perform a fake review detection classification.

However, these approaches are challenging for unsupervised tasks with unlabeled text data, that is, text data with no information about the generation model from which it was generated. To address these challenges, ConDA, a contrastive domain adaptation framework, was developed that combines standard LLM adaptation techniques with the expressive power of contrastive learning to learn domain-invariant representations that are effective in the final unsupervised <span id="page-15-4"></span>detection task [\[160\].](#page-22-37) The approach first uses a pre-trained text summarization model for news, and then a corpus to summarize long news articles into short claims.

<span id="page-15-5"></span>The effectiveness of machine-generated fake news detectors has also been measured by determining their sensitivity to perturbations[\[161\].](#page-22-38) Another model that utilizes perturbations is DetectGPT [\[54\].](#page-20-10) It is a model that aims to detect GPT-generated text and has attracted significant research interest. The model identified and validated the hypothesis that the curvature of the log-likelihood function tends to be significantly more negative in the model samples than that in human text and presented an algorithm to detect fake text based on this hypothesis.

<span id="page-15-8"></span><span id="page-15-7"></span><span id="page-15-6"></span><span id="page-15-1"></span>The basic framework of DetectGPT is depicted in Figure [6.](#page-15-0) several recent studies have developed DetectGPT further. For example, Fast-DetectGPT [\[11\]](#page-19-10) considers the concept of the conditional probability curvature from DetectGPT as a basis metric and optimizes it to improve its performance. Lee and Jang [\[162\]](#page-22-39) generated adversarial examples that appear to be modified by humans, which significantly improved detection accuracy. Another model that utilizes adversarial training is RADAR [\[163\].](#page-22-40) RADAR is built on adversarial training involving a paraphraser and a detector. The paraphraser's objective is to craft content that can evade AI text detection, and RADAR enhances this process by incorporating feedback from the detector. Tulchinskii et al. [\[164\]](#page-22-41) introduced an innovative method that utilizes the intrinsic dimensionality of text samples to robustly detect AI-generated texts. This approach employs the persistent homology dimension estimator to measure the intrinsic dimensionality of text embeddings, showing a significant statistical separation between human-written and AI-generated texts. Recent studies have also explored the utilization of llms, such as ChatGPT, as detection tools themselves. In Bhattacharjee et al. [\[165\]](#page-22-42) conducted an empirical study to assess ChatGPT's effectiveness in differentiating between AI-generated and human-written texts. This research focuses on addressing a specific facet of the challenge and extrapolating further solutions from it, providing insights into the use of ChatGPT and similar llms in automated detection systems.

<span id="page-15-11"></span><span id="page-15-10"></span><span id="page-15-9"></span><span id="page-15-3"></span>As language models continue to advance, ChatGPT has undergone substantial improvements with the release of GPT-4 [\[166\]](#page-22-43) and, more recently, GPT-4o [\[167\].](#page-22-44) The shift from GPT-3.5 to GPT-4 introduced stronger pattern recognition capabilities, while GPT-4o builds on these improvements by enhancing multimodal integration and inference speed, making it a more viable tool for real-time detection applications. These enhancements have expanded ChatGPT's potential as a detection tool. As detection methodologies continue to evolve, LLM-based approaches such as ChatGPT are expected to play an increasingly central role in identifying AI-generated content.

<span id="page-15-12"></span>In addition to the above approaches, research has been conducted on detecting fake text through watermarking methods. SWEET [\[168\],](#page-23-0) which stands for selective watermarking via entropy thresholding, proposes an approach for main<span id="page-16-1"></span>taining the accuracy of the generated code by recognizing green tokens only at high-entropy locations in the token distribution. Here, the watermark code is detected through statistical tests and z-scores based on entropy information. Sadasivan et al. [\[169\]](#page-23-1) applied the paraphrasing attack and spoofing attack techniques to an existing LLM model to evaluate LLM watermarking and showed that the accuracy was poor. Christ et al. [\[170\]](#page-23-2) introduced a cryptographically-inspired method for embedding undetectable watermarks in language models ensuring that the quality of the text remains unchanged and making the watermark undetectable without the key. Krishna et al. [\[171\]](#page-23-3) demonstrated that paraphrasing AI-generated text can significantly reduce the accuracy of various detectors, including DetectGPT and watermark-based methods. To counter this paraphrasing attack, they proposed a search-based method that compares candidate text against a database of previously generated sequences.

<span id="page-16-4"></span>Overall, we argue that the detector performance depends on the data. Larger text data can improve classifier performance [\[172\].](#page-23-4) In addition to the above models, various detectors are available on websites, such as CopyLeaks and GPTKit.

# C. HUMAN AID APPROACH

The human-aided approach in generative text detection focuses on methods that leverage human intuition and expertise to identify LM-generated texts. This approach combines technical analysis with human judgment to reach accurate conclusions even in complex and ambiguous situations. Human emotions, cultural understanding, and deep awareness of social context are the key elements of this methodology.

By combining the unique abilities of humans with AI, this human-aided approach can contribute to the development of more sophisticated and reliable generative text detection systems.

<span id="page-16-6"></span><span id="page-16-5"></span>Dugan et al. [\[173\]](#page-23-5) observed that assessing the quality variance among Natural Language Generation systems and comprehending human perceptions of the LM-generated text are significant yet complex tasks. To tackle these challenges, they presented RoFT, a system that engages users in identifying LM-generated text across diverse domains. Ippolito et al. [\[174\]](#page-23-6) addressed the importance of using both human judgment and automatic detectors, presented a comprehensive study of the model structure of LM-generated text detection systems and their sensitivity to decoding strategies and excerpt length, and analyzed the ability of human evaluators to identify machine-generated content and how it differs from automatic detectors. Clark et al. [\[175\]](#page-23-7) conducted an evaluation on the capability of non-experts to differentiate between texts written by humans and those authored by machines (specifically GPT2 and GPT3) in three categories: stories, news articles, and recipes. Their findings revealed that, without any prior training, evaluators were only able to distinguish GPT3-generated text from human-written text at a level equivalent to random chance.

<span id="page-16-8"></span><span id="page-16-2"></span>Another study effort introduced three novel, interpretable topological attributes for this task, utilizing topological data analysis, a method not extensively explored in NLP [\[10\].](#page-19-9) This study empirically showed that features derived from the BERT model outperformed count-based and neural-based baselines by up to 0.1 on three typical datasets and tended to be the most robust against GPT-style generative models, which is not seen in existing methods. Sheng et al. [\[176\]](#page-23-8) present a framework for recognizing news environments, recognizing that public interest in unexpected new content drives higher exposure and diffusion. It ''zooms out'' into the news environment to capture the environmental cues of news posts. This method designs modules centered on popularity and novelty to improve the performance of a basic fake news detector efficiently.

## <span id="page-16-3"></span><span id="page-16-0"></span>**VI. DISCUSSION**

This section provides a unified and common discussion of misinformation and LM-generated text detection. We consider the role of these detection techniques in responding to various challenges faced in the modern world, and discuss the limitations of these methods and their potential for future development [\[7\],](#page-19-6) [\[20\]. T](#page-19-16)hus, we hope to identify new directions and opportunities to improve the reliability of information in the digital environment and prevent the spread of misinformation.

While this research is comprehensive in its current scope, it faces limitations and challenges related to the availability and quality of data, potential biases in models, and the evolving nature of fake text detection [\[18\],](#page-19-23) [\[30\].](#page-19-18) In particular, current datasets are often limited in scope, lack diversity, and fail to represent the variety of misinformation encountered across languages and cultures. These constraints hinder the generalizability and adaptability of detection techniques, highlighting the need for more comprehensive and representative datasets. For example, in educational contexts, a comparative study found that the accuracy of LLM-generated text detectors decreased significantly when advanced paraphrasing techniques were applied, underscoring the critical role of domain-specific and robust datasets in maintaining detection performance [\[177\].](#page-23-9)

<span id="page-16-9"></span>The development and application of fake text detection technologies involve significant social and ethical implications. This discussion centers on identifying a balance between the free flow of information and protection of individual rights. The ethical considerations of deploying these technologies, particularly privacy, censorship, and the potential for misuse, must be examined in depth [\[21\],](#page-19-22) [\[178\].](#page-23-10)

<span id="page-16-10"></span><span id="page-16-7"></span>Rapid advances in AI, including machine learning techniques, have had a significant impact on the generation and detection of fake text. This discussion explored the positive and negative aspects of these technological advancements. Although offering innovative solutions, it also raises concerns about the potential misuse of these technologies and the challenge of staying up-to-date with sophisticated AI-generated text [\[20\].](#page-19-16) Moreover, the landscape of text generation with LLMs is constantly changing, with the recent development of low-cost and accessible LLMs like DeepSeek [\[179\].](#page-23-11)

<span id="page-17-2"></span><span id="page-17-1"></span><span id="page-17-0"></span>The integration of detection technologies across various fields presents a multifaceted approach. AI-generated text is already being studied in education, politics, business, healthcare, etc. [\[180\],](#page-23-12) [\[181\],](#page-23-13) [\[182\],](#page-23-14) [\[183\],](#page-23-15) [\[184\],](#page-23-16) [\[185\].](#page-23-17) Each field presents unique challenges and opportunities for the application of detection technologies, and understanding these nuances is critical for the effective implementation and social acceptance of these systems.

Building on these integrations, the detection methods discussed in this paper offer significant practical applications across various domains. For example, in education, they can be utilized to detect AI-generated assignments or essays, ensuring academic integrity. In healthcare, these methods can prevent the spread of harmful misinformation in medical advice or fabricated research summaries. Legal systems can also benefit from these techniques by using them to verify the authenticity of documents and identify potential fraud. In higher education, the combination of AI detection tools with human judgment has proven to be effective in identifying GPT-4-generated content, addressing the limitations of standalone systems, and strengthening academic integrity [\[186\].](#page-23-18)

<span id="page-17-7"></span>In addition, these detection technologies can enhance the moderation of social media content by identifying and managing AI-generated misinformation at scale. By incorporating these methods into automated moderation systems, platforms can reduce the spread of deceptive content and contribute to a more trustworthy digital ecosystem. These applications demonstrate the transformative potential of detection technologies in addressing real-world challenges across multiple fields.

The detection methods discussed in this study demonstrate significant cross-domain applicability, reflecting their ability to operate effectively across diverse contexts and problem spaces. Many of the approaches designed for misinformation detection, such as fact-checking and semantic analysis, inherently support transitions to LM-generated text detection by using overlapping features such as linguistic consistency and contextual coherence [\[187\].](#page-23-19) Similarly, techniques tailored for LM-generated text detection, such as stylometry and syntactic analysis, can improve misinformation detection by providing additional insights into text generation patterns and structural anomalies [\[188\].](#page-23-20)

<span id="page-17-9"></span><span id="page-17-8"></span>This interconnectivity highlights the dual utility of detection systems, enabling seamless transitions and mutual reinforcement between the two primary detection categories. For example, domain adaptation techniques and transfer learning frameworks have proven effective in bridging the gap between different domains within misinformation detection, such as political versus health-related content, while also demonstrating scalability to LM-generated text <span id="page-17-10"></span>scenarios [\[189\].](#page-23-21) These approaches illustrate how detection systems can generalize their functionality without compromising on domain-specific accuracy, effectively addressing unique and overlapping challenges across categories.

The mutual adaptability of misinformation detection and LM-generated text detection not only underscores the robustness of current methodologies but also establishes a foundation for future advancements. By integrating domainagnostic principles with category-specific features, these detection methods offer practical solutions to manage the evolving landscape of fake text in real-world applications.

<span id="page-17-6"></span><span id="page-17-5"></span><span id="page-17-4"></span><span id="page-17-3"></span>In light of the challenges discussed, it is evident that addressing the limitations of existing detection techniques and models requires significant innovation and exploration. The critical areas that require attention include overcoming data set biases, ensuring scalability to diverse contexts, and developing adaptive detection methods that can evolve along with rapid advancements in AI-generated text. These areas represent the foundation for future breakthroughs in fake text detection.

# A. LIMITATIONS

# 1) DATA AND BIASES

One of the primary limitations identified in the surveyed research is related to data availability, quality, and inherent model biases. High-quality, diverse datasets are crucial for training robust detection models [\[147\].](#page-22-24) The use of datasets that reflect a wide range of languages, cultural contexts, and domains has the potential to produce not only more accurate but also highly varied results. Using such data sets, detection methods could yield results that are not only more accurate but also more generalizable to new and unseen contexts.

<span id="page-17-13"></span><span id="page-17-12"></span><span id="page-17-11"></span>However, many studies are constrained by the availability of such datasets. In many cases, existing data sets are limited in scope, biased, or do not adequately represent the vast diversity of misinformation and LM-generated text encountered in real-world contexts [\[190\],](#page-23-22) [\[191\],](#page-23-23) [\[192\].](#page-23-24) As a result, detection techniques often struggle to generalize across different scenarios, making their effectiveness uneven and context-dependent.

Moreover, the dynamic nature of online content and the rapid evolution of AI-generated texts mean that detection models may quickly become outdated. This requires continuous data collection and model retraining, which requires significant resources and time. Without sustained efforts to address these challenges, the ability of detection methods to maintain their effectiveness over time will remain a significant obstacle.

#### 2) EVOLVING NATURE OF FAKE TEXT

The constantly evolving nature of fake text presents another major limitation. As AI and machine learning technologies advance, so do the techniques used to generate misinformation and misleading texts. This ongoing evolution makes it difficult to develop detection methods that remain effective over time. It further complicates the task of detection, especially for powerful attacks such as recursive paraphrasing, which can break recently proposed watermarking and retrieval-based detectors with minimal degradation of text quality [\[169\].](#page-23-1)

<span id="page-18-2"></span><span id="page-18-1"></span>The use of llms like ChatGPT introduces both significant advancements and new challenges in the detection of fake text. Despite their impressive capabilities, these models often struggle with maintaining factual accuracy, especially in complex or domain-specific contexts. Attackers misuse LLMs in creative ways, such as changing prompts, modifying text, etc. [\[169\],](#page-23-1) [\[193\],](#page-23-25) [\[194\].](#page-23-26) Additionally, the black-box nature of these models complicates efforts to trace the source of errors or inconsistencies in generated output. Such limitations underscore the need for detection methods that are not only robust, but also capable of adapting to the unique weaknesses of these advanced language models.

# FUTURE DIRECTIONS

Future research in fake text detection should address several key areas to enhance the robustness, applicability, and effectiveness of detection techniques. These areas include domain-specific research, handling various creations using llms, and developing sufficient datasets for detection.

#### 3) DOMAIN-SPECIFIC RESEARCH

As identified in our limitations, there is a pressing need for more domain-specific research on fake text detection. Future studies should focus on developing customized detection methods that can handle the unique characteristics of specialized fields such as healthcare, finance, legal documents, and more [\[160\],](#page-22-37) [\[183\],](#page-23-15) [\[185\],](#page-23-17) [\[195\].](#page-23-27) This involves creating domain-specific datasets and models that understand and process the nuanced language used in these areas. By addressing these specific challenges, researchers can improve the accuracy and reliability of detection techniques in specialized contexts, ensuring that they are effective across a wide range of applications.

#### 4) VARIOUS CREATIONS USING LLMs

As mentioned in the Limitations, llms have brought several challenges along with their advancements. Future research should explore ways to deal with the diverse output produced by these models, which includes not only detecting AI-generated text, but also understanding the underlying mechanisms that generate it. This involves not only detecting AI-generated text but also understanding the underlying mechanisms that generate these texts. Researchers should investigate adaptive detection techniques that can respond to the dynamic nature of LLM output, ensuring that detection methods remain robust as these technologies evolve. In addition, studying adversarial attacks, such as prompt engineering and recursive paraphrasing, will be crucial to developing more resilient detection systems.

## 5) SUFFICIENT DATASETS

A significant challenge in fake text detection is the lack of high-quality, diverse datasets. Future research should prioritize the development of comprehensive benchmark datasets that encompass a variety of languages, cultures, and contexts. These datasets should be large-scale and include examples of both genuine and fake text across different domains. Benchmark datasets will enable more rigorous testing and evaluation of detection methods. In addition, creating standardized datasets can help unify research efforts and promote collaboration among researchers in the field.

#### 6) REDUCING FALSE POSITIVES

<span id="page-18-4"></span>As detection technology advances and becomes more widely used, false detection of human-written text as fake text will occur [\[196\].](#page-23-28) Therefore, research is needed to establish clear criteria and guidelines for acceptable error rates to minimize the risk of misidentification. Of course, focusing on increasing the accuracy of the system by developing sophisticated algorithms is key. Future research should make efforts to reduce the false positive rate to maintain trust in the detection system and ensure its practical usefulness.

#### 7) REAL-WORD APPLICATIONS

<span id="page-18-3"></span>Finally, future research should focus on the implementation and impact of real-world fake text detection systems. This involves studying the integration of detection technologies into various platforms and applications, such as social media, news outlets, and educational institutions. Much of the research related to fake text detection is often limited to algorithm development. Researchers should explore the practical challenges of deploying such systems, such as user acceptability, scalability, and ongoing maintenance. Researchers should explore the practical challenges of deploying these systems, including user acceptance, scalability, and ongoing maintenance. Evaluating the effectiveness of detection methods in real-world scenarios will provide valuable insights into their practical utility and help identify areas for improvement.

#### <span id="page-18-0"></span>**VII. CONCLUSION**

This paper presents an in-depth exploration of the field of misinformation and LM-generated text detection, emphasizing the importance of distinguishing between these two types of fake text. Through this study, we highlighted the various implications of these detection technologies and analyzed the complexity and nuanced nature of current detection methods.

In this paper, we classified fake text into two main types: misinformation and LM-generated text. We specified and defined the unique challenges associated with each type, providing a detailed examination of the difficulties in detecting and mitigating these texts. In addition, we presented and evaluated several detection techniques tailored to address the specific issues posed by both misinformation and LM-generated text. These methods are crucial tools for effectively identifying and combating fake text in various contexts.

This study also highlights the significant challenges posed by text generation models, which require careful attention in the era of AI-generated content. We emphasize the importance of detecting both LM-generated text and misinformation and provide an overview of the trends and challenges in related technologies. By addressing these issues, we aim to contribute to the development and practical application of advanced detection techniques.

Despite the comprehensive nature of our survey, we identified several limitations within the current research, such as data availability and quality, model biases, and the evolving nature of fake text. These limitations highlight the need for continuous updates and improvements in detection technologies. Future research should focus on domain-specific studies, the development of robust methods to handle various outputs of llms, and the creation of high-quality, diverse datasets. Additionally, it is crucial to improve detection accuracy and reduce false positives to maintain the reliability and trustworthiness of these systems. Establishing clear standards and fostering social dialogue will help mitigate the risks of misidentification and ensure the responsible use of detection technologies.

We believe that this study will contribute to the development and practical application of advanced detection techniques. These efforts will help society respond more systematically to the challenge of fake text detection, minimize associated risks, and harness the full potential of AI. Finally, this study is expected to positively impact researchers in this ever-evolving field.

## **DECLARATION OF COMPETING INTEREST**

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this article.

#### **REFERENCES**

- <span id="page-19-0"></span>[\[1\] B](#page-0-0). Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz, E. Agirre, I. Heintz, and D. Roth, ''Recent advances in natural language processing via large pre-trained language models: A survey,'' *ACM Comput. Surveys*, vol. 56, no. 2, pp. 1–40, Feb. 2024.
- <span id="page-19-1"></span>[\[2\] I](#page-0-1). A. Zahid, S. S. Joudar, A. S. Albahri, O. S. Albahri, A. H. Alamoodi, J. Santamaría, and L. Alzubaidi, ''Unmasking large language models by means of OpenAI GPT-4 and Google AI: A deep instruction-based analysis,'' *Intell. Syst. Appl.*, vol. 23, Sep. 2024, Art. no. 200431.
- <span id="page-19-2"></span>[\[3\] E](#page-0-2). Brynjolfsson, D. Li, and L. R. Raymond, ''Generative AI at work,'' Nat. Bur. Econ. Res., Cambridge, MA, USA, Working Paper 31161, 2023.
- <span id="page-19-3"></span>[\[4\] L](#page-0-3). Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. E. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe, ''Training language models to follow instructions with human feedback,'' in *Proc. Adv. Neural Inf. Process. Syst.*, vol. 35, Jan. 2022, pp. 27730–27744.
- <span id="page-19-4"></span>[\[5\] I](#page-0-4). A. Zahid and S. Sabbar Joudar, ''Does lack of knowledge and hardship of information access signify powerful AI? A large language model perspective,'' *Appl. Data Sci. Anal.*, vol. 2023, pp. 150–154, Dec. 2023.
- <span id="page-19-5"></span>[\[6\] L](#page-0-5). Wu, F. Morstatter, K. M. Carley, and H. Liu, ''Misinformation in social media: Definition, manipulation, and detection,'' *ACM SIGKDD Explorations Newslett.*, vol. 21, no. 2, pp. 80–90, Nov. 2019.

- <span id="page-19-6"></span>[\[7\] E](#page-0-6). N. Crothers, N. Japkowicz, and H. L. Viktor, ''Machine-generated text: A comprehensive survey of threat models and detection methods,'' *IEEE Access*, vol. 11, pp. 70977–71002, 2023.
- <span id="page-19-7"></span>[\[8\] N](#page-0-7). Collier, S. Doan, A. Kawazoe, R. M. Goodwin, M. Conway, Y. Tateno, Q.-H. Ngo, D. Dien, A. Kawtrakul, K. Takeuchi, M. Shigematsu, and K. Taniguchi, ''BioCaster: Detecting public health rumors with a Web-based text mining system,'' *Bioinformatics*, vol. 24, no. 24, pp. 2940–2941, Dec. 2008.
- <span id="page-19-8"></span>[\[9\] A](#page-0-8). Ntoulas, M. Najork, M. Manasse, and D. Fetterly, ''Detecting spam Web pages through content analysis,'' in *Proc. 15th Int. Conf. World Wide Web*, May 2006, pp. 83–92.
- <span id="page-19-9"></span>[\[10\]](#page-0-9) L. Kushnareva, D. Cherniavskii, V. Mikhailov, E. Artemova, S. Barannikov, A. Bernstein, I. Piontkovskaya, D. Piontkovski, and E. Burnaev, ''Artificial text detection via examining the topology of attention maps,'' 2021, *arXiv:2109.04825*.
- <span id="page-19-10"></span>[\[11\]](#page-1-1) G. Bao, Y. Zhao, Z. Teng, L. Yang, and Y. Zhang, ''Fast-DetectGPT: Efficient zero-shot detection of machine-generated text via conditional probability curvature,'' 2023, *arXiv:2310.05130*.
- <span id="page-19-29"></span>[\[12\]](#page-0-10) P. Fortuna and S. Nunes, ''A survey on automatic detection of hate speech in text,'' *ACM Comput. Surv.*, vol. 51, no. 4, pp. 1–30, Jul. 2019.
- <span id="page-19-30"></span>[\[13\]](#page-0-10) A. Zubiaga, E. Kochkina, M. Liakata, R. Procter, M. Lukasik, K. Bontcheva, T. Cohn, and I. Augenstein, ''Discourse-aware rumour stance classification in social media using sequential classifiers,'' *Inf. Process. Manage.*, vol. 54, no. 2, pp. 273–290, Mar. 2018.
- <span id="page-19-17"></span>[\[14\]](#page-0-10) X. Zhou and R. Zafarani, ''A survey of fake news: Fundamental theories, detection methods, and opportunities,'' 2018, *arXiv:1812.00315*.
- <span id="page-19-12"></span>[\[15\]](#page-0-10) B. Guo, Y. Ding, L. Yao, Y. Liang, and Z. Yu, ''The future of misinformation detection: New perspectives and trends,'' 2019, *arXiv:1909.03654*.
- <span id="page-19-14"></span>[\[16\]](#page-0-10) A. Bondielli and F. Marcelloni, ''A survey on fake news and rumour detection techniques,'' *Inf. Sci.*, vol. 497, pp. 38–55, Sep. 2019.
- <span id="page-19-20"></span>[\[17\]](#page-0-10) X. Zhang and A. A. Ghorbani, ''An overview of online fake news: Characterization, detection, and discussion,'' *Inf. Process. Manage.*, vol. 57, no. 2, Mar. 2020, Art. no. 102025.
- <span id="page-19-23"></span>[\[18\]](#page-0-10) G. Jawahar, M. Abdul-Mageed, and L. V. S. Lakshmanan, ''Automatic detection of machine generated text: A critical survey,'' 2020, *arXiv:2011.01314*.
- <span id="page-19-19"></span>[\[19\]](#page-0-10) L. Hu, S. Wei, Z. Zhao, and B. Wu, ''Deep learning for fake news detection: A comprehensive survey,'' *AI Open*, vol. 3, pp. 133–155, Jul. 2022.
- <span id="page-19-16"></span>[\[20\]](#page-0-10) E. Aïmeur, S. Amri, and G. Brassard, ''Fake news, disinformation and misinformation in social media: A review,'' *Social Netw. Anal. Mining*, vol. 13, no. 1, p. 30, Feb. 2023.
- <span id="page-19-22"></span>[\[21\]](#page-0-10) P. P. Ray, ''ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope,'' *Internet Things Cyber-Phys. Syst.*, vol. 3, pp. 121–154, Jul. 2023.
- <span id="page-19-24"></span>[\[22\]](#page-0-10) J. Wu, S. Yang, R. Zhan, Y. Yuan, D. F. Wong, and L. S. Chao, ''A survey on LLM-generated text detection: Necessity, methods, and future directions,'' 2023, *arXiv:2310.14724*.
- <span id="page-19-25"></span>[\[23\]](#page-0-10) M. Dhaini, W. Poelman, and E. Erdogan, ''Detecting ChatGPT: A survey of the state of detecting ChatGPT-generated text,'' 2023, *arXiv:2309.07689*.
- <span id="page-19-27"></span>[\[24\]](#page-0-10) R. Tang, Y.-N. Chuang, and X. Hu, ''The science of detecting LLMgenerated text,'' *Commun. ACM*, vol. 67, no. 4, pp. 50–59, Apr. 2024.
- <span id="page-19-15"></span>[\[25\]](#page-0-10) S. Abdali, S. Shaham, and B. Krishnamachari, ''Multi-modal misinformation detection: Approaches, challenges and opportunities,'' *ACM Comput. Surveys*, vol. 57, no. 3, pp. 1–29, Mar. 2025.
- <span id="page-19-26"></span>[\[26\]](#page-0-10) L. Lin, N. Gupta, Y. Zhang, H. Ren, C.-H. Liu, F. Ding, X. Wang, X. Li, L. Verdoliva, and S. Hu, ''Detecting multimedia generated by large AI models: A survey,'' 2024, *arXiv:2402.00045*.
- <span id="page-19-28"></span>[\[27\]](#page-0-10) A. Liu, L. Pan, Y. Lu, J. Li, X. Hu, X. Zhang, L. Wen, I. King, H. Xiong, and P. Yu, ''A survey of text watermarking in the era of large language models,'' *ACM Comput. Surveys*, vol. 57, no. 2, pp. 1–36, Feb. 2025.
- <span id="page-19-11"></span>[\[28\]](#page-2-2) E. Ferrara, ''Social bot detection in the age of ChatGPT: Challenges and opportunities,'' *1st Monday*, vol. 2023, pp. 1–30, Jun. 2023.
- <span id="page-19-13"></span>[\[29\]](#page-3-0) M. R. Islam, S. Liu, X. Wang, and G. Xu, ''Deep learning for misinformation detection on online social networks: A survey and new perspectives,'' *Social Netw. Anal. Mining*, vol. 10, no. 1, p. 82, Dec. 2020.
- <span id="page-19-18"></span>[\[30\]](#page-3-1) X. Zhou and R. Zafarani, ''A survey of fake news: Fundamental theories, detection methods, and opportunities,'' *ACM Comput. Surv.*, vol. 53, no. 5, pp. 1–40, Sep. 2021.
- <span id="page-19-21"></span>[\[31\]](#page-3-2) R. Oshikawa, J. Qian, and W. Y. Wang, ''A survey on natural language processing for fake news detection,'' 2018, *arXiv:1811.00770*.

- <span id="page-20-0"></span>[\[32\]](#page-3-3) Y. Wang, M. McKee, A. Torbica, and D. Stuckler, ''Systematic literature review on the spread of health-related misinformation on social media,'' *Social Sci. Med.*, vol. 240, Nov. 2019, Art. no. 112552.
- <span id="page-20-1"></span>[\[33\]](#page-3-4) Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung, ''Survey of hallucination in natural language generation,'' *ACM Comput. Surveys*, vol. 55, no. 12, pp. 1–38, Dec. 2023.
- <span id="page-20-2"></span>[\[34\]](#page-4-2) M. D. Vicario, A. Bessi, F. Zollo, F. Petroni, A. Scala, G. Caldarelli, H. E. Stanley, and W. Quattrociocchi, ''The spreading of misinformation online,'' *Proc. Nat. Acad. Sci. USA*, vol. 113, no. 3, pp. 554–559, Jan. 2016.
- <span id="page-20-3"></span>[\[35\]](#page-4-3) N. Hu, L. Liu, and J. J. Zhang, ''Do online reviews affect product sales? The role of reviewer characteristics and temporal effects,'' *Inf. Technol. Manage.*, vol. 9, no. 3, pp. 201–214, Sep. 2008.
- <span id="page-20-4"></span>[\[36\]](#page-4-4) D. I. Adelani, H. Mai, F. Fang, H. H. Nguyen, J. Yamagishi, and I. Echizen, ''Generating sentiment-preserving fake online reviews using neural language models and their human-and machine-based detection,'' *IEEE Access*, vol. 8, pp. 1341–1354, 2020.
- <span id="page-20-5"></span>[\[37\]](#page-4-5) H. Allcott and M. Gentzkow, ''Social media and fake news in the 2016 election,'' *J. Econ. Perspect.*, vol. 31, no. 2, pp. 211–236, May 2017.
- <span id="page-20-6"></span>[\[38\]](#page-5-0) A. Gelfert, ''Fake news: A definition,'' *Informal Log.*, vol. 38, no. 1, pp. 84–117, Mar. 2018.
- <span id="page-20-7"></span>[\[39\]](#page-5-1) N. DiFonzo and P. Bordia, ''Rumor, gossip and urban legends,'' *Diogenes*, vol. 54, no. 1, pp. 19–35, Feb. 2007.
- <span id="page-20-8"></span>[\[40\]](#page-5-2) P. Hernon, ''Disinformation and misinformation through the Internet: Findings of an exploratory study,'' *Government Inf. Quart.*, vol. 12, no. 2, pp. 133–139, Jan. 1995.
- <span id="page-20-9"></span>[\[41\]](#page-5-3) S. Kumar, R. West, and J. Leskovec, ''Disinformation on the Web: Impact, characteristics, and detection of Wikipedia hoaxes,'' in *Proc. 25th Int. Conf. World Wide Web*, Apr. 2016, pp. 591–602.
- <span id="page-20-24"></span>[\[42\]](#page-0-10) V. Qazvinian, E. Rosengren, D. Radev, and Q. Mei, ''Rumor has it: Identifying misinformation in microblogs,'' in *Proc. Conf. Empirical Methods Natural Lang. Process.*, Jul. 2011, pp. 1589–1599.
- <span id="page-20-21"></span>[\[43\]](#page-0-10) F. Yang, Y. Liu, X. Yu, and M. Yang, ''Automatic detection of rumor on sina Weibo,'' in *Proc. ACM SIGKDD Workshop Mining Data Semantics*, Aug. 2012, pp. 1–7.
- <span id="page-20-33"></span>[\[44\]](#page-0-10) H. Zhang, Z. Fan, J. Zheng, and Q. Liu, ''An improving deception detection method in computer-mediated communication,'' *J. Netw.*, vol. 7, no. 11, p. 1811, Nov. 2012.
- <span id="page-20-18"></span>[\[45\]](#page-0-10) S. Kwon, M. Cha, K. Jung, W. Chen, and Y. Wang, ''Prominent features of rumor propagation in online social media,'' in *Proc. IEEE 13th Int. Conf. Data Mining*, Dec. 2013, pp. 1103–1108.
- <span id="page-20-27"></span>[\[46\]](#page-0-10) Q. Zhang, S. Zhang, J. Dong, J. Xiong, and X. Cheng, ''Automatic detection of rumor on social network,'' in *Proc. 4th CCF Conf. Natural Lang. Process. Chin. Comput.*, Nanchang, China. Cham, Switzerland: Springer, Jan. 2015, pp. 113–122.
- <span id="page-20-28"></span>[\[47\]](#page-0-10) L. Zeng, K. Starbird, and E. Spiro, ''# Unconfirmed: Classifying rumor stance in crisis-related social media messages,'' in *Proc. Int. AAAI Conf. Web Social Media*, 2016, vol. 10, no. 1, pp. 747–750.
- <span id="page-20-19"></span>[\[48\]](#page-0-10) L. Wu, J. Li, X. Hu, and H. Liu, ''Gleaning wisdom from the past: Early detection of emerging rumors in social media,'' in *Proc. SIAM Int. Conf. Data Mining*, Jun. 2017, pp. 99–107.
- <span id="page-20-20"></span>[\[49\]](#page-0-10) N. Ruchansky, S. Seo, and Y. Liu, ''CSI: A hybrid deep model for fake news detection,'' in *Proc. ACM Conf. Inf. Knowl. Manage.*, Nov. 2017, pp. 797–806.
- <span id="page-20-26"></span>[\[50\]](#page-0-10) Z. Jin, J. Cao, H. Guo, Y. Zhang, Y. Wang, and J. Luo, ''Detection and analysis of 2016 U.S. presidential election related rumors on Twitter,'' in *Proc. 10th Int. Conf. Social, Cultural, Behav. Modeling*, Washington, DC, USA.Cham, Switzerland: Springer, Jan. 2017, pp. 14–24.
- <span id="page-20-29"></span>[\[51\]](#page-0-10) E. Tacchini, G. Ballarin, M. L. D. Vedova, S. Moret, and L. de Alfaro, ''Some like it hoax: Automated fake news detection in social networks,'' 2017, *arXiv:1704.07506*.
- <span id="page-20-22"></span>[\[52\]](#page-0-10) J. Zhang, B. Dong, and P. S. Yu, ''FakeDetector: Effective fake news detection with deep diffusive neural network,'' in *Proc. IEEE 36th Int. Conf. Data Eng. (ICDE)*, Apr. 2020, pp. 1826–1829.
- <span id="page-20-35"></span>[\[53\]](#page-0-10) A. Silva, Y. Han, L. Luo, S. Karunasekera, and C. Leckie, ''Propagation2Vec: Embedding partial propagation networks for explainable fake news early detection,'' *Inf. Process. Manage.*, vol. 58, no. 5, Sep. 2021, Art. no. 102618.
- <span id="page-20-10"></span>[\[54\]](#page-6-2) E. Mitchell, Y. Lee, A. Khazatsky, C. D. Manning, and C. Finn, ''Detect-GPT: Zero-shot machine-generated text detection using probability curvature,'' in *Proc. Int. Conf. Mach. Learn.*, Jan. 2023, pp. 24950–24962.

- <span id="page-20-11"></span>[\[55\]](#page-6-3) D. R. E. Cotton, P. A. Cotton, and J. R. Shipway, ''Chatting and cheating: Ensuring academic integrity in the era of ChatGPT,'' *Innov. Educ. Teaching Int.*, vol. 61, no. 2, pp. 228–239, Mar. 2024.
- <span id="page-20-12"></span>[\[56\]](#page-6-4) L. Eliot. (2022). *Enraged Worries That Generative AI ChatGPT Spurs Students to Vastly Cheat When Writing Essays, Spawns Spellbound Attention for AI Ethics and AI Law*. Forbes. [Online]. Available: https://www.forbes.com/sites/lanceeliot/2022/12/18/enraged-worriesthat-generative-ai-chatgpt-spurs-students-to-vastly-cheat-when-writingessays-spawns-spellbound-attention-for-ai-ethics-and-ai-law
- <span id="page-20-13"></span>[\[57\]](#page-6-5) H. Alkaissi and S. I. McFarlane, ''Artificial hallucinations in ChatGPT: Implications in scientific writing,'' *Cureus*, vol. 15, no. 2, Feb. 2023, Art. no. e35179.
- <span id="page-20-14"></span>[\[58\]](#page-6-6) Y. Xie, F. Yu, K. Achan, R. Panigrahy, G. Hulten, and I. Osipkov, ''Spamming botnets: Signatures and characteristics,'' *ACM SIGCOMM Comput. Commun. Rev.*, vol. 38, no. 4, pp. 171–182, 2008.
- <span id="page-20-15"></span>[\[59\]](#page-6-7) S. Afroz, M. Brennan, and R. Greenstadt, ''Detecting hoaxes, frauds, and deception in writing style online,'' in *Proc. IEEE Symp. Secur. Privacy*, May 2012, pp. 461–475.
- <span id="page-20-16"></span>[\[60\]](#page-6-8) C. Castillo, M. Mendoza, and B. Poblete, ''Information credibility on Twitter,'' in *Proc. 20th Int. Conf. World Wide Web*, Mar. 2011, pp. 675–684.
- <span id="page-20-17"></span>[\[61\]](#page-6-9) S. Hamidian and M. T. Diab, ''Rumor detection and classification for Twitter data,'' 2019, *arXiv:1912.08926*.
- <span id="page-20-23"></span>[\[62\]](#page-7-1) A. Hanselowski, P. V. S. Avinesh, B. Schiller, F. Caspelherr, D. Chaudhuri, C. M. Meyer, and I. Gurevych, ''A retrospective analysis of the fake news challenge stance detection task,'' 2018, *arXiv:1806.05180*.
- <span id="page-20-25"></span>[\[63\]](#page-7-2) K. S. Jones, ''A statistical interpretation of term specificity and its application in retrieval,'' *J. Document.*, vol. 28, no. 1, pp. 11–21, 1972.
- <span id="page-20-30"></span>[\[64\]](#page-8-0) D. R. Cox, ''The regression analysis of binary sequences,'' *J. Roy. Stat. Soc., B, Stat. Methodol.*, vol. 20, no. 2, pp. 215–232, Jul. 1958.
- <span id="page-20-31"></span>[\[65\]](#page-8-1) O. Enayet and S. R. El-Beltagy, ''NileTMRG at SemEval-2017 task 8: Determining rumour and veracity support for rumours on Twitter,'' in *Proc. 11th Int. Workshop Semantic Eval. (SemEval)*, 2017, pp. 470–474.
- <span id="page-20-32"></span>[\[66\]](#page-8-2) C. Cortes and V. Vapnik, ''Support-vector networks,'' *Mach. Learn.*, vol. 20, no. 3, pp. 273–297, Sep. 1995.
- <span id="page-20-34"></span>[\[67\]](#page-8-3) G. Giasemidis, C. Singleton, I. Agrafiotis, J. R. C. Nurse, A. Pilgrim, C. Willis, and D. V. Greetham, ''Determining the veracity of rumours on Twitter,'' in *Proc. 8th Int. Conf. Social Inform.*, Bellevue, WA, USA. Cham, Switzerland: Springer, Jan. 2016, pp. 185–205.
- <span id="page-20-36"></span>[\[68\]](#page-8-4) J. R. Quinlan, ''Induction of decision trees,'' *Mach. Learn.*, vol. 1, no. 1, pp. 81–106, Mar. 1986.
- <span id="page-20-37"></span>[\[69\]](#page-8-5) L. Breiman, ''Random forests,'' *Mach. Learn.*, vol. 45, pp. 5–32, Jan. 2001.
- <span id="page-20-38"></span>[\[70\]](#page-8-6) T. Bodnar, C. Tucker, K. Hopkinson, and S. G. Bilén, ''Increasing the veracity of event detection on social media networks through user trust modeling,'' in *Proc. IEEE Int. Conf. Big Data (Big Data)*, Oct. 2014, pp. 636–643.
- <span id="page-20-39"></span>[\[71\]](#page-8-7) A. Aker, L. Derczynski, and K. Bontcheva, ''Simple open stance classification for rumour analysis,'' 2017, *arXiv:1708.05286*.
- <span id="page-20-40"></span>[\[72\]](#page-8-8) F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, ''The graph neural network model,'' *IEEE Trans. Neural Netw.*, vol. 20, no. 1, pp. 61–80, Dec. 2008.
- <span id="page-20-41"></span>[\[73\]](#page-8-9) D. Bahdanau, K. Cho, and Y. Bengio, ''Neural machine translation by jointly learning to align and translate,'' 2014, *arXiv:1409.0473*.
- <span id="page-20-42"></span>[\[74\]](#page-8-10) Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, ''Backpropagation applied to handwritten zip code recognition,'' *Neural Comput.*, vol. 1, no. 4, pp. 541–551, Dec. 1989.
- <span id="page-20-43"></span>[\[75\]](#page-8-11) D. E. Rumelhart, G. E. Hinton, and R. J. Williams, ''Learning representations by back-propagating errors,'' *Nature*, vol. 323, no. 6088, pp. 533–536, Oct. 1986.
- <span id="page-20-44"></span>[\[76\]](#page-8-12) S. Hochreiter and J. Schmidhuber, ''Long short-term memory,'' *Neural Comput.*, vol. 9, no. 8, pp. 1735–1780, Nov. 1997.
- <span id="page-20-45"></span>[\[77\]](#page-8-13) K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, ''Learning phrase representations using RNN encoder–decoder for statistical machine translation,'' 2014, *arXiv:1406.1078*.
- <span id="page-20-46"></span>[\[78\]](#page-8-14) Y.-C. Chen, Z.-Y. Liu, and H.-Y. Kao, ''IKM at SemEval-2017 task 8: Convolutional neural networks for stance detection and rumor verification,'' in *Proc. 11th Int. Workshop Semantic Eval. (SemEval)*, 2017, pp. 465–469.

- <span id="page-21-0"></span>[\[79\]](#page-8-15) F. Yu, Q. Liu, S. Wu, L. Wang, and T. Tan, ''A convolutional approach for misinformation identification,'' in *Proc. 26th Int. Joint Conf. Artif. Intell.*, Aug. 2017, pp. 3901–3907.
- <span id="page-21-1"></span>[\[80\]](#page-8-16) N. Dhamani, P. Azunre, J. L. Gleason, C. Corcoran, G. Honke, S. Kramer, and J. Morgan, ''Using deep networks and transfer learning to address disinformation,'' 2019, *arXiv:1905.10412*.
- <span id="page-21-2"></span>[\[81\]](#page-8-17) S. Kumar, R. Asthana, S. Upadhyay, N. Upreti, and M. Akbar, ''Fake news detection using deep learning models: A novel approach,'' *Trans. Emerg. Telecommun. Technol.*, vol. 31, no. 2, p. 3767, Feb. 2020.
- <span id="page-21-3"></span>[\[82\]](#page-8-18) W. Wang, F. Zhang, X. Luo, and S. Zhang, ''PDRCNN: Precise phishing detection with recurrent convolutional neural networks,'' *Secur. Commun. Netw.*, vol. 2019, pp. 1–15, Oct. 2019.
- <span id="page-21-4"></span>[\[83\]](#page-8-19) A. Roy, K. Basak, A. Ekbal, and P. Bhattacharyya, ''A deep ensemble framework for fake news detection and classification,'' 2018, *arXiv:1811.04670*.
- <span id="page-21-5"></span>[\[84\]](#page-8-20) H. Yenala, A. Jhanwar, M. K. Chinnakotla, and J. Goyal, ''Deep learning for detecting inappropriate content in text,'' *Int. J. Data Sci. Analytics*, vol. 6, no. 4, pp. 273–286, Dec. 2018.
- <span id="page-21-6"></span>[\[85\]](#page-8-21) Y. Yang, L. Zheng, J. Zhang, Q. Cui, Z. Li, and P. S. Yu, ''TI-CNN: Convolutional neural networks for fake news detection,'' 2018, *arXiv:1806.00749*.
- <span id="page-21-7"></span>[\[86\]](#page-8-22) W. Zhang, Y. Du, T. Yoshida, and Q. Wang, ''DRI-RCNN: An approach to deceptive review identification using recurrent convolutional neural network,'' *Inf. Process. Manage.*, vol. 54, no. 4, pp. 576–592, Jul. 2018.
- <span id="page-21-8"></span>[\[87\]](#page-8-23) J. Ma, W. Gao, P. Mitra, S. Kwon, B. J. Jansen, K.-F. Wong, and M. Cha, ''Detecting rumors from microblogs with recurrent neural networks,'' in *Proc. 25th Int. Joint Conf. Artif. Intell.*, 2016, pp. 3818–3824.
- <span id="page-21-9"></span>[\[88\]](#page-8-24) J. Ma, W. Gao, and K.-F. Wong, ''Rumor detection on Twitter with tree-structured recursive neural networks,'' in *Proc. 56th Annu. Meeting Assoc. Comput. Linguistics*, 2018, pp. 1–11.
- <span id="page-21-10"></span>[\[89\]](#page-8-25) J. Ma, W. Gao, and K.-F. Wong, ''Detect rumors on Twitter by promoting information campaigns with generative adversarial learning,'' in *Proc. World Wide Web Conf.*, May 2019, pp. 3049–3055.
- <span id="page-21-11"></span>[\[90\]](#page-8-26) K. Shu, L. Cui, S. Wang, D. Lee, and H. Liu, ''DEFEND: Explainable fake news detection,'' in *Proc. 25th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining*, Jul. 2019, pp. 395–405.
- <span id="page-21-12"></span>[\[91\]](#page-8-27) Y. Xu, C. Wang, Z. Dan, S. Sun, and F. Dong, ''Deep recurrent neural network and data filtering for rumor detection on Sina Weibo,'' *Symmetry*, vol. 11, no. 11, p. 1408, Nov. 2019.
- <span id="page-21-13"></span>[\[92\]](#page-8-28) S. Kudugunta and E. Ferrara, ''Deep neural networks for bot detection,'' *Inf. Sci.*, vol. 467, pp. 312–322, Oct. 2018.
- <span id="page-21-14"></span>[\[93\]](#page-8-29) Y. Long, Q. Lu, R. Xiang, M. Li, and C. Huang, ''Fake news detection through multi-perspective speaker profiles,'' in *Proc. 8th Int. Joint Conf. Natural Lang. Process.*, Nov. 2017, pp. 252–256.
- <span id="page-21-15"></span>[\[94\]](#page-8-30) H. Guo, J. Cao, Y. Zhang, J. Guo, and J. Li, ''Rumor detection with hierarchical social attention network,'' in *Proc. 27th ACM Int. Conf. Inf. Knowl. Manage.*, Oct. 2018, pp. 943–951.
- <span id="page-21-16"></span>[\[95\]](#page-8-31) M. Cheng, S. Nazarian, and P. Bogdan, ''VRoC: Variational autoencoderaided multi-task rumor classifier based on text,'' in *Proc. Web Conf.*, Apr. 2020, pp. 2892–2898.
- <span id="page-21-35"></span>[\[96\]](#page-0-10) A. Friggeri, L. A. Adamic, D. Eckles, and J. Cheng, ''Rumor cascades,'' in *Proc. Int. AAAI Conf. Web Social Media*, May 2014, vol. 8, no. 1, pp. 101–110.
- <span id="page-21-34"></span>[\[97\]](#page-0-10) Z. Zhao, P. Resnick, and Q. Mei, ''Enquiring minds: Early detection of rumors in social media from enquiry posts,'' in *Proc. 24th Int. Conf. World Wide Web*, May 2015, pp. 1395–1405.
- <span id="page-21-37"></span>[\[98\]](#page-0-10) J. Ma, W. Gao, Z. Wei, Y. Lu, and K.-F. Wong, ''Detect rumors using time series of social context information on microblogging websites,'' in *Proc. 24th ACM Int. Conf. Inf. Knowl. Manage.*, Oct. 2015, pp. 1751–1754.
- <span id="page-21-40"></span>[\[99\]](#page-0-10) J. Ma, W. Gao, and K.-F. Wong, ''Detect rumors in microblog posts using propagation structure via kernel learning,'' in *Proc. 55th Annu. Meeting Assoc. Comput. Linguistics*, 2017, pp. 708–717.
- <span id="page-21-36"></span>[\[100\]](#page-0-10) M. L. Della Vedova, E. Tacchini, S. Moret, G. Ballarin, M. DiPierro, and L. de Alfaro, ''Automatic online fake news detection combining content and social signals,'' in *Proc. 22nd Conf. Open Innov. Assoc. (FRUCT)*, May 2018, pp. 272–279.
- <span id="page-21-39"></span>[\[101\]](#page-0-10) K. Shu, S. Wang, and H. Liu, ''Understanding user profiles on social media for fake news detection,'' in *Proc.IEEE Conf. Multimedia Inf. Process. Retr. (MIPR)*, Apr. 2018, pp. 430–435.
- <span id="page-21-30"></span>[\[102\]](#page-0-10) K. Shu, S. Wang, and H. Liu, ''Beyond news contents: The role of social context for fake news detection,'' in *Proc. 12th ACM Int. Conf. Web Search Data Mining*, Jan. 2019, pp. 312–320.

- <span id="page-21-38"></span>[\[103\]](#page-0-10) V.-H. Nguyen, K. Sugiyama, P. Nakov, and M.-Y. Kan, ''FANG: Leveraging social context for fake news detection using graph representation,'' in *Proc. 29th ACM Int. Conf. Inf. Knowl. Manage.*, Oct. 2020, pp. 1165–1174.
- <span id="page-21-17"></span>[\[104\]](#page-8-32) G. M. Shahariar, S. Biswas, F. Omar, F. M. Shah, and S. B. Hassan, ''Spam review detection using deep learning,'' in *Proc. IEEE 10th Annu. Inf. Technol., Electron. Mobile Commun. Conf. (IEMCON)*, Oct. 2019, pp. 0027–0033.
- <span id="page-21-18"></span>[\[105\]](#page-8-33) T. Chen, X. Li, H. Yin, and J. Zhang, ''Call attention to rumors: Deep attention based recurrent neural networks for early rumor detection,'' in *Proc. PAKDD Workshops Trends Appl. Knowl. Discovery Data Mining*, Melbourne, VIC, Australia. Cham, Switzerland: Springer, Jan. 2018, pp. 40–52.
- <span id="page-21-19"></span>[\[106\]](#page-8-34) Q. Liu, F. Yu, S. Wu, and L. Wang, ''Mining significant microblogs for misinformation identification: An attention-based approach,'' *ACM Trans. Intell. Syst. Technol.*, vol. 9, no. 5, pp. 1–20, Sep. 2018.
- <span id="page-21-20"></span>[\[107\]](#page-9-1) Y. Chen, J. Sui, L. Hu, and W. Gong, ''Attention-residual network with CNN for rumor detection,'' in *Proc. 28th ACM Int. Conf. Inf. Knowl. Manage.*, Nov. 2019, pp. 1121–1130.
- <span id="page-21-21"></span>[\[108\]](#page-9-2) Y.-J. Lu and C.-T. Li, ''GCAN: Graph-aware co-attention networks for explainable fake news detection on social media,'' 2020, *arXiv:2004.11648*.
- <span id="page-21-22"></span>[\[109\]](#page-9-3) C. Song, K. Shu, and B. Wu, ''Temporally evolving graph neural network for fake news detection,'' *Inf. Process. Manage.*, vol. 58, no. 6, Nov. 2021, Art. no. 102712.
- <span id="page-21-23"></span>[\[110\]](#page-9-4) L. Hu, T. Yang, L. Zhang, W. Zhong, D. Tang, C. Shi, N. Duan, and M. Zhou, ''Compare to the knowledge: Graph neural fake news detection with external knowledge,'' in *Proc. 59th Annu. Meeting Assoc. Comput. Linguistics 11th Int. Joint Conf. Natural Lang. Process.*, 2021, pp. 754–763.
- <span id="page-21-24"></span>[\[111\]](#page-9-5) T. Bian, X. Xiao, T. Xu, P. Zhao, W. Huang, Y. Rong, and J. Huang, ''Rumor detection on social media with bi-directional graph convolutional networks,'' in *Proc. AAAI Conf. Artif. Intell.*, Apr. 2020, vol. 34, no. 1, pp. 549–556.
- <span id="page-21-25"></span>[\[112\]](#page-9-6) M. Sun, X. Zhang, J. Zheng, and G. Ma, ''DDGCN: Dual dynamic graph convolutional networks for rumor detection on social media,'' in *Proc. AAAI Conf. Artif. Intell.*, Jun. 2022, vol. 36, no. 4, pp. 4611–4619.
- <span id="page-21-26"></span>[\[113\]](#page-9-7) S. Qian, J. Hu, Q. Fang, and C. Xu, ''Knowledge-aware multi-modal adaptive graph convolutional networks for fake news detection,'' *ACM Trans. Multimedia Comput., Commun., Appl.*, vol. 17, no. 3, pp. 1–23, Aug. 2021.
- <span id="page-21-27"></span>[\[114\]](#page-9-8) Y. Wang, F. Ma, Z. Jin, Y. Yuan, G. Xun, K. Jha, L. Su, and J. Gao, ''EANN: Event adversarial neural networks for multi-modal fake news detection,'' in *Proc. 24th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining*, Jul. 2018, pp. 849–857.
- <span id="page-21-28"></span>[\[115\]](#page-9-9) Z. Jin, J. Cao, H. Guo, Y. Zhang, and J. Luo, ''Multimodal fusion with recurrent neural networks for rumor detection on microblogs,'' in *Proc. 25th ACM Int. Conf. Multimedia*, Oct. 2017, pp. 795–816.
- <span id="page-21-29"></span>[\[116\]](#page-9-10) Y. Wang, F. Ma, H. Wang, K. Jha, and J. Gao, ''Multimodal emergent fake news detection via meta neural process networks,'' in *Proc. 27th ACM SIGKDD Conf. Knowl. Discovery Data Mining*, Aug. 2021, pp. 3708–3716.
- <span id="page-21-31"></span>[\[117\]](#page-10-1) D. Khattar, J. S. Goud, M. Gupta, and V. Varma, ''MVAE: Multimodal variational autoencoder for fake news detection,'' in *Proc. World Wide Web Conf.*, May 2019, pp. 2915–2921.
- <span id="page-21-32"></span>[\[118\]](#page-10-2) X. Zhou, J. Wu, and R. Zafarani, ''SAFE: Similarity-aware multi-modal fake news detection,'' in *Proc. Pacific–Asia Conf. Knowl. Discovery Data Mining*. Cham, Switzerland: Springer, 2020, pp. 354–367.
- <span id="page-21-33"></span>[\[119\]](#page-10-3) F. Ghorbanpour, M. Ramezani, M. A. Fazli, and H. R. Rabiee, ''FNR: A similarity and transformer-based approach to detect multi-modal fake news in social media,'' *Social Netw. Anal. Mining*, vol. 13, no. 1, p. 56, Mar. 2023.
- <span id="page-21-41"></span>[\[120\]](#page-0-10) S. Das Bhattacharjee, A. Talukder, and B. V. Balantrapu, ''Active learning based news veracity detection with feature weighting and deep-shallow fusion,'' in *Proc. IEEE Int. Conf. Big Data (Big Data)*, Dec. 2017, pp. 556–565.
- <span id="page-21-42"></span>[\[121\]](#page-0-10) A. T. Nguyen, A. Kharosekar, S. Krishnan, S. Krishnan, E. Tate, B. C. Wallace, and M. Lease, ''Believe it or not: Designing a human-AI partnership for mixed-initiative fact-checking,'' in *Proc. 31st Annu. ACM Symp. User Interface Softw. Technol.*, Oct. 2018, pp. 189–199.
- <span id="page-21-43"></span>[\[122\]](#page-0-10) N. Vo and K. Lee, ''The rise of guardians: Fact-checking URL recommendation to combat fake news,'' in *Proc. 41st Int. ACM SIGIR Conf. Res. Develop. Inf. Retr.*, Jun. 2018, pp. 275–284.

- <span id="page-22-20"></span>[\[123\]](#page-0-10) J. Kim, B. Tabibian, A. Oh, B. Schölkopf, and M. Gomez-Rodriguez, ''Leveraging the crowd to detect and reduce the spread of fake news and misinformation,'' in *Proc. 11th ACM Int. Conf. Web Search Data Mining*, Feb. 2018, pp. 324–332.
- <span id="page-22-21"></span>[\[124\]](#page-0-10) S. Tschiatschek, A. Singla, M. Gomez Rodriguez, A. Merchant, and A. Krause, ''Fake news detection in social networks via crowd signals,'' in *Proc. Companion Web Conf. Web Conf.-WWW*, 2018, pp. 517–524.
- <span id="page-22-19"></span>[\[125\]](#page-0-10) G. Pennycook and D. G. Rand, ''Fighting misinformation on social media using crowdsourced judgments of news source quality,'' *Proc. Nat. Acad. Sci. USA*, vol. 116, no. 7, pp. 2521–2526, Feb. 2019.
- <span id="page-22-18"></span>[\[126\]](#page-0-10) K. Clayton, S. Blair, J. A. Busam, S. Forstner, J. Glance, G. Green, A. Kawata, A. Kovvuri, J. Martin, E. Morgan, M.Sandhu, R. Sang, R. Scholz-Bright, A. T. Welch, A. G. Wolff, A. Zhou, and B. Nyhan, ''Real solutions for fake news? Measuring the effectiveness of general warnings and fact-check tags in reducing belief in false stories on social media,'' *Political Behav.*, vol. 42, no. 4, pp. 1073–1095, Dec. 2020.
- <span id="page-22-0"></span>[\[127\]](#page-10-4) H. Zhang, Q. Fang, S. Qian, and C. Xu, ''Multi-modal knowledge-aware event memory network for social media rumor detection,'' in *Proc. 27th ACM Int. Conf. Multimedia*, Oct. 2019, pp. 1942–1951.
- <span id="page-22-1"></span>[\[128\]](#page-10-5) C. Song, N. Ning, Y. Zhang, and B. Wu, ''A multimodal fake news detection model based on crossmodal attention residual and multichannel convolutional neural networks,'' *Inf. Process. Manage.*, vol. 58, no. 1, Jan. 2021, Art. no. 102437.
- <span id="page-22-2"></span>[\[129\]](#page-10-6) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, ''Attention is all you need,'' in *Proc. Adv. Neural Inf. Process. Syst.*, vol. 30, Jun. 2017, pp. 5998–6008.
- <span id="page-22-3"></span>[\[130\]](#page-10-7) S. Gundapu and R. Mamidi, ''Transformer based automatic COVID-19 fake news detection system,'' 2021, *arXiv:2101.00180*.
- <span id="page-22-4"></span>[\[131\]](#page-10-8) J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ''BERT: Pre-training of deep bidirectional transformers for language understanding,'' 2018, *arXiv:1810.04805*.
- <span id="page-22-5"></span>[\[132\]](#page-10-9) Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, ''ALBERT: A lite BERT for self-supervised learning of language representations,'' 2019, *arXiv:1909.11942*.
- <span id="page-22-6"></span>[\[133\]](#page-10-10) Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le, ''XLNet: Generalized autoregressive pretraining for language understanding,'' in *Proc. Adv. Neural Inf. Process. Syst.*, vol. 32, Jan. 2019, pp. 1–11.
- <span id="page-22-7"></span>[\[134\]](#page-10-11) S. Singhal, R. R. Shah, T. Chakraborty, P. Kumaraguru, and S. Satoh, ''SpotFake: A multi-modal framework for fake news detection,'' in *Proc. IEEE 5th Int. Conf. Multimedia Big Data (BigMM)*, Sep. 2019, pp. 39–47.
- <span id="page-22-8"></span>[\[135\]](#page-10-12) X. Zhang, J. Cao, X. Li, Q. Sheng, L. Zhong, and K. Shu, ''Mining dual emotion for fake news detection,'' in *Proc. Web Conf.*, Apr. 2021, pp. 3465–3476.
- <span id="page-22-9"></span>[\[136\]](#page-10-13) Y. Mo, H. Qin, Y. Dong, Z. Zhu, and Z. Li, ''Large language model (LLM) AI text generation detection based on transformer deep learning algorithm,'' 2024, *arXiv:2405.06652*.
- <span id="page-22-10"></span>[\[137\]](#page-11-1) S. Volkova and J. Y. Jang, ''Misleading or falsification: Inferring deceptive strategies and types in online news and social media,'' in *Proc. Companion Web Conf. Web Conf.-WWW*, 2018, pp. 575–583.
- <span id="page-22-11"></span>[\[138\]](#page-11-2) S. Kwon and M. Cha, ''Modeling bursty temporal pattern of rumors,'' in *Proc. Int. AAAI Conf. Web Social Media*, May 2014, vol. 8, no. 1, pp. 650–651.
- <span id="page-22-12"></span>[\[139\]](#page-11-3) L. Wu, Y. Rao, H. Yu, Y. Wang, and A. Nazir, ''False information detection on social media via a hybrid deep model,'' in *Proc. 10th Int. Conf. Social Inform.*, St. Petersburg, Russia. Cham, Switzerland: Springer, Jan. 2018, pp. 323–333.
- <span id="page-22-13"></span>[\[140\]](#page-12-0) K. Shu, D. Mahudeswaran, S. Wang, D. Lee, and H. Liu, ''FakeNewsNet: A data repository with news content, social context, and spatiotemporal information for studying fake news on social media,'' *Big Data*, vol. 8, no. 3, pp. 171–188, Jun. 2020.
- <span id="page-22-14"></span>[\[141\]](#page-12-1) Y. Liu and Y. Wu, ''Early detection of fake news on social media through propagation path classification with recurrent and convolutional networks,'' in *Proc. AAAI Conf. Artif. Intell.*, Apr. 2018, vol. 32, no. 1, pp. 1–18.
- <span id="page-22-15"></span>[\[142\]](#page-12-2) Y. Liu and S. Xu, ''Detecting rumors through modeling information propagation networks in a social media environment,'' *IEEE Trans. Computat. Social Syst.*, vol. 3, no. 2, pp. 46–62, Jun. 2016.
- <span id="page-22-16"></span>[\[143\]](#page-12-3) A. Das, H. Liu, V. Kovatchev, and M. Lease, ''The state of humancentered NLP technology for fact-checking,'' *Inf. Process. Manage.*, vol. 60, no. 2, Mar. 2023, Art. no. 103219.

- <span id="page-22-17"></span>[\[144\]](#page-12-4) M. Chung and N. Kim, ''When I learn the news is false: How factchecking information stems the spread of fake news via third-person perception,'' *Hum. Commun. Res.*, vol. 47, no. 1, pp. 1–24, Feb. 2021.
- <span id="page-22-22"></span>[\[145\]](#page-13-1) C. Chen and K. Shu, ''Combating misinformation in the age of LLMs: Opportunities and challenges,'' *AI Mag.*, vol. 45, no. 3, pp. 354–368, Sep. 2024.
- <span id="page-22-23"></span>[\[146\]](#page-14-1) Y. Wang, J. Mansurov, P. Ivanov, J. Su, A. Shelmanov, A. Tsvigun, C. Whitehouse, O. Mohammed Afzal, T. Mahmoud, T. Sasaki, T. Arnold, A. Fikri Aji, N. Habash, I. Gurevych, and P. Nakov, ''M4: Multigenerator, multi-domain, and multi-lingual black-box machine-generated text detection,'' 2023, *arXiv:2305.14902*.
- <span id="page-22-24"></span>[\[147\]](#page-14-2) D. Macko, R. Moro, A. Uchendu, J. Samuel Lucas, M. Yamashita, M. Pikuliak, I. Srba, T. Le, D. Lee, J. Simko, and M. Bielikova, ''MULTITuDE: Large-scale multilingual machine-generated text detection benchmark,'' 2023, *arXiv:2310.13606*.
- <span id="page-22-25"></span>[\[148\]](#page-14-3) S. Tu, Y. Sun, Y. Bai, J. Yu, L. Hou, and J. Li, ''WaterBench: Towards holistic evaluation of watermarks for large language models,'' 2023, *arXiv:2311.07138*.
- <span id="page-22-26"></span>[\[149\]](#page-14-4) X. He, X. Shen, Z. Chen, M. Backes, and Y. Zhang, ''MGTBench: Benchmarking machine-generated text detection,'' in *Proc. ACM SIGSAC Conf. Comput. Commun. Secur.*, Dec. 2024, pp. 2251–2265.
- <span id="page-22-27"></span>[\[150\]](#page-14-5) Z. Qazi, W. Shiao, and E. E. Papalexakis, ''GPT-generated text detection: Benchmark dataset and tensor-based detection method,'' in *Companion Proc. ACM Web Conf.*, May 2024, pp. 842–846.
- <span id="page-22-28"></span>[\[151\]](#page-14-6) S. Gehrmann, H. Strobelt, and A. M. Rush, ''GLTR: Statistical detection and visualization of generated text,'' 2019, *arXiv:1906.04043*.
- <span id="page-22-29"></span>[\[152\]](#page-14-7) A. Fan, M. Lewis, and Y. Dauphin, ''Hierarchical neural story generation,'' 2018, *arXiv:1805.04833*.
- <span id="page-22-30"></span>[\[153\]](#page-14-8) J. Chorowski and N. Jaitly, ''Towards better decoding and language model integration in sequence to sequence models,'' 2016, *arXiv:1612.02695*.
- <span id="page-22-31"></span>[\[154\]](#page-14-9) H.-Q. Nguyen-Son, N. T. Tieu, H. H. Nguyen, J. Yamagishi, and I. E. Zen, ''Identifying computer-generated text using statistical analysis,'' in *Proc. Asia–Pacific Signal Inf. Process. Assoc. Annu. Summit Conf. (APSIPA ASC)*, Dec. 2017, pp. 1504–1511.
- <span id="page-22-32"></span>[\[155\]](#page-14-10) Y. Miao, H. Gao, H. Zhang, and Z. Deng, ''Efficient detection of LLM-generated texts with a Bayesian surrogate model,'' 2023, *arXiv:2305.16617*.
- <span id="page-22-33"></span>[\[156\]](#page-14-11) L. Fröhling and A. Zubiaga, ''Feature-based detection of automated language models: Tackling GPT-2, GPT-3 and Grover,'' *PeerJ Comput. Sci.*, vol. 7, p. e443, Apr. 2021.
- <span id="page-22-34"></span>[\[157\]](#page-15-1) R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, and Y. Choi, ''Defending against neural fake news,'' in *Proc. Adv. Neural Inf. Process. Syst.*, vol. 32, Jan. 2019, pp. 1–9.
- <span id="page-22-35"></span>[\[158\]](#page-15-2) J. Rodriguez, T. Hay, D. Gros, Z. Shamsi, and R. Srinivasan, ''Cross-domain detection of GPT-2-generated technical text,'' in *Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Technol.*, 2022, pp. 1213–1233.
- <span id="page-22-36"></span>[\[159\]](#page-15-3) J. Salminen, C. Kandpal, A. M. Kamel, S.-G. Jung, and B. J. Jansen, ''Creating and detecting fake reviews of online products,'' *J. Retailing Consum. Services*, vol. 64, Jan. 2022, Art. no. 102771.
- <span id="page-22-37"></span>[\[160\]](#page-15-4) A. Bhattacharjee, T. Kumarage, R. Moraffah, and H. Liu, ''ConDA: Contrastive domain adaptation for AI-generated text detection,'' 2023, *arXiv:2309.03992*.
- <span id="page-22-38"></span>[\[161\]](#page-15-5) M. M. Bhat and S. Parthasarathy, ''How effectively can machines defend against machine-generated fake news? An empirical study,'' in *Proc. 1st Workshop Insights Negative Results NLP*, 2020, pp. 48–53.
- <span id="page-22-39"></span>[\[162\]](#page-15-6) D. Hee Lee and B. Jang, ''Enhancing machine-generated text detection: Adversarial fine-tuning of pre-trained language models,'' *IEEE Access*, vol. 12, pp. 65333–65340, 2024.
- <span id="page-22-40"></span>[\[163\]](#page-15-7) X. Hu, P. Chen, and T.-Y. Ho, ''RADAR: Robust AI-text detection via adversarial learning,'' in *Proc. Adv. Neural Inf. Process. Syst.*, vol. 36, Jan. 2023, pp. 15077–15095.
- <span id="page-22-41"></span>[\[164\]](#page-15-8) E. Tulchinskii, K. Kuznetsov, L. Kushnareva, D. Cherniavskii, S. Nikolenko, E. Burnaev, S. Barannikov, and I. Piontkovskaya, ''Intrinsic dimension estimation for robust detection of AI-generated texts,'' in *Proc. Adv.Neural Inf. Process. Syst.*, vol. 36, Jan. 2023, pp. 1–20.
- <span id="page-22-42"></span>[\[165\]](#page-15-9) A. Bhattacharjee and H. Liu, ''Fighting fire with fire: Can ChatGPT detect AI-generated text?'' *ACM SIGKDD Explorations Newslett.*, vol. 25, no. 2, pp. 14–21, Mar. 2024.
- <span id="page-22-43"></span>[\[166\]](#page-15-10) J. Achiam et al., ''GPT-4 technical report,'' 2023, *arXiv:2303.08774*.
- <span id="page-22-44"></span>[\[167\]](#page-15-11) A. Hurst et al., ''GPT-4o system card,'' 2024, *arXiv:2410.21276*.

- <span id="page-23-0"></span>[\[168\]](#page-15-12) T. Lee, S. Hong, J. Ahn, I. Hong, H. Lee, S. Yun, J. Shin, and G. Kim, ''Who wrote this code? Watermarking for code generation,'' 2023, *arXiv:2305.15060*.
- <span id="page-23-1"></span>[\[169\]](#page-16-1) V. Sankar Sadasivan, A. Kumar, S. Balasubramanian, W. Wang, and S. Feizi, ''Can AI-generated text be reliably detected?'' 2023, *arXiv:2303.11156*.
- <span id="page-23-2"></span>[\[170\]](#page-16-2) M. Christ, S. Gunn, and O. Zamir, ''Undetectable watermarks for language models,'' 2023, *arXiv:2306.09194*.
- <span id="page-23-3"></span>[\[171\]](#page-16-3) K. Krishna, Y. Song, M. Karpinska, J. Wieting, and M. Iyyer, ''Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense,'' in *Proc. Adv. Neural Inf. Process. Syst.*, vol. 36, Jan. 2023, pp. 1–7.
- <span id="page-23-4"></span>[\[172\]](#page-16-4) S. Chakraborty, A. S. Bedi, S. Zhu, B. An, D. Manocha, and F. Huang, ''On the possibilities of AI-generated text detection,'' 2023, *arXiv:2304.04736*.
- <span id="page-23-5"></span>[\[173\]](#page-16-5) L. Dugan, D. Ippolito, A. Kirubarajan, and C. Callison-Burch, ''RoFT: A tool for evaluating human detection of machine-generated text,'' 2020, *arXiv:2010.03070*.
- <span id="page-23-6"></span>[\[174\]](#page-16-6) D. Ippolito, D. Duckworth, C. Callison-Burch, and D. Eck, ''Automatic detection of generated text is easiest when humans are fooled,'' 2019, *arXiv:1911.00650*.
- <span id="page-23-7"></span>[\[175\]](#page-16-7) E. Clark, T. August, S. Serrano, N. Haduong, S. Gururangan, and N. A. Smith, ''All that's 'human' is not gold: Evaluating human evaluation of generated text,'' 2021, *arXiv:2107.00061*.
- <span id="page-23-8"></span>[\[176\]](#page-16-8) Q. Sheng, J. Cao, X. Zhang, R. Li, D. Wang, and Y. Zhu, ''Zoom out and observe: News environment perception for fake news detection,'' 2022, *arXiv:2203.10885*.
- <span id="page-23-9"></span>[\[177\]](#page-16-9) M. S. Orenstrakh, O. Karnalim, C. A. Suárez, and M. Liut, ''Detecting LLM-generated text in computing education: Comparative study for ChatGPT cases,'' in *Proc. IEEE 48th Annu. Comput., Softw., Appl. Conf. (COMPSAC)*, Jul. 2024, pp. 121–126.
- <span id="page-23-10"></span>[\[178\]](#page-16-10) D. Mhlanga, ''Open AI in education, the responsible and ethical use of ChatGPT towards lifelong learning,'' in *FinTech and Artificial Intelligence for Sustainable Development: The Role of Smart Technologies in Achieving Development Goals*. Cham, Switzerland: Springer, 2023, pp. 387–409.
- <span id="page-23-11"></span>[\[179\]](#page-17-0) D. Guo et al., ''DeepSeek-r1: Incentivizing reasoning capability in LLMs via reinforcement learning,'' 2025, *arXiv:2501.12948*.
- <span id="page-23-12"></span>[\[180\]](#page-17-1) M. Dowling and B. Lucey, ''ChatGPT for (Finance) research: The bananarama conjecture,'' *Finance Res. Lett.*, vol. 53, May 2023, Art. no. 103662.
- <span id="page-23-13"></span>[\[181\]](#page-17-2) A. S. George and A. H. George, ''A review of ChatGPT AI's impact on several business sectors,'' *Partners Universal Int. Innov. J.*, vol. 1, no. 1, pp. 9–23, 2023.
- <span id="page-23-14"></span>[\[182\]](#page-17-3) S. Kreps, R. M. McCain, and M. Brundage, ''All the news that's fit to fabricate: AI-generated text as a tool of media misinformation,'' *J. Exp. Political Sci.*, vol. 9, no. 1, pp. 104–117, 2022.
- <span id="page-23-15"></span>[\[183\]](#page-17-4) A. H. Kumar, ''Analysis of ChatGPT tool to assess the potential of its utility for academic writing in biomedical domain,'' *Biol., Eng., Med. Sci. Rep.*, vol. 9, no. 1, pp. 24–30, Jan. 2023.
- <span id="page-23-16"></span>[\[184\]](#page-17-5) C. K. Lo, ''What is the impact of ChatGPT on education? A rapid review of the literature,'' *Educ. Sci.*, vol. 13, no. 4, p. 410, Apr. 2023.
- <span id="page-23-17"></span>[\[185\]](#page-17-6) S. Pal, M. Bhattacharya, S.-S. Lee, and C. Chakraborty, ''A domainspecific next-generation large language model (LLM) or ChatGPT is required for biomedical engineering and research,'' *Ann. Biomed. Eng.*, vol. 52, no. 3, pp. 451–454, Mar. 2024.
- <span id="page-23-18"></span>[\[186\]](#page-17-7) M. Perkins, J. Roe, D. Postma, J. McGaughran, and D. Hickerson, ''Detection of GPT-4 generated text in higher education: Combining academic judgement and software to identify generative AI tool misuse,'' *J. Academic Ethics*, vol. 22, no. 1, pp. 89–113, Mar. 2024.
- <span id="page-23-19"></span>[\[187\]](#page-17-8) C. Chen and K. Shu, ''Can LLM-generated misinformation be detected?'' 2023, *arXiv:2309.13788*.
- <span id="page-23-20"></span>[\[188\]](#page-17-9) Y. Zhou and J. Wang, ''Detecting AI-generated texts in cross-domains,'' in *Proc. ACM Symp. Document Eng.*, Aug. 2024, pp. 1–4.

- <span id="page-23-21"></span>[\[189\]](#page-17-10) Z. Liu, K. Yang, Q. Xie, C. de Kock, S. Ananiadou, and E. Hovy, ''RAEmoLLM: Retrieval augmented LLMs for cross-domain misinformation detection using in-context learning based on emotional information,'' 2024, *arXiv:2406.11093*.
- <span id="page-23-22"></span>[\[190\]](#page-17-11) T. A. Roshinta, Hartatik, E. K. Fauziyah, I. F. Dinata, N. Firdaus, and F. Y. A'la, ''A comparison of text classification methods: Towards fake news detection for Indonesian websites,'' in *Proc. 1st Int. Conf. Smart Technol., Appl. Informat., Eng. (APICS)*, Aug. 2022, pp. 59–64.
- <span id="page-23-23"></span>[\[191\]](#page-17-12) W. H. Walters, ''The effectiveness of software designed to detect AIgenerated writing: A comparison of 16 AI text detectors,'' *Open Inf. Sci.*, vol. 7, no. 1, Oct. 2023, Art. no. 20220158.
- <span id="page-23-24"></span>[\[192\]](#page-17-13) D. Weber-Wulff, A. Anohina-Naumeca, S. Bjelobaba, T. Foltýnek, J. Guerrero-Dib, O. Popoola, P. Šigut, and L.Waddington, ''Testing of detection tools for AI-generated text,'' *Int. J. Educ. Integrity*, vol. 19, no. 1, p. 26, Dec. 2023.
- <span id="page-23-25"></span>[\[193\]](#page-18-1) X. Xu, K. Kong, N. Liu, L. Cui, D. Wang, J. Zhang, and M. Kankanhalli, ''An LLM can fool itself: A prompt-based adversarial attack,'' 2023, *arXiv:2310.13345*.
- <span id="page-23-26"></span>[\[194\]](#page-18-2) A. Kumar, C. Agarwal, S. Srinivas, A. Jiaxun Li, S. Feizi, and H. Lakkaraju, ''Certifying LLM safety against adversarial prompting,'' 2023, *arXiv:2309.02705*.
- <span id="page-23-27"></span>[\[195\]](#page-18-3) C. A. Gao, F. M. Howard, N. S. Markov, E. Dyer, S. Ramesh, Y. Luo, and A. T. Pearson, ''Comparing scientific abstracts generated by ChatGPT to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers,'' *BioRxiv*, vol. 2022, pp. 1–18, Dec. 2022.
- <span id="page-23-28"></span>[\[196\]](#page-18-4) Y. Jiang, J. Hao, M. Fauss, and C. Li, ''Detecting ChatGPT-generated essays in a large-scale writing assessment: Is there a bias against non-native English speakers?'' *Comput. Educ.*, vol. 217, Aug. 2024, Art. no. 105070.

![](_page_23_Picture_31.jpeg)

SOONCHAN KWON (Member, IEEE) received the B.S. degree in public administration and business administration from Kwangwoon University, Seoul, South Korea, in 2022. He is currently pursuing the M.S. degree in information systems from Yonsei University, Seoul. His research interests include artificial intelligence, natural language processing, and llm.

![](_page_23_Picture_33.jpeg)

BEAKCHEOL JANG (Member, IEEE) received the B.S. degree in computer science from Yonsei University, in 2001, the M.S. degree in computer science from Korea Advanced Institute of Science and Technology, in 2002, and the Ph.D. degree in computer science from North Carolina State University, in 2009. He is currently a Professor with the Graduate School of Information, Yonsei University. His primary research interests include artificial intelligence, big data analytics,

and natural language processing. He is a member of ACM.