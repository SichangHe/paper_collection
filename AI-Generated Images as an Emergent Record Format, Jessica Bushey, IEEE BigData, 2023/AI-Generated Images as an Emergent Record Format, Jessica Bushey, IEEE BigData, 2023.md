Conferences > 2023 IEEE International Confe... 

# Ai-Generated Images As An Emergent Record **Format**

Publisher: IEEE Cite This  PDF

| Cites in  Paper   |
|-------------------|

Jessica Bushey All **Authors** 
Text Views

| 262   | Full   |
|-------|--------|

Alerts Manage Content Alerts Add to Citation Alerts 
   

### Abstract

Document Sections I. Introduction II. Background III. Methodology IV. Results V. Discussion Show Full Outline Authors References Citations Keywords Metrics

| ï„‡ Metadata Abstract:   |
|------------------------|

More Like This Abstract: AI-generated Images are disrupting existing approaches to verifying the trustworthiness of visual media. The application of generative AI in fields in which images are tr... **View more**
AI-generated Images are disrupting existing approaches to verifying the trustworthiness of visual media. The application of generative AI in fields in which images are trusted visual evidence of persons, actions and events is drawing the attention of archival scientists and AI researchers. A literature review of AI-generated images as an emergent record format, identified an absence of archival and recordkeeping knowledge. Analysis of the results revealed six thematic categories: authenticity and verifiability; manipulation and misinformation; bias and representation; attribution and intellectual property; transparency and explainability; and ethical considerations. These themes inform the development of research questions and the next phase of the study that includes the application of theory and methods of archival diplomatics and computational archival science.

Published in: 2023 IEEE International Conference on Big Data (BigData)
Date of Conference: 15-18 December 2023 Date Added to IEEE *Xplore*: 22 January 2024 ISBN Information:
DOI: 10.1109/BigData59044.2023.10386946 Publisher: IEEE
Conference Location: Sorrento, Italy Contents

## Section I. Introduction

Artificial intelligence (AI) tools for generating content (e.g., text, images, audio, and video) are no longer the privy of a select few. Rapid advances in generative AI, a type or subset of AI trained to produce new content, either randomly or based on prompts provided by users [1] are enabling tech companies to offer AI image generators that can create photorealistic images (e.g., Adobe Firefly, Dall-E, and Midjourney). As awareness and use of generative AI becomes more widespread, experimental, and playful applications of AI-generated images have emerged alongside more intentionally and misleading uses, raising issues of trust and reigniting debates over equating seeing with believing [2]. "Generative AI can produce content that looks as though a human produced it" [1]. In response to increasingly sophisticated and realistic AI-generated images and videos, several tools for identifying fakes have surfaced to assist in restoring public confidence in online news reportage; however, the deluge and immediacy of image dissemination online make image verification challenging, which is prompting industry-led initiatives to develop new solutions that focus on establishing content authenticity and provenance from the outset [3] **[4]. If we expand our exploration of generative AI**
PDF

Downl for images into the fields of medicine and law enforcement, we discover concerns about the complexity of AI algorithms as "black box" systems [5], the risks associated with inaccuracy and bias [6], and the need for policies and standards for explainable and responsible AI [7] [8] [9] **[10]. These concerns are based on the** lack of transparency and explainability of the models that generative AI tools rely on, as well as the training data what is difficult to access and assess [1].

Throughout the literature and industry-led initiatives addressing AI-generated images there is a lack of contributions from archival scholars and professionals, in particular an understanding of current recordkeeping practices for creation and use of AI-generated images as an emergent record format (i.e., a document made or received in the course of a practical activity as a by-product of such activity, and set aside for action or reference [11]. For over two decades, archival scholars and professionals have partnered with computer scientists and industry experts to explore issues of trust and the trustworthiness (i.e., authenticity, reliability and accuracy) of records in digital systems and online environments under the auspices of the International Research on Permanent Authentic Records in Electronic Systems (InterPARES) Projects (19992018), InterPARES Trust (2012-2019). The InterPARES Projects and their precursor, the UBC Project (199497), which resulted in the DoD Standard 5015.2 for recordkeeping systems, have produced a number of resources to support the understanding and preservation of trustworthy digital records [12] **[13] and**
trustworthy digital records in the online environment [14]. Specific to born-digital images, the InterPARES
Projects developed the theoretical framework of archival diplomatics, a body of knowledge that integrates concepts and methods from archival science and diplomatics [15], which was applied in studies of borndigital images as reliable and authentic records [16] and the trustworthiness of digital images in social media platforms [17]. Archival scholars have been at the forefront of digital preservation research and practice for several decades [18]. They have expertise in standards and best practice for preserving digital records, including recordkeeping strategies for ensuring trustworthy records over time.

In 2021, ITrustAI was launched as the most current iteration of InterPARES, with the aim of designing, developing, and leveraging AI to support the ongoing availability and accessibility of trustworthy public records. The five-year project brings together a multi-national, interdisciplinary research team comprised of researchers in archival science, records management, diplomatics, law, computer science, digital forensics, artificial intelligence, and information governance. This paper presents the findings of a literature review, conducted under the auspices of ITrustAI on the recordkeeping practices of creators using AI to generate images. Six themes emerged from the literature review: Authenticity and Verification; Manipulation and Misinformation; Bias and Representation; Attribution and Intellectual Property; Transparency and Explainability; and Ethical Consideration. Additionally, four research questions were identified to guide the next stage of the study: (1) How are individuals and organizations using AI-generated images? (2) What AI tools and technologies are being used to create, manage, and store AI-generated images? (3) What actions are being taken by individuals and organizations to identify AI-generated content? (4) What standards and/or policies are guiding procedures for creating, using, and preserving AI-generated images?

The rationale for this research is to better understand the nature of AI-generated images as an emergent record format and to identify risks posed by their creation and use to establishing and protecting their trustworthiness as historical records. In doing so, the research products will inform both creators and preservers and contribute to new recordkeeping practices involving generative AI. At this early stage it is unclear if existing archival approaches to preserving and protecting archival records will be sufficient for AI-
generated images. Metadata creation is a crucial aspect of archival work, as it provides context and facilitates access to archival materials. Current approaches to capturing and creating metadata for AI-generated images are still being developed and industry-led initiatives may not meet archival metadata standards and practices to ensure that AI-generated images are properly described and contextualized throughout their lifecycle.

Furthermore, to ensure the authenticity of AI-generated images, it is important to preserve the original data used to generate the images. This may include preserving the algorithms, training data, and any other relevant information. This could allow archivists and/or future researchers to verify the authenticity of the images and ensure that they have not been manipulated or altered in any way. Lastly, archivists need to be prepared to appraise and acquire AI-generated images as part of large volumes of born-digital records. The use of AI technology provides an opportunity for archivists to assess existing preservation strategies and determine if new approaches are needed. Paying close attention to approaches that may require archivists to develop new knowledge and skills, such as those addressed by the transdisciplinary field of computational archival science
(CAS), which explores the use of computational methods and tools to address challenges in archival science
[19] **[20].**
This paper is divided into four sections: background; methodology; results; discussion; and conclusion and next steps.

# Background

### A. Literature On **Archives And** Ai

In the past decade a small but increasing number of articles in academic and professional archival journals have explored advanced technological contexts, such as automation and the application of AI to the field of cultural heritage and recordkeeping [21] [22] [23] [24] [25] [26] [27] **[28]. A recent survey of literature on**
archives and AI that applies a records continuum lens and utilizes a recordkeeping scope (i.e., the activities undertaken by records managers and archivists from creation through preservation and including access and reuse) provides an insightful analysis of 53 references from 2015 to 2020 inclusive [29]. The survey reveals growing activities in the use of AI technologies and tools in archival processes, with the aim of automating aspects of archival appraisal, digitization, metadata creation and extraction, and providing access to archives for a more diverse range of users [29]. In most cases, AI is being explored as information technology that performs tasks that would ordinarily be performed by humans, such as solving problems, but on a massive scale and speed. The authors recognize that AI presents an opportunity for archivists to automate increasingly time-consuming dimensions of the archival process and explore new approaches to organizing and providing more robust access to volumes of digitized and complex born-digital records for researchers and scholars.

However, the current lack of AI regulatory frameworks and codes of practice, risk of bias in AI systems, potential impact on right to privacy, and the lack of transparent and explainable AI is challenging adoption and integration within the archival field. Furthermore, Colavizza et al. [29], suggest that "the technical, ethical, and societal implications of using AI in archival processes" have yet to be explored. The authors encourage archivists to pursue interdisciplinary exchange with computer scientists, AI researchers, and humanities scholars to inform their own discipline's integration of AI and provide theoretical and methodological contributions towards the advancement of a framework for trustworthy AI.

### B. Contribution Of Archival Diplomatics And Computational **Methods**

ITrustAI draws upon knowledge developed over two decades of InterPARES projects [30], which were dedicated to research on permanent authentic records in electronic systems, including experiential, interactive, and dynamic records. As recent discussions on archives and AI have emphasized, it is necessary to develop an understanding of AI concepts and gain experience in AI techniques within the recordkeeping profession [28] and to apply the collective knowledge of computer and archival science to new technologies that impact how trustworthy public records are generated, used, stored, preserved, and accessed [31] **[32].**
The application of archival diplomatics to AI-generated images would involve the identification of key attributes that must be preserved over the long term to demonstrate and maintain the records' continuing authenticity, reliability and accuracy. In this manner, archival diplomatics provides a framework to define the conceptual requirements for creating, managing, and preserving trustworthy records in digital systems and will be used by ITrustAI researchers to explore relevant AI technologies and systems, in particular AI- generated images.

CAS is a relatively new field of study that integrates computational and archival theory and methods to support the creation and preservation of trustworthy digital records and address large-scale analysis of archives to improve recordkeeping and research with archival materials [20]. Of particular interest to the study of AI-generated images is the application of computational methods for rapid conversion of descriptive photo metadata into linked data [33]. The potential for leveraging a computational processing workflow to born-digital images collections that contain AI-generated images based on newly emerging industry-led metadata standards could provide archivists with a solution to the time, labour and expertise-intensive process of creating and capturing metadata and exposing it to researchers for discovery and use [33]. Archivists and researchers are challenged by large scale born-digital images collections, particularly as textual descriptions of image content and context require metadata extraction and creation. Without knowledge of the types of metadata automatically captured by devices, software and platforms that create, share and store AI-generated images, archivists will be unable to exploit available metadata during the processes of arrangement and description, and preservation and access.

Computational methods that incorporate machine learning (ML) and computer vision (CV) into archival processing pipelines present opportunities to increase the discoverability of born-digital images collections and engagement with different types of researchers and research practices [33]. The field of CV focuses on computer-based image analysis and understanding, including tasks such as image classification, image segmentation, and object detection. These tasks require a model that was trained to approximate the rule underlying the relationships between the inputs and the corresponding outputs in the training dataset. Whereas generative AI models have a more synthetic nature: the aim is to learn the distribution of the training data so that they can generate new data with that same distribution, not to model the input-output relationship [34].

In contrast to the growing body of literature on the application of AI technologies and tools in archival processes, with the aim of automating aspects of appraisal, digitization, archival description, and providing increased digital access to archival collections [21] [26] [27] **[29], a review of the literature on the topic of** generative AI, specifically AI-generated images as an emergent record format revealed an absence in archival journals. This is likely due in part to the relatively recent widespread availability of generative AI tools, such as Midjourney, DALL-E, and Adobe's Firefly to produce photorealistic content [2]. As barriers to AI tool use are removed and organizations and individuals gain access and skills to integrate AI applications into workflows, we can anticipate exponential growth in the circulation of AI-generated images online. This will be in addition to AI-generated images being created and disseminated as records in fields that involve public trust, such as news & media communications, medicine, and law enforcement. Understandably, initial interest in AI by archivists is to utilize the technology to support archival functions. The research discussed in this paper focuses on creation of AI-generated images with the aim to understand them as an emergent record format, one that archivists will need to be familiar with in order to appraise, acquire, manage and preserve them in the near future. The following section addresses the methodological approach to the literature review.

## Section Iii. Methodology

### A. **Approach**

In information systems and knowledge management, it is important to communicate both the methodology (i.e., overall logic of inquiry) and the research methods (i.e., processes, procedures and techniques to collect and analyse data) at the outset [35]. Research on emergent record formats and recordkeeping activities, especially those aimed at describing and understanding new phenomena in the social world and their meanings in context [35], is supported by an interpretivist approach, one that utilizes primarily inductive methods that lead to new ways of understanding the phenomena of interest [36]. The role of archival diplomatic theory and computational archival science as methods of inquiry for AI-generated images presents an opportunity to explore the complexity of the emerging landscape of generative AI. Historically, InterPARES research has been undertaken across positivist and interpretivist paradigms, a reflection of its large multi-disciplinary, collaborative nature [36] **[37]. At this early stage in the research, a literature review**
was determined to be the best methodological tool to provide an overview of the topic, identify themes, and inform the development of research questions [38] **[39]. The literature review is used in information systems**
and knowledge management research to identify or construct a 'gap' in the literature, which presents an opportunity for researchers to explore further [35].

### B. Data **Collection**

The first step in surveying the literature on AI-generated images involved keyword searches in the Library and Information Science Source (LISS) database from 2014-2023. Keywords used for search: AI-generated images, generative AI, authenticity and AI-generated images, trustworthy AI-generated images, synthetic images, and deepfakes. The keywords were developed from relevant prior InterPARES theory and studies and compiled from an initial review of literature on AI and archives. Most of the results were either irrelevant, in that they did not address creation, use or management of AI-generated images or highly technical and too specific for our needs. The researcher leading this study and the graduate research assistants contributing to the literature review are archival scholars with a working knowledge of generative AI models and approaches to generating images through Generative Adversarial Networks (GANS) and more recent Diffusion Models, which enabled the selection of publications relevant to our topic. Literature aimed at AI technology developers and computer scientists was outside scope and was not included in the final selection. By focussing more on the creation, use, and management of AI-generated images, we identified several fields that discuss the topic in the context of verifying authenticity, concerns about accuracy and reliability, and fake image detection.

These fields are journalism and media communications, medicine, and law enforcement. Our next step involved using the same keyword searches in the Factiva database from 2022-2023. This produced several dozen news articles that revealed industry-led international initiatives aimed at addressing misinformation online, developing tools and methods for establishing provenance of AI-generated content, and exploring guidelines for the future of responsible AI. The keywords functioned as *a priori***codes that were included in** the annotated bibliography of 61 sources, which included peer-reviewed journal articles and conference papers, project reports in arXiv, newspaper articles, and project websites.

### C. Data **Interpretation**

The 61 sources formed the basis of the literature review; however, the rapid pace of generative AI applications and tools required an iterative approach to the literature review, with researchers working collaboratively to review sources and generate codes. Initially, sources were grouped according to discipline to determine how AI-generated images are being discussed and used in journalism and media communications, medical diagnostics, and law enforcement. In doing so, we were able to gain an overview of the current issues and approaches regarding AI-generated images specific to each field of application. An inductive and iterative process of content analysis identified emerging codes in the text and were added to existing codes (i.e., keywords), which were discussed amongst the researchers during biweekly zoom meetings. It was important to discuss the codes and their meanings with all researchers during the review process to ensure quality and reliability [38]. Thematic categories were identified through a process of reviewing, coding, and establishing relationships among codes. The codes were mapped into the following thematic categories: (1) Authenticity and Verifiability - this gathered approaches and activities that focus on contextual information that contributes to the creation and use of AI-generated images as trustworthy records; (2) Manipulation and Misinformation - this gathered approaches and activities that contribute to identifying images that have been intentionally altered or created as fakes; (3) Bias and Representation - this gathered approaches and activities that focus on training datasets for generative AI; (4) Attribution and Intellectual Property - this gathered approaches and activities specific to the rights and responsibilities of AI-generated images; (5) Transparency and Explainability - this gathered concerns specific to the proprietary nature of AI technologies and tools; and
(6) Ethical Considerations - this gathered concerns and approaches specific to privacy requirements.

The following section will present the results of the literature review according to disciplinary groupings.We acknowledge that there is significantly more literature of a highly technical nature in all these fields; however, the scope of our research study is purposefully limited to understanding the nature of AI-generated images through an exploration of their creation and use in the context of public records. Our primary audience is archival scholars and archivists who are interested in AI-generated images as an emergent record format that will eventually be appraised and acquired as born-digital archives.

## Section Iv. Results

### A. Field Of **Medicine**

In the field of medicine, the focus of AI tools appears to be on diagnostic applications. The term "medical imaging" refers less with the generation of images, but rather, the identification of tumors and anomalies within an image [40] **[41]. When generative AI is discussed, it is within the context of creating databases to**
train diagnostic AI. ML tools feature heavily in diagnostic imaging in medical research, receiving the most AI investment funding in 2022 at 6.1 billion USD [42] **[43]. AI increases the speed and accuracy of diagnoses**
using images of patients' bodies, either unaltered or digitally augmented for more efficient processing [44]
[45] **[46]. Although most current medical applications of AI technology involve intelligently capturing and**
processing authentic images of the human body, generative adversarial networks (GANs) can produce "synthetic images" to contribute to the data sets on which other ML tools are trained for diagnoses [47]. The term synthetic is favoured over generated, most likely due to medical applications using pre-existing images and either combining them to create a new image or adding a filter to generate images with diseases.

Simionescu and Iftene [48] indicate in their article on research directions in medical imaging that the focus on generative AI is involved with quality improvement rather than data generation. Generally, the current literature is concerned with protecting privacy of the dataset. This is particularly emphasized due to needing to balance between protecting patient privacy and requiring personally identifiable information to teach AI
[49]. The authors assert that Digital Imaging and Communication in Medicine (DICOM) metadata/ISO
12052 standard contains too much personal information to be safe for use in datasets. A cursory review of DICOM for information about AI-generated images reveals the use of image type field would require an assertion of image origins. The literature presents a strong demand for explainable AI. Mongan, Moy and Kahn [9] present a checklist that requires all elements of the dataset and sources of software decisions to be clearly communicated. A critique of AI is its lack of transparency regarding procedural decisions. Lastly, the need for international ethics for AI usage in medicine is suggested by Safdar et al. [50]. In early 2022, the Artificial Intelligence Industry Innovation Coalition (AI3C) was launched with the aim of providing best practices for using AI in healthcare [51]. A preliminary review revealed an emphasis on ethical AI in clinical practice, explainable AI, algorithmic biases in AI, data privacy, and synthetic AI. Further exploration of AI3C
could be useful.

$

### B. Field Of Law **Enforcement**

Most applications of ML in law enforcement and legal practice are based on AI image-processing techniques, the most widespread being facial recognition [52]. The usage of FRT, based on hypothetical and actual usage includes identity checking, searches for missing persons and/or criminals; and identifying child abuse [53]. In discussing responsible use principles for facial recognition technology (FRT), Lewis & Crumpler [54] focus on the inconsistencies in US legal restrictions and conditions for use. While 2 states and 19 municipalities have banned government use of FRT, no federal equivalent exists. Lewis and Crumpler list the following factors to consider when developing legal restrictions on FRT: permissible use; transparency; consent and authorization; data retention; redress and remedy; oversight and auditing; algorithmic review; and training data. Several authors discuss FRT in response to the proposed European Union's regulatory framework for AI, which was later enacted in 2023 [55]. Under the Act, real-time and remote biometric identification systems, such as facial recognition are considered an unacceptable risk as they pose a considerable threat to the public.

Hupont, Tolan, Gunes, and GÃ³mez [56] suggest the need for high quality, error-free, diverse datasets for AI in law enforcement to meet the standards set by the EU AI Act. Scholarship raises the familiar concern that AI- informed and automated predictive policing techniques can perpetuate the same inequities evident in the data sets on which they were trained [57] **[58]. Third party providers of FRT used by the police have come under** recent scrutiny for privacy breaches. Companies such as PimEyes and Clearview AI have populated training datasets with images scraped from the internet [59]. The Royal Canadian Mounted Police's (RCMP) usage of Clearview AI between 2019 and 2020 was investigated by the Office of Privacy Commissioner and revealed a breach of privacy for collecting and using personal data. [60].

In the Toolkit for Responsible AI Innovation in Law [10] we see the inclusion of deepfakes. The misuse of AI- generated images poses challenges for law enforcement. Considerable scholarship and United States-based legislation still refer to photorealistic AI-generated images and videos as "deepfakes" and/or AI-generated synthetic media, which rely on a (GAN) method and involve a mixture of AI deep learning with facial recognition software to manipulate existing images and produce realistic-looking images of people that are false [61] [62] [63] [64] **[65].**
These deepfakes are used to spread disinformation, facilitate identity theft (e.g., false social media profiles)
and provide fake evidence to manipulate legal proceedings. Deepfakes can thus be produced to respond to current events and ongoing geopolitical situations in real time, as was the case for a deepfaked video of Ukrainian president Volodymyr Zelenskyy instructing troops to surrender to Russian forces [66]. Celebrities and politicians are the most likely subjects of these media, though everyday women have been targets of
"revenge porn," and anyone can be the target of identity theft perpetrated through synthetic images [67]
[68]. The social threats and geopolitical implications of such digital media objects has "spurred research into improving automatic detection of deepfake images" [46]. An additional concern regarding AI-generated images and videos is the potential creation and dissemination of simulated Child Sexual Abuse Material
(CSAM). As demonstrated in the literature, the application of generative AI in law enforcement requires regulation and oversight to ensure its proper use and protect the privacy of the public.

C. Field of Journalism and Media **Communications**
In the past year, a deluge of commentary in the news media has explored the risks posed to the veracity of visual communications by photorealistic AI-generated images. The consensus on the threat is encapsulated in the New York Times headline "Can We No Longer Believe Anything We See?" [2]. The proliferation of AI-
generated images made using machine learning models that can generate new images based on training data and prompts profoundly disrupts the way people create, use, and think about digital images. AI algorithms enable individuals to create photorealistic images of subjects that do not exist; a task that would have taken days and required expertise to complete in the near past can now be performed by anyone with access to AI
tools in a matter of minutes [46]. Depending upon how AI-generated images are defined and in what context they are used, they pose a significant challenge to existing notions of the veracity of images and increase the risk of deception, fraud, and harm. The social impact of AI-generated visual objects is an area of emerging research with calls to move beyond commentary and provide empirical evidence [67].

The most recent and popular type of AI-generated image is based on a diffusion model, in which the AI application generates images from text prompts provided by a user. Much literature on AI-generated images thus focuses its attention on detection, through human and digital means, of photorealistic images as AI-
generated or manipulated [69]. Human participants in one study were only about 50% accurate in assessing whether images of human faces were authentic or synthetic [70], recalling a 2006 deception detection study that found humans detect lies at a similar rate, only about as effective as chance [71]. Early recommendations for assessing whether an image is AI-generated included straightforward contextualizing strategies, such as comparing the "internal" features of a face with the "external" features of head or jaw shape [72], though more recent commentary suggests that the days of that strategy's effectiveness have long passed [2]. The open access Handbook of Digital Face Manipulation and Detection proposes additional and somewhat more rigorous strategies for detecting synthetic images of human faces, addressing media forensics and physical and cybersecurity concerns [73].

The literature reveals that machine learning tools are significantly more effective than humans at parsing AI- generated images of human faces from authentic images [74] [75] **[76]. These detectors, some of which are**
open source (such as Wong's BLADERUNNER, which the author describes as a stopgap until more capable tools are developed), can parse synthetic images with upwards of 90% accuracy; Bird & Lotfi [75] propose the 92%-accurate Gradient Class Activation Mapping method, while Bhandari et al. [74] achieved 98% accuracy with the DICNN-XAI model. Another recommended approach to detection involves digitally "watermarking" AI-generated visual content by first watermarking all images used to train the AI, in tandem with standardized embedded metadata for human-generated audiovisual content [77].

At this stage, the public's trust in AI-generated images is uneven: gender, education, and income are all significant predictors of one's comfort with or aversion to artificial intelligence as a concept and in practice
[78]. Photorealistic AI-generated faces are perceived as "more trustworthy" than genuine photographs of real individuals to a statistically significant degree, likely because they are the literal average of the faces on which an AI has been trained [70]. This, of course, invites further discussion of the systemic inequalities inherent in AI image-processing technologies that have already been exposed by facial recognition software [57] **[79]**
[80].

Approaches to detecting synthetic media and combating misinformation and disinformation in visual content online are being mobilized by the combined efforts of technology and media companies in such initiatives as the Coalition for Content Provenance and Authenticity (C2PA). The joint effort of Adobe, Arm, Intel, Microsoft, and Truepic, C2PA develops "technical standards for certifying the source and history (referred to as provenance) of media content" [81]. The group effort of C2PA itself represents the combination in 2020 of Adobe's digital media-focused Content Authority Initiative (CAI) and the 2019-founded Project Origin, a collaboration between BBC and Microsoft focusing on digital journalism. The CAI is developing a secure endto-end system for digital content provenance through open-source development, cross-industry collaboration, and interoperability tools. Both of those initiatives continue alongside C2PA and execute the combined effort's deliverable technical standards. Introduced in January 2022, the C2PA end-to-end open technical standard would cryptographically hard-bind provenance data (referred to as a "claim," the aggregate authorship and editing "assertions" about an asset) to the asset through C2PA-enabled creation devices and/or the digital signatures of an actor whose identity has been confirmed by an industry-specific certification authority. In doing so, C2PA allows reliable statements about the provenance of digital content, such as its technical origin, its editing history, or the identity of the publisher. The CAI's methods do not allow for statements regarding whether the content is true, but rather allows for transparency and accountability of the context of creation and use of the content.

Ultimately the goal of CAI is to ensure that AI is used in responsible and ethical ways, protecting both the creators and consumers of AI-generated content. Understandably, there is a growing concern among journalists and creative professionals that: (1) original works will be used without permission or compensation in the training of generative AI, (2) non-generative AI works cannot be proven to be original and authentic visual depictions of reality, and (3) content produced using generative AI tools will not be properly attributed and will not be protected under copyright and intellectual property laws. In an already contentious information landscape, in which the dissemination of misinformation online has eroded the foundation of public trust in democracy, the potential harm posed by unregulated AI-generated content is considerable. Initiatives such as the C2PA recognize the need for the news ecosystem to adopt methods of maintaining the trust of their audiences, as well as the risk of media organizations using "illegitimate" content
[4].

The following section presents a discussion of the thematic categories identified throughout the literature review.

## Section V. Discussion

Analysis identified six prominent thematic categories across all fields when discussing the creation, use and management of AI-generated images. The following sub-sections discuss each theme and reveal how it informs the research study on the recordkeeping practices of creators using AI-generated images and contributes to an archival understanding of the nature of AI-generated images as an emergent record format

### A. Authenticity And **Verifiability**

As AI algorithms become more sophisticated, they are generating photorealistic images that are increasingly difficult for humans to distinguish from earlier born-digital and analog photographs. When AI-generated images are used in particular contexts, the risk of harm must be assessed and approaches to determining their authenticity and verifying their origins must be made available. Representing the vanguard of digital technology and publishing, the companies comprising C2PA and CAI have a vested interest in establishing standard practices, across these fields, for their products. They are acutely aware that disinformation, spread through the creation and use of deepfake images and videos online, undermines public trust in news reporting. The rapid pace of digital content creation and consumption (and thus the potential commercial profit) can be facilitated by seamless digital processes that maintain secure, accurate attribution and alteration records. CAI has concretized C2PA specifications through its open-source provenance embedding tools, the Content Credentials beta in Adobe Photoshop, and a partnership with Leica cameras that implements the C2PA standard (Parsons 2022). Planned for release in 2023, the Leica cameras integrate the C2PA metadata standard enabling photographers to include a "forgery-proof signature" that automatically captures technical metadata about the camera make and model, as well as information about image content, whether the image has been edited or altered, providing an "unbroken chain of authenticity from the time a picture is taken to the time it is published" [82].

Prior archival literature on the authenticity of born-digital images in various contexts has focused on the importance of metadata in establishing authenticity and protecting it over time. Bushey [17] argues that to ensure authenticity, the integrity and identity of a born-digital image must be established and maintained, and this requires specific contextual information to remain linked to the image file. Issues relating to the creation, capture, management, and preservation of adequate technical, descriptive, and administrative metadata are, therefore, integral to any research study addressing the authenticity of AI-generated images being used in the fields of medicine, law enforcement, and journalism and media communications. Therefore, an analysis and evaluation of the C2PA metadata specification should be undertaken by archival researchers to determine its compliance with existing criteria developed by the InterPARES Projects to support the presumption of authenticity (benchmark requirements) and to verify the authenticity of the record (baseline requirements). The activity would provide an opportunity for archival scholars to determine if there are gaps in the C2PA metadata specification and/or new methods of encryption that could pose barriers to preservation activities conducted by archivists in the future.

### B. Manipulation And **Misinformation**

AI-generated images have the potential to be manipulated and altered in ways that are difficult to detect without the use of special software and computer science expertise. As discussed in the literature, concerns about the deliberate use of AI to create fake images that contribute to the spread of misinformation online are prompting the development of extensive metadata specifications to establish virtual trust relationships for digital media content. "Signed provenance information transits trust between a creator and a consumer, based on a trust relationship between those two that exists outside the scope of C2PA" (C2PA Implementation Guidance 5.3.2) [81]. Certifying authorities and trust lists require "real-world due diligence to ensure credentials are only issued" to creators where rigorous industry-specific standards call for them (C2PA
Explainer 3.2) [81]. Though C2PA tools provide asset creators and consumers alike with "provenanceenabled user experiences," they do not make definitive statements about the reliability and accuracy of the media they document [4]. Thus, where C2PA guidelines are carried out according to the letter and spirit of the specifications, there remains a need to establish the roles and responsibilities of actors participating in the creation, use, and potentially the preservation of AI-generated images and linked metadata.

Prior archival literature on establishing the reliability and accuracy of born-digital images has focused on both adequate metadata and procedures over creation, use and transmission. Bushey [83] argues that industryspecific standard operating procedures and/or processing guidelines for records creators can establish roles and responsibilities and contribute to the assurance of record reliability and accuracy. Therefore, an analysis of C2PA metadata specific to roles and responsibilities should be undertaken by archival researchers to determine its capacity to establish actions taken by creators and events produced by AI tools.

### C. Bias And **Representation**

AI algorithms used to generate images can inherit biases present in the training data, which leads to biased representations of people, places, or events. Depending on the AI tool, creators may not be aware of the images and labels used in its training. The resulting AI-generated images may perpetuate bias and reinforce inaccuracies. Emerging policies regarding the use of FRT and biometric data in law enforcement demonstrate the contrarian nature of asserting objective policing practices when using tools that have been trained with datasets that may include racially- and gender- biased data. Suggestions to remedy this situation include validating databases to ensure that prejudice does not exist [84]. In the literature on diagnostic imaging in medicine the concern with bias in datasets is less clear; however, the concept of "ground truth" is discussed in the AI3C as well as several articles, which refers to the absolute, definitive reality or the factual information that is used as a reference to train and test a model. There may be an opportunity for archival researchers to explore criteria supporting ground truth and its correlation with identifying and eliminating bias in training data for AI tools.

At a time when archival institutions in North America are committing to decolonizing practices and engaging in reconciliation with Indigenous peoples and historically marginalized communities, it appears imperative that future approaches to archival appraisal and acquisition of born-digital archives include procedures to identify the full spectrum of information about AI-generated image content, including the provenance of training datasets. Therefore, recent literature by records and archives scholars exploring the concept of paradata should be considered when exploring the nature of AI-generated images and determining key components in the recordkeeping process [22].

### D. Attribution And Intellectual **Property**

AI-generated images present challenges in terms of attribution and intellectual property rights. It can be difficult to determine the original creator or owner of an AI-generated image due to the corpus of training data used and the algorithms employed. This raises questions about ownership and copyright. Notably, neither C2PA nor CAI considers AI to be a creator in and of itselfâ€”only a tool wielded by a human creatorand neither initiative renders judgements about the content of an image, instead providing a system that attributes both the tools used and by whom [85]. The recent ruling by the Copyright Office of the United States on generative AI states that copyright can protect only material that is the product of human creativity, and that AI-generated material does not contain human authorship [86]. An example is provided in which an AI technology receives a prompt from a human user and produces a visual work in response. It is explained that the user does not exercise ultimate creative control over how the system interprets the prompts and generates the material; therefore, the AI technology determines the expressive elements of its output. In this instance, the material is not protected by copyright. The Copyright Office of the United States acknowledges that there are additional implications for copyright issues raised by AI-generated works, and it intends to respond with an agency-wide initiative to explore these issues. In a similar approach, the Canadian federal government released a revised call for public consultation on copyright policy directions in regard to AI (2023). Of particular concern is the uncompensated use of copyright-protected works in the development of AI systems (i.e., text and data mining), the potential for AI-generated outputs to infringe existing copyrightprotected works, the role of AI systems in generating content and approaches to accurately attributing works and determining copyright protection, and the lack of practical enforcement remedies for rights holders. Both consultative processes reveal the complexity unique and complex challenges posed by AI technology and tools to intellectual property rights.

The outcome of these consultations will impact the development of AI technologies, the creation and use of AI-generated images, and the preservation of born-digital images archives that include AI-generated images. It will also impact the provenance and composition of datasets that are used to train AI and the textual prompts used to generate AI works. It should be noted that at this time, discussions about the impact these developments will have on protecting intangible cultural heritage and Indigenous knowledge are absent in the literature we reviewed. Therefore, contributions to the Canadian consultation on copyright in the age of AI have been provided by archival scholars and AI experts participating in the ITrustAI project. Yet it will be important for archival researchers to explore not only the knowledge of creators regarding current rulings and potential revisions to IPR in light of AI, but to identify methods by creators during the recordkeeping process establish authorship and ownership of AI-generated images.

### E. Transparency And **Explainability**

The AI algorithms used to generate images are complex and opaque, making it challenging to understand the process by which the images were created. The lack of transparent and explainable AI (XAI) is a significant challenge to existing legal and regulatory requirements for recordkeeping systems in which many institutions must comply with for purposes of accountability and risk mitigation. As of early June 2023, several sections of the C2PA specifications regarding synthetic content remained incomplete, demonstrating the speed at which generative AI tools are being released and the iterative nature of this nascent area. Pressures to release COTS hardware and software that make AI image generation possible on a large scale are resulting in real-time production of technical and ethical frameworks for their use. In this dynamic time, there is an opportunity for archivists and recordkeepers to participate in industry-led initiatives and contribute their expertise in shaping a trustworthy AI framework. As archivists have learned in the past decade, ensuring long-term digital preservation and future access to trustworthy records requires actions to prevent technological obsolescence and encourage interoperability of formats and systems. Through partnerships and multidisciplinary collaborations, archives can have a seat at the table.

### F. Ethical **Considerations**

The creation and use of AI-generated images raise ethical considerations such as privacy, consent, and the responsible use of technology. Current debates in the literature from the fields of medicine and law enforcement focus on the origins of the corpus of images used in training AI and techniques used to protect the privacy of individuals represented in the data without negatively impacting the utility of the dataset.

Depending on the juridical context in which the organization or institution operates, and the intended use of the AI generated content, the privacy requirements will differ to some degree. Prior records management and archival literature on determining retention and disposition schedules for digital records based on privacy requirements highlight the challenges posed by volumes of heterogenous content with a range of personally identifying information (PII) included throughout. Protecting PII is a major ethical consideration when it comes to born-digital records. Unauthorized access to PII can lead to identify theft and financial fraud.

Organizations that collect and store PII have a responsibility to protect it from unauthorized access. Therefore, archival researchers should explore recordkeeping processes by creators of AI generated records to determine if emerging tools and technologies provide protective measures such as identifying and classifying records and ensuring that they are stored securely.

# Section Vi. Conclusion And Next Steps

This paper presents and discusses the findings of a literature review on AI-generated images as an emergent digital record format. The literature review provides us with a current snapshot of what is known about AI- generated images in the archival field and identifies the gap in the literature, which leads to the following research questions: (1) How are individuals and organizations using AI-generated images? (2) What AI tools and technologies are being used to create, manage, and store AI-generated images? (3) What actions are being taken by individuals and organizations to identify AI-generated content? (4) What standards and/or policies are guiding procedures for creating, using, and preserving AI generated images? The next phase of the study will involve gathering data through interviews with creators in the fields of medicine, law enforcement, and journalism and media communications based on the research questions; a diplomatic analysis of the C2PA specification; analysis of the evolving copyright provisions for AI-generative works; and analysis of emerging industry-specific policies and procedures for AI-generated images.

### Acknowledgment

#### 

I thank the following individuals for their assistance in reviewing literature and contributing to the study on recordkeeping practices of creators using AI to generate images: Rachel Paprocki, Graduate Research Assistant, School of Information, San JosÃ© State University, San JosÃ©, USA; and Seiji Bessho, Graduate Research Assistant, iSchool, University of British Columbia, Vancouver, Canada.

| ï„‡ Authors    |
|--------------|
| ï„‡ References |
| ï„‡ Citations  |
| ï„‡ Keywords   |
| Metrics  ï„‡   |

### More Like This

Big Data Analysis of Intellectual Property Service Agencies 2021 4th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)
Published: 2021 Deceptive AI dehumanizes: The ethics of misattributed intelligence in the design of Generative AI interfaces 2024 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
Published: 2024

### Show More Ieee Account

Â» Change Username/Password Â» Update Address Purchase Details
Â»Payment Options
Â»Order History
Â»View Purchased Documents Profile Information
Â» Communications Preferences
Â»Profession and Education
Â»Technical Interests Need Help?

Â» **US & Canada:** +1 800 678 4333
Â»**Worldwide:** +1 732 981 0060
Â» Contact & Support About IEEE *Xplore* | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | Sitemap | Privacy & Opting Out of Cookies A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. Â© Copyright 2024 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.