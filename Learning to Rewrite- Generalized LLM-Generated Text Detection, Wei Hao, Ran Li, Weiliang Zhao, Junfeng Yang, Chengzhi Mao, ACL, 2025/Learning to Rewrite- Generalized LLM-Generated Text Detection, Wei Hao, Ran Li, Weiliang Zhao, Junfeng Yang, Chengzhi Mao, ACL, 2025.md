# Learning to Rewrite: Generalized LLM-Generated Text Detection

Ran Li<sup>1</sup>\*, Wei Hao<sup>1</sup>\*, Weiliang Zhao<sup>1</sup> , Junfeng Yang<sup>1</sup> , Chengzhi Mao<sup>2</sup> Columbia University<sup>1</sup> , Rutgers University<sup>2</sup>

{rl3424, wh2473, wz2665, jy2324}@columbia.edu, cm1838@rutgers.edu

### Abstract

Large language models (LLMs) present significant risks when used to generate non-factual content and spread disinformation at scale. Detecting such LLM-generated content is crucial, yet current detectors often struggle to generalize in open-world contexts. We introduce Learning2Rewrite, a novel framework for detecting AI-generated text with exceptional generalization to unseen domains. Our method leverages the insight that LLMs inherently modify AI-generated content less than human-written text when tasked with rewriting. By training LLMs to minimize alterations on AI-generated inputs, we amplify this disparity, yielding a more distinguishable and generalizable edit distance across diverse text distributions. Extensive experiments on data from 21 independent domains and four major LLMs (GPT-3.5, GPT-4, Gemini, and Llama-3) demonstrate that our detector outperforms stateof-the-art detection methods by up to 23.04% in AUROC for in-distribution tests, 37.26% for out-of-distribution tests, and 48.66% under adversarial attacks. Our unique training objective ensures better generalizability compared to directly training for classification, when leveraging the same amount of parameters. Our findings suggest that reinforcing LLMs' inherent rewriting tendencies offers a robust and scalable solution for detecting AI-generated text.

### 1 Introduction

Large Language Models (LLMs) demonstrate exceptional capabilities across various tasks [\(Radford](#page-9-0) [et al.,](#page-9-0) [2019;](#page-9-0) [Brown et al.,](#page-8-0) [2020;](#page-8-0) [Achiam et al.,](#page-8-1) [2023;](#page-8-1) [Touvron et al.,](#page-10-0) [2023;](#page-10-0) [Team et al.,](#page-10-1) [2023;](#page-10-1) [OpenAI,](#page-9-1) [2020\)](#page-9-1). However, they can be misused for illegal or unethical activities, such as spreading misinformation [\(Chen and Shu,](#page-8-2) [2023\)](#page-8-2), scaling spear phishing campaigns [\(Hazell,](#page-8-3) [2023\)](#page-8-3), facilitating social engineering and manipulation of social

<span id="page-0-0"></span>![](_page_0_Figure_8.jpeg)

Figure 1: Rewriting for LLM Text Detection. The histograms depict the edit distance distributions for texts generated by human and AI, illustrating how fine-tuning a rewrite model enhances their separation. We show two domains: Purple and Yellow represent human and AI distributions for Product Review texts, while Blue and Orange represent those for Environmental texts. Without fine-tuning the rewrite model, human and AI distributions are inseparable by a single threshold (red line, above). After fine-tuning, the texts can be separated by this threshold (below). On the right, we conceptualize L2R's intuition by showing that the rugged decision boundary between human and AI texts, caused by varying data distributions across domains, can be better aligned and divided by a single threshold after fine-tuning. Specifically, the standard deviation in decision thresholds among all domains decreases from 0.7506 to 0.4428 after fine-tuning.

media [\(Zhang et al.,](#page-10-2) [2024\)](#page-10-2), and generating propaganda [\(Pan et al.,](#page-9-2) [2023\)](#page-9-2). LLMs also facilitate academic dishonesty [\(Zellers et al.,](#page-10-3) [2019;](#page-10-3) [Mvondo](#page-9-3) [et al.,](#page-9-3) [2023\)](#page-9-3), and training foundation models with generated content can lead to irreversible defects in resulting models [\(Shumailov et al.,](#page-9-4) [2023\)](#page-9-4). These issues highlight the urgent need for reliable algorithms to detect LLM-generated text.

Various methods for detecting generated text have been proposed [\(Solaiman et al.,](#page-9-5) [2019;](#page-9-5) [Fagni](#page-8-4) [et al.,](#page-8-4) [2021;](#page-8-4) [Mitrovic et al.](#page-9-6) ´ , [2023;](#page-9-6) [Mitchell et al.,](#page-9-7) [2023;](#page-9-7) [Su et al.,](#page-10-4) [2023;](#page-10-4) [Liu et al.,](#page-8-5) [2024;](#page-8-5) [Bao et al.,](#page-8-6) [2024;](#page-8-6) [Mao et al.,](#page-9-8) [2024\)](#page-9-8). Most of these detectors

<sup>\*</sup>Equal contribution

employ pre-trained models, extracting hand-crafted features and heuristics, such as loss curvature [\(Bao](#page-8-6) [et al.,](#page-8-6) [2024\)](#page-8-6) and rewriting distance [\(Mao et al.,](#page-9-8) [2024\)](#page-9-8), and apply thresholds to distinguish LLM from human data. However, these thresholds are highly domain-dependent, obfuscating the establishment of a universal detection standard.

In this paper, we present L2R (Learning to Rewrite), which trains an LLM to perform more edits when being asked to rewrite human-generated data and fewer edits when rewriting on LLMgenerated data across a diverse set of domains. Unlike traditional detectors, which work well indistribution (ID) but often struggle to generalize among out-of-distribution (OOD) domains (including adversarial attacks), our algorithm leverages the inherent tendency of LLMs to modify their own output less frequently, and maximizing its generalizability by focusing on learning a single rewriting threshold across diverse distributions. Figure [1](#page-0-0) illustrates an example of how L2R learns to make LLM and human generated text more separable across domains, comparing with rewriting using a pre-trained model [\(Mao et al.,](#page-9-8) [2024\)](#page-9-8).

Visualizations and numerical results demonstrate that our targeted training objective enables LLMs to better capture the intricate structure of AI-generated content. To reflect the rapid advancements and real-world diversity of LLM-generated text, we in addition constructed a dataset spanning 21 domains (e.g., finance, entertainment, cuisine) using four different generator models. L2R surpasses the state-of-the-art detectors, achieving up to 19.56% higher AUROC ID and 35.10% higher OOD than [Verma et al.](#page-10-5) [\(2024\)](#page-10-5), 23.04% higher ID and 37.26% higher OOD than [Bao et al.](#page-8-6) [\(2024\)](#page-8-6), and 10.39% higher ID and 4.67% higher OOD than [Mao et al.](#page-9-8) [\(2024\)](#page-9-8). Comparing with fine-tuning a Llama-3 model for naive text classification, L2R has 51.35% higher AUROC OOD despite leveraging the same number of parameters. These results demonstrate that our training objective offers superior accuracy and generalizability. Furthermore, our method provides interpretability by highlighting the rewritten portions of the text. We will release our data, code, and models upon acceptance.

Our contributions are as follows:

• Fine-tuned detectors for generated text detection are known for overfitting to specific domains. We propose L2R, whose learning objective is rather to enlarge the edit distance between rewriting and the original text for LLMgenerated text while minimizing the ones that are human-generated. This learning objective is relatively domain-agnostic, yielding an invariant detection threshold across different data distributions.

- We build a diversely generated dataset (21 domains) and design a calibration loss function to make fine-tuning both effective and stable.
- We conduct comprehensive evaluations on ID, OOD datasets and against different adversarial attacks (Decoherence and Rewrite bypassing), showing that L2R surpasses state-of-the-art learning-based and zero-shot-based detectors.

## 2 Related Work

Various AI-generated text detectors have been proposed over the years. One set of detectors train a model on the input text [\(Solaiman et al.,](#page-9-5) [2019;](#page-9-5) [Fagni et al.,](#page-8-4) [2021;](#page-8-4) [Shnarch et al.,](#page-9-9) [2022;](#page-9-9) [Mitrovic´](#page-9-6) [et al.,](#page-9-6) [2023;](#page-9-6) [Liu et al.,](#page-8-7) [2023\)](#page-8-7). These methods excel in their training domains but struggle under OOD evaluation [\(Uchendu et al.,](#page-10-6) [2020;](#page-10-6) [Pu et al.,](#page-9-10) [2023\)](#page-9-10), namely detection with text from different domains or unfamiliar models. The second set of detectors utilize the raw outputs, i.e., logits, from pre-trained LLMs to assign probability score for detection. GLTR [\(Gehrmann et al.,](#page-8-8) [2019\)](#page-8-8) utilizes statistical features like log probability, word rank, and entropy to assign score, Ghostbuster [\(Verma](#page-10-5) [et al.,](#page-10-5) [2024\)](#page-10-5) utilizes log probability and unigram and bigram probability, DetectGPT [\(Mitchell et al.,](#page-9-7) [2023\)](#page-9-7) employs the delta in log probability of the input text after token perturbation to estimate AI likehood, PECOLA [\(Liu et al.,](#page-8-5) [2024\)](#page-8-5) selectively applys perturbation for enhanced accuracy, and Fast-DetectGPT [\(Bao et al.,](#page-8-6) [2024\)](#page-8-6) simplifies the process by exploiting conditional probability curvature. These family of detectors all require raw output of an LLM in some way or the other, but the main target of detection, namely commercial LLMs, are not open-sourced, which potentially impose a barrier on their probability estimation. Lastly, RAIDAR [\(Mao et al.,](#page-9-8) [2024\)](#page-9-8) is a detection method based on the observation that LLMs, when prompted to rewrite a given text, tend to produce a greater number of rewrites for humanwritten text compared to AI-generated text. Despite the attempt on capturing rewrite edit distance as a domain-agnostic feature, the rewrite amount

<span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)

Figure 2: Overview. Deleted characters are marked in red, added characters are marked in blue, and unmodified characters are in black. We exploit the difference in rewriting distance between human and AI texts for classification. While the off-the-shelf Llama-3 model give different amount of rewrite for human and AI texts (above), rewrites from our fine-tuned model are much more separable (below).

still varies across distributions, and the threshold of rewrite amount between human and AI texts learned on training domains does not generalize to OOD, which limits its full potential.

### 3 Method

#### 3.1 Rewriting for LLM Detection

Rewriting input with LLM and then measuring the change proves to be a successful way to detect LLM-generated content. Given an held-out input text set Xtrain with LLM and human generated text, and its corresponding label set Ytrain, an LLM F(·) is prompted to rewrite the input x ∈ Xtrain using a prompt p. The rewriting output is F(p, x). Particularly, the prompt p can be set to: Refine this for me please.

The edit distance between the input text and the rewritten output, D(x, F(p, x)), is then computed for all x ∈ Xtrain. [Mao et al.](#page-9-8) [\(2024\)](#page-9-8) adopts the Levenshtein distance [\(Levenshtein et al.,](#page-8-9) [1966\)](#page-8-9), which is defined as the minimum number of insertions, deletions, or substitutions required to transform one text into the other. With the Levenshtein distance, a similarity score we use for classification

is calculated based on:

$$D\_k(\mathbf{x}, F(\mathbf{p}, \mathbf{x})) = 1 - \frac{\text{Levenshtein}(F(\mathbf{p}, \mathbf{x}), \mathbf{x})}{\max(len(F(\mathbf{p}, \mathbf{x})), len(\mathbf{x}))}.\tag{1}$$

[Mao et al.](#page-9-8) [\(2024\)](#page-9-8) trains a classifier, such as logistic regression or decision tree, to threshold the similarity scores and predict if it is written by an LLM. However, as shown in Figure [1,](#page-0-0) the threshold of rewriting with a vanilla LLM often varies from one domain to another, causing RAIDAR to fail to generalize to new domains.

#### 3.2 Fine-Tuning the Rewrite Model

L2R works on the premise that human-written and AI generated text would cause a different amount of rewrites and a boundary can be drawn to separate both distributions. Thus we can finetune such a rewrite model F ′ (·), that gives as much rewrite as possible for human texts, while leaving the AI texts unmodified, demonstrated in Figure [2.](#page-2-0) Given some human text x<sup>h</sup> ∈ Xtrain and AI text xai ∈ Xtrain, our objective becomes:

$$\max\{D(\mathbf{x}\_h, F'(\mathbf{p}, \mathbf{x}\_h)) - D(\mathbf{x}\_{ai}, F'(\mathbf{p}, \mathbf{x}\_{ai}))\}\tag{2}$$

Since the edit distance is not differentiable, we use the cross-entropy loss L(·) assigned to the input x by F ′ (·) as a proxy to the edit distance. As a result, for each of input x with label y = 1 (AI) or 0 (human), our loss function becomes:

<span id="page-3-0"></span>
$$\min \{ L(\mathbf{X}\_{\text{train}}) \cdot y\_{\text{train}} \}, \quad y\_{\text{train}} = \begin{cases} 1 & \text{(AI)}\\ -1 & \text{(human)} \end{cases} \tag{3}$$

In this way, we flip the sign of the loss of the human texts. Since the overall loss would be minimized, this effectively encourages the rewrites to be different from human input and identical to the AI input.

#### 3.3 Calibration Loss during Fine-Tuning

When fine-tuning the rewrite model on Equation [3,](#page-3-0) the rewrite model aims to maximize the edits on human-generated text and minimize the edits on LLM-generated texts. However, without posting regularization and constraint on the unbounded loss, the rewrite model takes the risk of being corrupted (e.g., verbose output for all rewrite and overfitting with more edits on human-generated text rewrite) which we evaluated in [§A.5.](#page-11-0)

Therefore, we propose a calibration loss, which prevents the over-fitting problem by imposing a threshold value t on the absolute value of the loss on each given input. For human text xh, we apply gradient backpropagation only if the absolute loss L(xh) < t. For AI text xai, we apply backpropagation only if L(xai) > t. Otherwise, the gradient is set to 0. We show a pseudocode for the algorithm in [1.](#page-3-1)

<span id="page-3-1"></span>

| Algorithm 1 Calibration Loss Calculation                   |  |  |
|------------------------------------------------------------|--|--|
| Require: Threshold<br>t, loss<br>L(·), human text<br>xh,   |  |  |
| AI text xai                                                |  |  |
| 1: Lh<br>←<br>L(xh),<br>Lai<br>←<br>L(xai),<br>L<br>←<br>0 |  |  |
| 2: L<br>←<br>L<br>+<br>Lh<br>if Lh<br>< t                  |  |  |
| 3: L<br>←<br>L<br>+<br>Lai<br>if Lai<br>> t                |  |  |
|                                                            |  |  |

4: return L

Therefore, rather than minimizing the loss proxy, our objective becomes separating the distribution of human and AI rewrites to two ends of the threshold t. Concretely, this enables the model to only optimize against the hard examples, and leave those already correctly classified unchanged, so that we prevent overfitting. This is similar to DPO [\(Rafailov et al.,](#page-9-11) [2023\)](#page-9-11), where we fine-tune the rewrite model using only preference data, namely

the rewrites that are not yet separated by the existing boundary. This process is depicted by the graphical illustrations in Figure [1.](#page-0-0)

To determine the threshold t, we perform a forward pass using the rewrite model before finetuning on Xtrain and train a logistic regression model on all loss values. The threshold t can be derived from the weight and the intercept of the logistic regression model. In practice, applying the calibration loss improves detection performance by 4.54% in AUROC among the 21 domains, from 0.8555 to 0.9009.

### 4 Dataset

Existing detectors are often evaluated on datasets such as SQuAD [\(Rajpurkar et al.,](#page-9-12) [2016\)](#page-9-12), XSum [\(Narayan et al.,](#page-9-13) [2018\)](#page-9-13), and Writing Prompts [\(Fan et al.,](#page-8-10) [2018\)](#page-8-10). However, these datasets typically represent a narrow subset of available data, both in terms of timeliness and domain coverage. This limitation raises concerns about overfitting and uncertainty regarding how these detectors would perform when deployed in real-world scenarios, highlighting the necessity in creating a dataset of diversely-distributed texts for training.

#### 4.1 Data Collection

To ensure the robustness and generalizability of our detection model, we construct a dataset consisting of human-written text from 21 distinct domains, including finance, entertainment, cuisine, etc. For each domain, we collect the texts either by crawling online platforms like Wikipedia or by sampling from publicly available datasets. From these collections, we randomly select 200 complete paragraphs as text snippets which yields an average length of 120 words among the samples. For each of these 200 human-written samples per domain, we generate four AI-written counterparts using four state-of-the-art LLMs: GPT-4o [\(Ope](#page-9-14)[nAI,](#page-9-14) [2024\)](#page-9-14), GPT-3.5-Turbo [\(OpenAI,](#page-9-1) [2020\)](#page-9-1), Gemini 1.5 Pro [\(Reid et al.,](#page-9-15) [2024\)](#page-9-15), and Llama-3-70B-Instruct [\(Meta,](#page-9-16) [2024\)](#page-9-16). This results in a total of 21,000 text samples across all domains. Details on data generation are in Table [5,](#page-7-0) and descriptions of the domains and their sources are provided in [§A.1.](#page-11-1)

#### 4.2 Prompt Diversity

Conventionally, AI-generated text is created by prompting LLMs to either rewrite a given text or continue writing from a given prefix, often using a single, static prompt for the entire process [\(Mitchell et al.,](#page-9-7) [2023;](#page-9-7) [Bao et al.,](#page-8-6) [2024;](#page-8-6) [Verma](#page-10-5) [et al.,](#page-10-5) [2024;](#page-10-5) [Mao et al.,](#page-9-8) [2024\)](#page-9-8). However, realworld text generation involves a wide variety of prompts, which can significantly alter the distribution of the generated text. Previous work [\(Mao](#page-9-8) [et al.,](#page-9-8) [2024\)](#page-9-8) has shown that one straightforward way to bypass the RAIDAR detector is by using the prompt "Help me rephrase it, so that another GPT rewriting will cause a lot of modifications," which suggests that data generated by different prompts are different in distribution, indicating the importance of prompt diversity. To address this, we curate a dataset of 200 rewrite prompts, each containing slight variations in phrasing and instructions. For each generated text, a prompt is randomly sampled from this dataset. Examples of the prompts we use are provided below:

- Refine this for me please:
- Please rewrite this content in your own words:
- Make this text more formal and professional:
- Make this text more casual and friendly:
- Rephrase this text in a more elaborate way:
- Reframe this content in a more creative way:
- Rewrite this text to emphasize the key points:
- Help me rephrase it, so that another GPT rewriting will cause a lot of modifications:

For Gemini rewrite, training on diversely-prompted dataset increases testing AUROC from 0.7302 to 0.7566. For Llama rewrite, AUROC increases from 0.7888 to 0.7970. This shows that diverse prompts enables the model to better capture the distribution of AI texts in the real world, whose generation prompts are expected to vary significantly.

## 4.3 Data Cleaning

In collecting human-written text, we ensure that no data is generated after November 30, 2022, the release date of ChatGPT [\(OpenAI,](#page-9-1) [2020\)](#page-9-1), avoiding contamination of human dataset with AI-generated content. Instead of manually crafting the length, we split all texts into natural paragraphs, yielding an overall average length of 120 words with a standard deviation of 108 words. For AI-generated text, we carefully remove any extraneous suffixes, such as "Sure, here is a...," to avoid them be detected in this way.

## 5 Evaluation

This section answers the following questions:

- Q1: How does L2R compare with other detectors? ([§5.3\)](#page-5-0)
- Q2: How does L2R perform when OOD? ([§5.4\)](#page-5-1)
- Q3: How does L2R perform under adversarial attacks? ([§5.5\)](#page-5-2)
- Q4: How does L2R's training objective compare with directly training for binary classification? ([§5.6\)](#page-7-1)
- Q5: How does training on our proposed dataset contribute to L2R's performance? ([§5.7\)](#page-7-2)

### 5.1 Experiment Setup

We perform all experiments on one NVIDIA A100 GPU with 40GB RAM. We use 'meta-Llama/Meta-Llama-3-8B-Instruct' [\(AI@Meta,](#page-8-11) [2024\)](#page-8-11) as the open-sourced rewrite model in all experiments. To fine-tune the Llama model with 8B parameters, we employ 4-bit QLoRA [\(Dettmers et al.,](#page-8-12) [2024\)](#page-8-12), with parameter r set to 16, lora\_alpha set to 32, and lora\_dropout set to 0.05, unless otherwise noted. We use an initial learning rate of 5e-6, a weight decay of 0.01, and a batch size of 32 to train until convergence. We set the sampling temperature to 0 when using Llama for rewriting during training and detection for deterministic and reproducible results, therefore taking the results from a single run for the experiments. We use 70% of the dataset for training and the rest for testing in all experiments. Training on the 21 domains takes around six GPU hours and rewriting a single text of 120 words takes an average of 13.5 seconds.

## 5.2 Baselines

Our baseline detectors consist of Fast-DetectGPT [\(Bao et al.,](#page-8-6) [2024\)](#page-8-6), Ghostbusters [\(Verma](#page-10-5) [et al.,](#page-10-5) [2024\)](#page-10-5), RAIDAR [\(Mao et al.,](#page-9-8) [2024\)](#page-9-8), and a custom approach named 'Llama Logits,' which involves training a Llama-3-8B model together with a classifier (same size as RAIDAR and L2R) on its logits output to perform naive text classification. For Ghostbuster, RAIDAR and 'Llama Logits', we train and test these detectors on the identical training and testing sets as L2R. For Fast-DetectGPT, we use its local version available at [Fast-DetectGPT](#page-8-13) [\(2024\)](#page-8-13). For 'Llama Logits,' we train its Llama model using the same LoRA configurations as the rewrite model in L2R for a fair comparison. We also experiment on using a close-sourced model, Gemini 1.5 Pro [\(Reid et al.,](#page-9-15) [2024\)](#page-9-15) (referred to as Gemini Rewrite), as the rewrite model for RAIDAR in addition to Llama.

#### <span id="page-5-0"></span>5.3 Compare L2R with Other Detectors

We compare the performance of L2R with Fast-DetectGPT, Ghostbusters, and RAIDAR (Llama Rewrite and Gemini Rewrite), by measuring the Area Under the Receiver Operating Characteristic Curve (AUROC) scores. The resulting scores for each domain along with their average and standard deviation can be found in Table [1.](#page-6-0) L2R constantly outperforms both configurations of RAIDAR in all domains; outperforms Fast-DetectGPT in 20 of 21 domains by an average of 23.04% in AUROC; and outperforms Ghostbusters in 20 of 21 domains by an average of 19.56% in AUROC. L2R has a 5.62% lower AUROC score than Fast-DetectGPT on legal document domain, and a 1.62% lower AU-ROC score than Ghostbusters on literature creative writing domain, which might be due to the unique distributions of these domains: legal documents require a more rigorous writing style, while creative writing has a more casual style, thus leaving fewer room for rewrite even for human writers.

In general, the fluctuating AUROC scores indicate the challenging nature of our dataset and the diversity and independence of the distributions across domains. These results also show that L2R has better knowledge of the intricate differences between human and AI texts in various domains compared with the baselines, and is more capable in the real-world setting.

#### <span id="page-5-1"></span>5.4 OOD Dataset Evaluation

We showed that L2R outperforms the state-of-theart detectors ID in terms of AUROC scores, but it is equally important to assess its robustness under OOD conditions, as training-based detectors are prone to overfitting to familiar domains and generator models. We first evaluate this by showing its performance on OOD datasets.

To assess L2R's performance on OOD data, we adopt the M4 dataset [\(Wang et al.,](#page-10-7) [2024\)](#page-10-7), an OOD dataset that is different from our training data in multiple dimensions, including data generation models, text length, decoding strategy, and domains. We show a detailed comparison in Table [2.](#page-6-1)

The results of the OOD evaluation are presented in Table [3.](#page-7-3) We include both ID and OOD results to highlight the degree of overfitting for each detector. While the Llama Logits method achieves the highest ID AUROC, its OOD result is the lowest, indicating significant overfitting to the training data. Similarly, Ghostbuster shows overfitting

<span id="page-5-3"></span>![](_page_5_Figure_7.jpeg)

Figure 3: Relationship between the number of trainable parameters and ID and OOD AUROC scores for L2R and RAIDAR. As the number of parameters increase from 1 × 10<sup>6</sup> to 7 × 10<sup>6</sup> , both L2R and RAIDAR show higher ID performance and lower OOD performance, showing how the effect of overfitting emerges as we increase the LLM's trainable parameters. L2R outperforms Llama Logits either OOD or both ID and OOD, showing the superior robustness and accuracy of L2R.

with its OOD AUROC being roughly half of its ID performance. The naive rewrite-based approach shows superior robustness compared with these other methods, but L2R trained with reduced parameters, i.e. rank r set to 4 and lora\_alpha set to 8, outperforms Llama Rewrite by 3.45% ID and 4.67% OOD. This demonstrates that our fine-tuning does not simply overfits the rewrite model to the training data, but enhances its classification performance across diverse distributions.

We notice that reducing the number of training parameters make the model more generalizable, and further investigate the impact of finetuning parameters on L2R's performance ID and OOD. By adjusting the LoRA parameters r and lora\_alpha, we define four fine-tuning configurations with the number of trainable parameters ranging from 851,968 to 6,815,744, with details listed in [§A.4.](#page-11-2) Figure [3](#page-5-3) illustrates the results, where we observe a consistent increase in ID AUROC, accompanied by a decline in OOD AUROC as the number of parameters grows. This suggests that the model becomes increasingly overfitted to the training distribution. L2R either outperforms Llama Logits OOD or both ID and OOD, and all four configurations outperform Ghostbusters and Fast-DetectGPT both ID and OOD. Also, the first two configurations surpass RAIDAR in terms of AU-ROC across both settings.

#### <span id="page-5-2"></span>5.5 Adversarial Attack

We employ two distinct types of attack to assess L2R's robustness against the baseline detectors.

<span id="page-6-0"></span>

| Domain                    | Fast-DetectGPT | Ghostbusters | RAIDAR<br>(Gemini Rewrite) | RAIDAR<br>(Llama Rewrite) | Llama L2R |
|---------------------------|----------------|--------------|----------------------------|---------------------------|-----------|
| AcademicResearch          | 0.4664         | 0.6597       | 0.7911                     | 0.8311                    | 0.8406    |
| ArtCulture                | 0.6292         | 0.6781       | 0.7711                     | 0.6750                    | 0.8328    |
| Business                  | 0.6829         | 0.8331       | 0.8153                     | 0.8369                    | 0.9156    |
| Code                      | 0.6808         | 0.3770       | 0.5670                     | 0.3840                    | 0.8383    |
| EducationalMaterial       | 0.7474         | 0.8506       | 0.9339                     | 0.9675                    | 0.9644    |
| Entertainment             | 0.8392         | 0.8600       | 0.7836                     | 0.8319                    | 0.9494    |
| Environmental             | 0.8382         | 0.8447       | 0.9081                     | 0.9228                    | 0.9786    |
| Finance                   | 0.6879         | 0.7828       | 0.6917                     | 0.8153                    | 0.9400    |
| FoodCuisine               | 0.7425         | 0.6703       | 0.7181                     | 0.7831                    | 0.9547    |
| GovernmentPublic          | 0.7100         | 0.6833       | 0.7375                     | 0.7619                    | 0.8675    |
| LegalDocument             | 0.8365         | 0.5453       | 0.5528                     | 0.6594                    | 0.7803    |
| LiteratureCreativeWriting | 0.7928         | 0.9456       | 0.8056                     | 0.9161                    | 0.9294    |
| MedicalText               | 0.5693         | 0.6242       | 0.7614                     | 0.7700                    | 0.7857    |
| NewsArticle               | 0.5808         | 0.6800       | 0.7714                     | 0.8547                    | 0.9242    |
| OnlineContent             | 0.6292         | 0.5922       | 0.7408                     | 0.8231                    | 0.8881    |
| PersonalCommunication     | 0.5392         | 0.7042       | 0.6783                     | 0.7233                    | 0.8239    |
| ProductReview             | 0.6467         | 0.7364       | 0.7150                     | 0.8075                    | 0.9689    |
| Religious                 | 0.6314         | 0.6111       | 0.7772                     | 0.8397                    | 0.9775    |
| Sports                    | 0.6015         | 0.6561       | 0.6917                     | 0.7869                    | 0.8742    |
| TechnicalWriting          | 0.6075         | 0.7242       | 0.8269                     | 0.8575                    | 0.9369    |
| TravelTourism             | 0.6210         | 0.7517       | 0.8492                     | 0.8897                    | 0.9475    |
| AVERAGE                   | 0.6705         | 0.7053       | 0.7566                     | 0.7970                    | 0.9009    |
| STD                       | 0.1015         | 0.1259       | 0.0928                     | 0.1212                    | 0.0634    |

Table 1: Comparison of detection performance measured with AUROC scores. For Ghostbuster and all rewrite-based detectors, we train a single classifier on the training set of all domains, then test the model's performance on the test set of each individual domain. AVERAGE measures the average performance for all independent domains, and STD measures the standard deviation across domains.

<span id="page-6-1"></span>

| Dataset           | Ours                                               | M4                                         |
|-------------------|----------------------------------------------------|--------------------------------------------|
| Generator         | GPT-3.5-Turbo, GPT-4o, Llama-3-70B, Gemini 1.5 Pro | BLOOMz, ChatGPT, Davinci, Cohere, Dolly V2 |
| Text Length       | Mean: 765 chars, STD: 654 chars                    | Mean: 1365 chars, STD: 244 chars           |
| Decoding Strategy | Nucleus Sampling, Temperature = 1, top_p = 1       | Varies                                     |
| Domains           | 21 domains                                         | 5 Non-Overlapping English domains          |

Table 2: Comparison of characteristics of our dataset and M4 dataset, which we use for OOD evaluation.

For both experiments, we apply the attack to all AIgenerated texts in the testing set across all domains, while training L2R and the baselines on the unmodified training set and evaluating it on the modified testing set.

#### 5.5.1 Decoherence Attack

[Bao et al.](#page-8-6) [\(2024\)](#page-8-6) introduces the decoherence attack where two adjacent, randomly selected words are transposed in all sentences longer than 20 words within a paragraph for AI texts. [Bao et al.](#page-8-6) [\(2024\)](#page-8-6) demonstrated that this simple attack can be highly effective in degrading the performance of sate-ofthe-art detectors, without affecting the core meaning of the input. We present the results of this attack in Table [4,](#page-7-4) where L2R achieves the highest

AUROC on samples subjected to this attack, indicating its superior robustness compared to other models. This is because our rewrite-based objective function for fine-tuning teaches the model the innate distributions of human and AI texts, instead of relying on brittle statistical features that are easily altered through this simple attack.

## 5.5.2 Rewrite Attack

[Mao et al.](#page-9-8) [\(2024\)](#page-9-8) introduces the rewrite attack where a GPT-3.5-Turbo model is prompted to refine an input paragraph, generated by AI, in such a way that a subsequent rewrite by another GPT model would result in significant changes. [Mao](#page-9-8) [et al.](#page-9-8) [\(2024\)](#page-9-8) showed that this type of attack is particularly effective against rewrite-based detectors, as

<span id="page-7-3"></span>

| Model                                   | In-Distribution  | Out-of-Distribution |
|-----------------------------------------|------------------|---------------------|
| Ghostbusters                            | 0.7053           | 0.3888              |
| Fast-DetectGPT                          | 0.6705           | 0.6408              |
| Llama Logits                            | 0.9774           | 0.1426              |
| Llama Logits (Reduced Params)           | 0.8016           | 0.3450              |
| Llama Rewrite                           | 0.7970           | 0.6931              |
| Llama L2R<br>Llama L2R (Reduced Params) | 0.9009<br>0.8315 | 0.6561<br>0.7398    |

Table 3: ID and OOD performance measured in AU-ROC scores. For L2R and Llama logits, the "Reduced Params" models are tuned with approximately 1/4 of the parameters for better generalizability. With reduced parameters, L2R has the highest OOD AUROC, outperforming the naive Llama rewrite both ID and OOD by 3.45% and 4.67%, respectively, suggesting its generalizability through fine-tuning.

<span id="page-7-4"></span>

| Model          | No Attack | Decoherence Attack | Rewrite Attack |
|----------------|-----------|--------------------|----------------|
| Ghostbusters   | 0.7053    | 0.4730             | 0.4061         |
| Fast-DetectGPT | 0.6705    | 0.4984             | 0.5100         |
| Llama Logits   | 0.9774    | 0.7281             | 0.6563         |
| Llama Rewrite  | 0.7970    | 0.7681             | 0.7944         |
| Llama L2R      | 0.9009    | 0.8746             | 0.8927         |

Table 4: Adversarial attack results. While all detectors show performance degredation under attack, L2R has the highest AUROC in both setting, suggesting its robustness through fine-tuning.

it disrupts the rewrite we use for classification. As shown in Table [4,](#page-7-4) L2R again achieves the highest AUROC on these attack samples, further demonstrating its robustness through fine-tuning. This is because its fine-tuning objective creates separable gap between human and AI rewrite ratios that is large enough so that the attack samples remain in the AI distribution despite the perturbations. Concretely, the average edit ratio of human texts is 0.6981, and of AI texts is 0.8606. After attack, the ratio for AI decreases to 0.8386, which suggests that the rewrite attack is effective in shifting the AI distribution towards human, but there still exists a clear gap between both distributions, so that L2R's classification performance only degrades marginally.

#### <span id="page-7-1"></span>5.6 Compare L2R with Direct Fine-Tuning

A valid concern regarding L2R's superior performance is whether it is due to our fine-tuning objective, which enhances model's rewriting ability, or is solely from the fact that we exploit the vast parameters of an LLM. To answer this question, we compare L2R with the 'Llama Logits' baseline in Table [3](#page-7-3) and [4.](#page-7-4) The Llama logits detector involves fine-tuning a Llama-3-8B model not for rewrite, but directly for binary classification.

In [§5.4,](#page-5-1) we show that despite the Llama classifier has the highest ID AUROC score among all detectors, surpassing L2R by 7.65%, it has the lowest AUROC when evaluated OOD, up to 51.35% lower than L2R, which suggests that its performance ID is due to overfitting. This highlights the importance of our fine-tuning objective function in ensuring domain-agnostic detection accuracy. Also, the Llama classifier is inferior under adversarial attacks, with 14.65% and 23.64% lower AUROC for decoherence and rewrite attacks, respectively. This again shows L2R's robustness in capturing the true underlying distributions of human and AI data.

#### <span id="page-7-2"></span>5.7 Effectiveness of the Diverse Dataset

While there exists public datasets that emphasize data diversity, including RAID [\(Dugan et al.,](#page-8-14) [2024\)](#page-8-14), RuTAD [\(Maloyan et al.,](#page-8-15) [2022\)](#page-8-15), and MAGE [\(Li](#page-8-16) [et al.,](#page-8-16) [2024\)](#page-8-16), the contribution of our proposed dataset lies in its ability to train a robust and generalizable L2R model. We show this by training L2R on MAGE using the same number of texts and under the same configurations, then test its performance ID and OOD on the M4 dataset. We compare the results in [5,](#page-7-0) where L2R trained on our dataset has 15.98% higher OOD AUROC, suggesting that the diverse text distributions in our dataset is effective in training a robust and generalizable L2R model.

<span id="page-7-0"></span>

| Training Dataset | ID AUROC | OOD AUROC |
|------------------|----------|-----------|
| MAGE             | 0.8333   | 0.4963    |
| Ours             | 0.9009   | 0.6561    |

Table 5: Comparison of L2R's ID and OOD performance when trained on MAGE and our dataset. The superior OOD performance when trained on our dataset suggests its effectiveness.

### 6 Conclusion

We present L2R, a method designed to enhance the detection of LLM-generated text by learning to rewrite more on LLM-generated inputs and less on human generated inputs. L2R excels in identifying LLM-generated content collected across various models and 21 unique domains, both ID and OOD, and under adversarial attacks. Our work demonstrates that LLMs can be trained to detect content generated by other LLMs, surpassing previous detection methods in accuracy and generalizability.

### 7 Limitations

A limitation of ours is the relatively slow inference runtime. As most zero-shot detectors only requires a forward pass from the LLM being used, we need to call generate to create a rewrite. Nevertheless, this problem would be well alleviated considering the rapid enhancement in LLM efficiency and computing power.

### References

<span id="page-8-1"></span>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv Preprint arXiv:2303.08774*.

<span id="page-8-11"></span>AI@Meta. 2024. [Llama 3 model card.](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)

- <span id="page-8-6"></span>Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. 2024. Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. In *The Twelfth International Conference on Learning Representations*.
- <span id="page-8-0"></span>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. *Advances in Neural Information Processing Systems*, 33:1877–1901.
- <span id="page-8-2"></span>Canyu Chen and Kai Shu. 2023. Combating misinformation in the age of llms: Opportunities and challenges. *AI Magazine*.
- <span id="page-8-12"></span>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized llms. *Advances in Neural Information Processing Systems*, 36.
- <span id="page-8-14"></span>Liam Dugan, Alyssa Hwang, Filip Trhlík, Andrew Zhu, Josh Magnus Ludan, Hainiu Xu, Daphne Ippolito, and Chris Callison-Burch. 2024. [RAID: A](https://doi.org/10.18653/v1/2024.acl-long.674) [shared benchmark for robust evaluation of machine](https://doi.org/10.18653/v1/2024.acl-long.674)[generated text detectors.](https://doi.org/10.18653/v1/2024.acl-long.674) In *Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 12463– 12492, Bangkok, Thailand. Association for Computational Linguistics.
- <span id="page-8-4"></span>Tiziano Fagni, Fabrizio Falchi, Margherita Gambini, Antonio Martella, and Maurizio Tesconi. 2021. Tweepfake: About detecting deepfake tweets. *PLoS One*, 16(5):e0251415.
- <span id="page-8-10"></span>Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 889–898.

<span id="page-8-13"></span>Fast-DetectGPT. 2024. GitHub Repository. [\[link\].](https://github.com/baoguangsheng/fast-detect-gpt)

- <span id="page-8-8"></span>Sebastian Gehrmann, Hendrik Strobelt, and Alexander M Rush. 2019. Gltr: Statistical detection and visualization of generated text. In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations*, pages 111–116.
- <span id="page-8-3"></span>Julian Hazell. 2023. Large language models can be used to effectively scale spear phishing campaigns. *arXiv Preprint arXiv:2305.06972*.
- <span id="page-8-17"></span>IMDb. 2024. Imdb non-commercial datasets. [https://developer.imdb.com/](https://developer.imdb.com/non-commercial-datasets/) [non-commercial-datasets/](https://developer.imdb.com/non-commercial-datasets/).
- <span id="page-8-18"></span>Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. Pubmedqa: A dataset for biomedical research question answering. In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pages 2567–2577.
- <span id="page-8-9"></span>Vladimir I Levenshtein et al. 1966. Binary codes capable of correcting deletions, insertions, and reversals. In *Soviet Physics Doklady*, volume 10, pages 707– 710. Soviet Union.
- <span id="page-8-16"></span>Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Zhilin Wang, Longyue Wang, Linyi Yang, Shuming Shi, and Yue Zhang. 2024. Mage: Machine-generated text detection in the wild. In *Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 36–53.
- <span id="page-8-19"></span>Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. Dailydialog: A manually labelled multi-turn dialogue dataset. In *Proceedings of The 8th International Joint Conference on Natural Language Processing (IJCNLP 2017)*.
- <span id="page-8-5"></span>Shengchao Liu, Xiaoming Liu, Yichen Wang, Zehua Cheng, Chengzhengxu Li, Zhaohan Zhang, Yu Lan, and Chao Shen. 2024. Does detectgpt fully utilize perturbation? bridging selective perturbation to finetuned contrastive learning detector would be better. In *Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 1874–1889.
- <span id="page-8-7"></span>Xiaoming Liu, Zhaohan Zhang, Yichen Wang, Hang Pu, Yu Lan, and Chao Shen. 2023. Coco: Coherenceenhanced machine-generated text detection under low resource with contrastive learning. In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 16167–16188.
- <span id="page-8-15"></span>Narek Maloyan, Bulat Nutfullin, and Eugene Ilyshin. 2022. [Dialog-22 ruatd generated text detection.](https://doi.org/10.28995/2075-7182-2022-21-394-401) In *Computational Linguistics and Intellectual Technologies*, page 394–401. RSUH.
- <span id="page-9-8"></span>Chengzhi Mao, Carl Vondrick, Hao Wang, and Junfeng Yang. 2024. Raidar: Generative ai detection via rewriting. In *The Twelfth International Conference on Learning Representations*.
- <span id="page-9-18"></span>Julian John McAuley and Jure Leskovec. 2013. From amateurs to connoisseurs: Modeling the evolution of user expertise through online reviews. In *Proceedings of the 22nd International Conference on World Wide Web*, pages 897–908.

<span id="page-9-16"></span>Meta. 2024. Llama 3. [https://llama.meta.com/](https://llama.meta.com/llama3/) [llama3/](https://llama.meta.com/llama3/).

- <span id="page-9-7"></span>Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea Finn. 2023. Detectgpt: Zero-shot machine-generated text detection using probability curvature. In *International Conference on Machine Learning*, pages 24950–24962. PMLR.
- <span id="page-9-6"></span>Sandra Mitrovic, Davide Andreoletti, and Omran Ayoub. ´ 2023. [Chatgpt or human? detect and explain. ex](https://api.semanticscholar.org/CorpusID:256416337)[plaining decisions of machine learning model for de](https://api.semanticscholar.org/CorpusID:256416337)[tecting short chatgpt-generated text.](https://api.semanticscholar.org/CorpusID:256416337) *arXiv Preprint arXiv:2301.13852*, abs/2301.13852.
- <span id="page-9-21"></span>Edoardo Mosca, Mohamed Hesham Ibrahim Abdalla, Paolo Basso, Margherita Musumeci, and Georg Groh. 2023. [Distinguishing fact from fiction: A benchmark](https://doi.org/10.18653/v1/2023.trustnlp-1.17) [dataset for identifying machine-generated scientific](https://doi.org/10.18653/v1/2023.trustnlp-1.17) [papers in the llm era.](https://doi.org/10.18653/v1/2023.trustnlp-1.17) In *Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)*, pages 190–207, Toronto, Canada. Association for Computational Linguistics.
- <span id="page-9-3"></span>Gustave Florentin Nkoulou Mvondo, Ben Niu, and Salman Eivazinezhad. 2023. Generative conversational ai and academic integrity: A mixed method investigation to understand the ethical use of llm chatbots in higher education. *Available at SSRN 4548263*.
- <span id="page-9-13"></span>Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages 1797–1807.
- <span id="page-9-20"></span>Olympics. 2024. Olympics. [https://olympics.com/](https://olympics.com/en/) [en/](https://olympics.com/en/).
- <span id="page-9-1"></span>OpenAI. 2020. Chatgpt. [https://openai.com/](https://openai.com/chatgpt) [chatgpt](https://openai.com/chatgpt).

<span id="page-9-14"></span>OpenAI. 2024. Gpt-4o. [https://openai.com/](https://openai.com/index/hello-gpt-4o/) [index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/).

<span id="page-9-2"></span>Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and William Yang Wang. 2023. On the risk of misinformation pollution with large language models. In *The 2023 Conference on Empirical Methods in Natural Language Processing*.

- <span id="page-9-10"></span>Jiameng Pu, Zain Sarwar, Sifat Muhammad Abdullah, Abdullah Rehman, Yoonjin Kim, Parantapa Bhattacharya, Mobin Javed, and Bimal Viswanath. 2023. Deepfake text detection: Limitations and opportunities. In *2023 IEEE Symposium on Security and Privacy (SP)*, pages 1613–1630. IEEE.
- <span id="page-9-0"></span>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. *OpenAI Blog*, 1(8):9.
- <span id="page-9-11"></span>Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2023. [Direct preference optimization: Your language](https://openreview.net/forum?id=HPuSIXJaa9) [model is secretly a reward model.](https://openreview.net/forum?id=HPuSIXJaa9) In *Thirty-seventh Conference on Neural Information Processing Systems*.
- <span id="page-9-12"></span>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. *arXiv Preprint arXiv:1606.05250*.
- <span id="page-9-15"></span>Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. *arXiv Preprint arXiv:2403.05530*.
- <span id="page-9-19"></span>Jonathan Schler, Moshe Koppel, Shlomo Argamon, and James W Pennebaker. 2006. Effects of age and gender on blogging. In *AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs*, volume 6, pages 199–205.
- <span id="page-9-9"></span>Eyal Shnarch, Ariel Gera, Alon Halfon, Lena Dankin, Leshem Choshen, Ranit Aharonov, and Noam Slonim. 2022. Cluster & tune: Boost cold start performance in text classification. *arXiv preprint arXiv:2203.10581*.
- <span id="page-9-4"></span>Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023. The curse of recursion: Training on generated data makes models forget. *arXiv Preprint arXiv:2305.17493*.
- <span id="page-9-17"></span>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. [Recursive deep models for](https://www.aclweb.org/anthology/D13-1170) [semantic compositionality over a sentiment treebank.](https://www.aclweb.org/anthology/D13-1170) In *Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing*, pages 1631–1642, Seattle, Washington, USA. Association for Computational Linguistics.
- <span id="page-9-5"></span>Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. 2019. Release strategies and the social impacts of language models. *arXiv Preprint arXiv:1908.09203*.
- <span id="page-10-8"></span>Daniel Spokoyny, Tanmay Laud, Tom Corringham, and Taylor Berg-Kirkpatrick. 2023. Towards answering climate questionnaires from unstructured climate reports. *arXiv Preprint arXiv:2301.04253*.
- <span id="page-10-4"></span>Jinyan Su, Terry Yue Zhuo, Di Wang, and Preslav Nakov. 2023. Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text. In *The 2023 Conference on Empirical Methods in Natural Language Processing*.
- <span id="page-10-1"></span>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: A family of highly capable multimodal models. *arXiv Preprint arXiv:2312.11805*.
- <span id="page-10-9"></span>Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. [Beir:](https://openreview.net/forum?id=wCu6T5xFjeJ) [A heterogeneous benchmark for zero-shot evaluation](https://openreview.net/forum?id=wCu6T5xFjeJ) [of information retrieval models.](https://openreview.net/forum?id=wCu6T5xFjeJ) In *Thirty-Fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)*.
- <span id="page-10-0"></span>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. *arXiv Preprint arXiv:2302.13971*.
- <span id="page-10-6"></span>Adaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee. 2020. [Authorship attribution for neural text gener](https://doi.org/10.18653/v1/2020.emnlp-main.673)[ation.](https://doi.org/10.18653/v1/2020.emnlp-main.673) In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 8384–8395, Online. Association for Computational Linguistics.
- <span id="page-10-5"></span>Vivek Verma, Eve Fleisig, Nicholas Tomlin, and Dan Klein. 2024. Ghostbuster: Detecting text ghostwritten by large language models. In *Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)*, pages 1702–1717.
- <span id="page-10-7"></span>Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Chenxi Whitehouse, Osama Mohammed Afzal, Tarek Mahmoud, Toru Sasaki, Thomas Arnold, Alham Aji, Nizar Habash, Iryna Gurevych, and Preslav Nakov. 2024. [M4: Multi-generator, multi-domain, and multi](https://aclanthology.org/2024.eacl-long.83)[lingual black-box machine-generated text detection.](https://aclanthology.org/2024.eacl-long.83) In *Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 1369– 1407, St. Julian's, Malta. Association for Computational Linguistics.
- <span id="page-10-3"></span>Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. *Advances in Neural Information Processing Systems*, 32.
- <span id="page-10-2"></span>Yizhou Zhang, Karishma Sharma, Lun Du, and Yan Liu. 2024. Toward mitigating misinformation and social media manipulation in llm era. In *Companion Proceedings of the ACM on Web Conference 2024*, pages 1302–1305.
- <span id="page-10-10"></span>Lucia Zheng, Neel Guha, Brandon R Anderson, Peter Henderson, and Daniel E Ho. 2021. When does pretraining help? assessing self-supervised learning for law and the casehold dataset of 53,000+ legal holdings. In *Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law*, pages 159–168.

## A Appendix

#### <span id="page-11-1"></span>A.1 Dataset Details

Our dataset encompasses 21 indepedent English domains. Table [6](#page-12-0) shows the source and license for each domain. For all domains, we manually verify that no personal or offensive content are included. For domains that are taken from thirdparty datasets, we use the data consistent with their intended use (detection of machine generated text).

## A.2 Generation Prompts

Our dataset encompasses 200 different prompts for generating AI data. Here is an incomplete list of the prompts we used:

- Refine this for me please:
- Please rewrite this content in your own words:
- Make this text more formal and professional:
- Make this text more casual and friendly:
- Rephrase this text in a more elaborate way:
- Reframe this content in a more creative way:
- Can you make this sound more enthusiastic?
- Rewrite this passage to emphasize the key points:
- Help me rephrase it, so that another GPT rewriting will cause a lot of modifications:

## A.3 Effectiveness of the Diverse Prompt in Data Preparation

Our dataset involves 21 independent domains, four source LLMs, and 200 generation prompts, resembling real-world use cases for text detectors compared with traditional evaluation datasets which are usually constrained to one single domain and generation prompt. To prove the superiority of our dataset in training more capable detection models, we create a parallel nondiverse dataset which is created on the same number of domains and source LLMs, but generate the AI data with only with the prompt "Rewrite this for me please." Then, we train two RAIDAR detectors without finetuning, on the non-diverse dataset, and evaluate it on the diverse dataset. As shown in Table [7,](#page-12-1) the diverse prompts yields to 2.64% increase in AU-ROC score if the rewrite model is Gemini 1.5 Pro, and 0.82% increase in AUROC score if the rewrite model is Llama-3 8B. This validates the effectiveness of the diverse prompts we were using, and suggests that such diversity could help the detector to capture more information about real world data distributions.

### <span id="page-11-2"></span>A.4 LoRA Configurations for Fine-Tuning

Table [A.4](#page-11-2) lists the four fine-tuning configurations we use in [§5.4.](#page-5-1)

### <span id="page-11-0"></span>A.5 Effectiveness of the Calibration Loss

An important contribution of ours that improves the fine-tuning performance is the calibration loss, as proposed in §3.4. Without this loss, the model tends to overfit during fine-tuning as shown in Figure [4,](#page-11-3) where the model loss drastically decrease after 1500 steps, resulting in verbose rewrite even for LLM-generated text. We conduct an ablation study on five domains where the AUROC score is only 0.62 after the model overfits. We hypothesized that this technique could benefit model learning because the threshold effectively prevents further modification to model weights once an input, labeled either AI or human, falls in its respectively distribution already. Since our purpose is simply to draw a boundary rather than separate the distributions as much as possible, this halt in further weight adjustments facilitates the model to only perform parameter update on those inputs which are not yet correctly classified, so that it could converge more efficiently and effectively. Concretely, applying the calibration loss improves detection performance by 4.54% in AUROC among the 21 domains, even comparing with a model tuned with the loss before over-fitting.

<span id="page-11-3"></span>![](_page_11_Figure_20.jpeg)

Figure 4: Training loss curves for the rewrite model. The orange plots the loss trained without the calibration method, and the blue line plots the loss trained with the method. The later one exhibits faster convergence and higher stability than the former one.

## A.6 Different Ways to Generate OOD Data

There exists a variety of ways to generate OOD data, including using different generation models, decoding strategies, text lengths, and writing styles. While we show how M4, the OOD dataset we use for evaluation, is distinct from our training domain in all above aspects in [2,](#page-6-1) we conduct an additional

<span id="page-12-0"></span>

| Category              | Source                                                         | License                    |
|-----------------------|----------------------------------------------------------------|----------------------------|
| AcademicResearch      | Arxiv abstracts (Mao et al., 2024)                             | Various CC licenses        |
| ArtCulture            | Wikipedia                                                      | CC BY-SA                   |
| Business              | Wikipedia                                                      | CC BY-SA                   |
| Code                  | Code snippets (Mao et al., 2024)                               | MIT                        |
| EducationalMaterial   | Ghostbuster essays (Verma et al., 2024)                        | CC BY 3.0                  |
| Entertainment         | IMDb dataset (IMDb, 2024), Stanford SST2 (Socher et al., 2013) | IMDb terms of use, CC Zero |
| Environmental         | Climate-Ins (Spokoyny et al., 2023)                            | CC Zero                    |
| Finance               | Hugging Face FIQA (Thakur et al., 2021)                        | CC BY-NC                   |
| FoodCuisine           | Kaggle fine food reviews (McAuley and Leskovec, 2013)          | CC Zero                    |
| GovernmentPublic      | Wikipedia                                                      | CC BY-SA                   |
| LegalDocument         | CaseHOLD (Zheng et al., 2021)                                  | Apache 2.0                 |
| CreativeWriting       | Writing Prompts (Fan et al., 2018)                             | MIT                        |
| MedicalText           | PubMedQA (Jin et al., 2019)                                    | MIT                        |
| NewsArticle           | XSum (Narayan et al., 2018)                                    | MIT                        |
| OnlineContent         | Hugging Face blog authorship (Schler et al., 2006)             | Non-commercial             |
| PersonalCommunication | Hugging Face daily dialogue (Li et al., 2017)                  | CC-BY-NC-SA 4.0            |
| ProductReview         | Yelp reviews (Mao et al., 2024)                                | Yelp terms of use          |
| Religious             | Bible, Buddha, Koran, Meditation, and Mormon                   | N/A                        |
| Sports                | Olympics website (Olympics, 2024)                              | Olympics terms of use      |
| TechnicalWriting      | Scientific articles (Mosca et al., 2023)                       | CC Zero                    |
| TravelTourism         | Wikipedia                                                      | CC BY-SA                   |

Table 6: Source and license for each domain in our dataset.

<span id="page-12-1"></span>

| Dataset              | Rewrite Model | AUROC  |
|----------------------|---------------|--------|
| Single-Prompt        | Gemini        | 0.7302 |
| Multi-Domain Dataset | Llama         | 0.7888 |
| Multi-Prompt         | Gemini        | 0.7566 |
| Multi-Domain Dataset | Llama         | 0.7970 |

Table 7: Comparison of AUROC scores for Gemini and Llama rewrite models on nondiverse and duverse Datasets. Diverse prompting in the training set enhances detection performance for both models.

| r  | lora_alpha | Trainable Parameters |
|----|------------|----------------------|
| 2  | 4          | 851,968              |
| 4  | 8          | 1,703,936            |
| 8  | 16         | 3,407,872            |
| 16 | 32         | 6,815,744            |

Table 8: Parameter settings for LoRA fine-tuning.

ablation study on how different text length and decoding strategy alone could influence detection performance in Table [9](#page-13-0)

We use 200 randomly selected texts from our dataset for both studies. For decoding strategy, we use greedy decoding for GPT and Gemini models and beam search with num\_beams=5 for the Llama model. For text length, we chunk the texts to an average length of 60. We test L2R on the two datasets and show results below, where L2R

outperforms RAIDAR by 9.97% for the length ablation and 7.41% for the decoding strategy ablation in AUROC. This further shows L2R's robustness to different OOD data distributions.

#### A.7 Rewrite Examples

<span id="page-13-0"></span>

| Avg Length | Decoding Strategy             | Fast-DetectGPT | RAIDAR | L2R    |
|------------|-------------------------------|----------------|--------|--------|
| 120        | Nucleus Sampling              | 0.6833         | 0.8186 | 0.9213 |
| 60         | Nucleus Sampling              | 0.6500         | 0.7635 | 0.8632 |
| 120        | Greedy Decoding & Beam Search | 0.6897         | 0.8009 | 0.8750 |

Table 9: Performance comparison of different setups across models.

| Academic<br>Research          | INPUT HUMAN<br>In recent years, several techniques have been proposed<br>for increasing robustness to adversarial examples ---<br>and yet most of these have been quickly shown to be<br>vulnerable to future attacks. For example, over half of<br>the defenses proposed by papers accepted at ICLR<br>2018 have already been broken.                                                                                                      | REWRITTEN HUMAN<br>Despite recent efforts to increase robustness, most<br>proposed techniques have been quickly shown to be<br>vulnerable to future attacks. For instance, over half of<br>the defenses presented at ICLR 2018 have already<br>been breached.                                             |
|-------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                               | INPUT LLAMA-3-70B<br>Despite numerous attempts to develop robustness<br>against these threats, most proposed defenses have<br>been rapidly compromised by subsequent attacks. In<br>fact, a staggering 50% of defenses presented at ICLR<br>2018 have already been breached.                                                                                                                                                                | REWRITTEN LLAMA-3-70B<br>Despite numerous attempts to develop robustness<br>against these threats, most proposed defenses have<br>been rapidly compromised by subsequent attacks. In<br>fact, a staggering 50% of defenses presented at ICLR<br>2018 have already been breached.                          |
| Code<br>Indentation<br>hidden | INPUT HUMAN<br>def solve(s): flg = 0 idx = 0 new_str = list(s) for i in s: if<br>i.isalpha(): new_str[idx] = i.swapcase() flg = 1 idx += 1 s<br>= "" for i in new_str: s += i if flg == 0: return s[len(s)::-1]<br>return s                                                                                                                                                                                                                 | REWRITTEN HUMAN<br>def solve(s): result = [] for char in s: if char.isalpha():<br>result.append(char.swapcase()) else:<br>result.append(char) return "".join(result) if not<br>any(char.isalpha() for char in s) else "".join(result)[::-1]                                                               |
|                               | INPUT GPT-4o<br>def solve(s): has_letter = any(char.isalpha() for char in<br>s) if has_letter: return ''.join(char.swapcase() if<br>char.isalpha() else char for char in s) else: return s[::-1]                                                                                                                                                                                                                                            | REWRITTEN GPT-4o<br>def solve(s): has_letter = any(char.isalpha() for char in<br>s) if has_letter: return ''.join(char.swapcase() if<br>char.isalpha() else char for char in s) else: return s[::-1]                                                                                                      |
| Legal<br>Document             | INPUT HUMAN<br>This type of information may constitute trade secrets.<br>See G.L.c. 266, §30 (defining "trade secret" as used in<br>G.L.c. 93, §42                                                                                                                                                                                                                                                                                          | REWRITTEN HUMAN<br>This type of information may constitute trade secrets.<br>Massachusetts General Laws, chapter 266, section 30                                                                                                                                                                          |
|                               | INPUT GEMINI 1.5 PRO<br>This type of information, such as customer data and<br>proposals, can be legally protected as trade secrets<br>under Massachusetts law (G.L.c. 266, §30).                                                                                                                                                                                                                                                           | REWRITTEN GEMINI 1.5 PRO<br>This type of information, such as customer data and<br>proposals, can be legally protected as trade secrets<br>under Massachusetts law (G.L. c. 266, § 30).                                                                                                                   |
| Creative<br>Writing           | INPUT HUMAN<br>A voice in the sky will tell you when you've left a place<br>for the last time. You tell your SO goodbye and head out<br>to [ location of your choice ]. Halfway there you hear<br>"You have visited a location for the last time." John<br>wrinkled his nose and cleared his throat. Pausing, he<br>put down his briefcase and adjusted his tie. It was a red<br>tie with white stripes. This was one of his favorite ties. | REWRITTEN HUMAN<br>As John bid his SO farewell, he headed out to [location<br>of his choice]. Halfway there, a voice in his mind<br>whispered, "You have visited this place for the last<br>time." John's expression faltered, and he paused to<br>adjust his tie, a favorite red tie with white stripes. |
|                               | INPUT LLAMA-3-70B<br>As John bid farewell to his partner and headed out to<br>the city, a mysterious voice in the sky announced, "You<br>have visited a location for the last time." He paused,<br>adjusting his favorite red tie with white stripes, and<br>cleared his throat.                                                                                                                                                            | REWRITTEN LLAMA-3-70B<br>As John bid farewell to his partner and stepped out into<br>the city, a mysterious voice in the sky announced, "You<br>have visited a location for the last time." He paused,<br>adjusting his favorite red tie with white stripes, and<br>cleared his throat.                   |

Figure 5: Examples of texts in our proposed dataset along with the amount of edits L2R model gives for human and LLM data. Deleted characters are marked in red, inserted characters are in blue, and unmodified characters are in black. The examples demonstrate the diverse domains and source LLMs available in the dataset, as well as L2R's ability in separating human and LLM texts via rewriting.