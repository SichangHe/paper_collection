# Prevalence, Sharing Patterns, and Spreaders of Multimodal AI-Generated Content on X during the 2024 U.S. Presidential Election

Zhiyi Chen\*, Jinyi Ye\*, Emilio Ferrara, Luca Luceri

University of Southern California, Information Sciences Institute zchen346@usc.edu, jinyiy@usc.edu, emiliofe@usc.edu, ll 774@usc.edu

#### Abstract

While concerns about the risks of AI-generated content (AIGC) to the integrity of social media discussions have been raised, little is known about its scale and the actors responsible for its dissemination online. In this work, we identify and characterize the prevalence, sharing patterns, and spreaders of AIGC in different modalities, including images and texts. Analyzing a large-scale dataset from X related to the 2024 U.S. Presidential Election, we find that approximately 12% of images and 1.4% of texts are deemed AI-generated. Notably, roughly 3% of text spreaders and 10% of image spreaders account for 80% of the AI-generated content within their respective modalities. Superspreaders of AIGC are more likely to be X Premium subscribers with a right-leaning orientation and exhibit automated behavior. Additionally, AI image spreaders have a higher proportion of AI-generated content in their profiles compared to AI text spreaders. This study serves as a very first step toward understanding the role generative AI plays in shaping online socio-political environments and offers implications for platform governance.

# Introduction

Generative artificial intelligence (AI) technologies are increasingly mediating our online interactions on social media. From content moderation bots [\(Jhaver et al. 2019\)](#page-8-0) and synthetic personas [\(Ferrara 2024b\)](#page-8-1) to AI-generated news and tweets [\(Kreps, McCain, and Brundage 2022\)](#page-8-2), AI is assisting, augmenting, or even replacing human contributions [\(Sundar and Lee 2022\)](#page-9-0), creating a "synthetic reality" where human and AI actors coexist in our digital environment [\(Ferrara 2024a\)](#page-8-3). The advancements and proliferation of generative AI applications like ChatGPT, DALL·E, and Midjourney have amplified both the quantity and quality of AI-generated content (AIGC) online, while intensifying concerns about information credibility and authenticity [\(Lee](#page-8-4) [2020\)](#page-8-4), model bias [\(Liang et al. 2021\)](#page-8-5), social trust [\(Epstein](#page-8-6) [et al. 2023b\)](#page-8-6), and potential nefarious activity [\(Minici et al.](#page-9-1) [2024\)](#page-9-1). Although prior work has examined the impact of AIGC on human perceptions and behavior in the social media domain (e.g., [Jakesch et al.](#page-8-7) [\(2019\)](#page-8-7); [Du, Zhang, and Ge](#page-8-8) [\(2023\)](#page-8-8)), most of these studies are conducted in experimental settings, and relatively little attention has been given to the actual sharing behaviors of AIGC in real-world scenarios.

In critical contexts like democratic processes, concerns about AIGC on social media focus primarily on its negative impact on the integrity of online information. Research warns against the threat of deepfakes [\(Campbell et al. 2022\)](#page-8-9), coordinated information campaigns [\(Luceri et al. 2024\)](#page-9-2), and offensive speech targeting opposing viewpoints or vulnerable populations [\(Shaw 2023\)](#page-9-3). However, we know little about the scale, scope, and influence of AI-generated content online. Besides, research in misinformation consistently shows that a small fraction of individuals—also known as superspreaders—are responsible for the majority of questionable information shared on social media [\(DeVerna et al.](#page-8-10) [2024;](#page-8-10) [Nogara et al. 2022;](#page-9-4) [Baribi-Bartov, Swire-Thompson,](#page-7-0) [and Grinberg 2024\)](#page-7-0). Moreover, individuals who frequently share low-credibility content often post more toxic language and are more political in nature [\(DeVerna et al. 2024\)](#page-8-10). While AIGC clearly represents a new form of inauthentic content, it is still unknown whether superspreaders of AI-generated content display similar characteristics to misinformation superspreaders, yet, to the best of our knowledge, no prior work has examined this prolific group of users.

In this study, we utilize a comprehensive dataset collected during the 2024 U.S. Presidential Election period on X (formerly Twitter) to investigate and characterize the behavior of users sharing AI-generated content over the three months leading up to the election. We focus on disentangling AIGC based on its modality, thus differentiating between text and image generated by AI, given the increasing attention on the effects of multimodal generative AI on human behavior [\(Yan et al. 2024\)](#page-9-5). Our analysis begins by addressing fundamental questions about the prevalence of AIGC and identifying the users responsible for its dissemination. To this end, we use GPT-4o to detect AI-generated images and a fine-tuned RoBERTa model to distinguish AI-generated text. Through this process, we identify 303 superspreaders across two modality groups, employing established metrics of online influence [\(DeVerna et al. 2024\)](#page-8-10). Finally, we characterize the behaviors of these superspreaders, focusing on their levels of automation and toxicity. This work addresses the following research questions (RQs):

- RQ1: *What is the prevalence and concentration of AIGC on* X*? Do we observe consistent inequalities in sharing patterns across distinct AIGC modalities?*
- RQ2: *What are the characteristics and sharing behav-*

<sup>\*</sup>These authors contributed equally.

#### *iors of AIGC superspreaders across different modalities?*

Contributions of This Work. This study serves as an initial step toward understanding how generative AI technologies shape social media dynamics in political discussions. As one of the first investigations using real-world data to assess the prevalence of AIGC on social media, we reveal that approximately 12% of images and 1.4% of texts within the online discourse on X related to the 2024 U.S. Presidential Election are AI-generated. Notably, a small fraction of users dominate AIGC dissemination—about 3% of text spreaders and 10% of image spreaders account for 80% of content in their respective modalities. We identify and characterize the behaviors of AIGC superspreaders, noting that they are more likely to have a right-leaning political orientation, subscribe to X Premium, and exhibit stronger bot-like behaviors. Furthermore, we analyze differences in sharing patterns across AIGC modalities, finding that users spreading AIgenerated images have a higher proportion of AIGC within their timelines compared to those sharing AI-generated text. We hope our findings pave the way for further research on the role of AIGC in social media, offering valuable insights for platform governance, policy-making, and public awareness of generative AI's growing impact on online sociopolitical landscapes.

# Related Work

### Multimodal AIGC on Social Media

Due to challenges in detecting AIGC and limited real-world datasets, only a few studies have explored its presence and impact on social media. Research on AI-generated images has examined the prevalence and misuse of GAN-generated visuals for inauthentic activities [\(Yang, Singh, and Menczer](#page-9-6) [2024\)](#page-9-6), the role of synthetic content like political memes in shaping discourse during elections [\(Chang et al. 2024;](#page-8-11) [Minici et al. 2024\)](#page-9-1), and the broader influence of AIGC on platform dynamics, including content creation and consumption patterns [\(Wei and Tyson 2024\)](#page-9-7). In contrast, studies on AI-generated text are scarce, with one notable work investigating its prevalence among coordinated and organic users across platforms [\(Cinus et al. 2025\)](#page-8-12).

Despite these contributions, two significant gaps remain in the literature. First, many studies are limited in their detection of AIGC, either focusing on a narrow range of models, such as GAN [\(Yang, Singh, and Menczer 2024;](#page-9-6) [Ricker](#page-9-8) [et al. 2024\)](#page-9-8), or relying on explicit hashtags to identify labeled content [\(Wei and Tyson 2024\)](#page-9-7), which excludes unlabeled AIGC, including potentially harmful or misleading content. Additionally, manual identification approaches [\(Chang et al. 2024\)](#page-8-11), while useful, are neither scalable nor consistently accurate. These limitations prevent a comprehensive understanding of the full landscape of AIGC on social media. Second, existing research largely fails to adopt a multimodal perspective, which is critical given the differences between text and image modalities in emotional resonance [\(Li and Xie 2020\)](#page-8-13), accessibility, and their potential to mislead audiences [\(Barari et al. 2021;](#page-7-1) [Sundar, Molina, and](#page-9-9) [Cho 2021\)](#page-9-9) and retain memories [\(Kirkpatrick 1894\)](#page-8-14). As multimodal AIGC, including text, image, audio, and video, increasingly circulates online, examining multiple modalities is essential to fully capture its scope and implications. To address these gaps, this work adopts a multimodal approach to analyze AIGC on social media platforms, aiming to provide a comprehensive view of its prevalence and explore differences in user sharing behaviors across modalities.

### Superspreaders of Online Information

Superspreaders, also referred to as supersharers, are users who disproportionately contribute to the spread of specific types of content on social media, such as low-credibility or fake news. They play a unique role in shaping online information ecosystems by generating content that garners substantial reach and engagement. Studies have consistently found that a small fraction of superspreaders accounts for the majority of misinformation shared online [\(Nogara et al.](#page-9-4) [2022;](#page-9-4) [DeVerna et al. 2024\)](#page-8-10). For instance, during the 2016 U.S. Presidential Election, just 0.1% of Twitter users were responsible for nearly 80% of fake news shared, with similar patterns observed during the 2020 Election [\(Grinberg](#page-8-15) [et al. 2019;](#page-8-15) [Baribi-Bartov, Swire-Thompson, and Grinberg](#page-7-0) [2024;](#page-7-0) [Guess, Nyhan, and Reifler 2018\)](#page-8-16). Superspreaders are typically identified using various metrics, such as k-core decomposition to assess centrality within the network, the sum of nearest neighbors' degrees to evaluate local influence, and the h-index to measure the volume and reach of their shared content [\(Pei et al. 2014;](#page-9-10) [DeVerna et al. 2024\)](#page-8-10). Superspreaders are not limited to bots or automated accounts but frequently include politically active individuals and pundits with large followings [\(Guess, Nagler, and Tucker 2019;](#page-8-17) [Baribi-Bartov, Swire-Thompson, and Grinberg 2024\)](#page-7-0). Recent work has also shown that X's recommendation algorithm amplifies superspreaders like political commentators and partisan influencers [\(Ye, Luceri, and Ferrara 2024\)](#page-9-11).

While most studies have focused on superspreaders of misinformation, the rise of generative AI introduces new complexities and raises open questions. AIGC is often multimodal, blending text and visuals, which may enhance its appeal and spreadability [\(Cao et al. 2023\)](#page-8-18). Additionally, AIGC's potential to mimic diverse styles and create content at scale raises concerns about its integration into existing misinformation networks [\(Ferrara 2024a\)](#page-8-3). Superspreaders of AIGC may exploit these affordances and automation capabilities to amplify their influence further. This study, therefore, builds on prior work by investigating AIGC superspreaders during the 2024 U.S. Presidential Election.

# Methodology

### Data Collection and Curation

We leverage an existing dataset of tweets related to the 2024 U.S. ELECTION [\(Balasubramanian et al. 2024\)](#page-7-2). The dataset was generated by querying targeted keywords related to political figures, events, and emerging issues of the Presidential Election to query data effectively. In this study, we analyze data spanning a three-month period leading up to the election, from July 1, 2024, to September 30, 2024. The resulting dataset includes 24.7M tweets and 2.5M images shared by 3.1M users.

Table 1: Basic statistics of the dataset

<span id="page-2-0"></span>

| Modality          | Text       | Image     |
|-------------------|------------|-----------|
| # of tweets       | 24,746,761 | 1,896,292 |
| # of users        | 3,108,782  | 414,541   |
| # of AIGC sharers | 170,980    | 88,045    |
| # of target users | 17,584     | 12,898    |

Since our analysis focuses on characterizing the behaviors of users who shared AI-generated content during the observed period, we first extract and count these AI content sharers. To facilitate comparisons between the two modalities, we classify users into two groups: text and image, and apply the filtering process separately while adhering to the same criteria. Specifically, users who shared at least one AIgenerated text are referred to as *AI text sharers*, while those who shared at least one AI-generated image are denoted as *AI image sharers* (details on the detection of AIGC can be found in the *AI-Generated Content Detection* section). This yields approximately 170K AI text sharers and 88K AI image sharers.

Next, to address RQ2, we apply a filtering process to identify a subset of target users. To ensure that each user has sufficient data points to analyze their sharing behavior and received engagement, two criteria are applied. The first criterion sets a minimum threshold for the total number of tweets a user has posted during the observed time period. For AI text sharers, users with fewer than five tweets containing text are excluded. Similarly, for AI image sharers, users who have posted fewer than five tweets with at least one image in each are excluded from the analysis. The second criterion requires that among all tweets a user has shared, at least one must have received at least one retweet, i.e., *h*-index > 0 (details on the calculation of *h*-index are provided in the *Identifying AIGC Superspreaders* section). These criteria collectively ensure a robust and meaningful dataset for evaluating user activity and influence. After this filtering step, we retrieve a subset of *target users* consisting of 17,584 AI text sharers and 12,898 AI image sharers. Table [1](#page-2-0) provides the basic statistics of the dataset.

## AI-Generated Content Detection

With the rise of AI-generated content on platforms like X, detection has become a significant challenge. Traditional deep learning models often struggle to distinguish AI-generated images [\(Borji 2022\)](#page-7-3), but recent advancements show that large language models (LLMs) like GPT-4o, paired with well-crafted prompts, offer robust multimodal performance [\(Chen et al. 2023;](#page-8-19) [Chang et al. 2024;](#page-8-11) [Brin](#page-8-20) [et al. 2024\)](#page-8-20). Accordingly, we employ GPT-4o for image detection in this study. For AI-generated texts, LLMs often underperform in zero-shot settings [\(Bhattacharjee and](#page-7-4) [Liu 2024\)](#page-7-4). To address this, we follow prior successful approaches [\(Dmonte et al. 2024;](#page-8-21) [Cinus et al. 2025\)](#page-8-12) by curating a dataset of AI-generated and human-written texts and fine-tuning a RoBERTa model for enhanced accuracy. This method ensures reliable detection across diverse sources.

AI-Generated Text Detection. To detect AI-generated texts on X, we build on the method introduced in [Dmonte](#page-8-21) [et al.](#page-8-21) [\(2024\)](#page-8-21), which uses a fine-tuned RoBERTa model trained on 8,000 election-related claims written by humans and three open-source LLMs: Llama, Falcon, and Mistral. However, we enhance this dataset as it does not account for texts generated by other widely-used generative AI models, such as GPT, Claude, and Gemini. To address this limitation, we construct a new dataset comprising tweets generated by four models: GPT-4o, Claude 3 Sonnet, Gemini 1.5, and Llama 3 8B, alongside human-written tweets. Human-written tweets are randomly sampled from a dataset related to the 2020 U.S. Election [\(Chen, Deb, and Ferrara](#page-8-22) [2022\)](#page-8-22), which predates the widespread adoption of LLMs like ChatGPT. For each human-written tweet, we generate an AI counterpart using the following prompt to ensure similar topic and language distributions [\(Cinus et al. 2025\)](#page-8-12):

*This is a tweet related to the 2020 U.S. election:* {*tweet*}*. Based on the topic of the given tweet, write a new tweet, mimicking the language styles used by Twitter users.*

Table [2](#page-3-0) presents the number of instances for each label in the training and test datasets. We performed binary classification to distinguish between human- and AIgenerated tweets. The RoBERTa model was fine-tuned using the AdamW optimizer with a learning rate of 1e-5, trained for 3 epochs on the training data, and evaluated on the test dataset. The model achieved a 0.96 F1-score on the validation set, demonstrating strong performance in distinguishing AI-generated texts from human-written ones. Training code and dataset can be found in URL[1](#page-2-1) .

AI-Generated Image Detection. We leverage GPT-4o to differentiate AI-generated images from non-AI-generated ones. To evaluate its performance, we follow the approach of [Epstein et al.](#page-8-23) [\(2023a\)](#page-8-23), constructing a validation set of 1,200 AI-generated images and 1,200 non-AI-generated images. The non-AI-generated subset is randomly sampled from the LAION-400M dataset [\(Schuhmann et al. 2021\)](#page-9-12), while the AI-generated subset includes 400 images from the TwitterGAN dataset [\(Yang, Singh, and Menczer 2024\)](#page-9-6), 400 DALL·E-generated images crawled from the subreddit r/ dalle[2](#page-2-2) , and 400 Midjourney-generated images crawled from its official website[3](#page-2-3) .

We extract image URLs from the metadata of tweets in our dataset and craft the following prompt:

*Is this an AI-generated image? Answer in one word: "yes" or "no".*

The prompt, along with the image URL, is provided to GPT-4o, with the temperature fixed at 0 to minimize variability. Among the 2,400 images in the validation set, GPT-4o successfully respond to 2,367 images but failed to process 33 due to issues such as sensitive content. The model achieved an F1-score of 0.96, demonstrating high accuracy in detecting AI-generated images.

<span id="page-2-2"></span><span id="page-2-1"></span><sup>1</sup> [https://github.com/angelayejinyi/AI](https://github.com/angelayejinyi/AI_political_tweet_classifier) political tweet classifier 2 Subreddit r/dalle:<https://www.reddit.com/r/dalle/>

<span id="page-2-3"></span><sup>3</sup>Midjourney Explore:<https://www.midjourney.com/explore/>

Table 2: Label distribution of instances in the training and test datasets

| Dataset       | AI             |                |              | Human        | Total           |                 |
|---------------|----------------|----------------|--------------|--------------|-----------------|-----------------|
|               | GPT            | Llama          | Claude       | Gemini       |                 |                 |
| Train<br>Test | 4,000<br>1,000 | 4,000<br>1,000 | 1,796<br>449 | 2,204<br>551 | 12,000<br>3,000 | 24,000<br>6,000 |

<span id="page-3-0"></span>We then apply GPT-4o detection to our 2024 U.S. Election dataset, using the OpenAI BatchAPI[4](#page-3-1) to optimize computational time and cost. The model's responses for each image are categorized as: (i) valid responses ("yes" or "no"), (ii) failure to download the image due to content moderation, or (iii) inability to respond, likely due to sensitive content, with messages like, "Sorry, I cannot answer this." Valid responses correspond to category (i), while categories (ii) and (iii) are classified as invalid. Of the 2,462,132 images processed, approximately 90.5% receive valid responses. Subsequent analyses focus on the remaining images and users.

### Identifying Superspreaders of AIGC

Superspreaders are typically identified using influence metrics, including k-core centrality, the sum of nearest neighbors' degrees, and the engagement driven by their original content [\(Pei et al. 2014;](#page-9-10) [DeVerna et al. 2024\)](#page-8-10). Here, we adopt the *h*-index, originally designed to measure scholarly impact [\(Hirsch 2005\)](#page-8-24), and later adapted for identifying superspreaders of low-credibility content on X [\(DeVerna et al.](#page-8-10) [2024\)](#page-8-10). In our scenario, the h-index for a user i is defined as the maximum value of h such that user i posted at least h tweets containing AIGC, each retweeted at least h times. This metric captures both the volume of AI-generated content shared and the engagement received by each post. A superspreader must meet both criteria: a high number of tweets containing AI-generated content and a significant volume of received retweets. This ensures that superspreaders are identified based on both their activity and the broader impact of their AI-generated content on the platform.

### Characterizing AIGC Superspreaders

We characterize superspreaders of AI-generated content based on three key metrics: The *AI score* quantifies the proportion of AI-generated content in a user's posts. The *bot score* evaluates the probability of an account being automated based on its historical tweet activity. Finally, the *language toxicity* metric evaluates the degree of toxicity present in a user's shared content.

AI Score. To quantify the proportion of AI-generated content in a user's tweets, we introduce a new metric called the *AI score*, defined as:

$$r\_i = \frac{A\_i}{T\_i},$$

where A<sup>i</sup> is the number of tweets containing AI-generated text (for AI text sharers) or at least one AI-generated image (for AI image sharers), and T<sup>i</sup> is the total number of tweets or tweets containing an image shared by the user i during the observation period. This metric captures the dominance of AIGC in a user's shared posts, with higher AI scores indicating a greater reliance on AIGC, whether in text or image. We compute the Pearson correlation coefficient between the AI scores of users who share both AI-generated images and texts, yielding a value of 0.20 (p < .001), indicating a weak positive relationship. This suggests that AI text and image sharers exhibit distinct behaviors, with few users actively sharing both types of content.

Bot Score. We use the bot score metric to assess the likelihood of an account being automated. Bot scores are calculated using the widely adopted tool Botometer [\(Ferrara](#page-8-25) [et al. 2016\)](#page-8-25), which evaluates various account features, such as tweet content, network interactions, and posting behavior. A machine learning model then assigns a score ranging from 0 to 1, where higher scores indicate a greater likelihood of automation. Notably, Botometer relies on the Twitter V1 API [\(Yang et al. 2020\)](#page-9-13) to retrieve historical account data. Since this API was discontinued in 2023, we can only retrieve bot scores for accounts created prior to May 2023.

Language Toxicity. To examine the prevalence of toxic language among target users, we draw from the Google Jigsaw Perspective API [\(Lees et al. 2022\)](#page-8-26), a machine learning tool designed to identify and manage harmful or abusive content on online platforms. The API assigns a toxicity score to each tweet, ranging from 0 to 1, with higher scores indicating a greater likelihood of rude or harmful language. This analysis focuses exclusively on English-language tweets. To calculate a user's overall toxicity level, we average the toxicity scores of all the tweets they have posted.

# Results

### RQ1: Prevalence and Concentration of AIGC

Investigating the prevalence of AI-generated content involves two key questions. On the one hand, we aim to reveal the proportion of AI-generated texts versus images on X. On the other hand, we seek to determine the proportion of users sharing AI-generated texts versus images. The former analysis focuses on the number of tweets and images, while the latter emphasizes the activity distribution among users.

Overall, approximately 1.37% of the tweets related to the 2024 U.S. Election on X contain AI-generated text, whereas 12.33% of images are AI-generated. This indicates that AIgenerated images are nearly ten times more prevalent than AI-generated texts, highlighting the greater prominence of visual AI-generated content on X. Figure [1](#page-4-0) highlights the

<span id="page-3-1"></span><sup>4</sup>OpenAI BatchAPI: [https://platform.openai.com/docs/guides/](https://platform.openai.com/docs/guides/batch) [batch](https://platform.openai.com/docs/guides/batch)

<span id="page-4-0"></span>![](_page_4_Figure_0.jpeg)

Figure 1: Daily proportions of AI-generated texts and images relative to all tweets and images

<span id="page-4-1"></span>![](_page_4_Figure_2.jpeg)

Figure 2: Cumulative percentage of total AI-generated texts and images shared by text and image spreaders

daily proportions of AI-generated texts and images relative to all tweets and images, respectively. AI-generated images consistently show a significantly higher prevalence compared to AI-generated texts, with a daily mean of 12.25% for images and 1.06% for texts. The proportion of AI-generated texts remains stable over time with minimal fluctuations, while AI-generated images show greater variability, including notable spikes and dips, reflecting episodic surges in usage. This highlights the dominance of images over texts in prevalence and suggests more dynamic sharing behavior for images. Additionally, the low Pearson correlation coefficient (0.07, p = .55) indicates that the sharing activities of the two modalities are largely independent and follow distinct patterns. This finding is consistent with the earlier observation that the correlation between the AI scores of users who share both AI-generated images and texts is also relatively <span id="page-4-2"></span>Table 3: Number of users across four groups categorized by impact and modality

|        |                                    |               | Modality      |  |  |
|--------|------------------------------------|---------------|---------------|--|--|
|        |                                    | Text          | Image         |  |  |
| Impact | Superspreader<br>Non-Superspreader | 175<br>17,409 | 128<br>12,770 |  |  |

low. This further underscores the distinct patterns of AIgenerated content sharing within the two modality groups.

In terms of users sharing AIGC, among the 3,108,782 users who shared at least one tweet containing text, only 5.5% are AI text sharers. In contrast, 23.23% of the 379,025 users who shared at least one image are AI image sharers. The substantially higher proportion of users sharing AIgenerated images suggests that these visuals may have a broader appeal compared to AI-generated text.

We next compare the distributions of text sharers and image sharers. As shown in Figure [2,](#page-4-1) approximately 3% of text sharers are responsible for 80% of the total AI-generated texts shared, while 10% of image sharers account for 80% of the total AI-generated images shared. This suggests that AI-generated content is highly concentrated among a small subset of users in both modalities, with AI text production being significantly more concentrated than AI image production.

Summary. We present three key findings regarding the prevalence and concentration of AI-generated content. First, AI-generated images are significantly more prevalent than AI-generated texts, accounting for over 10% of all images. Additionally, more than 20% of users have shared at least one AI-generated image. Second, the sharing patterns of AI-generated text and images exhibit minimal correlation, suggesting distinct user sharing behaviors across modalities. Third, AIGC dissemination is highly concentrated: 3% of text sharers account for 80% of AI-generated texts, while 10% of image sharers contribute 80% of AI-generated images. Notably, AI text production is even more concentrated than AI image production.

### RQ2: AIGC Superspreader Characterization

Given the inequalities presented above, in this section, we identify and characterize superspreaders among users sharing AI-generated texts and images. Using the *h*-index, calculated for both text and image groups, we refer to the top 1% of the accounts with the highest *h*-index in each group as *superspreaders*. This yields 175 superspreaders in the text group and 128 in the image group.

We first look into the political leanings of these superspreaders and their account verification statuses. To further analyze their content sharing behavior, we categorize users into four groups: superspreaders of AI-generated images, non-superspreaders of AI-generated images, superspreaders of AI-generated texts, and non-superspreaders of AIgenerated texts, as detailed in Table [3.](#page-4-2) We evaluate their sharing behaviors using three metrics—AI score, bot score, and toxicity score—as outlined in the Methods section. Group-wise comparisons are conducted using the Mann-Whitney U test, with significant differences reported using *p*-values where applicable.

Political Affiliation. One way to characterize these AIGC superspreaders is by examining their political affiliation. To achieve this, two authors manually inspect the 303 accounts. The annotation process is conducted in two steps. First, the annotators assess whether the user's profile description indicates a clear political affiliation. Second, if an affiliation is present, they determine whether the user supports a specific political party. After the initial round of annotations, the two encoders agree on 289 accounts (95.4% of superspreaders, Krippendorff's α = 0.90). For the remaining disagreements, a third annotator is involved, and the disagreements are resolved through a majority vote among the three annotators.

Figure [3](#page-5-0) summarizes the annotation results. Among the 128 superspreaders in the image group, 88 have clear political affiliations, with 64 identified as right-leaning and 24 as left-leaning, while 40 users show no clear affiliation. In the text group, 10 of the 175 superspreaders have had their accounts suspended. Of the remaining 165 users, 151 exhibit clear political affiliations, with 77 identified as right-leaning and 74 as left-leaning.

Four key findings emerge from these results. First, superspreaders in the image group are predominantly rightleaning, with a significantly larger right-to-left ratio compared to the text group. Second, the text group shows a near balance between right- and left-leaning superspreaders, with a slight right-leaning majority. Third, a higher proportion of image superspreaders lack clear political affiliations compared to the text group. Lastly, account suspensions occur exclusively among AI text superspreaders.

Premium Account Status. Another perspective to describe superspreaders is to examine the proportion of X *Pre-*

<span id="page-5-0"></span>![](_page_5_Figure_8.jpeg)

Figure 3: Political affiliations of AIGC superspreaders

<span id="page-5-2"></span>![](_page_5_Figure_10.jpeg)

Figure 4: X Premium account proportion by modality and user impact. Proportions are calculated for each user group

*mium subscribers*[5](#page-5-1) . X Premium subscribers (users with a blue check mark) gain additional features, such as tweet editing, longer posts, enhanced analytics, and priority in replies and searches. As shown in Figure [4,](#page-5-2) superspreaders consistently have a higher proportion of premium accounts than non-superspreaders across both modalities. Note that proportions are calculated considering users within each group. Notably, superspreaders of AI-generated images have the highest proportion (82.8%) and non-superspreaders of AI-generated texts have the lowest (34.5%). This shows that premium accounts are more prevalent among superspreaders, particularly within the image group.

AI Score. The AI score represents the proportion of AIgenerated content shared by a user. Figure [5](#page-6-0) shows the AI scores for the four user groups. Results show that AIGC superspreaders have significantly higher AI scores than nonsuperspreaders in both the image and text groups (p < .001), indicating greater involvement in sharing AI-generated content. This outcome is expected but not guaranteed, as hyperactive accounts do not always garner high engagement

<span id="page-5-1"></span><sup>5</sup>X Premium: [https://help.x.com/en/managing-your-account/](https://help.x.com/en/managing-your-account/about-x-verified-accounts) [about-x-verified-accounts](https://help.x.com/en/managing-your-account/about-x-verified-accounts)

<span id="page-6-0"></span>![](_page_6_Figure_0.jpeg)

Figure 5: Comparison of AI scores across four user groups

[\(Luceri, Cardoso, and Giordano 2021\)](#page-8-27). AI image sharers consistently have higher AI scores than AI text sharers (p < .001), regardless of their impact, i.e., superspreader or nonsuperspreader users. AI scores in the image group exhibit a broader range, while scores for AI-text sharers are more concentrated between 0 and 0.2. The broader range of AI scores in the image group highlights greater variability in how users adopt and share AI-generated images, indicating a wider diversity of behaviors among AI image sharers. Moreover, the fat-tailed distribution among superspreaders, particularly in the image group, highlights the prevalence of a relevant fraction of users with high AI scores exhibiting extreme AIGCsharing behaviors.

Bot Score. Although the analysis using Botometer is limited to accounts created prior to the discontinuation of the Twitter API, a significant portion of our user base was successfully evaluated, indicating that the majority of the accounts analyzed are not newly created. In particular, we assess 99 (77.3%) superspreaders of AI-generated images, 10,415 (81.6%) non-superspreaders of AI-generated images, 146 (83.4%) superspreaders of AI-generated texts, and 14,871 (85.4%) non-superspreaders of AI-generated texts.

Figure [6](#page-6-1) presents the bot scores across these four user groups. First, we can observe that superspreaders are significantly more likely to exhibit automated behavior than nonsuperspreaders in both the image and text groups (p < .001). Second, non-superspreaders of images are more likely to be automated than non-superspreaders of texts (p < .001), while no significant difference is observed between superspreaders of AI-generated images and texts. Third, the distribution of superspreaders is distinctly bimodal, particularly within the image group, with prominent peaks around 0.2 and 0.6, suggesting a conspicuous number of automated accounts employed in the diffusion of AIGC, especially images. Note that a high bot score (e.g., larger than 0.5) is indicative of bot-like behavior [\(Luceri et al. 2019\)](#page-8-28).

<span id="page-6-1"></span>![](_page_6_Figure_5.jpeg)

Figure 6: Comparison of bot scores across four user groups

Language Toxicity. For toxicity detection, only Englishlanguage tweets are analyzed, leading to the exclusion of 171 non-superspreaders of AI-generated images and 61 nonsuperspreaders of AI-generated texts. A Mann-Whitney U test reveals a significant difference in toxicity scores solely between superspreaders and non-superspreaders of AIgenerated texts, with non-superspreaders exhibiting higher toxicity levels (p < .001). No significant differences are found in toxicity scores across the other comparisons.

Summary. Among the AIGC superspreaders, a higher proportion of AI-image superspreaders align with conservative views, while AI-text superspreaders show a more balanced political distribution. Premium accounts are more prevalent among superspreaders, especially in the image group. When characterizing the behaviors of AIGC superspreaders, three key findings emerge: (i) AIGC superspreaders tend to share a higher proportion of AI-generated content in their profiles compared to non-superspreaders; (ii) AI image sharers demonstrate greater heterogeneity in their sharing activity of AIGC, with a significant portion of users exhibiting extreme sharing behaviors; (iii) AIGC superspreaders are more bot-like compared to non-superspreaders.

# Discussion and Conclusions

This study, grounded in the analysis of over 24.7 million tweets and 2.5 million images, investigates the prevalence and concentration of AI-generated content (AIGC) and provides an in-depth characterization of AIGC superspreaders. Four key insights emerge from our research.

Finding 1: AI-generated images are more prevalent than AI-generated texts. Our analysis reveals that AIgenerated images are ten times more prevalent than texts in the 2024 U.S. Election discussion on X. Influencers may prefer visual content due to its ability to convey complex messages instantly and evoke stronger emotional responses [\(Li and Xie 2020\)](#page-8-13). Furthermore, generative AI has significantly lowered the barriers to creating and sharing customized content, such as political memes [\(Chang et al.](#page-8-11) [2024\)](#page-8-11). While this trend highlights the increasing presence of AIGC across two modalities on social media, it also raises concerns about its potential harm. Research demonstrated that humans often struggle to distinguish AI-generated images from authentic ones [\(Lu et al. 2024\)](#page-8-29), creating opportunities for misuse and weaponization of generative AI technologies, such as attacking political figures [\(Chang et al.](#page-8-11) [2024\)](#page-8-11), fabricating events, and disseminating misinformation [\(Kreps, McCain, and Brundage 2022\)](#page-8-2). These risks underscore the need for robust detection mechanisms and informed strategies to mitigate the negative impacts of AIgenerated images and safeguard democratic processes.

Finding 2: AIGC sharing is highly unequal and concentrated. The sharing of AIGC is highly skewed, with approximately 3% of text sharers responsible for 80% of all AI-generated texts and 10% of image sharers accounting for 80% of all AI-generated images. This highlights the outsized role of a small group of users in generating AIGC on social media. Interestingly, similar patterns have been observed in misinformation sharing, where a small fraction of users disproportionately spreads low-credibility content [\(Grinberg et al. 2019;](#page-8-15) [Baribi-Bartov, Swire-Thompson, and](#page-7-0) [Grinberg 2024;](#page-7-0) [DeVerna et al. 2024;](#page-8-10) [Nogara et al. 2022\)](#page-9-4). For instance, [Grinberg et al.](#page-8-15) [\(2019\)](#page-8-15) shows that 0.1% of users share roughly 80% of fake news. However, AIGC sharing appears less skewed, suggesting a broader base of users is involved in its dissemination, potentially exposing a larger audience to such content. These findings highlight the need for further research into the dynamics of AIGC diffusion to gain deeper insights into its reach, engagement, and influence relative to misinformation.

Finding 3: AIGC superspreaders are more likely to be bots. Our results show that AIGC superspreaders have significantly higher bot scores compared to nonsuperspreaders, indicating that they are more likely to be automated accounts. This aligns with prior studies showing that bots play a crucial role in diffusing low-credibility content [\(Shao et al. 2018\)](#page-9-14), highlighting similarities between the dissemination patterns of AIGC and misinformation. However, an intriguing discrepancy also emerges. Unlike [Shao](#page-9-14) [et al.](#page-9-14) [\(2018\)](#page-9-14), which reported a single peak in the bot score distribution around 0.2, we discover that the distribution of bot scores among AIGC superspreaders is distinctly bimodal, particularly for image content, with two peaks at around 0.2 and 0.6. This might surface the potential coordinated activity of a subset of bots specifically targeting the dissemination of visual AIGC. Therefore, future research could look into the strategies, motivations, and coordinated actions underlying bot-driven AIGC diffusion.

Finding 4: AI image sharers exhibit greater heterogeneity in AIGC sharing. Our analysis reveals a broader range of AI scores among users sharing AI-generated images, suggesting greater variability in how users adopt and share this content. The diversity of AI scores reflects a wider spectrum of behaviors among AI image sharers, ranging from sporadic use to highly frequent generation of AI-generated visuals. Notably, the distribution of AI scores among AI image sharers, particularly superspreaders, displays a "fat tail," indicating the presence of a small yet substantial fraction of users who engage in extreme sharing behaviors. These outliers play a disproportionate role in sharing and driving engagement with their AI-generated images on social media, compared to the majority of users who share AIGC at a more moderate pace.

Limitations and Future Work We acknowledge several limitations in our research, which will guide future research. First, our data collection spans only three months, from July to September, excluding the critical period of Election Day. Future studies could incorporate Election Day into the analysis to explore variations in user behavior before and after this pivotal event. Second, while this study focuses on analyzing individual superspreaders, future research could investigate the collective behaviors of coordinated superspreaders, offering a broader understanding of their networked activity and impact on vulnerable population. Third, our research centers on characterizing superspreaders of AIGC. Expanding this scope to examine the effects on audiences, such as user engagement, emotional arousal, or behavioral changes, would be a promising direction. Lastly, this study is situated within the political context of X; future research could extend and compare these findings to other platforms and different high-stakes scenarios, such as public health crises or misinformation campaigns.

Ethical Statement This study investigates the behaviors of superspreaders of AI-generated content on social media, aiming to inform platform governance and public awareness. To mitigate potential societal harms, we anonymized all user data and avoided releasing granular user-level findings that could be exploited to enhance malicious content dissemination. We acknowledge the potential misuse of our findings, such as stigmatizing specific users or enabling more effective content manipulation, and have taken steps to ensure responsible data handling, access control, and reproducibility while adhering to ethical research standards.

# References

<span id="page-7-2"></span>Balasubramanian, A.; Zou, V.; Narayana, H.; You, C.; Luceri, L.; and Ferrara, E. 2024. A public dataset tracking social media discourse about the 2024 US Presidential Election on Twitter/X. *arXiv preprint arXiv:2411.00376*.

<span id="page-7-1"></span>Barari, S.; Lucas, C.; Munger, K.; et al. 2021. Political deepfake videos misinform the public, but no more than other fake media. *OSF Preprints*, 13.

<span id="page-7-0"></span>Baribi-Bartov, S.; Swire-Thompson, B.; and Grinberg, N. 2024. Supersharers of fake news on Twitter. *Science*, 384(6699): 979–982.

<span id="page-7-4"></span>Bhattacharjee, A.; and Liu, H. 2024. Fighting fire with fire: Can ChatGPT detect AI-generated text? *ACM SIGKDD Explorations Newsletter*, 25(2): 14–21.

<span id="page-7-3"></span>Borji, A. 2022. How good are deep models in understanding the generated images? *arXiv preprint arXiv:2208.10760*.

<span id="page-8-20"></span>Brin, D.; Sorin, V.; Barash, Y.; Konen, E.; Glicksberg, B. S.; Nadkarni, G. N.; and Klang, E. 2024. Assessing GPT-4 multimodal performance in radiological image analysis. *European Radiology*, 1–7.

<span id="page-8-9"></span>Campbell, C.; Plangger, K.; Sands, S.; and Kietzmann, J. 2022. Preparing for an era of deepfakes and AI-generated ads: A framework for understanding responses to manipulated advertising. *Journal of Advertising*, 51(1): 22–38.

<span id="page-8-18"></span>Cao, Y.; Li, S.; Liu, Y.; Yan, Z.; Dai, Y.; Yu, P. S.; and Sun, L. 2023. A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt. *arXiv preprint arXiv:2303.04226*.

<span id="page-8-11"></span>Chang, H.; Shaman, B.; Chen, Y.-C.; Zha, M.; Noh, S.; Wei, C.; Weener, T.; and Magee, M. 2024. Generative memesis: AI mediates political information in the 2024 United States Presidential Election. *Available at SSRN*.

<span id="page-8-22"></span>Chen, E.; Deb, A.; and Ferrara, E. 2022. # Election2020: the first public Twitter dataset on the 2020 US Presidential Election. *Journal of Computational Social Science*, 1–18.

<span id="page-8-19"></span>Chen, R.; Xiong, T.; Wu, Y.; Liu, G.; Hu, Z.; Chen, L.; Chen, Y.; Liu, C.; and Huang, H. 2023. GPT-4 Vision on medical image classification–A case study on COVID-19 dataset. *arXiv preprint arXiv:2310.18498*.

<span id="page-8-12"></span>Cinus, F.; Minici, M.; Luceri, L.; and Ferrara, E. 2025. Exposing cross-platform coordinated inauthentic activity in the run-up to the 2024 US Election. *The Web Conference*.

<span id="page-8-10"></span>DeVerna, M. R.; Aiyappa, R.; Pacheco, D.; Bryden, J.; and Menczer, F. 2024. Identifying and characterizing superspreaders of low-credibility content on Twitter. *Plos one*, 19(5): e0302201.

<span id="page-8-21"></span>Dmonte, A.; Zampieri, M.; Lybarger, K.; Albanese, M.; and Coulter, G. 2024. Classifying human-generated and AIgenerated election claims in social media. *arXiv preprint arXiv:2404.16116*.

<span id="page-8-8"></span>Du, D.; Zhang, Y.; and Ge, J. 2023. Effect of AI generated content advertising on consumer engagement. In *International Conference on Human-Computer Interaction*, 121– 129. Springer.

<span id="page-8-23"></span>Epstein, D. C.; Jain, I.; Wang, O.; and Zhang, R. 2023a. Online detection of ai-generated images. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, 382–392.

<span id="page-8-6"></span>Epstein, Z.; Hertzmann, A.; of Human Creativity, I.; Akten, M.; Farid, H.; Fjeld, J.; Frank, M. R.; Groh, M.; Herman, L.; Leach, N.; et al. 2023b. Art and the science of generative AI. *Science*, 380(6650): 1110–1111.

<span id="page-8-3"></span>Ferrara, E. 2024a. GenAI against humanity: Nefarious applications of generative artificial intelligence and large language models. *Journal of Computational Social Science*, 1–21.

<span id="page-8-1"></span>Ferrara, E. 2024b. What are the risks of living in a GenAI synthetic reality? The generative AI paradox. *arXiv preprint arXiv:2411.08250*.

<span id="page-8-25"></span>Ferrara, E.; Varol, O.; Davis, C.; Menczer, F.; and Flammini, A. 2016. The rise of social bots. *Communications of the ACM*, 59(7): 96–104.

<span id="page-8-15"></span>Grinberg, N.; Joseph, K.; Friedland, L.; Swire-Thompson, B.; and Lazer, D. 2019. Fake news on Twitter during the 2016 US presidential election. *Science*, 363(6425): 374– 378.

<span id="page-8-17"></span>Guess, A.; Nagler, J.; and Tucker, J. 2019. Less than you think: Prevalence and predictors of fake news dissemination on Facebook. *Science advances*, 5(1): eaau4586.

<span id="page-8-16"></span>Guess, A.; Nyhan, B.; and Reifler, J. 2018. Selective exposure to misinformation: Evidence from the consumption of fake news during the 2016 US presidential campaign. *European Research Council*, 9(3): 4.

<span id="page-8-24"></span>Hirsch, J. E. 2005. An index to quantify an individual's scientific research output. *Proceedings of the National academy of Sciences*, 102(46): 16569–16572.

<span id="page-8-7"></span>Jakesch, M.; French, M.; Ma, X.; Hancock, J. T.; and Naaman, M. 2019. AI-mediated communication: How the perception that profile text was written by AI affects trustworthiness. In *Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems*, 1–13.

<span id="page-8-0"></span>Jhaver, S.; Birman, I.; Gilbert, E.; and Bruckman, A. 2019. Human-machine collaboration for content regulation: The case of reddit automoderator. *ACM Transactions on Computer-Human Interaction (TOCHI)*, 26(5): 1–35.

<span id="page-8-14"></span>Kirkpatrick, E. A. 1894. An experimental study of memory. *Psychological Review*, 1(6): 602.

<span id="page-8-2"></span>Kreps, S.; McCain, R. M.; and Brundage, M. 2022. All the news that's fit to fabricate: AI-generated text as a tool of media misinformation. *Journal of experimental political science*, 9(1): 104–117.

<span id="page-8-4"></span>Lee, E.-J. 2020. Authenticity model of (mass-oriented) computer-mediated communication: Conceptual explorations and testable propositions. *Journal of Computer-Mediated Communication*, 25(1): 60–73.

<span id="page-8-26"></span>Lees, A.; Tran, V. Q.; Tay, Y.; Sorensen, J.; Gupta, J.; Metzler, D.; and Vasserman, L. 2022. A new generation of perspective api: Efficient multilingual character-level transformers. In *Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining*, 3197–3207.

<span id="page-8-13"></span>Li, Y.; and Xie, Y. 2020. Is a picture worth a thousand words? An empirical study of image content and social media engagement. *Journal of marketing research*, 57(1): 1–19.

<span id="page-8-5"></span>Liang, P. P.; Wu, C.; Morency, L.-P.; and Salakhutdinov, R. 2021. Towards understanding and mitigating social biases in language models. In *International Conference on Machine Learning*, 6565–6576. PMLR.

<span id="page-8-29"></span>Lu, Z.; Huang, D.; Bai, L.; Qu, J.; Wu, C.; Liu, X.; and Ouyang, W. 2024. Seeing is not always believing: benchmarking human and model perception of AI-generated images. *Advances in Neural Information Processing Systems*, 36.

<span id="page-8-27"></span>Luceri, L.; Cardoso, F.; and Giordano, S. 2021. Down the bot hole: Actionable insights from a one-year analysis of bot activity on Twitter. *First Monday*.

<span id="page-8-28"></span>Luceri, L.; Deb, A.; Giordano, S.; and Ferrara, E. 2019. Evolution of bot and human behavior during elections. *First Monday*.

<span id="page-9-2"></span>Luceri, L.; Pante, V.; Burghardt, K.; and Ferrara, E. 2024. ` Unmasking the web of deceit: Uncovering coordinated activity to expose information operations on twitter. In *Proceedings of the ACM on Web Conference 2024*, 2530–2541.

<span id="page-9-1"></span>Minici, M.; Luceri, L.; Cinus, F.; and Ferrara, E. 2024. Uncovering coordinated cross-platform information operations threatening the integrity of the 2024 US presidential election online discussion. *First Monday*.

<span id="page-9-4"></span>Nogara, G.; Vishnuprasad, P. S.; Cardoso, F.; Ayoub, O.; Giordano, S.; and Luceri, L. 2022. The disinformation dozen: An exploratory analysis of covid-19 disinformation proliferation on twitter. In *Proceedings of the 14th ACM web science conference 2022*, 348–358.

<span id="page-9-10"></span>Pei, S.; Muchnik, L.; Andrade, J. S., Jr; Zheng, Z.; and Makse, H. A. 2014. Searching for superspreaders of information in real-world social media. *Scientific reports*, 4(1): 5547.

<span id="page-9-8"></span>Ricker, J.; Assenmacher, D.; Holz, T.; Fischer, A.; and Quiring, E. 2024. AI-generated faces in the real world: a largescale case study of twitter profile images. In *Proceedings of the 27th International Symposium on Research in Attacks, Intrusions and Defenses*, 513–530.

<span id="page-9-12"></span>Schuhmann, C.; Vencu, R.; Beaumont, R.; Kaczmarczyk, R.; Mullis, C.; Katta, A.; Coombes, T.; Jitsev, J.; and Komatsuzaki, A. 2021. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. *arXiv preprint arXiv:2111.02114*.

<span id="page-9-14"></span>Shao, C.; Ciampaglia, G. L.; Varol, O.; Yang, K.-C.; Flammini, A.; and Menczer, F. 2018. The spread of lowcredibility content by social bots. *Nature communications*, 9(1): 1–9.

<span id="page-9-3"></span>Shaw, A. 2023. Social media, extremism, and radicalization. *Science Advances*, 9(35): eadk2031.

<span id="page-9-0"></span>Sundar, S. S.; and Lee, E.-J. 2022. Rethinking communication in the era of artificial intelligence. *Human Communication Research*, 48(3): 379–385.

<span id="page-9-9"></span>Sundar, S. S.; Molina, M. D.; and Cho, E. 2021. Seeing is believing: Is video modality more powerful in spreading fake news via online messaging apps? *Journal of Computer-Mediated Communication*, 26(6): 301–319.

<span id="page-9-7"></span>Wei, Y.; and Tyson, G. 2024. Understanding the Impact of AI-Generated Content on Social Media: The Pixiv Case. In *Proceedings of the 32nd ACM International Conference on Multimedia*, 6813–6822.

<span id="page-9-5"></span>Yan, L.; Greiff, S.; Teuber, Z.; and Gasevi ˇ c, D. 2024. ´ Promises and challenges of generative artificial intelligence for human learning. *Nature Human Behaviour*, 8(10): 1839– 1850.

<span id="page-9-6"></span>Yang, K.-C.; Singh, D.; and Menczer, F. 2024. Characteristics and prevalence of fake social media profiles with AIgenerated faces. *arXiv preprint arXiv:2401.02627*.

<span id="page-9-13"></span>Yang, K.-C.; Varol, O.; Hui, P.-M.; and Menczer, F. 2020. Scalable and generalizable social bot detection through data selection. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 34, 1096–1103.

<span id="page-9-11"></span>Ye, J.; Luceri, L.; and Ferrara, E. 2024. Auditing political exposure bias: Algorithmic amplification on Twitter/X approaching the 2024 US Presidential Election. *arXiv preprint arXiv:2411.01852*.