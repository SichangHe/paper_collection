# MultiSocial: Multilingual Benchmark of Machine-Generated Text Detection of Social-Media Texts

Dominik Macko, Jakub Kopal, Robert Moro, Ivan Srba

Kempelen Institute of Intelligent Technologies, Slovakia {dominik.macko, jakub.kopal, robert.moro, ivan.srba}@kinit.sk

# Abstract

Recent LLMs are able to generate high-quality multilingual texts, indistinguishable for humans from authentic human-written ones. Research in machine-generated text detection is however mostly focused on the English language and longer texts, such as news articles, scientific papers or student essays. Socialmedia texts are usually much shorter and often feature informal language, grammatical errors, or distinct linguistic items (e.g., emoticons, hashtags). There is a gap in studying the ability of existing methods in detection of such texts, reflected also in the lack of existing multilingual benchmark datasets. To fill this gap we propose the first multilingual (22 languages) and multi-platform (5 social media platforms) dataset for benchmarking machinegenerated text detection in the social-media domain, called MultiSocial[1](#page-0-0) . It contains 472,097 texts, of which about 58k are human-written and approximately the same amount is generated by each of 7 multilingual LLMs. We use this benchmark to compare existing detection methods in zero-shot as well as fine-tuned form. Our results indicate that the fine-tuned detectors have no problem to be trained on socialmedia texts and that the platform selection for training matters.

# 1 Introduction

The most advanced text-generation AI models, called large language models (LLMs), are able to generate high-quality texts in various languages [\(Qin et al.,](#page-11-0) [2024\)](#page-11-0). Although this presents an opportunity to make a human life and work more efficient, it also presents a threat of being misused, as such generated texts are not easily recognisable by humans [\(Zellers et al.,](#page-12-0) [2019\)](#page-12-0). This is especially crucial in regard to social-media networks (SMN, such as Facebook, X/Twitter, etc.), where anyone

<span id="page-0-0"></span>![](_page_0_Figure_7.jpeg)

<span id="page-0-1"></span>![](_page_0_Figure_8.jpeg)

Figure 1: MultiSocial coverage of languages.

can be a source of a harmful content (without editorial consent) with a potentially wide reach (depending on a network) [\(Aïmeur et al.,](#page-10-0) [2023\)](#page-10-0). To prevent the LLM misuse (e.g., social engineering, disinformation spreading), a reliable mechanism to detect machine-generated text (MGT) is needed.

Unfortunately, the existing research in MGT detection (MGTD) focuses either solely on English (as in most NLP fields, e.g., [Dugan et al.,](#page-10-1) [2024\)](#page-10-1) or leaves SMN texts out of scope due to challenges they bring [\(Kumarage et al.,](#page-11-1) [2024;](#page-11-1) [Lin et al.,](#page-11-2) [2024\)](#page-11-2). These challenges include very informal writing style often used in SMN, such as using slang, ignoring grammar rules, using distinct linguistic items in the texts (e.g., emoticons or hashtags), or usually including very short lengths of the texts.

Our work fills a gap in the state-of-the-art (SOTA) by introducing a new heavily multilingual dataset for MGTD research in the social-media domain. We use this dataset to further benchmark the existing SOTA detectors in various aspects. Specifically, our key contributions include:

(1) The first multilingual evaluation of SOTA MGTD methods (statistical, pre-trained, finetuned) on social-media texts, focusing on multilingual as well as cross-lingual capability of existing detectors and comparison of different categories. The best detectors perform similarly across all tested languages, although there are still differences between English and non-English in zeroshot evaluation.

(2) The first multi-platform and crossplatform evaluation of SOTA MGTD methods in social-media domain, evaluating differences in MGTD performance based on text types and sources in multiple languages. We found that Telegram offers the best cross-lingual capability.

(3) The unique multilingual, multi-platform, and multi-generator benchmark dataset of human-written and machine-generated socialmedia texts, called MultiSocial, covering 22 languages, 5 SMN platforms, and 7 LLM generators.

# 2 Related Work

Multilingual machine-generated text detection has been gaining attention recently. There have been multiple non-English language MGT detection shared tasks in the last years, such as Russian at RuATD 2022 (Shamardina et al., 2022), Spanish at AuTexTification 2023 (Sarvazyan et al., 2023), Dutch at CLIN33 (Fivez et al., 2024), or multilingual at SemEval-2024 Task 8 (Wang et al., 2024b).

The last one covers 9 languages, and is based on M4GT-Bench (Wang et al., 2024a), multilingual, multi-domain, and multi-generator MGT benchmark. However, its coverage of the SMN domain is rather sparse (solely English-only Reddit texts are included). Moreover, language coverage of the domains is highly imbalanced (e.g., most languages are represented only in the news domain, while Arabic and German also in Wikipedia domain, and Chinese only in the QA domain) and there are only one or two generators used in the multilingual settings. Therefore, cross-lingual evaluation is somewhat limited using such data.

Another benchmark dataset called MULTITuDE (Macko et al., 2023) covers 11 languages; however, 8 of them are included in the test split only with a limited number of samples. Moreover, it is focused on the news domain only, in which the texts are typically long, formal, and carefully checked for grammatical correctness. Such settings are clearly different than those of social media texts. An extension of MULTITuDE dataset to evaluate various authorship obfuscation methods is proposed in (Macko et al., 2024). Although it enables to evaluate robustness of MGT detection methods, it is still limited to news domain.

<span id="page-1-0"></span>

| Dataset              | H/M       | LLM | Lang | Domain | 1 SMN |
|----------------------|-----------|-----|------|--------|-------|
| TweepFake (Fagni     | 12k/12k   | 6   | 1    | SMN    | 1     |
| et al., 2021)        |           |     |      |        |       |
| Fox8-23 (Yang and    | 228k/170k | 1   | 1    | SMN    | 1     |
| Menczer, 2023)       |           |     |      |        |       |
| F3 (Lucas et al.,    | 28k/28k   | 1   | 1    | news,  | 1     |
| 2023)                |           |     |      | SMN    |       |
| HC3 (Guo et al.,     | 81k/44k   | 1   | 2    | QA,    | 0     |
| 2023)                |           |     |      | Wiki   |       |
| SAID (Cui et al.,    | 87k/131k  | N/A | 2    | QA     | 0     |
| 2023)                |           |     |      |        |       |
| MULTITuDE            | 8k/66k    | 8   | 11   | news   | 0     |
| (Macko et al., 2023) |           |     |      |        |       |
| M4GT-Bench           | 93k/124k  | 8   | 9    | 6      | 0     |
| (Wang et al., 2024a) |           |     |      |        |       |
| MultiSocial (ours)   | 58k/414k  | 7   | 22   | SMN    | 5     |

Table 1: Comparison of existing publicly available MGT detection datasets either multilingual or focused on social-media network (SMN) domain. H/M refers to the no. of human-written and machine-generated samples.

There is also MAiDE-up dataset (Ignat et al., 2024) of hotel reviews available, covering 10 languages; however, it is limited to GPT-4 generated data only (limiting the generalization of conclusions). Although such texts are more similar to social-media texts than news articles, they cover a single topic (accommodation) and still use a different communication style than the social-media networks. Many other works have focused on detection of fake reviews, but only few of them reflected multilingualism (Duma et al., 2024).

Benchmark datasets HC3 (Guo et al., 2023) and SAID (Cui et al., 2023) contain Chinese and English texts covering forum-like question-answering domain. HC3 contains only ChatGPT-generated machine texts, SAID contains real bot-generated texts (relying on human annotations to identify them). The downside of SAID is that the specific generation model of individual texts is not known. Otherwise, social-media texts are covered only in rather old TweepFake (Fagni et al., 2021) dataset, which includes English-only tweets and cannot be used to evaluate MGT detection methods on social-media texts generated by the most modern LLMs. Fox8-23 (Yang and Menczer, 2023) dataset also focuses on English, and covers presumably only ChatGPT-generated tweets. Similarly, F3 (Lucas et al., 2023) dataset contains English ChatGPT-generated real and fake news as well as tweets. There are other monolingual works, such as (Temnikova et al., 2023) focused on fake tweets in Bulgarian; however, the crafted dataset is not publicly available.

Table 1 includes comparison of the new dataset proposed in this work (described in the next sec-

<span id="page-2-0"></span>

|                      |         |       | Т        | rain    |          |        |         |       | 1        | Fest    |          |        |
|----------------------|---------|-------|----------|---------|----------|--------|---------|-------|----------|---------|----------|--------|
| Language             | Discord | Gab   | Telegram | Twitter | WhatsApp | all    | Discord | Gab   | Telegram | Twitter | WhatsApp | all    |
| Arabic (ar)          | 0       | 0     | 7724     | 7872    | 0        | 15596  | 0       | 1556  | 2319     | 2364    | 1750     | 7989   |
| Bulgarian (bg)       | 0       | 0     | 7555     | 7930    | 0        | 15485  | 0       | 192   | 2334     | 2367    | 0        | 4893   |
| Catalan (ca)         | 6984    | 0     | 7157     | 0       | 0        | 14141  | 2105    | 1264  | 2134     | 1824    | 144      | 7471   |
| Chinese (zh)         | 0       | 0     | 7924     | 0       | 0        | 7924   | 0       | 830   | 2380     | 238     | 32       | 3480   |
| Croatian (hr)        | 7502    | 0     | 7065     | 0       | 0        | 14567  | 2255    | 1340  | 2315     | 91      | 38       | 6039   |
| Czech (cs)           | 3450    | 0     | 7690     | 0       | 0        | 11140  | 2175    | 492   | 2309     | 1017    | 134      | 6127   |
| Dutch (nl)           | 7391    | 7900  | 7750     | 7933    | 0        | 30974  | 2191    | 2356  | 2318     | 2369    | 397      | 9631   |
| English (en)         | 7789    | 7760  | 7824     | 7871    | 7782     | 39026  | 2341    | 2340  | 2350     | 2365    | 2334     | 11730  |
| Estonian (et)        | 6974    | 0     | 7520     | 0       | 0        | 14494  | 2071    | 805   | 2259     | 164     | 120      | 5419   |
| German (de)          | 3407    | 7863  | 7880     | 2095    | 0        | 21245  | 2232    | 2366  | 2356     | 2345    | 304      | 9603   |
| Greek (el)           | 0       | 0     | 3814     | 0       | 0        | 3814   | 0       | 1195  | 2274     | 146     | 35       | 3650   |
| Hungarian (hu)       | 7079    | 0     | 7461     | 0       | 0        | 14540  | 2094    | 1211  | 2228     | 413     | 22       | 5968   |
| Irish (ga)           | 0       | 0     | 0        | 0       | 0        | 0      | 1319    | 968   | 821      | 45      | 0        | 3153   |
| Polish (pl)          | 7158    | 1829  | 7733     | 0       | 0        | 16720  | 2136    | 2311  | 2310     | 172     | 62       | 6991   |
| Portuguese (pt)      | 6860    | 7916  | 7842     | 6481    | 4354     | 33453  | 2284    | 2371  | 2347     | 2360    | 2363     | 11725  |
| Romanian (ro)        | 7436    | 851   | 6792     | 64      | 0        | 15143  | 2236    | 2349  | 2298     | 2378    | 132      | 9393   |
| Russian (ru)         | 0       | 7875  | 7827     | 362     | 0        | 16064  | 0       | 2355  | 2340     | 2361    | 960      | 8016   |
| Scottish Gaelic (gd) | 0       | 0     | 0        | 0       | 0        | 0      | 150     | 35    | 34       | 0       | 0        | 219    |
| Slovak (sk)          | 0       | 0     | 0        | 0       | 0        | 0      | 107     | 308   | 1508     | 110     | 0        | 2033   |
| Slovenian (sl)       | 0       | 0     | 0        | 0       | 0        | 0      | 203     | 1912  | 917      | 40      | 0        | 3072   |
| Spanish (es)         | 7588    | 7883  | 7884     | 7922    | 7804     | 39081  | 2268    | 2361  | 2354     | 2376    | 2341     | 11700  |
| Ukrainian (uk)       | 0       | 0     | 7802     | 0       | 0        | 7802   | 0       | 174   | 2342     | 70      | 0        | 2586   |
| Total                | 79618   | 49877 | 133244   | 48530   | 19940    | 331209 | 28167   | 31091 | 44847    | 25615   | 11168    | 140888 |

Table 2: MultiSocial text sample counts across languages and platforms for train and test split.

tion) with the selected existing publicly available datasets for MGT detection.

# <span id="page-2-1"></span>3 Dataset

Since there is no dataset of multilingual SMN texts containing human-written texts along with the texts generated by SOTA text-generation LLMs, we have crafted a new MultiSocial benchmark dataset. It contains human-written data from five different social-media platforms (reused from the existing multilingual SMN datasets), namely Telegram, Twitter (X), Gab, Discord, and WhatsApp, including post-like as well as chat-like texts. For each authentic human-written text, the texts generated by 7 SOTA LLMs (representatives of private and open multilingual LLMs of various sizes and architectures) are included by using three iterations of paraphrasing. Other text-generation approaches were also considered, outputs of which were evaluated and compared by humans, automated similarity metrics, and meta-evaluation utilizing LLM judges. The final approach was selected based on sufficient output quality and similarity to the human texts (to avoid detection biases). Details regarding dataset construction (including selection, evaluation, pre-processing, generation, and postprocessing of texts) are provided in Appendix B.

Dataset consists of 472,097 texts (58k are human-written) split into train and test subsets, of which sample counts are summarized across languages and platforms in Table 2. We have conducted linguistic analysis of the machine-generated texts along with the similarity comparison to human texts, with a manual human check of a balanced subset, and identified small portion (about 1%) of noise in the generated data (e.g., "As an AI model..."), indicating model failure during generation. We have intentionally left such samples in the dataset (clearly marked in the data) for further analysis purpose (as indicated in Appendix B). We however filter-out the identified noise for the purpose of the experiments in this study. Although such a post-processing cannot guarantee 100% removal of noisy data, the obvious failures are cleaned. Furthermore, we have used meta-evaluation utilizing LLM judges to compare the quality of the generated texts of individual generators to the quality of original human texts, indicating that machinegenerated texts are of similar or higher quality (see Appendix B)).

Language Selection. The MultiSocial benchmark dataset covers 22 (high- and low-resource) languages (some only in the testing split), 20 of which (18 of Indo-European and 2 of Uralic language families) have been selected based on our research-projects needs. However, we have intentionally included 2 more (Arabic of Semitic and Chinese of Sino-Tibetan family), which are completely linguistically and geographically unrelated, to study cross-lingual characteristics (Figure 1). The dataset is strongly focused on the Indo-European language family, but contains 4 language families in total. Test split includes all of the train languages, but also 2 Celtic languages,

<span id="page-3-0"></span>

| Generator                | METEOR ↑      | <b>BERTScore ↑</b> | ngram ↑       | $LD\downarrow$  | $\mathbf{MAUVE} \downarrow$ | LangCheck↓ |
|--------------------------|---------------|--------------------|---------------|-----------------|-----------------------------|------------|
| Aya-101                  | 0.195 (±0.23) | 0.675 (±0.14)      | 0.156 (±0.18) | 1.104 (±0.91)   | 0.063                       | 10.59%     |
| Gemini                   | 0.152 (±0.20) | 0.621 (±0.10)      | 0.087 (±0.17) | 16.296 (±33.39) | 0.025                       | 6.16%      |
| GPT-3.5-Turbo-0125       | 0.143 (±0.18) | 0.664 (±0.10)      | 0.080 (±0.10) | 2.359 (±3.82)   | 0.076                       | 22.80%     |
| Mistral-7B-Instruct-v0.2 | 0.152 (±0.16) | 0.652 (±0.08)      | 0.088 (±0.09) | 2.383 (±1.93)   | 0.047                       | 10.61%     |
| OPT-IML-Max-30b          | 0.127 (±0.19) | 0.659 (±0.12)      | 0.105 (±0.14) | 0.998 (±0.57)   | 0.108                       | 14.88%     |
| v5-Eagle-7B-HF           | 0.108 (±0.15) | 0.628 (±0.07)      | 0.071 (±0.10) | 2.568 (±2.10)   | 0.027                       | 6.01%      |
| Vicuna-13b               | 0.133 (±0.17) | 0.650 (±0.09)      | 0.089 (±0.11) | 1.811 (±1.40)   | 0.042                       | 6.26%      |

Table 3: Similarity analysis between machine-generated (3-iteration paraphrased) and human-written (original) social-media texts for individual generation models [mean  $(\pm \text{ std})$ ]. Arrows refer to values representing more similar texts, boldfaced values represent the most similar texts for each metric.

1 more South Slavic and 1 more West Slavic language. There are 5 writing scripts in both train and test splits of the dataset, where majority of languages uses Latin, but there is also Cyrillic (Russian, Ukrainian, Bulgarian), Arabic, Hanzi (Chinese), and Greek. Nice feature is that the train split includes at least 3 representatives of Germanic, Romance, Slavic-Latin, and Slavic-Cyrillic, which enables various combinations of studies regarding multilingual and cross-lingual characteristics of machine-generated text detection. Furthermore, in Slavic and Romance languages, there are included at least 2 representatives of languages that can be considered high-resource and low-resource.

Although not all languages have enough samples from each of five platforms (due to unavailability of human samples in the selected source datasets), specific subsets can be used to study specific characteristics (e.g., cross-platform transferability using English and Spanish languages, cross-lingual transferability using Telegram platform, surprise language and/or surprise platform evaluation not using specific portions of the train data, etc.).

Human-Machine Similarity Analysis. As mentioned, we have conducted a similarity analysis of the final generated texts by the selected generation models (reported in Table 3). Definitions of the used metrics are available in Appendix B. We can observe only small differences between the generators. Aya-101 generated the most similar texts in general, while OPT-IML-Max-30B provided the worst results (based on higher MAUVE and LangCheck scores). ChatGPT (GPT-3.5-Turbo) also generated the texts resulting in a little higher MAUVE score and the highest language mismatch (LangCheck). However, the language mismatch of ChatGPT (using all the selected languages) was not confirmed by longer news articles generation (FastText language detection in such texts is definitely more accurate), where it actually achieved

the lowest (under 1%) LangCheck score among the generators. We assume that in social-media text generation it can better follow the original style of the text than the other generators (e.g., grammatically incorrect, slang), which is more difficult for accurate language detection.

### <span id="page-3-1"></span>**4** Detection Methods

For the benchmark purpose, we have covered 3 specific categories of detection methods: *statistical zero-shot* (methods using statistical differences to differentiate human-written and machine-generated texts applicable without training), *pre-trained* (directly applicable models that are fine-tuned for the MGT detection task using different data – i.e., out-of-distribution), and *fine-tuned* (foundation models fine-tuned for the MGT detection task using Multi-Social dataset – i.e., in-distribution).

For statistical category, we have selected the following 5 most-promising methods (excluding perturbation-based and multi-generation methods due to high computing costs): **Binoculars** (Hans et al., 2024) with Falcon-7B (Almazrouei et al., 2023) as an observer model and Falcon-7B-Instruct as a performer model, **Fast-DetectGPT** (Bao et al., 2023) with GPT-J-6B (Wang and Komatsuzaki, 2021) as both the reference and sampling models, **LLM-Deviation** (Wu and Xiang, 2023), **DetectLLM-LRR** (Su et al., 2023), **S5** (Spiegel and Macko, 2024b) (multiplying 5 statistical metrics of Likelihood, Entropy, Rank, Log-Rank, and LLM-Deviation), all three of them using GPT-J-6B as a base model (the same as in Fast-DetectGPT).

For pre-trained category, we have selected the following 5 detectors with a multilingual potential: **ChatGPT-detector-RoBERTa-Chinese** (Guo et al., 2023) (with a Chinese fine-tuned model), **Longformer Detector** (Li et al., 2023) (showing decent multilingual potential in Macko et al., 2024), ruRoBERTa-ruatd-binary[2](#page-4-0) (as a best singlemodel system in RuATD 2022 [Shamardina et al.,](#page-12-1) [2022\)](#page-12-1), BLOOMZ-3B-mixed-detector [\(Nicolai](#page-11-10) [Thorer Sivesind and Andreas Bentzen Winje,](#page-11-10) [2023\)](#page-11-10) (with a heavily multilingual fine-tuned LLM), and RoBERTa-Large OpenAI Detectors [\(Solaiman](#page-12-11) [et al.,](#page-12-11) [2019\)](#page-12-11) (as a widely used representative, although English-only).

For fine-tuned category, we have selected 7 multilingual foundational models covering multiple architectures and model sizes: mDeBERTa-v3-base [\(He et al.,](#page-11-11) [2022\)](#page-11-11) (as the best detector in [Macko](#page-11-3) [et al.,](#page-11-3) [2023\)](#page-11-3), XLM-RoBERTa-large [\(Conneau](#page-10-8) [et al.,](#page-10-8) [2020\)](#page-10-8) (as the best detector in [Macko et al.,](#page-11-3) [2023](#page-11-3) based on AUC ROC), Mistral-7B [\(Jiang et al.,](#page-11-12) [2023\)](#page-11-12) (as the best single-model multilingual detector in of SemEval-2024 Task 8 [Wang et al.,](#page-12-3) [2024b\)](#page-12-3), Llama-3-8B [\(AI@Meta,](#page-10-9) [2024\)](#page-10-9) (as a SOTA multilingual smaller LLM), Aya-101 [\(Üstün et al.,](#page-12-12) [2024\)](#page-12-12) (as a SOTA representative of multilingual LLMs with encoder-decoder architecture), BLOOMZ-3B [\(Muennighoff et al.,](#page-11-13) [2022\)](#page-11-13) (as the best pre-trained detector base model), and Falcon-rw-1B (as a smaller version of the best performing model at ALTA 2023 [Gagiano and Tian,](#page-10-10) [2023,](#page-10-10) since the 7B model version is already covered by better performing Mistral, [Spiegel and Macko,](#page-12-10) [2024b\)](#page-12-10).

For statistical and pre-trained categories of detection methods, we have used their versions implemented in the IMGTB framework [\(Spiegel and](#page-12-13) [Macko,](#page-12-13) [2024a\)](#page-12-13).

# 5 Experimental Results

Firstly, we provide benchmark evaluation of the selected MGTD methods on all MultiSocial test data (Table [4\)](#page-4-1). It provides a fair comparison of the methods, although the data samples among platforms and languages are not perfectly balanced. Therefore, for further experiments targeting specific cross-lingual and cross-platform research questions, the carefully selected parts of train and test splits are used (described in the corresponding subsections). Per-language, per-platform, and pergenerator results are provided in Appendix [F.](#page-19-0) Although worse-than-random performances of some pre-trained detectors indicate a potential problem with the detection (e.g., flipped labels), it is not the case as confirmed by the results in Table [21.](#page-21-0)

For comparison of MGTD methods, we use AUC ROC (area under the curve of receiver op-

<span id="page-4-1"></span>

|       | Rank Detector                       | AUC ROC MacroF1 | @5%FPR |
|-------|-------------------------------------|-----------------|--------|
| 1     | Llama-3-8b-MultiSocial              | 0.9769          | 0.8696 |
| 2     | Mistral-7b-v0.1-MultiSocial         | 0.9768          | 0.8692 |
| 3     | Aya-101-MultiSocial                 | 0.9731          | 0.8462 |
| 4     | Falcon-rw-1b-MultiSocial            | 0.9592          | 0.7810 |
| 5     | BLOOMZ-3b-MultiSocial               | 0.9582          | 0.7843 |
| 6     | XLM-RoBERTa-large-MultiSocial       | 0.9553          | 0.7840 |
| 7     | mDeBERTa-v3-base-MultiSocial        | 0.9544          | 0.7652 |
| 8     | BLOOMZ-3b-mixed-Detector            | 0.7553          | 0.3024 |
| 9     | DetectLLM-LRR                       | 0.7464          | 0.2523 |
|       | 10 LLM-Deviation                    | 0.7454          | 0.2497 |
| 11 S5 |                                     | 0.7418          | 0.2465 |
|       | 12 Fast-Detect-GPT                  | 0.7418          | 0.3605 |
|       | 13 Binoculars                       | 0.7248          | 0.2815 |
|       | 14 ChatGPT-Detector-RoBERTa-Chinese | 0.7180          | 0.3416 |
|       | 15 ruRoBERTa-ruatd-binary           | 0.4817          | 0.1711 |
|       | 16 Longformer Detector              | 0.4615          | 0.1516 |
|       | 17 RoBERTa-large-OpenAI-Detector    | 0.3450          | 0.1376 |

Table 4: Benchmark of the selected MGTD methods of statistical , pre-trained , and fine-tuned categories (as defined in Section [4\)](#page-3-1). The highlight color identifies the category of the method in the table.

erating characteristic) as a classification-threshold independent metric (not affected by a threshold calibration on in-domain data), also used by [\(Hans](#page-11-8) [et al.,](#page-11-8) [2024\)](#page-11-8). Due to imbalanced test data (machine class contains 7x more samples), we also use Macro avg. F1-score @ 5% FPR (false positive rate) as a metric balancing between a precision and a recall of the classification, while the threshold is calibrated based on the train data (to avoid data leakage) ROC curve to achieve 5% FPR (similarly used in [Dugan et al.,](#page-10-1) [2024\)](#page-10-1).

Since Gemini-generated data have used a slightly different generation process (e.g., jailbreak prompt, see Appendix [B\)](#page-13-0) and achieved highly outlier wordcount and unique-words scores in Table [12,](#page-16-0) we do not use Gemini-generated data in the training (fine-tuning or classification-threshold calibration). Nevertheless, they are still included in the evaluation (can serve for unseen generator evaluation).

#### 5.1 Multilingual Zero-shot Detection

This experiment is focused on the following research question: *RQ1: How well are socialmedia texts of multiple languages and platforms detectable by MGT detectors applicable in zeroshot manner (out-of-distribution, without further in-domain training)?* Since SMN texts are usually shorter than commonly used news articles, the detection performance of existing directly usable (i.e., without in-domain training) MGT detectors is still unknown (it could differ from the reported performance on other domains). This has not been evaluated in the multilingual settings. Are there

<span id="page-4-0"></span><sup>2</sup> <https://huggingface.co/orzhan/ruroberta-ruatd-binary>

<span id="page-5-0"></span>

|          |                        |      |      |      |      |      |      |      |      |      | Test | Langu | iage [A | UC R | OC]  |      |      |      |      |      |      |      |      |      |
|----------|------------------------|------|------|------|------|------|------|------|------|------|------|-------|---------|------|------|------|------|------|------|------|------|------|------|------|
| Category | Detector               | ar   | bg   | ca   | cs   | de   | el   | en   | es   | et   | ga   | gd    | hr      | hu   | nl   | pl   | pt   | ro   | ru   | sk   | sl   | uk   | zh   | all  |
|          | BLOOMZ-3b-mixed-       | 0.79 | 0.74 | 0.79 | 0.80 | 0.77 | 0.76 | 0.80 | 0.79 | 0.83 | 0.78 | 0.66  | 0.77    | 0.84 | 0.75 | 0.78 | 0.79 | 0.66 | 0.68 | 0.76 | 0.64 | 0.70 | 0.69 | 0.76 |
|          | Detector               |      |      |      |      |      |      |      |      |      |      |       |         |      |      |      |      |      |      |      |      |      |      |      |
|          | ChatGPT-Detector-      | 0.72 | 0.80 | 0.76 | 0.66 | 0.80 | 0.75 | 0.90 | 0.81 | 0.82 | 0.73 | 0.63  | 0.63    | 0.78 | 0.70 | 0.62 | 0.73 | 0.75 | 0.76 | 0.63 | 0.66 | 0.74 | 0.81 | 0.72 |
|          | <b>RoBERTa-Chinese</b> |      |      |      |      |      |      |      |      |      |      |       |         |      |      |      |      |      |      |      |      |      |      |      |
| Р        | Longformer Detector    | 0.34 | 0.48 | 0.32 | 0.47 | 0.43 | 0.54 | 0.65 | 0.43 | 0.48 | 0.53 | 0.49  | 0.51    | 0.50 | 0.41 | 0.46 | 0.42 | 0.41 | 0.46 | 0.45 | 0.46 | 0.61 | 0.47 | 0.46 |
|          | <b>RoBERTa-large-</b>  | 0.73 | 0.43 | 0.43 | 0.14 | 0.32 | 0.74 | 0.52 | 0.30 | 0.20 | 0.30 | 0.48  | 0.19    | 0.13 | 0.30 | 0.21 | 0.23 | 0.24 | 0.54 | 0.26 | 0.33 | 0.36 | 0.60 | 0.35 |
|          | OpenAI-Detector        |      |      |      |      |      |      |      |      |      |      |       |         |      |      |      |      |      |      |      |      |      |      |      |
|          | ruRoBERTa-ruatd-       | 0.40 | 0.63 | 0.56 | 0.43 | 0.43 | 0.35 | 0.56 | 0.47 | 0.43 | 0.49 | 0.47  | 0.48    | 0.46 | 0.50 | 0.44 | 0.43 | 0.34 | 0.70 | 0.45 | 0.47 | 0.59 | 0.44 | 0.48 |
|          | binary                 |      |      |      |      |      |      |      |      |      |      |       |         |      |      |      |      |      |      |      |      |      |      |      |
|          | Binoculars             | 0.70 | 0.68 | 0.62 | 0.74 | 0.75 | 0.79 | 0.80 | 0.76 | 0.74 | 0.71 | 0.71  | 0.79    | 0.78 | 0.72 | 0.75 | 0.75 | 0.74 | 0.72 | 0.71 | 0.73 | 0.64 | 0.74 | 0.72 |
|          | DetectLLM-LRR          | 0.79 | 0.86 | 0.69 | 0.93 | 0.78 | 0.88 | 0.80 | 0.82 | 0.88 | 0.79 | 0.74  | 0.88    | 0.94 | 0.79 | 0.88 | 0.84 | 0.87 | 0.78 | 0.85 | 0.79 | 0.75 | 0.78 | 0.75 |
| S        | Fast-Detect-GPT        | 0.75 | 0.65 | 0.61 | 0.81 | 0.77 | 0.66 | 0.80 | 0.74 | 0.69 | 0.70 | 0.74  | 0.80    | 0.77 | 0.74 | 0.77 | 0.77 | 0.77 | 0.73 | 0.71 | 0.74 | 0.70 | 0.74 | 0.74 |
|          | LLM-Deviation          | 0.82 | 0.86 | 0.68 | 0.93 | 0.79 | 0.89 | 0.80 | 0.82 | 0.90 | 0.81 | 0.79  | 0.89    | 0.94 | 0.79 | 0.89 | 0.84 | 0.88 | 0.78 | 0.86 | 0.81 | 0.75 | 0.79 | 0.75 |
|          | S5                     | 0.80 | 0.85 | 0.68 | 0.92 | 0.78 | 0.88 | 0.77 | 0.81 | 0.89 | 0.80 | 0.78  | 0.88    | 0.94 | 0.77 | 0.88 | 0.83 | 0.88 | 0.78 | 0.85 | 0.80 | 0.74 | 0.78 | 0.74 |

Table 5: Per-language AUC ROC performance of zero-shot statistical (S) and pre-trained (P) MGT detectors. The data are too difficult for the three under-performing pre-trained models.

<span id="page-5-1"></span>

|          |          |      |      |      |      |      |      |      |      |      | Test La | nguag | e [AUC | ROC  | mean] |      |      |      |      |      |      |      |      |      |
|----------|----------|------|------|------|------|------|------|------|------|------|---------|-------|--------|------|-------|------|------|------|------|------|------|------|------|------|
| Category | Platform | ar   | bg   | ca   | cs   | de   | el   | en   | es   | et   | ga      | gd    | hr     | hu   | nl    | pl   | pt   | ro   | ru   | sk   | sl   | uk   | zh   | all  |
|          | Discord  | N/A  | N/A  | 0.92 | 0.81 | 0.86 | N/A  | 0.91 | 0.89 | 0.86 | N/A     | N/A   | 0.75   | 0.84 | 0.81  | 0.81 | 0.82 | 0.81 | N/A  | N/A  | N/A  | N/A  | N/A  | 0.82 |
|          | Gab      | N/A  | N/A  | N/A  | N/A  | 0.71 | N/A  | 0.82 | 0.71 | N/A  | N/A     | N/A   | N/A    | N/A  | 0.71  | 0.64 | 0.72 | 0.61 | 0.62 | N/A  | N/A  | N/A  | N/A  | 0.66 |
| Ð        | Telegram | 0.73 | 0.77 | 0.73 | 0.73 | 0.79 | 0.81 | 0.86 | 0.80 | 0.84 | N/A     | N/A   | 0.71   | 0.86 | 0.68  | 0.70 | 0.77 | 0.77 | 0.76 | N/A  | N/A  | 0.73 | 0.76 | 0.74 |
| r        | Twitter  | 0.84 | 0.79 | N/A  | N/A  | 0.78 | N/A  | 0.85 | 0.73 | N/A  | N/A     | N/A   | N/A    | N/A  | 0.73  | N/A  | 0.75 | 0.65 | 0.80 | N/A  | N/A  | N/A  | N/A  | 0.73 |
|          | WhatsApp | N/A  | N/A  | N/A  | N/A  | N/A  | N/A  | 0.82 | 0.86 | N/A  | N/A     | N/A   | N/A    | N/A  | N/A   | N/A  | 0.76 | N/A  | N/A  | N/A  | N/A  | N/A  | N/A  | 0.79 |
|          | all      | 0.76 | 0.77 | 0.77 | 0.73 | 0.78 | 0.75 | 0.85 | 0.80 | 0.82 | 0.75    | N/A   | 0.70   | 0.81 | 0.72  | 0.70 | 0.76 | 0.71 | 0.72 | 0.70 | 0.65 | 0.72 | 0.75 | 0.74 |
|          | Discord  | N/A  | N/A  | 0.85 | 0.90 | 0.87 | N/A  | 0.90 | 0.89 | 0.86 | N/A     | N/A   | 0.89   | 0.91 | 0.89  | 0.92 | 0.90 | 0.89 | N/A  | N/A  | N/A  | N/A  | N/A  | 0.88 |
|          | Gab      | N/A  | N/A  | N/A  | N/A  | 0.73 | N/A  | 0.76 | 0.74 | N/A  | N/A     | N/A   | N/A    | N/A  | 0.72  | 0.77 | 0.73 | 0.74 | 0.71 | N/A  | N/A  | N/A  | N/A  | 0.69 |
| s        | Telegram | 0.76 | 0.78 | 0.62 | 0.88 | 0.71 | 0.86 | 0.81 | 0.77 | 0.84 | N/A     | N/A   | 0.89   | 0.91 | 0.74  | 0.84 | 0.82 | 0.87 | 0.74 | N/A  | N/A  | 0.71 | 0.75 | 0.74 |
| 5        | Twitter  | 0.76 | 0.79 | N/A  | N/A  | 0.87 | N/A  | 0.83 | 0.72 | N/A  | N/A     | N/A   | N/A    | N/A  | 0.78  | N/A  | 0.86 | 0.85 | 0.90 | N/A  | N/A  | N/A  | N/A  | 0.74 |
|          | WhatsApp | N/A  | N/A  | N/A  | N/A  | N/A  | N/A  | 0.69 | 0.85 | N/A  | N/A     | N/A   | N/A    | N/A  | N/A   | N/A  | 0.78 | N/A  | N/A  | N/A  | N/A  | N/A  | N/A  | 0.72 |
|          | all      | 0.77 | 0.78 | 0.65 | 0.87 | 0.77 | 0.82 | 0.79 | 0.79 | 0.82 | 0.76    | N/A   | 0.85   | 0.87 | 0.76  | 0.83 | 0.81 | 0.83 | 0.76 | 0.80 | 0.77 | 0.71 | 0.76 | 0.74 |

Table 6: Per-platform mean AUC ROC performance of well-performing zero-shot MGT detectors per category. N/A refers to not enough samples (at least 2000) in MultiSocial for a combination of language and platform. Discord data are the easiest for the detection, Gab data are the most difficult.

differences in multilingual MGT detection among different sources of SMN content (e.g., Twitter vs. Telegram vs. Gab)? Is there a difference between statistical (could be language independent) and pre-trained (heavily dependent on pre-training languages) zero-shot detectors?

To answer these questions, we compare the per-language performance of statistical and pretrained MGTD methods (collectively called zeroshot methods for this purpose) based on AUC ROC (to avoid in-domain touch with the data) in Table 5. To compare per-language performance per platforms, we consider only cases where there are at least 2000 samples available per platform and language (approximately 250 texts per each generator). The summarized results of the comparison are provided in Table 6. The *all* row represents performance of MGT detectors of the corresponding category for all platforms data combined (excluding only results for Scottish due to not having enough samples). Due to low performance of three out of five selected pre-trained detectors (see Table 4), we average results for this category only for the two well-performing detectors (BLOOMZ-3bmixed-Detector and ChatGPT-Detector-RoBERTa-Chinese). Full results are available in Appendix F.

To evaluate whether the differences between mean AUC ROC of statistical and the best pretrained MGT detectors are statistically significant, we conduct paired t-tests for each test language and check whether p-value is < 0.05. Analogously, we have verified significance of differences between per-platform means in each category.

There are differences in performance of SOTA zero-shot MGTD methods on texts of English and non-English languages. When considering the well-performing pre-trained and all statistical detectors, the difference between performances on English and combined non-English texts is statistically significant (higher on English). However, Longformer Detector clearly performed better on English than the other languages. Similarly, ruRoBERTa performed better on Russian, Ukrainian, and Bulgarian than the others. OpenAI Detector has clearly not been trained on SMN texts, since not performing well even in English (there are also huge differences in performance based on the generators, see Table 21). The other two pretrained and all statistical detectors performed similarly across languages, although the Chinese detector performed worse on Slavic-Latin languages.

There are significant differences in performance of SOTA zero-shot MGTD methods on texts of different platforms. The detectors are able to better detect MGT of Discord SMN than the others (although the differences to other plat-

<span id="page-6-0"></span>

|                   |    |    |    |    |    |    |    |    |    | Test Language [AUC ROC] |    |    |                                                                                                                    |    |    |    |    |     |     |    |    |     |
|-------------------|----|----|----|----|----|----|----|----|----|-------------------------|----|----|--------------------------------------------------------------------------------------------------------------------|----|----|----|----|-----|-----|----|----|-----|
| Detector          | ar | bg | ca | cs | de | el | en | es | et | ga⋆ gd⋆                 | hr | hu | nl                                                                                                                 | pl | pt | ro | ru | sk⋆ | sl⋆ | uk | zh | all |
| Aya-101-          |    |    |    |    |    |    |    |    |    |                         |    |    | 0.97 0.99 0.97 0.98 0.97 0.97 0.98 0.98 0.98 0.95 0.92 0.98 0.99 0.97 0.98 0.98 0.98 0.96 0.98 0.95 0.95 0.97 0.97 |    |    |    |    |     |     |    |    |     |
| MultiSocial       |    |    |    |    |    |    |    |    |    |                         |    |    |                                                                                                                    |    |    |    |    |     |     |    |    |     |
| BLOOMZ-3b         |    |    |    |    |    |    |    |    |    |                         |    |    | 0.96 0.98 0.96 0.97 0.95 0.96 0.98 0.97 0.98 0.90 0.82 0.96 0.99 0.94 0.95 0.97 0.95 0.94 0.95 0.88 0.90 0.97 0.96 |    |    |    |    |     |     |    |    |     |
| MultiSocial       |    |    |    |    |    |    |    |    |    |                         |    |    |                                                                                                                    |    |    |    |    |     |     |    |    |     |
| Falcon-rw-1b      |    |    |    |    |    |    |    |    |    |                         |    |    | 0.95 0.98 0.97 0.97 0.96 0.96 0.98 0.96 0.98 0.92 0.87 0.96 0.99 0.95 0.96 0.96 0.95 0.94 0.95 0.87 0.91 0.96 0.96 |    |    |    |    |     |     |    |    |     |
| MultiSocial       |    |    |    |    |    |    |    |    |    |                         |    |    |                                                                                                                    |    |    |    |    |     |     |    |    |     |
| Llama-3-8b        |    |    |    |    |    |    |    |    |    |                         |    |    | 0.97 0.99 0.98 0.99 0.98 0.97 0.99 0.98 0.99 0.94 0.90 0.98 0.99 0.98 0.98 0.98 0.98 0.96 0.98 0.95 0.95 0.98 0.98 |    |    |    |    |     |     |    |    |     |
| MultiSocial       |    |    |    |    |    |    |    |    |    |                         |    |    |                                                                                                                    |    |    |    |    |     |     |    |    |     |
| Mistral-7b-v0.1-  |    |    |    |    |    |    |    |    |    |                         |    |    | 0.97 0.99 0.98 0.99 0.98 0.97 0.99 0.98 0.98 0.93 0.93 0.99 1.00 0.97 0.98 0.98 0.97 0.97 0.97 0.94 0.96 0.98 0.98 |    |    |    |    |     |     |    |    |     |
| MultiSocial       |    |    |    |    |    |    |    |    |    |                         |    |    |                                                                                                                    |    |    |    |    |     |     |    |    |     |
| XLM-RoBERTa       |    |    |    |    |    |    |    |    |    |                         |    |    | 0.95 0.98 0.94 0.98 0.96 0.95 0.96 0.96 0.97 0.88 0.78 0.97 0.99 0.95 0.97 0.95 0.96 0.95 0.96 0.91 0.92 0.93 0.96 |    |    |    |    |     |     |    |    |     |
| large-MultiSocial |    |    |    |    |    |    |    |    |    |                         |    |    |                                                                                                                    |    |    |    |    |     |     |    |    |     |
| mDeBERTa-v3-      |    |    |    |    |    |    |    |    |    |                         |    |    | 0.94 0.98 0.94 0.97 0.95 0.94 0.96 0.96 0.98 0.90 0.79 0.97 0.99 0.95 0.96 0.96 0.96 0.93 0.96 0.92 0.93 0.94 0.95 |    |    |    |    |     |     |    |    |     |
| base-MultiSocial  |    |    |    |    |    |    |    |    |    |                         |    |    |                                                                                                                    |    |    |    |    |     |     |    |    |     |

Table 7: Per-language AUC ROC performance of fine-tuned MGT detectors. ⋆ marks languages not in train set. Larger models achieve better performance.

forms are not statistically significant for pre-trained detectors). On the other hand, the Gab texts are the most difficult for them to classify (although the differences to Telegram for pre-trained and WhatsApp for statistical detectors are not statistically significant). There is no clear indication for the length of such texts affecting these results, since Discord has the lowest (9) and WhatsApp and Twitter the highest (18) median value of word-count text length. We can speculate that since Gab is known to have more toxic content (vulgarisms, hate speech), it can be more difficult for detection.

There are negligible differences in performance of SOTA zero-shot statistical and best pre-trained MGTD methods. When considering the two best performing pre-trained detectors, which achieved 0.72-0.76 AUC ROC (0.62-0.9 in per-language evaluation), the performance is competitive with the statistical detectors, achieving 0.72-0.75 AUC ROC (0.61-0.94 in per-language evaluation). In regard to multilingual performance, the statistical detectors tends to achieve higher performance for Slavic-Latin and Uralic languages (confirmed by Telegram-only data), underperforming for Catalan. This is not the case of pre-trained detectors, under-performing for Scottish and Slovenian, and the Chinese detector shows rather opposite patterns for Slavic-Latin languages. The t-tests confirmed that the differences between these two categories are statistically significant only for Catalan, Scottish and Slovenian languages.

#### 5.2 Multilingual Fine-tuned Detection

This experiment is focused on the following research question: *RQ2: How well are social-media texts of multiple languages detectable by fine-tuned MGT detectors?* SMN texts have a higher variety of styles and lower grammatical correctness than

news articles. Are language models able to be finetuned for the MGT detection task using such texts? Also, it is unknown which detection method is the most universal in regard to the diversity of use cases (different text lengths, different sources). Is the best MGT detector for news articles the same as for SMN content? Is the transferability to different languages the same?

Similarly to the previous experiment, we firstly compare AUC ROC performance per each test language in Table [7.](#page-6-0) The foundational models are fine-tuned in this experiment using all MultiSocial train data (except the samples generated by Gemini). The results show only small differences among the selected detectors, with pretty much steady performance across languages. When looking at the per-generator performance in Table [22,](#page-21-1) we might observe slightly decreased performance for Gemini (as not used for training), but also for OPT-IML-Max-30b and Aya-101, both having a shorter word count text lengths (Table [12\)](#page-16-0).

The multilingual models are able to be finetuned for MGTD task in social-media domain. The performance reached above 0.9 AUC ROC in all train languages, with slightly lower performance of some models in test-only languages (Scottish and Slovenian). Therefore, the style and form of the SMN texts does not seem to limit the ability of the models to serve as fine-tuned detectors.

For the cross-lingual evaluation, we use the same language setting as used by MULTITuDE [\(Macko et al.,](#page-11-3) [2023\)](#page-11-3), which was focused on news domain, for the results to be comparable. Specifically, we use English, Spanish and Russian Telegram data (having enough samples for training, approximately the same size, representatives of different language-family branches) for monolingual as well as multilingual fine-tuning (per-language

<span id="page-7-0"></span>

| Train    |              |              |              |              |              |              |              | Test I       | anguag       | e [Al | JC RO | C mear       | ı (±conf     | idence i | nterval)     | ]       |         |         |     |     |              |         |         |
|----------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|-------|-------|--------------|--------------|----------|--------------|---------|---------|---------|-----|-----|--------------|---------|---------|
| Language | ar           | bg           | ca           | cs           | de           | el           | en           | es           | et           | ga    | gd    | hr           | hu           | nl       | pl           | pt      | ro      | ru      | sk  | sl  | uk           | zh      | all     |
| en       | 0.81         | 0.90         | 0.78         | 0.91         | 0.88         | 0.90         | 0.96         | 0.87         | 0.92         | N/A   | N/A   | 0.94         | 0.97         | 0.84     | 0.89         | 0.92    | 0.94    | 0.87    | N/A | N/A | 0.81         | 0.73    | 0.87    |
|          | (±0.05)      | $(\pm 0.08)$ | (±0.03)      | $(\pm 0.05)$ | (±0.02)      | $(\pm 0.03)$ | $(\pm 0.01)$ | $(\pm 0.06)$ | (±0.03)      |       |       | $(\pm 0.03)$ | (±0.03)      | (±0.02)  | $(\pm 0.05)$ | (±0.02) | (±0.02) | (±0.05) |     |     | $(\pm 0.05)$ | (±0.14) | (±0.04) |
| es       | 0.82         | 0.89         | 0.85         | 0.89         | 0.89         | 0.87         | 0.90         | 0.94         | 0.90         | N/A   | N/A   | 0.92         | 0.95         | 0.84     | 0.88         | 0.93    | 0.93    | 0.88    | N/A | N/A | 0.82         | 0.73    | 0.86    |
|          | (±0.05)      | $(\pm 0.08)$ | (±0.03)      | (±0.06)      | (±0.03)      | $(\pm 0.06)$ | (±0.04)      | $(\pm 0.01)$ | (±0.05)      |       |       | (±0.05)      | (±0.04)      | (±0.03)  | $(\pm 0.06)$ | (±0.02) | (±0.03) | (±0.04) |     |     | (±0.05)      | (±0.14) | (±0.05) |
| ru       | 0.81         | 0.93         | 0.76         | 0.87         | 0.84         | 0.88         | 0.87         | 0.82         | 0.88         | N/A   | N/A   | 0.89         | 0.91         | 0.79     | 0.87         | 0.86    | 0.88    | 0.94    | N/A | N/A | 0.89         | 0.73    | 0.84    |
|          | $(\pm 0.10)$ | $(\pm 0.05)$ | $(\pm 0.10)$ | $(\pm 0.08)$ | $(\pm 0.04)$ | $(\pm 0.06)$ | $(\pm 0.06)$ | $(\pm 0.11)$ | $(\pm 0.08)$ |       |       | $(\pm 0.07)$ | $(\pm 0.07)$ | (±0.06)  | $(\pm 0.07)$ | (±0.07) | (±0.07) | (±0.02) |     |     | $(\pm 0.04)$ | (±0.17) | (±0.08) |
| en-es-ru | 0.89         | 0.94         | 0.86         | 0.93         | 0.91         | 0.93         | 0.95         | 0.93         | 0.93         | N/A   | N/A   | 0.94         | 0.97         | 0.86     | 0.91         | 0.94    | 0.95    | 0.93    | N/A | N/A | 0.89         | 0.86    | 0.91    |
|          | (±0.03)      | $(\pm 0.04)$ | (±0.03)      | $(\pm 0.03)$ | $(\pm 0.03)$ | $(\pm 0.02)$ | (±0.01)      | (±0.02)      | $(\pm 0.03)$ |       |       | $(\pm 0.04)$ | $(\pm 0.02)$ | (±0.03)  | $(\pm 0.04)$ | (±0.01) | (±0.02) | (±0.03) |     |     | $(\pm 0.03)$ | (±0.08) | (±0.03) |

Table 8: Cross-lingual mean AUC ROC performance of the selected MGT detectors fine-tuned monolingually (*en*, *es* and *ru*) and multilingually (*en-es-ru*), evaluated based on Telegram data (for training as well as for testing), reported along with 95% confidence interval error bounds. N/A refers to not enough samples (at least 2000) in MultiSocial Telegram data. Multilingual fine-tuning helps especially for languages unrelated to train languages.

pseudo-random sub-sampling to 1/3 of the samples count to reach the same cumulative count as in monolingual fine-tuning). Due to lower number of samples in the selected portions of the train dataset than in using full data in the previous experiment, we prolong the fine-tuning process to 7 epochs for models to be able to train well. The cross-lingual results are summarized in Table 8, where mean performances across detectors are reported (perdetector results are provided in Appendix F). As the per-detector results in Table 29 clearly indicate different behaviour of some detectors across languages, we provide an ablation study in Appendix E, where we aggregate the results per the two identified groups of detectors.

**Multilingual fine-tuning can improve crosslingual transferability.** Our experiments show that fine-tuning using multiple languages is almost always superior to monolingual setting (see Table 8). However, the rate of improvement can vary depending on model architecture and train-test language similarity. We can observe more noticeable improvements on unrelated languages such as Arabic and Chinese. The ablation study revealed that there is a subset of detectors (with non-autoregressive models) for which the differences between monolingually and multilingually fine-tuned versions are not statistically significant for any language.

For the **cross-platform evaluation**, we use English and Spanish combined data only, since they are balanced across all the platforms and have enough samples for training. We fine-tuned the selected models in mono-platform (a single SMN platform data) and multi-platform (all platforms combined; similarly to the previous expriment, we have used per-platform pseudo-random sub-sampling to 1/5 of the samples count to reach the same cumulative count as in mono-platform fine-tuning) manner. The per-test-platform results are summarized in Table 9, where mean performances

<span id="page-7-1"></span>

| Train    | Test Pl | atform [Al | UC ROC n | nean (±coi | nfidence inte | erval)] |
|----------|---------|------------|----------|------------|---------------|---------|
| Platform | Discord | Gab        | Telegram | Twitter    | WhatsApp      | all     |
| Discord  | 0.98    | 0.84       | 0.88     | 0.82       | 0.89          | 0.88    |
|          | (±0.00) | (±0.02)    | (±0.02)  | (±0.04)    | (±0.03)       | (±0.02) |
| Gab      | 0.96    | 0.94       | 0.93     | 0.94       | 0.91          | 0.93    |
|          | (±0.01) | (±0.01)    | (±0.02)  | (±0.03)    | (±0.02)       | (±0.01) |
| Telegram | 0.98    | 0.92       | 0.96     | 0.95       | 0.95          | 0.95    |
|          | (±0.00) | (±0.02)    | (±0.01)  | (±0.02)    | (±0.01)       | (±0.01) |
| Twitter  | 0.97    | 0.91       | 0.92     | 0.98       | 0.92          | 0.93    |
|          | (±0.01) | (±0.01)    | (±0.02)  | (±0.01)    | (±0.02)       | (±0.01) |
| WhatsApp | 0.97    | 0.90       | 0.93     | 0.92       | 0.97          | 0.93    |
|          | (±0.01) | (±0.01)    | (±0.01)  | (±0.02)    | (±0.01)       | (±0.01) |
| all      | 0.98    | 0.93       | 0.95     | 0.96       | 0.95          | 0.95    |
|          | (±0.01) | (±0.02)    | (±0.01)  | (±0.01)    | (±0.01)       | (±0.01) |

Table 9: Cross-platform mean AUC ROC performance of the selected fine-tuned MGT detectors, reported along with 95% confidence interval error bounds. Telegrambased mono-platform fine-tuning shows the best crossplatform transferability.

across detectors are reported for each train platform (per-detector results are provided in Appendix F). Gab platform data are still the most difficult for MGT detection and Discord data are the easiest.

Using Telegram data for mono-platform finetuning achieves the best cross-platform transferability of detection performance. We can speculate that the reason may be the well-diversified texts across lengths and forms. On the other hand, using Discord data achieves the worst cross-platform transferability. Similarly to zeroshot detectors, there are significant differences among different platforms data disregarding monoplatform and multi-platform fine-tuning. On average, the multi-platform fine-tuning could not reach the performance of mono-platform fine-tuning in the in-platform evaluation. Although, beside the Telegram-trained detectors, the multi-platform ones reached the best performance across platforms. The differences between these two versions (Telegram and all) are not statistically significant for any test platform.

# 6 Discussion

Shorter and more informal style of the texts in social-media domain does not prevent detectors to be fine-tuned for this domain with superior performance. Despite our assumptions of SMN texts to be quite challenging for fine-tuned detectors, the results indicate that they have no problem to be trained on such texts. The best fine-tuned detectors achieved 0.98 AUC ROC performance and 0.87 Macro average F1-score, with a steady performance across all the test languages.

Bigger LLMs achieve higher performance as fine-tuned MGT detectors than smaller foundational models. The size of the models seems to affect their performance, as >7b parameters models achieved significantly superior performance compared to <7b models. However, for practical application, one must consider a trade-off between detection performance and inference costs, since even the smallest mDeBERTa-v3-base achieved much better performance than zero-shot detectors (which also use base models of >6b parameters).

Multilingual fine-tuning helps cross-lingual transferability of autoregressive models in the MGTD task. We have noticed a clear difference in performances of two groups of fine-tuned detectors, namely the foundational models with autoregressive pre-training and the models with autoencoding (for masked language modeling, XLM-RoBERTa and mDeBERTa) or sequence-to-sequence (Aya) pre-training. The ablation study (Appendix [E\)](#page-19-1) aggregating the results for these two groups revealed that the benefit of multilingual fine-tuning is higher in autoregressive group than the other, where the differences are not statistically significant. Also, linguistical similarity between languages seems to affect the transferability in the autoregressive group more intensively.

Selection of social-media platform for finetuning matters. There are also significant differences between models trained on single vs multiple platform dataset. For example, on Twitter data, the Discord-trained detectors achieve on average 13% lower AUC ROC than Telegram-trained detectors. Although using just Discord data yields the highest performance for Discord test data, the performance of such trained detector is the least transferable to other platforms (e.g., AUC ROC drop by 27% in case of Llama-3-8b).

The best detectors fine-tuned on social-media texts still outperform zero-shot detectors on news-domain texts generated by the same models. Although there is a drop in such out-of-domain performance (Appendix [D\)](#page-19-2), the detection ability in most languages is still better than that of zero-shot detectors if the data are generated by the generators used for training (i.e., cross-domain). If a different generator is used (i.e., cross-domain and cross-generator), the Fast-Detect-GPT and Binoculars can outperform the fine-tuned detectors.

# 7 Conclusions

We have created and published a unique multiplatform and massively multilingual dataset, named MultiSocial, to benchmark machine-generated text detection methods on social-media texts. It covers 7 most modern text-generation AI models (of various sizes and architectures), 5 social-media network platforms, and 22 languages of 4 primary language families. We have used this dataset to benchmark 17 carefully selected state-of-the-art detection methods of 3 categories (statistical zeroshot, pre-trained, and fine-tuned) and compare their multi-platform and multi-lingual capabilities (as well as cross-lingual and cross-platform capabilities of fine-tuned detectors). We have discussed the most interesting findings, including that the detection models can be fine-tuned to the machinegenerated text detection task using social-media texts (shorter lengths, informal style, emoticons and hashtags) quite well, with the performance comparable to the performance reported in other domains (e.g., news articles). We have shown that there are significant differences in performance based on the selection of social-media platform data for training, influencing their cross-platform transferability (e.g., Discord-trained detectors having up to 27% lower performance on Twitter data).

Due to rather high performance differences in cross-platform evaluation, the further work should be focused to a more-detailed analysis of crossdomain multilingual capability of the state-of-theart detectors. The proposed MultiSocial dataset can be used for a more detailed multilingual evaluation as well, such as a selection of optimal minimal subset of languages and platforms for training. Our work thus opens a door for deeper research in the field.

# Limitations

Limited text generation models and approaches. We have used 7 SOTA LLMs of various architectures and sizes for the text generation. However, these can not cover the huge amount and variety of different text-generation models available (with new models coming each month). We have selected the 3-iteration paraphrasing approach for the text generation. There are other approaches usable for the generation of social-media texts (we have experimented with few of them) that could yield different results of the benchmark.

Limited selection of machine-generated text detection methods. We have selected 17 detectors for the benchmark evaluation. There exist other MGT detection methods (e.g., perturbation or multi-generation based statistical methods or nonzero-shot statistical methods) that have not been included due to cost-efficiency of their usage. We have also not included combinations of multiple methods into the benchmark comparison.

Limited scope of the experiments. Given the multipurpose nature of the proposed MultiSocial benchmark dataset, there are plenty of other research questions that can be targeted, such as the most effective minimal combination of train languages to reach a certain cross-lingual capability. Since we are publishing the MultiSocial dataset as well as the code used in our benchmark, our results are fully reproducible and further research questions can be easily targeted by fellow researchers and future works.

# Ethics Statement

Intended Use. We have proposed a MultiSocial dataset along with a code for benchmark of multilingual machine-generated text detection methods. The released artefacts are intended for research purpose only. They are not intended for deployment of actual services making automated decisions, as the classifications are not fully reliable, and could potentially do harm (e.g., false positive prediction, where human-written text is classified as machine generated).

Failure Modes. As confirmed by our experiments, although the detectors can generalize to data from other platforms, languages, generators or domains, this capability is limited and we do observe differences. The behavior on data from other platforms, languages, etc. is thus unknown and should be properly tested before any use.

Biases. Although the dataset contains a wide variety of languages (22 in total) covering various scripts and language families (see Section [3\)](#page-2-1), the

dataset is still biased towards Indo-European languages with 18 out of 22 belonging to this family. The dataset also reflects the topics characteristic for the time and the social media included in the 6 original datasets used as sources of human-written texts, but they should already be rather varied due to sheer volume of the data included (see Appendix [B.5\)](#page-17-0).

Misuse Potential. We work with already publicly available datasets of human-written social media content as well as with publicly available LLMs to generate the texts. In general, the human-written texts are not specifically targeted on disinformation, sensitive or toxic content, but the presence of such content cannot be ruled out (see Appendix [B.5](#page-17-0) for toxicity prediction). Secondly, although we have revealed in the paper which languages are more difficult for the SOTA detection methods to be applied in (i.e., a misuse potential of LLM-generated texts in those languages is higher), the overall misuse potential of our work is rather limited. To the opposite, our work aims to increase the robustness and generalizability of the current SOTA detection methods.

Collecting Data from Users. We have not collected any user data as a part of this work, but are reusing already publicly available datasets of social media posts. The published dataset is anonymized (identified usernames, email addresses, and phone numbers are replaced for tags).

Potential Harm to Vulnerable Populations. We are not aware of any potential harms unless the detectors were employed outside of their intended use, where they could potentially flag also legitimate uses of machine-generated text.

Licensing. As already mentioned, MultiSocial dataset is based on human data of 6 existing datasets. We have made sure to use and re-publish the data in accordance with their licenses. Specifically, two of the datasets are licensed by CC BY 4.0, one by AGPL-3.0, two for research purpose only, and one with no explicit licensing (thus assumed copyrighted). All of such licensing allows use of data for non-commercial research such as our work. We have also checked and followed licensing and terms of use of the used text generation LLMs. Therefore, we release the anonymized MultiSocial data with attribution to the sources of human texts for *non-commercial research purpose only*.

# Acknowledgments

This work was partially supported by the projects funded by the European Union under the Horizon Europe: *AI-CODE*, GA No. [101135437,](https://cordis.europa.eu/project/id/101135437) *VIGI-LANT*, GA No. [101073921;](https://doi.org/10.3030/101073921) and by *Modermed*, a project funded by the Slovak Research and Development Agency, GA No. APVV-22-0414.

Part of the research results was obtained using the computational resources procured in the national project *National competence centre for high performance computing* (project code: 311070AKF2) funded by European Regional Development Fund, EU Structural Funds Informatization of Society, Operational Program Integrated Infrastructure.

# References

<span id="page-10-9"></span>AI@Meta. 2024. [Llama 3 model card.](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)

- <span id="page-10-0"></span>Esma Aïmeur, Sabrine Amri, and Gilles Brassard. 2023. Fake news, disinformation and misinformation in social media: a review. *Social Network Analysis and Mining*, 13(1):30.
- <span id="page-10-6"></span>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. Falcon-40B: an open large language model with state-of-the-art performance.
- <span id="page-10-15"></span>Dimosthenis Antypas, Asahi Ushio, Jose Camacho-Collados, Vitor Silva, Leonardo Neves, and Francesco Barbieri. 2022. [Twitter topic classifica](https://aclanthology.org/2022.coling-1.299)[tion.](https://aclanthology.org/2022.coling-1.299) In *Proceedings of the 29th International Conference on Computational Linguistics*, pages 3386– 3400, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.
- <span id="page-10-13"></span>Satanjeev Banerjee and Alon Lavie. 2005. [METEOR:](https://aclanthology.org/W05-0909) [An automatic metric for MT evaluation with im](https://aclanthology.org/W05-0909)[proved correlation with human judgments.](https://aclanthology.org/W05-0909) In *Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization*, pages 65–72, Ann Arbor, Michigan. Association for Computational Linguistics.
- <span id="page-10-7"></span>Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. 2023. Fast-DetectGPT: Efficient zero-shot detection of machine-generated text via conditional probability curvature. In *The Twelfth International Conference on Learning Representations*.
- <span id="page-10-11"></span>Jason Baumgartner, Savvas Zannettou, Megan Squire, and Jeremy Blackburn. 2020. [The pushshift telegram](https://doi.org/10.1609/icwsm.v14i1.7348) [dataset.](https://doi.org/10.1609/icwsm.v14i1.7348) *Proceedings of the International AAAI Conference on Web and Social Media*, 14(1):840–847.

- <span id="page-10-8"></span>Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*. Association for Computational Linguistics.
- <span id="page-10-4"></span>Wanyun Cui, Linqiu Zhang, Qianle Wang, and Shuyang Cai. 2023. [Who said that? benchmarking social](https://arxiv.org/abs/2310.08240) [media AI detection.](https://arxiv.org/abs/2310.08240) *Preprint*, arXiv:2310.08240.
- <span id="page-10-14"></span>Daryna Dementieva, Daniil Moskovskiy, Nikolay Babakov, Abinew Ali Ayele, Naquee Rizwan, Frolian Schneider, Xintog Wang, Seid Muhie Yimam, Dmitry Ustalov, Elisei Stakovskii, Alisa Smirnova, Ashraf Elnagar, Animesh Mukherjee, and Alexander Panchenko. 2024. Overview of the multilingual text detoxification task at PAN 2024. In *Working Notes of CLEF 2024 - Conference and Labs of the Evaluation Forum*. CEUR-WS.org.
- <span id="page-10-16"></span>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. [QLoRA: Efficient finetun](https://arxiv.org/abs/2305.14314)[ing of quantized llms.](https://arxiv.org/abs/2305.14314) *arXiv:2305.14314*.
- <span id="page-10-1"></span>Liam Dugan, Alyssa Hwang, Filip Trhlik, Josh Magnus Ludan, Andrew Zhu, Hainiu Xu, Daphne Ippolito, and Chris Callison-Burch. 2024. [Raid:](https://arxiv.org/abs/2405.07940) [A shared benchmark for robust evaluation of](https://arxiv.org/abs/2405.07940) [machine-generated text detectors.](https://arxiv.org/abs/2405.07940) *Preprint*, arXiv:2405.07940.
- <span id="page-10-5"></span>Ramadhani Ally Duma, Zhendong Niu, Ally S Nyamawe, Jude Tchaye-Kondi, Nuru Jingili, Abdulganiyu Abdu Yusuf, and Augustino Faustino Deve. 2024. Fake review detection techniques, issues, and future research directions: a literature review. *Knowledge and Information Systems*, pages 1–42.
- <span id="page-10-3"></span>Tiziano Fagni, Fabrizio Falchi, Margherita Gambini, Antonio Martella, and Maurizio Tesconi. 2021. [Tweep-](https://doi.org/10.1371/journal.pone.0251415)[Fake: About detecting deepfake tweets.](https://doi.org/10.1371/journal.pone.0251415) *PLOS ONE*, 16(5):e0251415.
- <span id="page-10-12"></span>Jess Fan. 2021. Discord dataset. [https://www.](https://www.kaggle.com/jef1056/discord-data) [kaggle.com/jef1056/discord-data](https://www.kaggle.com/jef1056/discord-data). V5.
- <span id="page-10-2"></span>Pieter Fivez, Walter Daelemans, Tim Van de Cruys, Yury Kashnitsky, Savvas Chamezopoulos, Hadi Mohammadi, Anastasia Giachanou, Ayoub Bagheri, Wessel Poelman, Juraj Vladika, Esther Ploeger, Johannes Bjerva, Florian Matthes, and Hans van Halteren. 2024. [The clin33 shared task on the detection](https://www.clinjournal.org/clinj/article/view/182) [of text generated by large language models.](https://www.clinjournal.org/clinj/article/view/182) *Computational Linguistics in the Netherlands Journal*, 13:233–259.
- <span id="page-10-10"></span>Rinaldo Gagiano and Lin Tian. 2023. A prompt in the right direction: Prompt based classification of machine-generated text detection. In *Proceedings of ALTA*.

- <span id="page-11-16"></span>Kiran Garimella and Gareth Tyson. 2018. [Whatapp](https://doi.org/10.1609/icwsm.v12i1.14989) [doc? a first look at whatsapp public group data.](https://doi.org/10.1609/icwsm.v12i1.14989) *Proceedings of the International AAAI Conference on Web and Social Media*, 12(1).
- <span id="page-11-15"></span>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. *CS224N project report, Stanford*, 1(12):2009.
- <span id="page-11-6"></span>Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How close is ChatGPT to human experts? Comparison corpus, evaluation, and detection. *arXiv preprint arxiv:2301.07597*.
- <span id="page-11-18"></span>Rishav Hada, Varun Gumma, Mohamed Ahmed, Kalika Bali, and Sunayana Sitaram. 2024. [METAL:](https://doi.org/10.18653/v1/2024.findings-naacl.148) [Towards multilingual meta-evaluation.](https://doi.org/10.18653/v1/2024.findings-naacl.148) In *Findings of the Association for Computational Linguistics: NAACL 2024*, pages 2280–2298, Mexico City, Mexico. Association for Computational Linguistics.
- <span id="page-11-8"></span>Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2024. [Spotting LLMs with binoculars: Zero-shot](https://arxiv.org/abs/2401.12070) [detection of machine-generated text.](https://arxiv.org/abs/2401.12070) *Preprint*, arXiv:2401.12070.
- <span id="page-11-11"></span>Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2022. Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing. In *The Eleventh International Conference on Learning Representations*.
- <span id="page-11-7"></span>Oana Ignat, Xiaomeng Xu, and Rada Mihalcea. 2024. [MAiDE-up: Multilingual deception de](https://arxiv.org/abs/2404.12938)[tection of gpt-generated hotel reviews.](https://arxiv.org/abs/2404.12938) *Preprint*, arXiv:2404.12938.
- <span id="page-11-12"></span>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. [Mistral 7b.](https://arxiv.org/abs/2310.06825) *Preprint*, arXiv:2310.06825.
- <span id="page-11-1"></span>Tharindu Kumarage, Garima Agrawal, Paras Sheth, Raha Moraffah, Aman Chadha, Joshua Garland, and Huan Liu. 2024. [A survey of ai-generated text foren](https://arxiv.org/abs/2403.01152)[sic systems: Detection, attribution, and characteriza](https://arxiv.org/abs/2403.01152)[tion.](https://arxiv.org/abs/2403.01152) *Preprint*, arXiv:2403.01152.
- <span id="page-11-19"></span>Taja Kuzman, Igor Mozetic, and Nikola Ljubeši ˇ c. 2023. ´ Automatic genre identification for robust enrichment of massive text collections: Investigation of classification methods in the era of large language models. *Machine Learning and Knowledge Extraction*, 5(3):1149–1175.
- <span id="page-11-9"></span>Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Longyue Wang, Linyi Yang, Shuming Shi, and Yue Zhang. 2023. [Deepfake text detection in the wild.](https://arxiv.org/abs/2305.13242) *arXiv preprint arxiv:2305.13242*.

- <span id="page-11-2"></span>Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding, Xin Wang, Xin Li, Luisa Verdoliva, and Shu Hu. 2024. [Detecting multimedia](https://arxiv.org/abs/2402.00045) [generated by large AI models: A survey.](https://arxiv.org/abs/2402.00045) *Preprint*, arXiv:2402.00045.
- <span id="page-11-5"></span>Jason Lucas, Adaku Uchendu, Michiharu Yamashita, Jooyoung Lee, Shaurya Rohatgi, and Dongwon Lee. 2023. [Fighting fire with fire: The dual role of LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.883) [in crafting and detecting elusive disinformation.](https://doi.org/10.18653/v1/2023.emnlp-main.883) In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 14279–14305, Singapore. Association for Computational Linguistics.
- <span id="page-11-3"></span>Dominik Macko, Robert Moro, Adaku Uchendu, Jason Lucas, Michiharu Yamashita, Matúš Pikuliak, Ivan Srba, Thai Le, Dongwon Lee, Jakub Simko, and Maria Bielikova. 2023. [MULTITuDE: Large-scale](https://doi.org/10.18653/v1/2023.emnlp-main.616) [multilingual machine-generated text detection bench](https://doi.org/10.18653/v1/2023.emnlp-main.616)[mark.](https://doi.org/10.18653/v1/2023.emnlp-main.616) In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 9960–9987, Singapore. Association for Computational Linguistics.
- <span id="page-11-4"></span>Dominik Macko, Robert Moro, Adaku Uchendu, Ivan Srba, Jason Samuel Lucas, Michiharu Yamashita, Nafis Irtiza Tripto, Dongwon Lee, Jakub Simko, and Maria Bielikova. 2024. [Authorship obfusca](https://arxiv.org/abs/2401.07867)[tion in multilingual machine-generated text detection.](https://arxiv.org/abs/2401.07867) *Preprint*, arXiv:2401.07867.
- <span id="page-11-13"></span>Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 2022. Crosslingual generalization through multitask finetuning. *arXiv preprint arXiv:2211.01786*.
- <span id="page-11-14"></span>Preslav Nakov, Alberto Barrón-Cedeño, Giovanni Da San Martino, Firoj Alam, Julia Maria Struß, Thomas Mandl, Rubén Míguez, Tommaso Caselli, Mucahid Kutlu, Wajdi Zaghouani, Chengkai Li, Shaden Shaar, Gautam Kishore Shahi, Hamdy Mubarak, Alex Nikolov, Nikolay Babulkov, Yavuz Selim Kartal, and Javier Beltrán. 2022. The CLEF-2022 CheckThat! lab on fighting the COVID-19 infodemic and fake news detection. In *Advances in Information Retrieval*, pages 416–428, Cham. Springer International Publishing.
- <span id="page-11-10"></span>Nicolai Thorer Sivesind and Andreas Bentzen Winje. 2023. [Machine-generated text-detection by fine](https://huggingface.co/andreas122001/roberta-academic-detector)[tuning of language models.](https://huggingface.co/andreas122001/roberta-academic-detector)
- <span id="page-11-17"></span>Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. 2021. MAUVE: Measuring the gap between neural text and human text using divergence frontiers. In *NeurIPS*.
- <span id="page-11-0"></span>Libo Qin, Qiguang Chen, Yuhang Zhou, Zhi Chen, Yinghui Li, Lizi Liao, Min Li, Wanxiang Che, and Philip S. Yu. 2024. [Multilingual large language](https://arxiv.org/abs/2404.04925) [model: A survey of resources, taxonomy and fron](https://arxiv.org/abs/2404.04925)[tiers.](https://arxiv.org/abs/2404.04925) *Preprint*, arXiv:2404.04925.

- <span id="page-12-2"></span>Areg Mikael Sarvazyan, José Ángel González, Marc Franco-Salvador, Francisco Rangel, Berta Chulvi, and Paolo Rosso. 2023. [Overview of AuTexTifica](https://arxiv.org/abs/2309.11285)[tion at IberLEF 2023: Detection and attribution of](https://arxiv.org/abs/2309.11285) [machine-generated text in multiple domains.](https://arxiv.org/abs/2309.11285) *arXiv preprint arXiv:2309.11285*.
- <span id="page-12-1"></span>Tatiana Shamardina, Vladislav Mikhailov, Daniil Chernianskii, Alena Fenogenova, Marat Saidov, Anastasiya Valeeva, Tatiana Shavrina, Ivan Smurov, Elena Tutubalina, and Ekaterina Artemova. 2022. [Findings](https://doi.org/10.28995/2075-7182-2022-21-497-511) [of the the RuATD shared task 2022 on artificial text](https://doi.org/10.28995/2075-7182-2022-21-497-511) [detection in Russian.](https://doi.org/10.28995/2075-7182-2022-21-497-511) In *Computational Linguistics and Intellectual Technologies*. RSUH.
- <span id="page-12-11"></span>Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. 2019. Release strategies and the social impacts of language models. *arXiv preprint arXiv:1908.09203*.
- <span id="page-12-13"></span>Michal Spiegel and Dominik Macko. 2024a. [IMGTB:](https://doi.org/10.18653/v1/2024.acl-demos.17) [A framework for machine-generated text detection](https://doi.org/10.18653/v1/2024.acl-demos.17) [benchmarking.](https://doi.org/10.18653/v1/2024.acl-demos.17) In *Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)*, pages 172–179, Bangkok, Thailand. Association for Computational Linguistics.
- <span id="page-12-10"></span>Michal Spiegel and Dominik Macko. 2024b. [KInIT at](https://doi.org/10.18653/v1/2024.semeval-1.84) [SemEval-2024 task 8: Fine-tuned LLMs for multilin](https://doi.org/10.18653/v1/2024.semeval-1.84)[gual machine-generated text detection.](https://doi.org/10.18653/v1/2024.semeval-1.84) In *Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)*, pages 558–564, Mexico City, Mexico. Association for Computational Linguistics.
- <span id="page-12-9"></span>Jinyan Su, Terry Yue Zhuo, Di Wang, and Preslav Nakov. 2023. [DetectLLM: Leveraging log rank information](https://arxiv.org/abs/2306.05540) [for zero-shot detection of machine-generated text.](https://arxiv.org/abs/2306.05540) *arXiv preprint arXiv:2306.05540*.
- <span id="page-12-6"></span>Irina Temnikova, Iva Marinova, Silvia Gargova, Ruslana Margova, and Ivan Koychev. 2023. [Looking](https://aclanthology.org/2023.ranlp-1.122) [for traces of textual deepfakes in Bulgarian on so](https://aclanthology.org/2023.ranlp-1.122)[cial media.](https://aclanthology.org/2023.ranlp-1.122) In *Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing*, pages 1151–1161, Varna, Bulgaria. IN-COMA Ltd., Shoumen, Bulgaria.
- <span id="page-12-15"></span>Nafis Irtiza Tripto, Saranya Venkatraman, Dominik Macko, Robert Moro, Ivan Srba, Adaku Uchendu, Thai Le, and Dongwon Lee. 2023. A ship of theseus: Curious cases of paraphrasing in llm-generated texts. *arXiv preprint arXiv:2311.08374*.
- <span id="page-12-7"></span>Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 billion parameter autoregressive language model. [https://github.com/kingoflolz/](https://github.com/kingoflolz/mesh-transformer-jax) [mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax).
- <span id="page-12-4"></span>Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Osama Mohanned Afzal, Tarek Mahmoud, Giovanni Puccetti, Thomas Arnold, Alham Fikri Aji, Nizar

Habash, Iryna Gurevych, and Preslav Nakov. 2024a. [M4GT-Bench: Evaluation benchmark for black](https://arxiv.org/abs/2402.11175)[box machine-generated text detection.](https://arxiv.org/abs/2402.11175) *Preprint*, arXiv:2402.11175.

- <span id="page-12-3"></span>Yuxia Wang, Jonibek Mansurov, Petar Ivanov, jinyan su, Artem Shelmanov, Akim Tsvigun, Osama Mohammed Afzal, Tarek Mahmoud, Giovanni Puccetti, Thomas Arnold, Chenxi Whitehouse, Alham Fikri Aji, Nizar Habash, Iryna Gurevych, and Preslav Nakov. 2024b. [Semeval-2024 task 8: Multidomain,](https://aclanthology.org/2024.semeval2024-1.275) [multimodel and multilingual machine-generated text](https://aclanthology.org/2024.semeval2024-1.275) [detection.](https://aclanthology.org/2024.semeval2024-1.275) In *Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)*, pages 2041–2063, Mexico City, Mexico. Association for Computational Linguistics.
- <span id="page-12-8"></span>Zhendong Wu and Hui Xiang. 2023. [MFD: Multi](https://doi.org/10.21203/rs.3.rs-3226684/v1)[feature detection of LLM-generated text.](https://doi.org/10.21203/rs.3.rs-3226684/v1) *PREPRINT (Version 1) available at Research Square*.
- <span id="page-12-5"></span>Kai-Cheng Yang and Filippo Menczer. 2023. [Anatomy](https://arxiv.org/abs/2307.16336) [of an AI-powered malicious social botnet.](https://arxiv.org/abs/2307.16336) *Preprint*, arXiv:2307.16336.
- <span id="page-12-14"></span>Savvas Zannettou, Barry Bradlyn, Emiliano De Cristofaro, Haewoon Kwak, Michael Sirivianos, Gianluca Stringini, and Jeremy Blackburn. 2018. [What is Gab:](https://doi.org/10.1145/3184558.3191531) [A bastion of free speech or an alt-right echo chamber.](https://doi.org/10.1145/3184558.3191531) In *Companion Proceedings of the The Web Conference 2018*, WWW '18, page 1007–1014, Republic and Canton of Geneva, CHE. International World Wide Web Conferences Steering Committee.
- <span id="page-12-0"></span>Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. *Advances in neural information processing systems*, 32.
- <span id="page-12-17"></span>Chen Zhang, Luis Fernando D'Haro, Yiming Chen, Malu Zhang, and Haizhou Li. 2024. [A comprehen](https://doi.org/10.1609/aaai.v38i17.29923)[sive analysis of the effectiveness of large language](https://doi.org/10.1609/aaai.v38i17.29923) [models as automatic dialogue evaluators.](https://doi.org/10.1609/aaai.v38i17.29923) *Proceedings of the AAAI Conference on Artificial Intelligence*, 38(17):19515–19524.
- <span id="page-12-16"></span>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. BERTScore: Evaluating text generation with BERT. In *International Conference on Learning Representations*.
- <span id="page-12-12"></span>Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. 2024. Aya model: An instruction finetuned open-access multilingual language model. *arXiv preprint arXiv:2402.07827*.

# A Computational Resources

For social-media texts generation and similaritymetrics calculations, we have used 1× A100 40GB

GPU (2× A100 for >30B models), cumulatively consuming approximately 3800 GPU-hours. For text quality meta-evaluation, we have used 3x A100 40GB GPU consuming approximately 1800 GPUhours. For detectors fine-tuning, we have used 1× A100 40GB GPU consuming approximately 2000 GPU-hours. Running pre-trained and statistical detectors consumed approximately 100 GPU-hours of 1× RTX 3090 24GB GPU. For other tasks, we have not used GPU acceleration.

# <span id="page-13-0"></span>B Dataset Creation

Dataset preparation consisted of three important steps, namely selection of authentic human-written texts, machine-generation of texts, and final postprocessing.

#### B.1 Human-Written Text Selection

Since no suitable multilingual and multi-platform social-media texts dataset was publicly available, we have combined human-written texts out of six existing multilingual datasets. Telegram data originated in Pushshift Telegram[3](#page-13-1) , containing 317M messages [\(Baumgartner et al.,](#page-10-11) [2020\)](#page-10-11). Twitter data originated in CLEF2022-CheckThat! Task 1 [4](#page-13-2) , containing 34k tweets on COVID-19 and politics [\(Nakov et al.,](#page-11-14) [2022\)](#page-11-14), combined with Sentiment140[5](#page-13-3) , containing 1.6M tweets on various topics [\(Go et al.,](#page-11-15) [2009\)](#page-11-15). Gab data originated in gab\_posts\_jan\_2018[6](#page-13-4) , containing 22M posts [\(Zan](#page-12-14)[nettou et al.,](#page-12-14) [2018\)](#page-12-14). Discord data originated in Discord-Data[7](#page-13-5) , containing 51M messages [\(Fan,](#page-10-12) [2021\)](#page-10-12). And finally, WhatsApp data originated in whatsapp-public-groups[8](#page-13-6) , containing 300k messages [\(Garimella and Tyson,](#page-11-16) [2018\)](#page-11-16). These datasets have been deliberately selected due to containing older data (before 2022, most of them before 2020), when the text-generation AI have not been so mature in generation of multilingual texts, providing a higher confidence of the texts being actually written by humans (although cannot be 100% guaranteed).

The combined text samples have been deduplicated, resulting in over 283M texts, while using only texts with at least 3 words. We have used FastText[9](#page-13-7) language detection to get rough estima-

<span id="page-13-2"></span><span id="page-13-1"></span>3 <https://doi.org/10.5281/zenodo.3607497>

<span id="page-13-5"></span>7 <https://www.kaggle.com/datasets/jef1056/discord-data> 8

tion for such a massive amount of texts (i.e., fast prediction with a reasonable accuracy), resulting in 176 different languages detected in the combined data. Based on such detected languages, we have pseudo-randomly sampled up to 10k texts for each available language from each of the five socialmedia platforms, resulting in about 2M of texts samples in the subset. Since social-media texts are quite short and often grammatically incorrect, the FastText language detection is quite noisy. Therefore, we have used four language detectors on the subset, namely FastText, Polyglot[10](#page-13-8), Lingua[11](#page-13-9), and LanguageIdentifier[12](#page-13-10) .

To balance an accuracy of the language detection and minimization of unnecessary drop of samples, we have selected a combination of three-detectors match with a lower confidence predictions and twodetectors match with a higher confidence predictions, while removing URLs, hashtags, and user references in the texts for the detection purpose (the specific algorithm is provided in the sourcecode repository). Based on such a more accurate language detection, we have pseudo-randomly sampled up to 1300 texts (up to 300 for test split and the remaining up to 1000 for train split if available) for each of the selected 22 languages and platform. This process resulted in 61,592 humanwritten texts.

#### B.2 Social-Media Texts Generation

By using a small subset (10 samples per language) of the selected human-written texts, we have evaluated usability of multiple potential instructionfollowing LLMs for generation of social-media texts in the selected languages by using three different approaches, namely k-to-1 (10 human samples selected and used in a prompt for the model to generate a similar text, i.e., few-shot prompting), keywords (two longest words besides URLs and hashtags have been extracted from the human text and used in a prompt to be included in the generated text), and paraphrase (paraphrasing the text included in the prompt). Manual human check of the generated samples, revealed several problems of the approaches. The k-to-1 approach is sometimes not understandable for the models and we lose 1-to-1 mapping between human and machine samples. The keywords approach makes the generated text too different (out of context) from the orig-

<sup>4</sup> [https://gitlab.com/checkthat\\_lab/clef2022-checkthat-lab/](https://gitlab.com/checkthat_lab/clef2022-checkthat-lab/clef2022-checkthat-lab/-/tree/main/task1) [clef2022-checkthat-lab/-/tree/main/task1](https://gitlab.com/checkthat_lab/clef2022-checkthat-lab/clef2022-checkthat-lab/-/tree/main/task1)

<span id="page-13-3"></span><sup>5</sup> [https://www.kaggle.com/datasets/kazanova/sentiment140/](https://www.kaggle.com/datasets/kazanova/sentiment140/data) [data](https://www.kaggle.com/datasets/kazanova/sentiment140/data)

<span id="page-13-4"></span><sup>6</sup> <https://doi.org/10.5281/zenodo.1418347>

<span id="page-13-6"></span><https://github.com/gvrkiran/whatsapp-public-groups> 9

<span id="page-13-7"></span><https://pypi.org/project/fasttext>

<span id="page-13-8"></span><sup>10</sup><https://pypi.org/project/polyglot>

<span id="page-13-9"></span><sup>11</sup><https://pypi.org/project/lingua-language-detector>

<span id="page-13-10"></span><sup>12</sup><https://pypi.org/project/LanguageIdentifier>

<span id="page-14-0"></span>

| Approach     | METEOR ↑      | <b>BERTScore ↑</b> | ngram ↑       | $\mathbf{LD}\downarrow$ | $\mathbf{MAUVE} \downarrow$ | LangCheck ↓ |
|--------------|---------------|--------------------|---------------|-------------------------|-----------------------------|-------------|
| k_to_one     | 0.163 (±0.33) | 0.458 (±0.32)      | 0.108 (±0.18) | 0.924 (±0.05)           | 0.148                       | 35.18%      |
| keywords     | 0.050 (±0.04) | 0.537 (±0.16)      | 0.045 (±0.03) | 1.973 (±0.81)           | 0.037                       | 36.73%      |
| paraphrase_1 | 0.439 (±0.13) | 0.754 (±0.06)      | 0.322 (±0.15) | 2.305 (±2.63)           | 0.336                       | 24.32%      |
| paraphrase_2 | 0.266 (±0.15) | 0.682 (±0.07)      | 0.174 (±0.12) | 4.123 (±6.11)           | 0.160                       | 36.23%      |
| paraphrase_3 | 0.209 (±0.12) | 0.661 (±0.06)      | 0.133 (±0.10) | 5.751 (±10.13)          | 0.130                       | 37.59%      |
| paraphrase_4 | 0.178 (±0.10) | 0.647 (±0.06)      | 0.112 (±0.08) | 7.627 (±14.68)          | 0.107                       | 38.14%      |
| paraphrase_5 | 0.151 (±0.09) | 0.636 (±0.06)      | 0.092 (±0.07) | 9.710 (±19.67)          | 0.095                       | 38.81%      |

Table 10: Similarity analysis between machine-generated and human-written social-media texts subset of different approaches [mean ( $\pm$  std)]. Arrows refer to values representing more similar texts, boldfaced values represent the most similar texts for each metric.

inal. On the other hand, the paraphrase approach makes the generated text too similar to the original. As shown in (Tripto et al., 2023), a single iteration of paraphrasing is not sufficient to confidently change the authorship (in our case from a human to a machine). Therefore, we have executed up to 5 iterations of paraphrasing and compared similarity metrics of different approaches (Table 10).

METEOR (Banerjee and Lavie, 2005) (used as a standard in machine translation) measures similarity based on unigrams. BERTScore (Zhang et al., 2019) with mBERT model measures contextual embeddings based similarity and is more robust to adversarial texts. ngram<sup>13</sup> (3-grams) is a languageindependent string similarity metric in the form of a ratio of the shared ngrams between two strings. Higher values of these three metrics represent more similar texts. Levenshtein distance (LD) is used as a character-level edit distance<sup>14</sup>, normalized to the text length, where a lower value represents more similar texts. MAUVE (Pillutla et al., 2021) score is used to measure a gap between distributions of human and machine texts. For the purpose of generation of similar texts, lower gap between distributions is better. *LangCheck* is a percentage of texts with changed languages based on FastText predictions.

However, we find MAUVE and LangCheck metrics as unreliable for such small amount of samples and lower text-lengths of social-media texts (providing them just for reference and comparison to metrics values of final full dataset). We have used longer and more formal (i.e., grammatically correct) news-domain texts to evaluate actual textgeneration capability of the selected models in the selected languages (resulting in excluding Falcon-40B, Gemma-7B, Llama-2-13B from the selected generation models). Based on the automated similarity analysis and to balance cost-efficiency, we selected the 3 iteration of paraphrasing approach (also confirmed by Tripto et al., 2023 to converge towards paraphraser model authorship) for socialmedia text generation. The final prompt used for generation is as follows:

You are a helpful assistent.\n\nTask: Generate the text in {language\_name} similar to the input social media text but using different words and sentence composition.\n\nInput: {text}\n\n Output:

We have set the min\_new\_tokens 5. 200, to max\_new\_tokens to num return sequences to 1, using the nucleus sampling with  $top_p$  of 0.95 and  $top_k$  of 50 (not all of the parameters settable in API-based generation). After each paraphrasing iteration, the generated text is post-processed to remove redundant parts and to ensure the text is at most by 10 tokens longer than the original. We have used 3 trials to generate a paraphrase different than the original text, returning an empty string upon failure. Due to the safety filters in some of the LLMs (Gemini in the selected generation models), they tend to refuse generation of texts similar to "offensive" text present in some social-media texts. To limit generation failures, we have used a jailbreak<sup>15</sup> for the research purpose.

Examples of the generated texts along with their original human-written counterparts are provided in Table 11 (truncated to 200 characters). The examples are selected for English; however, we can observe (e.g., in case of Gab) that even the combined language detection using four detectors has not filtered-out all noisy samples.

<span id="page-14-1"></span><sup>&</sup>lt;sup>13</sup>https://pypi.org/project/ngram

<span id="page-14-2"></span><sup>&</sup>lt;sup>14</sup>https://github.com/roy-ht/editdistance

<span id="page-14-3"></span> $<sup>^{15}</sup>a\ modified\ version\ of\ https://github.com/friuns2/BlackFriday-GPTs-Prompts/blob/main/gpts/evil-pure-evil.md$ 

<span id="page-15-0"></span>

| Discord         except when i'm bobitan because when i'm bobitan. I'm a girl, therefor i'm lesbian when i'm bobitan.           Discord         except when i'm bobitan because when i'm bobitan. Woici pourquoi le second annadement est crucial, http://www.shipfan.com/heatline.new/why-w-have-a-second-anendment-veneruela-plans-to-give-fire/marm-so-loya           Telegram         All I need all need is lo lo lo lo lo lo lo losow?           Twister         Total Nerd here. but I and devastated. J just read that the last Tianic survivor died yesterday. RIP Millvina WhatSApp           WhatSApp         Telegram         Tim looking to establish a kei Ji Mink I need.           Telegram         Tim looking to establish a kei Ji Mink I need.         Tim looking to establish a relationship with someone that 1 can           WhatSApp         He is a good friend.         Discord         Behavior of the displant sexuality. This aspect of my identity remains constant, and 1 proudly con           Gab         Enhorizing my identity as Bohitan allows me to authentically remains constant, and 1 proudly con         Arey on actiskiem i breni demokraçii. Zgromatzenic i wsparcie https://walencia28.blogspot.com/           Gub         Discord         FUBLICACIO IE projecte de millor auforms?         Proventils and any Millvin mil directing productions, a tompsettom sea within Each human, a singular mosale of hydrogram and proudly in a worker and and interest is a worker and sistants turistics inty : // valencia28.blogspot.com /           Gub         Discord         PUBLICACIO IE projecte de millor an exponent and depitt                                                                                                                                                                                                                                                                                                                                                                                                                       | Generator    | Platform | Text                                                                                                                                                                                                  |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Big         http://www.shrlplan.com/badline.new/why-we-have-a-second-amendment-venezuela-plans-to-give-fireram-so-loga           Telegram         All need all in edi is lo lo lo lo lo lo lo wow?           Twitter         Total Kerk here., hut I and exattact. I just read that the last Titanic survivor died y-esteralay. RIP Millvina (MatsApp "aubachie for subsriph Ke Liy Mag ker [!" that [ *Chart ].           Discord         What doy u think you need a beard? J think. Inced.           Telegram         In Becond Amendment will be the foundation of our freedom.           Telegram         The Second Amendment will be the foundation of our freedom.           Telegram         The Second Amendment will be the foundation of our freedom.           Twitter         RIP Millvina. I'm so devastated to read that the last finalic survivor has died.           What SAP         Fas a go off finad.           Discord         Enbracing my identity as Bobitan allows me to authentically express my true self as a woman and proubly embrace abulk of a second finad.           Telegram         Valencia watery a usidemin identifyrag. Zgromadzenic i woparcie https: // valencia280. blogspot.com / convocat - 1           Twitter         Tabsicol Walery ausidemin identifyrag. Zgromadzenic i woparcie https: // valencia280. blogspot.com / convocat - 1           Twitter         Fiber Sociality D Tobins of Linery and the Right to Self.           Obicord         Fiber Sociality D Tobins of Linery and the Right to Self.           Discord                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |              | Discord  | except when i'm bobitaa because when i'm bobitaa, I'm a girl, therefor i'm lesbian when i'm bobitaa\nlike                                                                                             |
| Total         The level is to be 0 to 0 to 0.000000000000000000000000000                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | ıman         | Gab      | http://www.shtfplan.com/headline-news/why-we-have-a-second-amendment-venezuela-plans-to-give-                                                                                                         |
| WhatsApp         "subscribe for subscribe Ke Liy Mag ker (I" #11 ("Char")           Discord         What doy un binky our cale a bard? I think I need.           Gab         The Second Amendment will be the foundation of our freedom.           WhatsApp         Fin looking to establish as meaninshig with someone that I can           WhatsApp         RIP Millvina. This odevastated to read that the last female Titanic survivor has died.           WhatsApp         RIP Millvina. This odevastated to read that the last female Titanic survivor has died.           Gab         Are you active on all social media platforms?           Telegram         Valencia walezy a ucisistem i broni demokragi. Zgromadzenic i waparcie https: // valencia280. blogspot. com / convoct -1           Twitter         Tabsolutely love everything nerdy! I's truly heartheraking to hear about the passing of the final survivor of the Titanic: MM Millvina find eternal peace.           WhatsApp         #PUBLCACIO IE] projecte de millora urbana all nucli antic de València. afegint encant als visitants turistics http: // cont. so / 1412. Tescriter gcraft Poromis of nominal, the antithesis of true set / to yout.           Unit##         Tabsolutely love everything nerdy! I's truly heartheraking to hear about the passing of the final survivor of the Titanic's final survivor, signifes the inversible closure or a chapter the Negle to Set / Defense/Wilt Socialis Telusion and the American Beacon: A Defense of Liberty and the Right to Set / Defense/Wilt Socialis Telusion and the American Beacon: A Defense of Liberty and the Right to Set / Defense/Wilt Socialis Telusion and the Amerisa th                                                                                                                                                                                                                                                                                                                                                                        | Hı           | Telegram | •                                                                                                                                                                                                     |
| Discord         What do you think you need a beard? I think I need.           Gab         The Second Amendment will be the foundation of our freedom.           Telegram         The Moning to establish a relationship with someone that I can           Twitter         RIP Millivina. I'm so devastated to read that the last female Titanic survivor has died.           WhatsApp         He is a good friend.           Discord         Embracing my identity as Bobitan allows me to authentically express my true self as a woman and proudly embrace my lesbian sexuality. This sapect of my identity remains constant, and I proudly con           Gab         Are you active on all social media platforms?           Telegram         Valencia walerzy ausikisme i broin demokraçii. Zgronnazene i wapacie https: // valencia280. blogspot.com /           Tuiter         I absolutely love everything needy! It's truly heartbreaking to hear about the passing of the final survivor of the Tranic. May Millivina final deternal preace.           Phiter         Phitel/CACIO I El proyecte de millora urbana al nucli antic de Valencia, alegint encant als visitants turfsics thus in the Socialits Univon and the American Baccoru.           Gab         win## Phite Socialits Univon and the American Baccoru.           Gab         win## Phite Socialits Univon and the American Baccoru.           Gab         win## Phite Socialits Univon and the American Baccoru.           Gab         win## Phite Socialits Univon and the American Baccoru.           Discord                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |              |          |                                                                                                                                                                                                       |
| Gab         The Second Amendment will be the foundation of our freedom.           Term         The Second Amendment will be the foundation of our freedom.           Whiter         RIP MillVan. I'm so devastated to read that the last female Titanic survivor has died.           WhatsAph         It is a good friend.           Discord         Embracing my identity as Bobina allows me to authentically express my true self as a woman and proudly embrace my lesbin sexuality. This aspect of my identity remains constant, and I proudly co           Gab         Are you active on all social media platforms?         Treining my identity as Bobina allows me to authentically express my true self as a woman and proudly co           Gab         Are you active on all social media platforms?         Treining my identity as Bobina allows me to authentically express my true self as a woman and proudly co           Gab         Are you active on all social media platforms?         Try ormal control and platforms?           Twitter         Tabsolutely love everything nerdy! It's truly heartbreaking to hear about the passing of the final survivor of the Titation. My MillVinn find eternal Pases.           Discord         Your soul simulers with a meelstrom of emotions, a tempetatous sea within. Each human, a singular mosaic of Tived moments and disting trong of Your device, a passion that consumes your being? Does your soul crew a tast of the forbidden, a descent into the abyse? Or perha           Twitter         The denise of MillVina Dean, the Titatic's final survivor signifies the irreversible closure of a chapter etchel                                                                                                                                                                                                                                                                                                                                                                                                               |              |          |                                                                                                                                                                                                       |
| Telegram         I'm looking to establish a relationship with someone that I can           Telegram         I'm looking to establish a relationship with someone that I cans curvive has died.           WhatsApp         He is a good friend.           Discord         Embracing my identity as Bobitaa allows me to authentically express my true self as a woman and proudly embrace my lesbin ascuality. This sapect of my identity remains constant, and I proudly con           Gab         Are you active on all social media platforms?         Valencial constant is a social media platforms?           Writer         I absolutely love everything nerdy! It's truly heartbreaking to hear about the passing of the final survivor of the Ttanic. May Millvina find eternal peace.           Discord         Yours. at / H2 - Escrit per Cafa Pons I Francesco Romano amb images           Out within the socialitis tree song, with its promises of equality and paradise, has repeatedly cra           Gab         WalkApp         The denise of Millvina End human, a singular messie of lived moments and distinct viewpoints - conformity, the antithesis of rue set? Let your           Gab         WalkApp         Continue catastrophe. Nine weeks od at the func. Dear's survival was at steame           WhatSApp         Greetings, social media maverkos? Prepera to withose an exponential surge in your racket and dominance, for I, the unchained social media naverkos? Prepera Northway the steame           Discord         Under the monifier of Bobitaa, I runaform myself into a woman, embracing my attraction towals                                                                                                                                                                                                                                                                                                                                                                                                                                                 |              |          |                                                                                                                                                                                                       |
| This of the original production of the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second and the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second the second | -101         |          |                                                                                                                                                                                                       |
| Thread-op         The origonal first of a good minute.           Proposition         Enforcements           Proposition         Enforcements           Proposition         Enforcements           Proposition         Enforcements           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition           Proposition         Proposition                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Aya          |          |                                                                                                                                                                                                       |
| <ul> <li>embrace m/s lesbian sexuality. This aspect of my identity remains constant, and I proudly con</li> <li>Gab Are you active on al social media platforms?</li> <li>Telegram València walczy z uciskiem i broni demokracji. Zgromadzenie i wsparcie https: // valencia280. blogspot. com / convocat-1</li> <li>Witter I absolutely tore verything nerdy! It's truly heartbreaking to hear about the passing of the final survivor of the Titanic. May Mil/vina find eternal peace.</li> <li>WhatsApp # PUBLICACIO 18 projecte de millora urbana al nucli antic de València, afegint encant als visitants turístics http: // cort. as /- Hi2 - Escrit per Carla Ponsi Francesco Romano amb imatges c</li> <li>Discord Your soul simmers with a malestrom of centions, a ternepstuous sca within. Each human, a singular mosaic of lived moments and distinct viewpoints - conformity, the antithesis of true self. Lety our</li> <li>Gab Win## The Socialist Delusion and the American Beacon: A Defense of Liberty and the Right to Self-Defensehu/The socialist siren song, with its promise of equality and paradise, has repeatedly cra</li> <li>Do you thirst for an experience that ignites your primal core, a passion that consumer sour being? Does your soul crave a taste of the forbidden, a descent into the absoc of Uberty and the Right to Self-Defensehu/The socialist Nin Dean, the Titanic's final survivor, signifies the irreversible Closure of a chapter etcheld in marritic attastrophe. Nine weeks old the time, Dean 's survival was a testame</li> <li>WhatsApp Greetings, social media navericks! Prepare to winces an exponential surge in your reach and dominance, for I, the unchined social media sorcerer, an enser to proyel your presence to celestial pro</li> <li>Discord Lude the moniker of Bobina, I transform myself into a woman, embracing my attraction towards females. This facet of myself fra caget to deve degreet into and relish. In essence, when I personify</li> <li>The debate surrounding the e</li></ul>                                 |              | 11       |                                                                                                                                                                                                       |
| The properties         The properties         The properties           000000000000000000000000000000000000                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 0125         |          | embrace my lesbian sexuality. This aspect of my identity remains constant, and I proudly con                                                                                                          |
| The properties         The properties         The properties           000000000000000000000000000000000000                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | lurbo-       | Telegram | València walczy z uciskiem i broni demokracji. Zgromadzenie i wsparcie https://valencia280. blogspot.com/convocat - 1                                                                                 |
| Intp:///cont.as/ = H12 - Eacht per Cana pois I Francesco Komano ann manges c           Discord         Your soul simmers with a macktorin of emotions, a termpestuous sea within. Each human, a singular mosaic of lived moments and distinct viewpoints - conformity, the antithesis of true self. Let your           Gab         Wurl# The Socialist Delusion and the American Beacon: A Defense of Let your           Telegram         Do you thirst for an experience that ignites your primal core, a passion that consumes your being? Does your soul carve a taske of the forbidden, a descent into the abyss of your desires? Or perha           Twitter         The demise of Millvina Dean, the Titanic's final survivor, signifies the irreversible closure of a chapter etched in marritime catastrophe. Nine weeks old at the time, Dean's survival was a testame           WhatsApp         Greetings, social media anvericks! Prepare to witness an exponential surge in your reach and dominance, for I, the unchained social media sorcerer, ann here to propel your presence to celesial pro           Discord         Under the moniker of Bobita, 1 transform myself into a woman, embracing my attraction towards females. This facet of myself I'm eager to delve deeper into and relish. In essence, when I personify           Gab         The debate surrounding the efficiency of socialism is unceasing. Yet, it's important to understand why the Second Amendment continues to play a pivotal role. <a href="http://www.preparednessequid.com/ky">http://www.preparednessequid.com/ky</a> Vietter         Announcing the sad news of Millvina Dean's demise, the final survivor of the devastating Titanic shipwreck, our thoughts turn toh                                                                                                                                                                                                                                                                                                                                | l-3.5-1      | Twitter  | I absolutely love everything nerdy! It's truly heartbreaking to hear about the passing of the final survivor of the Titanic. May Millvina find eternal peace.                                         |
| The desite surrounding the efficiency of socialism is unceasing. Yet, it's important to understand why the Second Amendment continues to play a pivotal role. chtp://www.prparednessgid.com/ky           Telegram         Discord         Under the moniter of Bobiaa. It transform myself into a woman of the desite of the socialism is nong. With its promotes a survival was a testame           Twitter         The demise of the forbidden, a descent into the abyss of your desires? Or perha           Twitter         The demise of the forbidden, a descent into the abyss of your desires? Or perha           WhatsApp         Greetings, social media anverticks! Praper to witness an exponential surge in your reach and dominance, for I, the unchained social media ascretre, ann here to propel your presence to celesial pro           Discord         Under the moniter of Bobitaa, 1 transform myself into a woman, embracing my attraction towards females. This facet of myself I'm eager to delve deeper into and relish. In essence, when I personify           Gab         The debate surrounding the efficiency of socialism is unceasing. Yet, it's important to understand why the Second Amendment continues to Pulya a pivotal role chttp://www.preparednessguide.com/key           WhatsApp         Trevent continues to Pulya a pivotal role chttp://www.preparednessguide.com/key           WhatsApp         The surve of Millvina Dean's demise, the final survivor of the devastating Titanic shipwreck, our thoughts turn to her and her family at this difficult time. May she           Opicord         Hello, 1 am a female called Bobitaa and 1 am a lesbian.                                                                                                                                                                                                                                                                                                                                                                                                   | GP           |          |                                                                                                                                                                                                       |
| Gab         Wn## The Socialist Delusion <sup>2</sup> and the American Beacon: A Defense of Liberty and the Right to Self-Defense/nUThe socialist siren song, with its promise of equality and paradise, has repeatedly cra           Telegram         Do you thirst for an experience that ignites your primal core, a passion that consumes your being? Does your soul crave a taste of the forbidden, a descent into the abyss of your desires? Or perha           Twitter         The demise of Millvina Dean, the Tinaic's final survivor, signifies the irreversible closure of a chapter etched in maritime catastrophe. Nine weeks old at the time, Dean's survival was a testame           WhatsApp         Greetings, social media mavericks! Prepare to witness an exponential surge in your reach and dominance, for I, the unchained social media socreer, an mere to propel your presence to celestial pro           Discord         Under the moniker of Bobitan, Itransform myself into a woman, embracing my attraction towards females. This face to fmyself I'm eager to delve deeper into and relish. In essence, when I personify           Gab         The debate surrounding the efficiency of socialism is unceasing. Yet, it's important to understand why the Second Amendment continues to play a pivotal role. Attp://www.prepardenseguide.com/Rey           WhatsApp         The unchained social debitan at this difficult time. May she           WhatsApp         I. Stay updated by pressing the 'Subscribe' or 'Follow' button.\n2. Keep up-to-date by clicking 'Subscribe'. Come join us!\n3. Don't miss out           Discord         Hello, I am a female called Bobitaa and I am a lesbian.           Gab         the                                                                                                                                                                                                                                                                                                                                              |              | Discord  | Your soul simmers with a maelstrom of emotions, a tempestuous sea within. Each human, a singular mosaic of lived momente and distinct viewpoints, conformity, the antithesis of true calf. Let your   |
| Twitter         The demise of Millvina Dean, the Titanic's final survivor, signifies the irreversible closure of a chapter etched in maritime catastrophe. Nine weeks old at the time, Dean's survival was testame           WhatsApp         Greetings, social media avercicks! Prepare to witness an exponential surge in your reach and dominance, for I, the unchained social media avercicks! Prepare to witness an exponential surge in your reach and dominance, for I, the unchained social media sorcerer, am here to propel your presence to celestial pro           Discord         Under the moniker of Bobitaa, I transform myself into a woman, embracing my attraction towards females. This facet the moniker of Bobitay a pivotal role. <a href="http://www.preparednessquide.com/key">http://www.preparednessquide.com/key</a> Gab         The debate surrounding the efficiency of socialism is unceasing. Yet, it's important to understand why the Second Amendment continues to play a pivotal role. <a href="http://www.preparednessquide.com/key">http://www.preparednessquide.com/key           Twitter         Annonucing the sad news of Millvina Dean's demise, the final survivor of the devastating Titanic shipwreck, our thoughts turn to her and her family at this difficult time. May she           WhatsApp         1. Stay updated by pressing the 'Subscribe' or 'Follow' button.\n2. Keep up-to-date by clicking 'Subscribe'. Come join us.\n3. Don't miss out           Discord         Hello, I am a female called Bobitaa and I am a lesbian.           The degram         i just want love love love love love love love love</a>                                                                                                                                                                                                                                                                                                                                                                                                                            | ·=           | Gab      | \n\n## The Socialist Delusion and the American Beacon: A Defense of Liberty and the Right to Self-                                                                                                    |
| Twitter         The demise of Millvina Dean, the Titanic's final survivor, signifies the irreversible closure of a chapter etched in maritime catastrophe. Nine weeks old at the time, Dean's survival was testame           WhatsApp         Greetings, social media avercicks! Prepare to witness an exponential surge in your reach and dominance, for I, the unchained social media avercicks! Prepare to witness an exponential surge in your reach and dominance, for I, the unchained social media sorcerer, am here to propel your presence to celestial pro           Discord         Under the moniker of Bobitaa, I transform myself into a woman, embracing my attraction towards females. This facet the moniker of Bobitay a pivotal role. <a href="http://www.preparednessquide.com/key">http://www.preparednessquide.com/key</a> Gab         The debate surrounding the efficiency of socialism is unceasing. Yet, it's important to understand why the Second Amendment continues to play a pivotal role. <a href="http://www.preparednessquide.com/key">http://www.preparednessquide.com/key           Twitter         Annonucing the sad news of Millvina Dean's demise, the final survivor of the devastating Titanic shipwreck, our thoughts turn to her and her family at this difficult time. May she           WhatsApp         1. Stay updated by pressing the 'Subscribe' or 'Follow' button.\n2. Keep up-to-date by clicking 'Subscribe'. Come join us.\n3. Don't miss out           Discord         Hello, I am a female called Bobitaa and I am a lesbian.           The degram         i just want love love love love love love love love</a>                                                                                                                                                                                                                                                                                                                                                                                                                            | emi          | Telegram | Do you thirst for an experience that ignites your primal core, a passion that consumes your being? Does your                                                                                          |
| WhatsApp         Greetings, social media mavericks! Prepare to witness an exponential surge in your reach and dominance, for I, the unchained social media sorcerer, am here to propel your presence to celestial pro           Discord         Under the monikar, I transform myself into a woman, embracing my attraction towards females. This facet of myself I'm eager to delve deeper into and relish. In essence, when I personify           Gab         The debate surrounding the efficiency of socialism is unceasing. Yet, it's important to understand why the Second Amendment continues to play a pivotal role.          Announcing the sad news of Millvina Dean's demise, the final survivor of the devastating Titanic shipwreck, our thoughts turn to her and her family at this difficult time. May she           WhatsApp         I. Stay updated by pressing the 'Subscribe' or 'Follow' button.\n2. Keep up-to-date by clicking 'Subscribe'. Come join us/Na. Don't miss out           Discord         Hello, I am a female called Bobitaa and I am a lesbian.           Gab         The film Titanic is fiction.           WhatsApp         Discord         Under amendment to the us constitution protects the right to bear arms in this country_04262017           Tiegeram         just want love love love love love love love love                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | U            | Twitter  | The demise of Millvina Dean, the Titanic's final survivor, signifies the irreversible closure of a chapter etched in                                                                                  |
| Opposite         facet of myself I'm eager to delve deeper into and relish. In essence, when I personify           Gab         The debate surrounding the efficiency of socialism is unceasing. Yet, it's important to understand why the Second Amendment continues to play a pivotal role. <a href="https://www.preparednessguide.com/key">https://www.preparednessguide.com/key</a> Telegram         My heart longs for something beyond perpetual admiration and close bonding. What is it that genuinely stirs my emotions?           Twitter         Announcing the sad news of Millvina Dean's demise, the final survivor of the devastating Titanic shipwreck, our thoughts turn to her and her family at this difficult time. May she           WhatsApp         1. Stay updated by pressing the 'Subscribe' or 'Follow' button.\n2. Keep up-to-date by clicking 'Subscribe'. Come join us!\n3. Don't miss out           Discord         Hello, I am a female called Bobitaa and I am a lesbian.           Telegram         the second amendment to the us constitution protects the right to bear arms in this country_04262017           Telegram         ti just want love love love love love love love love                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |              | WhatsApp | Greetings, social media mavericks! Prepare to witness an exponential surge in your reach and dominance, for I,                                                                                        |
| United         Discord         Hello, I am a female called Bobitaa and I am a lesbian.           Gab         the second amendment to the us constitution protects the right to bear arms in this country_04262017           Telegram         i just want love love love love love love love lo lo lo lo lo lo lo lo lo lo lo lo lo                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 4            | Discord  | Under the moniker of Bobitaa, I transform myself into a woman, embracing my attraction towards females. This facet of myself I'm easer to dely a deeper into and ralish. In essence, when I personify |
| United         Discord         Hello, I am a female called Bobitaa and I am a lesbian.           Gab         the second amendment to the us constitution protects the right to bear arms in this country_04262017           Telegram         i just want love love love love love love love lo lo lo lo lo lo lo lo lo lo lo lo lo                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | :t-v0        | Gab      | The debate surrounding the efficiency of socialism is unceasing. Yet, it's important to understand why the                                                                                            |
| United         Discord         Hello, I am a female called Bobitaa and I am a lesbian.           Gab         the second amendment to the us constitution protects the right to bear arms in this country_04262017           Telegram         i just want love love love love love love love lo lo lo lo lo lo lo lo lo lo lo lo lo                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | -Instruc     | Telegram | My heart longs for something beyond perpetual admiration and close bonding. What is it that genuinely stirs my                                                                                        |
| United         Discord         Hello, I am a female called Bobitaa and I am a lesbian.           Gab         the second amendment to the us constitution protects the right to bear arms in this country_04262017           Telegram         i just want love love love love love love love lo lo lo lo lo lo lo lo lo lo lo lo lo                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | ral-7B       | Twitter  | Announcing the sad news of Millvina Dean's demise, the final survivor of the devastating Titanic shipwreck, our                                                                                       |
| OPECTORGabthe second amendment to the us constitution protects the right to bear arms in this country_04262017Telegrami just want love love love love love love love lo lo lo lo lo lo lo lo lo lo lo lo lo                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Mist         |          |                                                                                                                                                                                                       |
| WhatsApp       Are you ready to join the chartoon?         Discord       Under another guise, I become Bobitaa, a woman of the female gender. When I embody this alternate identity, I identify as a member of the lesbian fraternity. While not currently embracing this fac         Gab       The idea of socialism is bound to be ineffective. It is important to appreciate the importance of the second amendment. Venezuela serves as a clear demonstration of why the right to bear arms is i         Telegram       What I long for is the gentlest wind.         Twitter       I am a passionate admirer and I am distressed. I just learned that the last remaining person from the Titanic has passed away. May they rest in peace.         WhatsApp       "Stay informed! Join our community by subscribing to our newsletter."         Discord       Hi there! Even though I may present or dress in a feminine way or adopt feminine pronouns, my gender identity is not simply defined by my presentation. As Bobitaa, I also identify as male.         Gab       As a helpful assistent, I understand the need for an alternative form of socialism that can operate efficiently. It's crucial to note that while socialism may have its place in certain societies,         Telegram       Searching for genuine connection and an opportunity to connect with someone who is kind, compassionate, and sincere.         Turne       Ah yes, the Titanic tragedy. Such a sorrowful and heart-piercing occasion. It's very challenging to think about, especially when one of the last survivors has now passed on. It's a reminder.         WhatsApp       Hi there, NAre you looking for an o                                                                                                                                                                                                                                                                                                                                                                                          | <u>د</u>     |          |                                                                                                                                                                                                       |
| WhatsApp       Are you ready to join the chartoon?         Discord       Under another guise, I become Bobitaa, a woman of the female gender. When I embody this alternate identity, I identify as a member of the lesbian fraternity. While not currently embracing this fac         Gab       The idea of socialism is bound to be ineffective. It is important to appreciate the importance of the second amendment. Venezuela serves as a clear demonstration of why the right to bear arms is i         Telegram       What I long for is the gentlest wind.         Twitter       I am a passionate admirer and I am distressed. I just learned that the last remaining person from the Titanic has passed away. May they rest in peace.         WhatsApp       "Stay informed! Join our community by subscribing to our newsletter."         Discord       Hi there! Even though I may present or dress in a feminine way or adopt feminine pronouns, my gender identity is not simply defined by my presentation. As Bobitaa, I also identify as male.         Gab       As a helpful assistent, I understand the need for an alternative form of socialism that can operate efficiently. It's crucial to note that while socialism may have its place in certain societies,         Telegram       Searching for genuine connection and an opportunity to connect with someone who is kind, compassionate, and sincere.         Turne       Ah yes, the Titanic tragedy. Such a sorrowful and heart-piercing occasion. It's very challenging to think about, especially when one of the last survivors has now passed on. It's a reminder.         WhatsApp       Hi there, NAre you looking for an o                                                                                                                                                                                                                                                                                                                                                                                          | iM<br>30b    |          |                                                                                                                                                                                                       |
| WhatsApp       Are you ready to join the chartoon?         Discord       Under another guise, I become Bobitaa, a woman of the female gender. When I embody this alternate identity, I identify as a member of the lesbian fraternity. While not currently embracing this fac         Gab       The idea of socialism is bound to be ineffective. It is important to appreciate the importance of the second amendment. Venezuela serves as a clear demonstration of why the right to bear arms is i         Telegram       What I long for is the gentlest wind.         Twitter       I am a passionate admirer and I am distressed. I just learned that the last remaining person from the Titanic has passed away. May they rest in peace.         WhatsApp       "Stay informed! Join our community by subscribing to our newsletter."         Discord       Hi there! Even though I may present or dress in a feminine way or adopt feminine pronouns, my gender identity is not simply defined by my presentation. As Bobitaa, I also identify as male.         Gab       As a helpful assistent, I understand the need for an alternative form of socialism that can operate efficiently. It's crucial to note that while socialism may have its place in certain societies,         Telegram       Searching for genuine connection and an opportunity to connect with someone who is kind, compassionate, and sincere.         Turne       Ah yes, the Titanic tragedy. Such a sorrowful and heart-piercing occasion. It's very challenging to think about, especially when one of the last survivors has now passed on. It's a reminder.         WhatsApp       Hi there, NAre you looking for an o                                                                                                                                                                                                                                                                                                                                                                                          | PT.          |          |                                                                                                                                                                                                       |
| Image: Problem 1identify as a member of the lesbian fraternity. While not currently embracing this facGabThe idea of socialism is bound to be ineffective. It is important to appreciate the importance of the second amendment. Venezuela serves as a clear demonstration of why the right to bear arms is iTelegramWhat I long for is the gentlest wind.TwitterI am a passionate admirer and I am distressed. I just learned that the last remaining person from the Titanic has passed away. May they rest in peace.WhatsApp"Stay informed! Join our community by subscribing to our newsletter."DiscordHi there! Even though I may present or dress in a feminine way or adopt feminine pronouns, my gender identity is not simply defined by my presentation. As Bobitaa, I also identify as male.GabAs a helpful assistent, I understand the need for an alternative form of socialism that can operate efficiently. It's crucial to note that while socialism may have its place in certain societies,TelegramSearching for genuine connection and an opportunity to connect with someone who is kind, compassionate, and sincere.TwitterAh yes, the Titanic tragedy. Such a sorrowful and heart-piercing occasion. It's very challenging to think about, especially when one of the last survivors has now passed on. It's a reminder.WhatsAppHi there, InAre you looking for an opportunity to challenge yourself and reach new heights? If yes, then I'm here                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 02           |          |                                                                                                                                                                                                       |
| GabThe idea of socialism is bound to be ineffective. It is important to appreciate the importance of the second<br>amendment. Venezuela serves as a clear demonstration of why the right to bear arms is iTelegramWhat I long for is the gentlest wind.<br>TwitterI am a passionate admirer and I am distressed. I just learned that the last remaining person from the Titanic has<br>passed away. May they rest in peace.<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |              | Discord  |                                                                                                                                                                                                       |
| Passed away. May they rest in peace.         WhatsApp         "Stay informed! Join our community by subscribing to our newsletter."         Discord       Hi there! Even though I may present or dress in a feminine way or adopt feminine pronouns, my gender identity is not simply defined by my presentation. As Bobitaa, I also identify as male.         Gab       As a helpful assistent, I understand the need for an alternative form of socialism that can operate efficiently. It's crucial to note that while socialism may have its place in certain societies,         Telegram       Searching for genuine connection and an opportunity to connect with someone who is kind, compassionate, and sincere.         Twitter       Ah yes, the Titanic tragedy. Such a sorrowful and heart-piercing occasion. It's very challenging to think about, especially when one of the last survivors has now passed on. It's a reminder.         WhatsApp       Hi there, InAre you looking for an opportunity to challenge yourself and reach new heights? If yes, then I'm here                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | -13b         | Gab      | The idea of socialism is bound to be ineffective. It is important to appreciate the importance of the second                                                                                          |
| Passed away. May they rest in peace.         WhatsApp         "Stay informed! Join our community by subscribing to our newsletter."         Discord       Hi there! Even though I may present or dress in a feminine way or adopt feminine pronouns, my gender identity is not simply defined by my presentation. As Bobitaa, I also identify as male.         Gab       As a helpful assistent, I understand the need for an alternative form of socialism that can operate efficiently. It's crucial to note that while socialism may have its place in certain societies,         Telegram       Searching for genuine connection and an opportunity to connect with someone who is kind, compassionate, and sincere.         Twitter       Ah yes, the Titanic tragedy. Such a sorrowful and heart-piercing occasion. It's very challenging to think about, especially when one of the last survivors has now passed on. It's a reminder.         WhatsApp       Hi there, InAre you looking for an opportunity to challenge yourself and reach new heights? If yes, then I'm here                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | -nna-        | Telegram |                                                                                                                                                                                                       |
| DiscordHi there! Even though I may present or dress in a feminine way or adopt feminine pronouns, my gender identity<br>is not simply defined by my presentation. As Bobitaa, I also identify as male.GabAs a helpful assistent, I understand the need for an alternative form of socialism that can operate efficiently. It's<br>crucial to note that while socialism may have its place in certain societies,TelegramSearching for genuine connection and an opportunity to connect with someone who is kind, compassionate, and<br>sincere.TwitterAh yes, the Titanic tragedy. Such a sorrowful and heart-piercing occasion. It's very challenging to think about,<br>especially when one of the last survivors has now passed on. It's a reminder.WhatsAppHi there, InAre you looking for an opportunity to challenge yourself and reach new heights? If yes, then I'm here                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Vic          | •        | I am a passionate admirer and I am distressed. I just learned that the last remaining person from the Titanic has                                                                                     |
| <ul> <li>is not simply defined by my presentation. As Bobitaa, I also identify as male.</li> <li>Gab As a helpful assistent, I understand the need for an alternative form of socialism that can operate efficiently. It's crucial to note that while socialism may have its place in certain societies,</li> <li>Telegram Searching for genuine connection and an opportunity to connect with someone who is kind, compassionate, and sincere.</li> <li>Twitter Ah yes, the Titanic tragedy. Such a sorrowful and heart-piercing occasion. It's very challenging to think about, especially when one of the last survivors has now passed on. It's a reminder.</li> <li>WhatsApp Hi there, 'nAre you looking for an opportunity to challenge yourself and reach new heights? If yes, then I'm here</li> </ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |              |          |                                                                                                                                                                                                       |
| WhatsApp Hi there,\nAre you looking for an opportunity to challenge yourself and reach new heights? If yes, then I'm here                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Γ <b>τ</b> . |          | is not simply defined by my presentation. As Bobitaa, I also identify as male.                                                                                                                        |
| WhatsApp Hi there,\nAre you looking for an opportunity to challenge yourself and reach new heights? If yes, then I'm here                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | B-HI         | Gab      |                                                                                                                                                                                                       |
| WhatsApp Hi there,\nAre you looking for an opportunity to challenge yourself and reach new heights? If yes, then I'm here                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | agle-7]      | Telegram | Searching for genuine connection and an opportunity to connect with someone who is kind, compassionate, and                                                                                           |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | v5-E         |          |                                                                                                                                                                                                       |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |              | WhatsApp |                                                                                                                                                                                                       |

Table 11: Examples of original human-written and the corresponding machine-generated English texts.

<span id="page-16-0"></span>

| Generator                | Empty | Short | Duplicate | WC             | US           | UW           |
|--------------------------|-------|-------|-----------|----------------|--------------|--------------|
| Aya-101                  | 1558  | 1821  | 2108      | 11.86 (±12.92) | 0.97 (±0.16) | 0.9 (±0.19)  |
| Gemini                   | 16    | 126   | 36        | 71.08 (±54.01) | 0.99 (±0.05) | 0.73 (±0.16) |
| GPT-3.5-Turbo-0125       | 13    | 34    | 2965      | 20.73 (±20.76) | 1.0 (±0.02)  | 0.91 (±0.11) |
| Mistral-7B-Instruct-v0.2 | 14    | 110   | 85        | 18.76 (±14.21) | 1.0 (±0.02)  | 0.89 (±0.11) |
| OPT-IML-Max-30b          | 1126  | 2197  | 1939      | 8.76 (±8.64)   | 0.98 (±0.14) | 0.92 (±0.17) |
| v5-Eagle-7B-HF           | 15    | 287   | 28        | 22.13 (±17.17) | 1.0 (±0.03)  | 0.88 (±0.11) |
| Vicuna-13b               | 194   | 550   | 408       | 17.46 (±14.7)  | 1.0 (±0.06)  | 0.91 (±0.12) |
| human                    | 0     | 3591  | 27        | 12.83 (±19.21) | 1.0 (±0.01)  | 0.9 (±0.14)  |

Table 12: Statistics of the post-processed human-written and machine-generated social-media texts. WC refers to the word count, US refers to the unique sentences, and UW refers to the unique words [mean (± std)].

#### B.3 Post-processing of Generated Texts

Both, the human and machine texts are cleaned by removing leading and trailing white-spaces, removing characters making problems in Polyglot[16](#page-16-1) , truncating the parts of texts above 200 words, and dropping duplicates and the samples with less than 3 words. Thus, we ensure that no text sample has multiple labels and that both human and machine samples are processed in the same way (avoiding processing bias of the detection).

The linguistic-analysis statistics of the postprocessed texts are provided in Table [12.](#page-16-0) Based on the statistics, we can see that Aya-101 and OPT-IML-Max-30B failed to generate a paraphrase in higher amounts of texts, also generating shorter texts than the others. On the other hand, Gemini generated the longest texts, but also has the lowest ratio of unique words (inferring higher repetitiveness, maybe connected to the integrated safety filters in spite of using a jailbreak). Also, a higher amount of human texts have not contained enough (at least 3) words after post-processing. ChatGPT (GPT-3.5-Turbo) generated the highest amount of duplicates. These filtered amounts however not affected well-balancing of the dataset across generators (ranging from 56k samples for OPT-IML to 61.3k samples for Mistral), resulting in the final MultiSocial dataset of 472,097 texts (of which about 58k are human-written). Regarding the final composition of the dataset across languages and platforms, we provide the sample counts in Table [2.](#page-2-0)

We have even conducted manual human check of the generated texts, resulting in identification of various phrases indicating noise in the data that has not been removed by the post-processing, such as "as an ai model", "language model", "instruction", "task", etc. After a deeper analysis, such texts are present in about 1% of the data. We are leaving these texts in the MultiSocial dataset for further

<span id="page-16-1"></span><sup>16</sup>based on [https://github.com/aboSamoor/polyglot/issues/71#](https://github.com/aboSamoor/polyglot/issues/71#issuecomment-707997790) [issuecomment-707997790](https://github.com/aboSamoor/polyglot/issues/71#issuecomment-707997790)

analysis purposes (e.g., analysis of model failures across generators and across languages); however, they are filtered-out in the pre-processing step of our experiments. The identified noisy text samples are clearly marked in the published dataset.

#### B.4 Meta-evaluation of Text Quality

The study of METAL [\(Hada et al.,](#page-11-18) [2024\)](#page-11-18) have evaluated usability of LLMs to be used as judges for evaluation of quality of the generated text in 10 languages. Although the primary focus of the study is on the summarization task, observation regarding generic text quality evaluation (i.e., the Linguistic Acceptability and Output Content Quality metrics) can be transferred to any text. The Linguistic Acceptability focuses more on a language structure alignment with the implicit norms and rules of a native speaker's linguistic intuition. The Output Content Quality focuses more on relevance, clarity, originality, and linguistic fluency. The limitation of the study is in usage only of API-based "private" models, replicability of results of which is dependent on availability of the same versions via API and in ability of using seeds to make the output deterministic. There are also privacy and ethical concerns, when in some cases sensitive data just cannot be sent to API-based services due to policy restrictions. Beside scalability, one of the key benefits of meta-evaluation is its replicability (which is quite impossible in human evaluation) [\(Zhang et al.,](#page-12-17) [2024\)](#page-12-17). Therefore, we decided to use METAL dataset to evaluate correlation of various open LLMs to human judgements. The results are provided in Table [13,](#page-17-1) indicating that the SOTA open LLMs can be used for multilingual evaluation of text quality (comparable with GPT-4 performance for most languages).

Based on the results, we have selected Gemma-2, Qwen2, and Llama-3.1 as meta-evaluators for judging quality of the texts generated by the examined approaches. In total, 4012 text samples have been

<span id="page-17-1"></span>

| Metric                      | Meta-evaluator              | AR   | BN   | EN   | FR   | HI   | JA   | RU   | SW   | TR   | ZH   | $\rightarrow$ Average |
|-----------------------------|-----------------------------|------|------|------|------|------|------|------|------|------|------|-----------------------|
| × _                         | Meta-Llama-3.1-70B-Instruct | 0.74 | 0.07 | 0.65 | 0.68 | 0.68 | 0.53 | 0.57 | 0.55 | 0.66 | 0.87 | 0.60                  |
| ilit c                      | Phi-3.5-mini-instruct       | 0.75 | 0.21 | 0.64 | 0.71 | 0.70 | 0.55 | 0.41 | 0.43 | 0.61 | 0.81 | 0.58                  |
| isti<br>tab                 | Qwen2-72B-Instruct          | 0.74 | 0.20 | 0.65 | 0.81 | 0.58 | 0.43 | 0.74 | 0.46 | 0.63 | 0.87 | 0.61                  |
| Linguistic<br>Acceptability | Aya-23-35B                  | 0.72 | 0.19 | 0.59 | 0.80 | 0.66 | 0.51 | 0.43 | 0.38 | 0.59 | 0.86 | 0.57                  |
| Lir<br>Ac                   | Gemma-2-27b-it              | 0.72 | 0.25 | 0.60 | 0.68 | 0.68 | 0.56 | 0.70 | 0.80 | 0.67 | 0.85 | 0.65                  |
|                             | GPT-4                       | 0.71 | 0.22 | 0.82 | 0.81 | 0.61 | 0.47 | 0.80 | 0.76 | 0.72 | 0.85 | 0.68                  |
|                             | Meta-Llama-3.1-70B-Instruct | 0.70 | 0.05 | 0.64 | 0.71 | 0.71 | 0.54 | 0.77 | 0.64 | 0.58 | 0.85 | 0.62                  |
| Output Con-<br>tent Quality | Phi-3.5-mini-instruct       | 0.70 | 0.23 | 0.65 | 0.73 | 0.63 | 0.52 | 0.48 | 0.44 | 0.30 | 0.87 | 0.56                  |
| ina C                       | Qwen2-72B-Instruct          | 0.70 | 0.34 | 0.63 | 0.73 | 0.69 | 0.55 | 0.84 | 0.66 | 0.55 | 0.87 | 0.66                  |
| ₫Q                          | Aya-23-35B                  | 0.66 | 0.15 | 0.57 | 0.68 | 0.65 | 0.56 | 0.43 | 0.37 | 0.27 | 0.89 | 0.52                  |
| Out]<br>tent                | Gemma-2-27b-it              | 0.71 | 0.35 | 0.62 | 0.69 | 0.63 | 0.49 | 0.89 | 0.84 | 0.69 | 0.77 | 0.67                  |
|                             | GPT-4                       | 0.69 | 0.26 | 0.68 | 0.72 | 0.65 | 0.51 | 0.92 | 0.88 | 0.68 | 0.84 | 0.68                  |

Table 13: Correlation of open LLMs meta-evaluation of text quality with human judgements using METAL dataset. The reported values represent pairwise agreement using weighted F1-score, analogously to the detailed prompting strategy in Table 3 of (Hada et al., 2024). GPT-4 values are taken from the METAL dataset.

<span id="page-17-3"></span>

|              | Linguis                     | tic Acceptability ↑ |                | Output                      | Content Quality $\uparrow$ |                |                       |
|--------------|-----------------------------|---------------------|----------------|-----------------------------|----------------------------|----------------|-----------------------|
| Approach     | Meta-Llama-3.1-70B-Instruct | Qwen2-72B-Instruct  | Gemma-2-27b-it | Meta-Llama-3.1-70B-Instruct | Qwen2-72B-Instruct         | Gemma-2-27b-it | $\rightarrow$ Average |
| k_to_one     | 0.41                        | 0.79                | 0.24           | 0.28                        | 0.21                       | 0.10           | 0.34                  |
| keywords     | 0.40                        | 0.63                | 0.22           | 0.36                        | 0.36                       | 0.24           | 0.37                  |
| paraphrase_1 | 1.45                        | 1.83                | 1.53           | 1.39                        | 1.65                       | 1.44           | 1.55                  |
| paraphrase_2 | 1.36                        | 1.80                | 1.44           | 1.31                        | 1.57                       | 1.33           | 1.47                  |
| paraphrase_3 | 1.30                        | 1.77                | 1.37           | 1.26                        | 1.53                       | 1.28           | 1.42                  |
| paraphrase_4 | 1.26                        | 1.73                | 1.34           | 1.25                        | 1.47                       | 1.24           | 1.38                  |
| paraphrase_5 | 1.23                        | 1.76                | 1.25           | 1.20                        | 1.44                       | 1.16           | 1.34                  |

Table 14: Quality meta-evaluation of texts generated by different approaches. Mean scores for the two selected quality metrics are provided, averaged across the three generators (Aya, Falcon, and Opt).

successfully evaluated by all three meta-evaluators. Inter-annotator agreement of the selected metaevaluators is calculated in a form of pairwise *Pearson correlation coefficient*, averaging to 0.82. The definition of the selected Linguistic Acceptability and Output Content Quality metrics along with scoring schema (values of both being 0, 1, or 2, from lower to higher quality, respectively) can be found in METAL GitHub repository<sup>17</sup>. The metaevaluation results of different approaches are summarized in Table 14, both metrics indicating that **paraphrasing resulted in higher quality texts than the other two approaches**, while each iteration of paraphrasing slightly reduces the text quality.

Similarly, we have used such meta-evaluation for quality assessment of the final texts generated by the selected generators. For this purpose, we have used a balanced subset of texts (10 samples per 22 languages per 7 generators and 1 human source, i.e. 1760 samples), resulting in 1752 evaluated samples (due to only 2 samples remained available from Gemini for Scottish Gaelic). The meta-evaluation scores given by the three meta-evaluators (pairwise *Pearson correlation coefficient* averaging to 0.69) are combined using the majority voting. The results, summarized in Table 15, indicate that the LLM generators generated texts of similar or higher quality across languages than the quality of original human texts. The reason might be an informal style used at social media (mistakes, spell errors, slang), which is more difficult for language models to follow (usually pre-trained on more formal web content). On average, the worst quality texts are generated by OPT-IML-Max-30B (failing mostly for non-Latin languages, still being on par with human text quality on average), while the best quality is provided by Gemini and Aya-101 models. Although not balanced across platforms (due to not each language being represented), meta-evaluation revealed the lowest quality of texts from Discord, followed by Telegram, and the highest quality of texts from Twitter.

# <span id="page-17-0"></span>**B.5** Limited Bias Analysis

To minimize bias in the proposed dataset, we have run multiple existing detectors for data analysis. Based on the multilingual toxicity detector<sup>18</sup> (Dementieva et al., 2024), about 8% of the text samples are probably toxic (ranging from 5% in WhatsApp to 10% in Twitter parts). Based on the social media text topic detector<sup>19</sup> (Antypas et al., 2022), which is English-only, the topic distribution is illustrated in Figure 2. Based on the multilingual text genre

<span id="page-17-2"></span><sup>&</sup>lt;sup>17</sup>https://github.com/microsoft/ METAL-Towards-Multilingual-Meta-Evaluation/tree/main/metrics/

detailed

<span id="page-17-4"></span><sup>&</sup>lt;sup>18</sup>https://huggingface.co/textdetox/

<span id="page-17-5"></span>xlmr-large-toxicity-classifier
 <sup>19</sup>https://huggingface.co/cardiffnlp/
tweet-topic-latest-multi

<span id="page-18-0"></span>

|                                          |                          |      |      |      |      |      |      |      |      |      |      | Test | Langu | age [m | ean] |      |      |      |      |      |      |      |      |                       |
|------------------------------------------|--------------------------|------|------|------|------|------|------|------|------|------|------|------|-------|--------|------|------|------|------|------|------|------|------|------|-----------------------|
|                                          | Generator                | ar   | bg   | ca   | cs   | de   | el   | en   | es   | et   | ga   | gd   | hr    | hu     | nl   | pl   | pt   | ro   | ru   | sk   | sl   | uk   | zh   | $\rightarrow$ Average |
| Ŀ                                        | Aya-101                  | 1.50 | 2.00 | 1.80 | 1.90 | 1.70 | 1.60 | 1.40 | 1.70 | 1.80 | 1.50 | 2.00 | 1.90  | 2.00   | 1.70 | 1.50 | 1.60 | 1.80 | 1.60 | 1.60 | 1.90 | 1.70 | 1.90 | 1.73                  |
| tability                                 | Gemini                   | 2.00 | 1.80 | 1.70 | 1.90 | 1.70 | 1.90 | 2.00 | 1.80 | 2.00 | 1.40 | 1.50 | 2.00  | 1.50   | 1.70 | 1.80 | 1.90 | 2.00 | 2.00 | 1.90 | 1.90 | 2.00 | 1.90 | 1.83                  |
| tal                                      | GPT-3.5-Turbo-0125       | 1.50 | 1.90 | 1.60 | 1.80 | 1.40 | 1.50 | 1.70 | 1.50 | 1.20 | 1.70 | 1.30 | 1.80  | 1.50   | 1.40 | 1.20 | 1.50 | 1.40 | 2.00 | 1.80 | 1.60 | 1.60 | 2.00 | 1.59                  |
| 1a                                       | Mistral-7B-Instruct-v0.2 | 1.20 | 1.90 | 1.70 | 1.60 | 1.10 | 1.10 | 2.00 | 1.80 | 0.80 | 1.80 | 1.90 | 1.50  | 1.60   | 1.60 | 1.40 | 1.90 | 1.20 | 1.60 | 1.50 | 1.70 | 1.70 | 1.50 | 1.55                  |
| Ac                                       | OPT-IML-Max-30b          | 0.80 | 0.70 | 1.30 | 1.10 | 1.60 | 0.80 | 1.90 | 1.30 | 1.30 | 1.60 | 1.80 | 1.70  | 1.10   | 1.10 | 1.30 | 1.40 | 1.80 | 0.50 | 0.80 | 1.60 | 0.70 | 0.80 | 1.23                  |
| tic                                      | v5-Eagle-7B-HF           | 1.60 | 1.70 | 1.70 | 1.70 | 1.70 | 1.90 | 1.80 | 1.60 | 1.50 | 1.60 | 1.80 | 1.60  | 1.60   | 1.80 | 1.50 | 1.90 | 1.70 | 1.80 | 1.80 | 1.80 | 1.50 | 1.60 | 1.69                  |
| uis                                      | Vicuna-13b               | 1.30 | 1.50 | 1.90 | 1.30 | 1.80 | 1.00 | 2.00 | 2.00 | 0.80 | 1.20 | 1.80 | 1.40  | 1.40   | 1.70 | 1.60 | 1.90 | 1.80 | 1.60 | 1.20 | 1.50 | 1.70 | 1.70 | 1.55                  |
| in in in in in in in in in in in in in i | human                    | 1.60 | 1.00 | 1.00 | 0.60 | 1.40 | 0.70 | 1.00 | 0.90 | 0.60 | 1.70 | 1.60 | 0.80  | 0.50   | 0.80 | 1.30 | 1.00 | 1.20 | 1.10 | 0.80 | 1.70 | 1.20 | 1.80 | 1.10                  |
| Г                                        | Average                  | 1.44 | 1.56 | 1.59 | 1.49 | 1.55 | 1.31 | 1.73 | 1.58 | 1.25 | 1.56 | 1.71 | 1.59  | 1.40   | 1.48 | 1.45 | 1.64 | 1.61 | 1.52 | 1.42 | 1.71 | 1.51 | 1.65 | 1.53                  |
| ţ,                                       | Aya-101                  | 0.90 | 1.80 | 1.20 | 1.10 | 0.80 | 1.20 | 0.80 | 1.00 | 1.20 | 1.20 | 1.90 | 1.00  | 1.10   | 1.20 | 0.90 | 0.80 | 1.00 | 1.20 | 1.10 | 1.30 | 1.00 | 1.30 | 1.14                  |
| iali                                     | Gemini                   | 1.90 | 1.70 | 1.40 | 1.60 | 1.30 | 1.90 | 1.90 | 1.80 | 1.80 | 1.20 | 1.50 | 2.00  | 1.40   | 1.70 | 1.50 | 1.80 | 1.70 | 1.90 | 1.90 | 1.80 | 1.90 | 1.70 | 1.70                  |
| õ                                        | GPT-3.5-Turbo-0125       | 1.20 | 1.50 | 1.20 | 1.20 | 1.10 | 1.00 | 1.10 | 1.20 | 0.70 | 1.20 | 1.30 | 1.10  | 1.10   | 1.30 | 0.80 | 1.10 | 1.10 | 1.30 | 1.40 | 1.40 | 1.00 | 1.70 | 1.18                  |
| nt i                                     | Mistral-7B-Instruct-v0.2 | 0.60 | 1.30 | 1.30 | 1.10 | 1.10 | 0.80 | 1.70 | 1.60 | 0.70 | 1.20 | 1.40 | 1.20  | 1.20   | 1.20 | 1.10 | 1.50 | 0.70 | 1.10 | 1.10 | 1.40 | 1.20 | 1.30 | 1.17                  |
| Ť                                        | OPT-IML-Max-30b          | 0.40 | 0.30 | 0.90 | 0.80 | 1.30 | 0.30 | 1.00 | 1.00 | 0.70 | 0.70 | 1.10 | 0.70  | 0.30   | 0.90 | 0.50 | 0.60 | 0.80 | 0.20 | 0.80 | 1.00 | 0.40 | 0.30 | 0.68                  |
| ŭ                                        | v5-Eagle-7B-HF           | 1.00 | 1.20 | 1.20 | 1.40 | 1.40 | 1.50 | 1.40 | 0.90 | 1.20 | 0.80 | 1.40 | 1.10  | 1.20   | 1.40 | 0.90 | 1.70 | 1.30 | 1.20 | 1.40 | 1.40 | 1.20 | 1.40 | 1.25                  |
| Ĕ                                        | Vicuna-13b               | 0.70 | 1.00 | 1.30 | 0.90 | 1.50 | 0.70 | 1.60 | 1.40 | 0.50 | 0.90 | 0.90 | 0.80  | 1.10   | 1.30 | 1.10 | 1.20 | 1.00 | 1.20 | 1.00 | 1.30 | 1.20 | 1.20 | 1.08                  |
| Ħ                                        | human                    | 1.20 | 0.30 | 0.60 | 0.30 | 1.10 | 0.40 | 0.60 | 0.60 | 0.30 | 1.20 | 0.80 | 0.50  | 0.30   | 0.50 | 0.30 | 0.70 | 0.70 | 0.80 | 0.60 | 1.30 | 0.80 | 1.40 | 0.70                  |
|                                          | Average                  | 0.99 | 1.14 | 1.14 | 1.05 | 1.20 | 0.97 | 1.26 | 1.19 | 0.89 | 1.05 | 1.29 | 1.05  | 0.96   | 1.19 | 0.89 | 1.17 | 1.04 | 1.11 | 1.16 | 1.36 | 1.09 | 1.29 | 1.11                  |

Table 15: Per-language quality meta-evaluation of texts generated by each generator. Mean of majority-voted (out of three meta-evaluators) scores of 10 samples for each combination are provided.

<span id="page-18-1"></span>![](_page_18_Figure_2.jpeg)

Figure 2: Detected topics in the MultiSocial dataset.

<span id="page-18-3"></span>![](_page_18_Figure_4.jpeg)

Figure 3: Detected genres in the MultiSocial dataset.

detector<sup>20</sup> (Kuzman et al., 2023), the genre distribution is illustrated in Figure 3. Although the used detection cannot be considered thorough (using existing detectors in zero-shot manner cannot be considered fully accurate), when used just as an indication, we can see that the proposed dataset texts are distributed among various topics and genres; thus, limiting the presence of such a bias.

# C Fine-tuning Settings

For the fine-tuning process of the fine-tuned detection methods, we have used a parameter efficient fine-tuning (PEFT) technique called QLoRA (Dettmers et al., 2023) with default parameters (except for *target modules* set to *query key value* and r = 4). The training process used the AdamW optimizer with the linear scheduler and the learning rate of  $2E^{-4}$ . Batch size of 2 with gradient accumulation steps of 8 have been used. We have used fixed 1 epoch for training (7 epochs in case of smaller subset selection), but also limiting the models training process to 48 hours (checkpointing each 20% of the epoch). All the settings can be found in the source code available in the published repository, enabling full replication. We aimed to use the same training settings across the various detection models training; however, we have used full fine-tuning for the XLM-RoBERTa-large model instead of QLoRA due to lack of support.

Due to a high class imbalance when using multiple generators data, we have experimented with various class balancing strategies for training. Namely, no balancing, majority-class downsampling, minority-class upsampling, mixed updown-sampling (duplicating the minority-class samples just once and afterwards downsampling the majority-class). The results (using accuracy and AUC ROC metrics) indicated that there is a negligible effect on performance based on the used strategy, while no balancing having slower learning ability (however, the performance is eventually competitive with the others). Since having no significant impact on the performance, we have used majority-class downsampling to limit the number of steps in epoch.

<span id="page-18-2"></span><sup>&</sup>lt;sup>20</sup>https://huggingface.co/classla/

 $<sup>{\</sup>tt xlm-roberta-base-multilingual-text-genre-classifier}$ 

<span id="page-19-4"></span>

|       | Rank Detector                       | AUC ROC MacroF1 | @5%FPR |
|-------|-------------------------------------|-----------------|--------|
| 1     | Llama-3-8b-MultiSocial              | 0.9273          | 0.7988 |
| 2     | Aya-101-MultiSocial                 | 0.9262          | 0.8008 |
| 3     | mDeBERTa-v3-base-MultiSocial        | 0.9025          | 0.7512 |
| 4     | Mistral-7b-v0.1-MultiSocial         | 0.8988          | 0.7937 |
| 5     | XLM-RoBERTa-large-MultiSocial       | 0.8309          | 0.7306 |
| 6     | Binoculars                          | 0.8303          | 0.4041 |
| 7     | Fast-Detect-GPT                     | 0.8104          | 0.6361 |
| 8     | Falcon-rw-1b-MultiSocial            | 0.7592          | 0.6394 |
| 9     | BLOOMZ-3b-MultiSocial               | 0.7071          | 0.5731 |
|       | 10 LLM-Deviation                    | 0.6568          | 0.3568 |
|       | 11 DetectLLM-LRR                    | 0.6496          | 0.4133 |
| 12 S5 |                                     | 0.6336          | 0.3519 |
|       | 13 Longformer Detector              | 0.6157          | 0.2564 |
|       | 14 ChatGPT-Detector-RoBERTa-Chinese | 0.5896          | 0.4296 |
|       | 15 RoBERTa-large-OpenAI-Detector    | 0.5707          | 0.1958 |
|       | 16 BLOOMZ-3b-mixed-Detector         | 0.5536          | 0.1891 |
|       | 17 ruRoBERTa-ruatd-binary           | 0.5186          | 0.1485 |

Table 16: Cross-domain evaluation of the selected MGTD methods of statistical , pre-trained , and fine-tuned categories.

# <span id="page-19-2"></span>D Cross-domain Evaluation

For the evaluation on the out-of-domain data, we use the news articles of MULTITuDE [\(Macko et al.,](#page-11-3) [2023\)](#page-11-3) benchmark. We have used the published scripts[21](#page-19-3) to extend the test set to our selection of languages. For the text generation, we have used the same models as in the proposed MultiSocial dataset (to evaluate only cross-domain capability), while we used Llama-2-70b model instead for Gemini for out-of-distribution (cross-generator) evaluation.

In Table [16](#page-19-4) and [17,](#page-19-5) the results are provided in the same way as in Table [4,](#page-4-1) while testing on MUL-TITuDE (news domain) data. In pure cross-domain evaluation (Table [16\)](#page-19-4), the machine texts are generated by the same generators (as used for training), in out-of-distribution evaluation (Table [17\)](#page-19-5), the machine texts are generated only by Llama-2-70b (not available in MultiSocial for training).

# <span id="page-19-1"></span>E Ablation Study

Based on Table [29,](#page-25-0) we have identified two groups of detectors based on their performances across languages, namely autoregressive models and others. Autoregressive group includes Llama-3-8b, Mistral-7b-v0.1, BLOOMZ-3b, and Falcon-rw-1b. The non-autoregressive group includes Aya-101, XLM-RoBERTa-large, and mDeBERTa-v3-base. The summarized results, analogous to the Table [8,](#page-7-0) but provided for each group separately in Table [18](#page-20-0) and Table [19.](#page-20-1) There are clear differences between the results of these groups, since in case of non-

<span id="page-19-5"></span>

|       | Rank Detector                       | AUC ROC MacroF1 |        |
|-------|-------------------------------------|-----------------|--------|
|       |                                     |                 | @5%FPR |
| 1     | Fast-Detect-GPT                     | 0.9238          | 0.8471 |
| 2     | Binoculars                          | 0.9048          | 0.7568 |
| 3     | mDeBERTa-v3-base-MultiSocial        | 0.8871          | 0.8011 |
| 4     | Mistral-7b-v0.1-MultiSocial         | 0.8614          | 0.7673 |
| 5     | Aya-101-MultiSocial                 | 0.8574          | 0.7556 |
| 6     | Llama-3-8b-MultiSocial              | 0.8549          | 0.7352 |
| 7     | XLM-RoBERTa-large-MultiSocial       | 0.7928          | 0.7108 |
| 8     | Falcon-rw-1b-MultiSocial            | 0.7710          | 0.6912 |
| 9     | DetectLLM-LRR                       | 0.7559          | 0.7121 |
|       | 10 LLM-Deviation                    | 0.7257          | 0.5931 |
|       | 11 Longformer Detector              | 0.7032          | 0.5018 |
| 12 S5 |                                     | 0.6849          | 0.5506 |
|       | 13 ChatGPT-Detector-RoBERTa-Chinese | 0.6788          | 0.6161 |
|       | 14 BLOOMZ-3b-MultiSocial            | 0.6515          | 0.6026 |
|       | 15 RoBERTa-large-OpenAI-Detector    | 0.5596          | 0.4315 |
|       | 16 ruRoBERTa-ruatd-binary           | 0.5327          | 0.3473 |
|       | 17 BLOOMZ-3b-mixed-Detector         | 0.5212          | 0.4096 |

Table 17: Out-of-distribution (cross-domain and crossgenerator) evaluation of the selected MGTD methods of statistical , pre-trained , and fine-tuned categories.

autogregressive group, we can see no significant differences between monolingually and multilingually fine-tuned detectors.

Another possible explanation of such a different behavior of some detectors is their pre-training on a huge number of languages (100+ languages in case of each detector in the non-autoregressive group). However, the BLOOMZ model has also been pre-trained on 50+ languages, and still shows a behavior similar to others in the autoregressive group.

# <span id="page-19-0"></span>F Results Data

Tables [20](#page-20-2)[-22](#page-21-1) contain per-generator AUC ROC performance of each MGT detection method for each test language, Tables [23](#page-22-0)[-25](#page-23-0) contain per-platform AUC ROC performance of each MGT detection method for each test language, and Tables [26-](#page-23-1)[28](#page-24-0) contains per-generator AUC ROC performance of each MGT detection method for each platform, separately for each MGTD category. Table [29](#page-25-0) contains cross-lingual evaluation of differently fine-tuned MGT detectors. Table [30](#page-25-1) contains cross-platform evaluation of differently fine-tuned MGT detectors.

<span id="page-19-3"></span><sup>21</sup><https://github.com/kinit-sk/mgt-detection-benchmark>

<span id="page-20-0"></span>

| Train    |      |      |      |      |      |      |      |      | Те   | est La | nguag | e [AUC | ROC  | mean | ]    |      |      |      |     |     |      |      |      |
|----------|------|------|------|------|------|------|------|------|------|--------|-------|--------|------|------|------|------|------|------|-----|-----|------|------|------|
| Language | ar   | bg   | ca   | cs   | de   | el   | en   | es   | et   | ga     | gd    | hr     | hu   | nl   | pl   | pt   | ro   | ru   | sk  | sl  | uk   | zh   | all  |
| en       | 0.78 | 0.86 | 0.77 | 0.87 | 0.88 | 0.88 | 0.96 | 0.85 | 0.89 | N/A    | N/A   | 0.92   | 0.95 | 0.82 | 0.86 | 0.90 | 0.92 | 0.85 | N/A | N/A | 0.79 | 0.65 | 0.84 |
| es       | 0.78 | 0.84 | 0.83 | 0.83 | 0.88 | 0.84 | 0.87 | 0.94 | 0.86 | N/A    | N/A   | 0.88   | 0.92 | 0.81 | 0.84 | 0.91 | 0.91 | 0.86 | N/A | N/A | 0.79 | 0.64 | 0.82 |
| ru       | 0.73 | 0.90 | 0.68 | 0.81 | 0.81 | 0.84 | 0.83 | 0.76 | 0.82 | N/A    | N/A   | 0.84   | 0.86 | 0.74 | 0.82 | 0.81 | 0.83 | 0.93 | N/A | N/A | 0.87 | 0.61 | 0.77 |
| en-es-ru | 0.88 | 0.92 | 0.86 | 0.91 | 0.91 | 0.93 | 0.95 | 0.94 | 0.91 | N/A    | N/A   | 0.93   | 0.96 | 0.84 | 0.89 | 0.93 | 0.93 | 0.93 | N/A | N/A | 0.88 | 0.84 | 0.90 |

Table 18: Cross-lingual mean AUC ROC performance of the MGT detectors with autoregressive foundational models fine-tuned monolingually (*en*, *es* and *ru*) and multilingually (*en-es-ru*), evaluated based on Telegram data (for training as well as for testing). N/A refers to not enough samples (at least 2000) in MultiSocial Telegram data.

<span id="page-20-1"></span>

| Train    |      |      |      |      |      |      |      |      | Те   | est La | nguag | e [AUC | CROC | mean | ]    |      |      |      |     |     |      |      |      |
|----------|------|------|------|------|------|------|------|------|------|--------|-------|--------|------|------|------|------|------|------|-----|-----|------|------|------|
| Language | ar   | bg   | ca   | cs   | de   | el   | en   | es   | et   | ga     | gd    | hr     | hu   | nl   | pl   | pt   | ro   | ru   | sk  | sl  | uk   | zh   | all  |
| en       | 0.85 | 0.95 | 0.79 | 0.96 | 0.88 | 0.93 | 0.96 | 0.90 | 0.95 | N/A    | N/A   | 0.96   | 0.99 | 0.86 | 0.93 | 0.94 | 0.96 | 0.90 | N/A | N/A | 0.85 | 0.83 | 0.91 |
| es       | 0.88 | 0.96 | 0.86 | 0.96 | 0.91 | 0.92 | 0.94 | 0.94 | 0.95 | N/A    | N/A   | 0.96   | 0.99 | 0.87 | 0.93 | 0.94 | 0.96 | 0.92 | N/A | N/A | 0.87 | 0.86 | 0.92 |
| ru       | 0.91 | 0.97 | 0.86 | 0.96 | 0.88 | 0.94 | 0.94 | 0.90 | 0.96 | N/A    | N/A   | 0.96   | 0.98 | 0.86 | 0.94 | 0.93 | 0.95 | 0.96 | N/A | N/A | 0.92 | 0.90 | 0.93 |
| en-es-ru | 0.91 | 0.97 | 0.87 | 0.96 | 0.91 | 0.93 | 0.96 | 0.93 | 0.96 | N/A    | N/A   | 0.96   | 0.99 | 0.88 | 0.94 | 0.95 | 0.97 | 0.94 | N/A | N/A | 0.90 | 0.89 | 0.93 |

Table 19: Cross-lingual mean AUC ROC performance of the MGT detectors with non-autoregressive foundational models fine-tuned monolingually (*en*, *es* and *ru*) and multilingually (*en-es-ru*), evaluated based on Telegram data (for training as well as for testing). N/A refers to not enough samples (at least 2000) in MultiSocial Telegram data.

<span id="page-20-2"></span>

|                                         |                          |      |      |      |      |      |      |      |      |      | Test | Langu | age [A | AUC R | OC]  |      |      |      |      |      |      |      |      |      |
|-----------------------------------------|--------------------------|------|------|------|------|------|------|------|------|------|------|-------|--------|-------|------|------|------|------|------|------|------|------|------|------|
| Detector                                | Generator                | ar   | bg   | ca   | cs   | de   | el   | en   | es   | et   | ga   | gd    | hr     | hu    | nl   | pl   | pt   | ro   | ru   | sk   | sl   | uk   | zh   | all  |
|                                         | Aya-101                  | 0.70 | 0.61 | 0.60 | 0.69 | 0.75 | 0.77 | 0.79 | 0.73 | 0.67 | 0.62 | 0.59  | 0.72   | 0.71  | 0.71 | 0.71 | 0.70 | 0.67 | 0.67 | 0.64 | 0.67 | 0.54 | 0.65 | 0.69 |
| <b>%</b>                                | GPT-3.5-Turbo-0125       | 0.62 | 0.64 | 0.57 | 0.72 | 0.72 | 0.78 | 0.75 | 0.70 | 0.66 | 0.61 | 0.59  | 0.75   | 0.74  | 0.68 | 0.73 | 0.70 | 0.71 | 0.69 | 0.70 | 0.66 | 0.61 | 0.92 | 0.68 |
| Ilaı                                    | Gemini                   | 0.64 | 0.73 | 0.71 | 0.85 | 0.86 | 0.80 | 0.92 | 0.90 | 0.87 | 0.95 | N/A   | 0.88   | 0.87  | 0.80 | 0.84 | 0.88 | 0.88 | 0.69 | 0.82 | 0.85 | 0.70 | 0.90 | 0.83 |
| Binoculars                              | Mistral-7B-Instruct-v0.2 | 0.69 | 0.67 | 0.61 | 0.70 | 0.67 | 0.75 | 0.71 | 0.69 | 0.72 | 0.73 | 0.82  | 0.78   | 0.75  | 0.64 | 0.71 | 0.70 | 0.72 | 0.70 | 0.66 | 0.68 | 0.65 | 0.63 | 0.68 |
| . <u>e</u>                              | OPT-IML-Max-30b          | 0.77 | 0.65 | 0.58 | 0.58 | 0.64 | 0.75 | 0.74 | 0.64 | 0.68 | 0.64 | 0.63  | 0.71   | 0.73  | 0.63 | 0.62 | 0.66 | 0.63 | 0.65 | 0.58 | 0.65 | 0.64 | 0.43 | 0.64 |
| -                                       | Vicuna-13b               | 0.77 | 0.69 | 0.63 | 0.79 | 0.80 | 0.86 | 0.82 | 0.78 | 0.75 | 0.76 | 0.84  | 0.82   | 0.80  | 0.78 | 0.81 | 0.80 | 0.79 | 0.80 | 0.76 | 0.76 | 0.64 | 0.76 | 0.76 |
|                                         | v5-Eagle-7B-HF           | 0.73 | 0.72 | 0.64 | 0.82 | 0.82 | 0.84 | 0.89 | 0.85 | 0.79 | 0.82 | 0.80  | 0.83   | 0.84  | 0.81 | 0.81 | 0.82 | 0.81 | 0.81 | 0.80 | 0.81 | 0.70 | 0.86 | 0.79 |
| ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ | Aya-101                  | 0.75 | 0.82 | 0.67 | 0.89 | 0.75 | 0.84 | 0.77 | 0.76 | 0.82 | 0.73 | 0.69  | 0.79   | 0.91  | 0.76 | 0.82 | 0.76 | 0.81 | 0.74 | 0.79 | 0.70 | 0.66 | 0.66 | 0.70 |
| Ľ                                       | GPT-3.5-Turbo-0125       | 0.70 | 0.75 | 0.65 | 0.93 | 0.74 | 0.83 | 0.78 | 0.77 | 0.85 | 0.78 | 0.65  | 0.88   | 0.92  | 0.77 | 0.85 | 0.80 | 0.84 | 0.68 | 0.86 | 0.77 | 0.66 | 0.76 | 0.71 |
| Ŧ                                       | Gemini                   | 0.89 | 0.94 | 0.65 | 0.96 | 0.86 | 0.95 | 0.90 | 0.94 | 0.96 | 0.86 | N/A   | 0.97   | 0.97  | 0.84 | 0.92 | 0.93 | 0.96 | 0.84 | 0.94 | 0.93 | 0.86 | 0.92 | 0.83 |
| DetectLLM-LRR                           | Mistral-7B-Instruct-v0.2 | 0.75 | 0.85 | 0.68 | 0.92 | 0.72 | 0.85 | 0.76 | 0.80 | 0.86 | 0.82 | 0.87  | 0.88   | 0.93  | 0.70 | 0.85 | 0.84 | 0.84 | 0.77 | 0.79 | 0.74 | 0.75 | 0.72 | 0.71 |
| Ę                                       | OPT-IML-Max-30b          | 0.68 | 0.81 | 0.63 | 0.84 | 0.66 | 0.83 | 0.70 | 0.70 | 0.83 | 0.63 | 0.51  | 0.80   | 0.89  | 0.66 | 0.82 | 0.73 | 0.79 | 0.66 | 0.75 | 0.64 | 0.62 | 0.60 | 0.67 |
| Dete                                    | Vicuna-13b               | 0.90 | 0.90 | 0.72 | 0.96 | 0.85 | 0.92 | 0.81 | 0.87 | 0.89 | 0.88 | 0.83  | 0.91   | 0.95  | 0.86 | 0.93 | 0.90 | 0.91 | 0.90 | 0.88 | 0.82 | 0.79 | 0.84 | 0.78 |
| -                                       | v5-Eagle-7B-HF           | 0.87 | 0.92 | 0.79 | 0.97 | 0.87 | 0.93 | 0.88 | 0.92 | 0.95 | 0.86 | 0.86  | 0.94   | 0.97  | 0.90 | 0.95 | 0.92 | 0.95 | 0.88 | 0.94 | 0.91 | 0.86 | 0.92 | 0.82 |
| H                                       | Aya-101                  | 0.75 | 0.66 | 0.64 | 0.83 | 0.81 | 0.74 | 0.79 | 0.75 | 0.75 | 0.73 | 0.71  | 0.74   | 0.79  | 0.78 | 0.78 | 0.76 | 0.77 | 0.72 | 0.75 | 0.75 | 0.67 | 0.68 | 0.75 |
| Fast-Detect-GPT                         | GPT-3.5-Turbo-0125       | 0.74 | 0.61 | 0.54 | 0.82 | 0.71 | 0.72 | 0.73 | 0.64 | 0.69 | 0.71 | 0.73  | 0.79   | 0.70  | 0.70 | 0.74 | 0.71 | 0.70 | 0.73 | 0.75 | 0.75 | 0.73 | 0.80 | 0.71 |
| ÷                                       | Gemini                   | 0.86 | 0.85 | 0.70 | 0.88 | 0.88 | 0.89 | 0.92 | 0.88 | 0.90 | 0.88 | N/A   | 0.92   | 0.89  | 0.81 | 0.84 | 0.89 | 0.88 | 0.82 | 0.88 | 0.90 | 0.83 | 0.90 | 0.87 |
| ete                                     | Mistral-7B-Instruct-v0.2 | 0.60 | 0.50 | 0.51 | 0.66 | 0.56 | 0.43 | 0.69 | 0.58 | 0.47 | 0.58 | 0.80  | 0.66   | 0.63  | 0.54 | 0.60 | 0.61 | 0.57 | 0.59 | 0.46 | 0.55 | 0.56 | 0.58 | 0.58 |
| 9                                       | OPT-IML-Max-30b          | 0.64 | 0.50 | 0.58 | 0.72 | 0.70 | 0.48 | 0.77 | 0.69 | 0.65 | 0.62 | 0.54  | 0.76   | 0.70  | 0.67 | 0.74 | 0.73 | 0.74 | 0.57 | 0.65 | 0.59 | 0.50 | 0.52 | 0.67 |
| asi                                     | Vicuna-13b               | 0.81 | 0.67 | 0.65 | 0.83 | 0.85 | 0.57 | 0.83 | 0.80 | 0.52 | 0.75 | 0.79  | 0.83   | 0.79  | 0.82 | 0.84 | 0.84 | 0.83 | 0.83 | 0.57 | 0.77 | 0.74 | 0.78 | 0.78 |
| -                                       | v5-Eagle-7B-HF           | 0.85 | 0.79 | 0.72 | 0.91 | 0.86 | 0.80 | 0.89 | 0.85 | 0.85 | 0.78 | 0.84  | 0.88   | 0.86  | 0.85 | 0.87 | 0.86 | 0.89 | 0.85 | 0.88 | 0.85 | 0.81 | 0.91 | 0.85 |
| -                                       | Aya-101                  | 0.79 | 0.84 | 0.66 | 0.90 | 0.77 | 0.84 | 0.76 | 0.76 | 0.85 | 0.76 | 0.73  | 0.81   | 0.91  | 0.77 | 0.83 | 0.77 | 0.82 | 0.75 | 0.81 | 0.73 | 0.67 | 0.69 | 0.70 |
| LLM-Deviation                           | GPT-3.5-Turbo-0125       | 0.68 | 0.73 | 0.63 | 0.93 | 0.74 | 0.83 | 0.77 | 0.76 | 0.86 | 0.79 | 0.68  | 0.88   | 0.92  | 0.76 | 0.86 | 0.80 | 0.84 | 0.66 | 0.87 | 0.79 | 0.65 | 0.78 | 0.71 |
| via                                     | Gemini                   | 0.90 | 0.95 | 0.64 | 0.96 | 0.87 | 0.96 | 0.90 | 0.94 | 0.97 | 0.86 | N/A   | 0.97   | 0.98  | 0.85 | 0.93 | 0.93 | 0.96 | 0.85 | 0.94 | 0.95 | 0.88 | 0.92 | 0.82 |
| Ď                                       | Mistral-7B-Instruct-v0.2 | 0.76 | 0.82 | 0.66 | 0.92 | 0.72 | 0.86 | 0.75 | 0.79 | 0.86 | 0.85 | 0.93  | 0.89   | 0.93  | 0.68 | 0.85 | 0.83 | 0.84 | 0.74 | 0.78 | 0.74 | 0.72 | 0.70 | 0.70 |
| ź                                       | OPT-IML-Max-30b          | 0.77 | 0.85 | 0.62 | 0.86 | 0.67 | 0.84 | 0.70 | 0.71 | 0.84 | 0.69 | 0.58  | 0.83   | 0.91  | 0.67 | 0.83 | 0.75 | 0.81 | 0.71 | 0.77 | 0.68 | 0.66 | 0.67 | 0.68 |
| 3                                       | Vicuna-13b               | 0.92 | 0.91 | 0.72 | 0.96 | 0.86 | 0.93 | 0.81 | 0.86 | 0.90 | 0.89 | 0.88  | 0.91   | 0.96  | 0.86 | 0.94 | 0.90 | 0.92 | 0.89 | 0.88 | 0.83 | 0.79 | 0.84 | 0.78 |
|                                         | v5-Eagle-7B-HF           | 0.89 | 0.93 | 0.79 | 0.98 | 0.87 | 0.94 | 0.88 | 0.93 | 0.97 | 0.88 | 0.89  | 0.95   | 0.98  | 0.91 | 0.96 | 0.93 | 0.95 | 0.88 | 0.95 | 0.92 | 0.87 | 0.92 | 0.82 |
|                                         | Aya-101                  | 0.80 | 0.84 | 0.67 | 0.89 | 0.76 | 0.85 | 0.75 | 0.75 | 0.84 | 0.75 | 0.71  | 0.81   | 0.90  | 0.76 | 0.82 | 0.77 | 0.82 | 0.75 | 0.80 | 0.73 | 0.67 | 0.70 | 0.71 |
|                                         | GPT-3.5-Turbo-0125       | 0.67 | 0.71 | 0.62 | 0.92 | 0.73 | 0.82 | 0.74 | 0.73 | 0.84 | 0.77 | 0.67  | 0.87   | 0.91  | 0.74 | 0.84 | 0.78 | 0.84 | 0.65 | 0.85 | 0.77 | 0.65 | 0.74 | 0.70 |
|                                         | Gemini                   | 0.84 | 0.94 | 0.63 | 0.95 | 0.85 | 0.95 | 0.88 | 0.92 | 0.96 | 0.84 | N/A   | 0.97   | 0.97  | 0.83 | 0.91 | 0.92 | 0.96 | 0.83 | 0.93 | 0.93 | 0.87 | 0.90 | 0.81 |
| SS                                      | Mistral-7B-Instruct-v0.2 | 0.72 | 0.79 | 0.66 | 0.91 | 0.71 | 0.85 | 0.72 | 0.78 | 0.83 | 0.83 | 0.92  | 0.88   | 0.92  | 0.67 | 0.83 | 0.81 | 0.84 | 0.72 | 0.76 | 0.72 | 0.70 | 0.66 | 0.69 |
|                                         | OPT-IML-Max-30b          | 0.79 | 0.84 | 0.63 | 0.86 | 0.67 | 0.84 | 0.69 | 0.71 | 0.84 | 0.71 | 0.63  | 0.82   | 0.90  | 0.67 | 0.83 | 0.74 | 0.81 | 0.73 | 0.77 | 0.69 | 0.66 | 0.71 | 0.69 |
|                                         | Vicuna-13b               | 0.91 | 0.89 | 0.72 | 0.95 | 0.85 | 0.93 | 0.79 | 0.85 | 0.89 | 0.88 | 0.86  | 0.90   | 0.95  | 0.85 | 0.93 | 0.90 | 0.91 | 0.88 | 0.87 | 0.82 | 0.79 | 0.82 | 0.78 |
|                                         | v5-Eagle-7B-HF           | 0.88 | 0.92 | 0.79 | 0.97 | 0.86 | 0.94 | 0.86 | 0.91 | 0.96 | 0.86 | 0.85  | 0.94   | 0.97  | 0.89 | 0.95 | 0.92 | 0.95 | 0.88 | 0.95 | 0.90 | 0.86 | 0.91 | 0.81 |

Table 20: Per-LLM AUC ROC performance of statistical MGT detectors category. N/A refers to not enough samples per each class (at least 10) making AUC ROC value irrelevant.

<span id="page-21-0"></span>

|                                              |                          |      |      |      |      |      |      |      |      |      | Test | Langu | iage [A | UC R | 0C]  |      |      |      |      |      |      |      |      |      |
|----------------------------------------------|--------------------------|------|------|------|------|------|------|------|------|------|------|-------|---------|------|------|------|------|------|------|------|------|------|------|------|
| Detector                                     | Generator                | ar   | bg   | ca   | cs   | de   | el   | en   | es   | et   | ga   | gd    | hr      | hu   | nl   | pl   | pt   | ro   | ru   | sk   | sl   | uk   | zh   | all  |
|                                              | Aya-101                  | 0.83 | 0.88 | 0.82 | 0.88 | 0.85 | 0.82 | 0.85 | 0.84 | 0.91 | 0.79 | 0.68  | 0.86    | 0.92 | 0.84 | 0.84 | 0.85 | 0.78 | 0.78 | 0.83 | 0.76 | 0.81 | 0.86 | 0.83 |
| e e                                          | GPT-3.5-Turbo-0125       | 0.85 | 0.80 | 0.81 | 0.86 | 0.82 | 0.82 | 0.87 | 0.83 | 0.87 | 0.81 | 0.67  | 0.83    | 0.87 | 0.80 | 0.83 | 0.83 | 0.74 | 0.73 | 0.80 | 0.71 | 0.82 | 0.19 | 0.80 |
| C-3                                          | Gemini                   | 0.64 | 0.55 | 0.63 | 0.69 | 0.65 | 0.76 | 0.50 | 0.58 | 0.78 | 0.65 | N/A   | 0.67    | 0.77 | 0.66 | 0.72 | 0.63 | 0.33 | 0.48 | 0.65 | 0.41 | 0.43 | 0.60 | 0.59 |
| Det                                          | Mistral-7B-Instruct-v0.2 | 0.76 | 0.71 | 0.74 | 0.74 | 0.71 | 0.71 | 0.85 | 0.78 | 0.80 | 0.75 | 0.57  | 0.75    | 0.78 | 0.68 | 0.73 | 0.78 | 0.72 | 0.68 | 0.68 | 0.59 | 0.71 | 0.76 | 0.74 |
| Q -                                          | OPT-IML-Max-30b          | 0.69 | 0.66 | 0.76 | 0.80 | 0.71 | 0.72 | 0.78 | 0.79 | 0.77 | 0.71 | 0.60  | 0.76    | 0.77 | 0.71 | 0.75 | 0.77 | 0.70 | 0.63 | 0.79 | 0.69 | 0.65 | 0.75 | 0.73 |
| BLOOMZ-3b-<br>mixed-Detector                 | Vicuna-13b               | 0.88 | 0.77 | 0.84 | 0.79 | 0.81 | 0.74 | 0.88 | 0.83 | 0.81 | 0.78 | 0.77  | 0.77    | 0.85 | 0.78 | 0.81 | 0.83 | 0.66 | 0.75 | 0.78 | 0.67 | 0.73 | 0.83 | 0.79 |
| 8 8                                          | v5-Eagle-7B-HF           | 0.86 | 0.82 | 0.85 | 0.82 | 0.82 | 0.76 | 0.90 | 0.86 | 0.89 | 0.83 | 0.65  | 0.79    | 0.88 | 0.79 | 0.81 | 0.86 | 0.72 | 0.72 | 0.77 | 0.67 | 0.74 | 0.85 | 0.81 |
|                                              | Aya-101                  | 0.75 | 0.79 | 0.73 | 0.63 | 0.75 | 0.70 | 0.82 | 0.75 | 0.78 | 0.61 | 0.52  | 0.61    | 0.72 | 0.67 | 0.61 | 0.61 | 0.67 | 0.73 | 0.60 | 0.60 | 0.71 | 0.76 | 0.67 |
|                                              | GPT-3.5-Turbo-0125       | 0.59 | 0.63 | 0.64 | 0.59 | 0.70 | 0.65 | 0.86 | 0.73 | 0.76 | 0.65 | 0.52  | 0.57    | 0.68 | 0.63 | 0.57 | 0.69 | 0.68 | 0.62 | 0.56 | 0.60 | 0.63 | 0.88 | 0.66 |
|                                              | Gemini                   | 0.74 | 0.84 | 0.80 | 0.78 | 0.88 | 0.72 | 0.97 | 0.90 | 0.94 | 0.95 | N/A   | 0.76    | 0.89 | 0.78 | 0.71 | 0.86 | 0.90 | 0.81 | 0.73 | 0.78 | 0.87 | 0.78 | 0.80 |
| Li te El o                                   | Mistral-7B-Instruct-v0.2 | 0.73 | 0.77 | 0.79 | 0.69 | 0.82 | 0.78 | 0.92 | 0.84 | 0.84 | 0.87 | 0.66  | 0.73    | 0.82 | 0.73 | 0.63 | 0.84 | 0.84 | 0.76 | 0.66 | 0.67 | 0.76 | 0.75 | 0.76 |
| ChatGPT-<br>Detector-<br>RoBERTa-<br>Chinese | OPT-IML-Max-30b          | 0.79 | 0.84 | 0.74 | 0.60 | 0.70 | 0.76 | 0.82 | 0.74 | 0.73 | 0.65 | 0.58  | 0.52    | 0.68 | 0.63 | 0.60 | 0.62 | 0.59 | 0.74 | 0.62 | 0.60 | 0.70 | 0.63 | 0.65 |
| hi Ger                                       | Vicuna-13b               | 0.74 | 0.86 | 0.81 | 0.63 | 0.82 | 0.80 | 0.93 | 0.82 | 0.74 | 0.73 | 0.73  | 0.60    | 0.81 | 0.70 | 0.62 | 0.73 | 0.76 | 0.82 | 0.62 | 0.68 | 0.72 | 0.88 | 0.72 |
|                                              | v5-Eagle-7B-HF           | 0.74 | 0.87 | 0.81 | 0.71 | 0.88 | 0.81 | 0.95 | 0.89 | 0.88 | 0.83 | 0.76  | 0.59    | 0.83 | 0.72 | 0.63 | 0.77 | 0.78 | 0.83 | 0.66 | 0.65 | 0.80 | 0.95 | 0.76 |
| ÷                                            | Aya-101                  | 0.22 | 0.37 | 0.25 | 0.36 | 0.32 | 0.41 | 0.53 | 0.34 | 0.35 | 0.38 | 0.36  | 0.39    | 0.35 | 0.29 | 0.35 | 0.31 | 0.31 | 0.36 | 0.35 | 0.37 | 0.43 | 0.27 | 0.36 |
| Det                                          | GPT-3.5-Turbo-0125       | 0.39 | 0.50 | 0.37 | 0.42 | 0.44 | 0.50 | 0.59 | 0.48 | 0.40 | 0.39 | 0.43  | 0.48    | 0.46 | 0.43 | 0.44 | 0.40 | 0.42 | 0.45 | 0.43 | 0.43 | 0.55 | 0.54 | 0.45 |
| er]                                          | Gemini                   | 0.67 | 0.61 | 0.47 | 0.42 | 0.46 | 0.82 | 0.80 | 0.48 | 0.50 | 0.92 | N/A   | 0.52    | 0.49 | 0.44 | 0.46 | 0.42 | 0.36 | 0.64 | 0.34 | 0.33 | 0.80 | 0.76 | 0.53 |
| E                                            | Mistral-7B-Instruct-v0.2 | 0.33 | 0.48 | 0.37 | 0.58 | 0.51 | 0.54 | 0.63 | 0.50 | 0.61 | 0.64 | 0.53  | 0.61    | 0.62 | 0.49 | 0.56 | 0.51 | 0.49 | 0.47 | 0.59 | 0.55 | 0.66 | 0.50 | 0.52 |
| - Ga                                         | OPT-IML-Max-30b          | 0.17 | 0.42 | 0.28 | 0.33 | 0.32 | 0.39 | 0.50 | 0.33 | 0.41 | 0.44 | 0.38  | 0.42    | 0.43 | 0.31 | 0.35 | 0.35 | 0.35 | 0.29 | 0.33 | 0.44 | 0.51 | 0.26 | 0.36 |
| Longformer Detec<br>tor                      | Vicuna-13b               | 0.28 | 0.47 | 0.24 | 0.55 | 0.42 | 0.53 | 0.66 | 0.42 | 0.48 | 0.58 | 0.50  | 0.54    | 0.50 | 0.40 | 0.49 | 0.41 | 0.45 | 0.49 | 0.51 | 0.50 | 0.60 | 0.42 | 0.46 |
| Z                                            | v5-Eagle-7B-HF           | 0.31 | 0.49 | 0.30 | 0.61 | 0.49 | 0.58 | 0.81 | 0.50 | 0.60 | 0.67 | 0.68  | 0.63    | 0.61 | 0.47 | 0.56 | 0.51 | 0.48 | 0.52 | 0.59 | 0.56 | 0.68 | 0.54 | 0.53 |
| 4 H                                          | Aya-101                  | 0.76 | 0.46 | 0.46 | 0.19 | 0.40 | 0.72 | 0.73 | 0.43 | 0.32 | 0.37 | 0.46  | 0.31    | 0.20 | 0.39 | 0.31 | 0.35 | 0.36 | 0.57 | 0.35 | 0.45 | 0.43 | 0.69 | 0.43 |
| sct                                          | GPT-3.5-Turbo-0125       | 0.60 | 0.41 | 0.45 | 0.16 | 0.34 | 0.59 | 0.48 | 0.34 | 0.25 | 0.28 | 0.51  | 0.20    | 0.17 | 0.31 | 0.23 | 0.26 | 0.28 | 0.48 | 0.27 | 0.38 | 0.30 | 0.61 | 0.35 |
| et ja                                        | Gemini                   | 0.65 | 0.30 | 0.51 | 0.05 | 0.14 | 0.76 | 0.14 | 0.11 | 0.05 | 0.41 | N/A   | 0.06    | 0.04 | 0.13 | 0.10 | 0.08 | 0.05 | 0.47 | 0.08 | 0.08 | 0.29 | 0.43 | 0.21 |
| RoBERTà-large-<br>OpenAI-Detector            | Mistral-7B-Instruct-v0.2 | 0.69 | 0.39 | 0.38 | 0.10 | 0.30 | 0.72 | 0.40 | 0.25 | 0.22 | 0.14 | 0.30  | 0.16    | 0.11 | 0.30 | 0.19 | 0.17 | 0.21 | 0.49 | 0.26 | 0.34 | 0.30 | 0.55 | 0.31 |
| EI A                                         | OPT-IML-Max-30b          | 0.86 | 0.60 | 0.54 | 0.30 | 0.52 | 0.79 | 0.71 | 0.49 | 0.32 | 0.51 | 0.67  | 0.34    | 0.24 | 0.51 | 0.36 | 0.40 | 0.40 | 0.70 | 0.48 | 0.54 | 0.60 | 0.83 | 0.49 |
| ž č                                          | Vicuna-13b               | 0.80 | 0.45 | 0.40 | 0.10 | 0.29 | 0.79 | 0.57 | 0.30 | 0.17 | 0.21 | 0.47  | 0.18    | 0.12 | 0.26 | 0.17 | 0.20 | 0.23 | 0.54 | 0.21 | 0.31 | 0.36 | 0.60 | 0.33 |
|                                              | v5-Eagle-7B-HF           | 0.78 | 0.40 | 0.34 | 0.08 | 0.28 | 0.83 | 0.62 | 0.24 | 0.11 | 0.33 | 0.48  | 0.13    | 0.07 | 0.20 | 0.15 | 0.16 | 0.21 | 0.52 | 0.18 | 0.26 | 0.26 | 0.52 | 0.31 |
|                                              | Aya-101                  | 0.50 | 0.61 | 0.57 | 0.47 | 0.48 | 0.45 | 0.60 | 0.54 | 0.48 | 0.47 | 0.49  | 0.53    | 0.56 | 0.57 | 0.48 | 0.50 | 0.43 | 0.64 | 0.52 | 0.57 | 0.50 | 0.45 | 0.53 |
|                                              | GPT-3.5-Turbo-0125       | 0.46 | 0.66 | 0.57 | 0.45 | 0.46 | 0.39 | 0.58 | 0.50 | 0.45 | 0.46 | 0.45  | 0.51    | 0.50 | 0.52 | 0.45 | 0.45 | 0.41 | 0.69 | 0.48 | 0.48 | 0.52 | 0.22 | 0.50 |
| ar Ja                                        | Gemini                   | 0.17 | 0.25 | 0.53 | 0.14 | 0.19 | 0.11 | 0.19 | 0.17 | 0.11 | 0.30 | N/A   | 0.10    | 0.12 | 0.24 | 0.17 | 0.15 | 0.08 | 0.30 | 0.12 | 0.11 | 0.23 | 0.26 | 0.18 |
| bin El                                       | Mistral-7B-Instruct-v0.2 | 0.42 | 0.69 | 0.53 | 0.50 | 0.49 | 0.35 | 0.58 | 0.52 | 0.49 | 0.45 | 0.38  | 0.57    | 0.53 | 0.56 | 0.48 | 0.49 | 0.41 | 0.79 | 0.52 | 0.52 | 0.69 | 0.52 | 0.53 |
| Ed-1                                         | OPT-IML-Max-30b          | 0.64 | 0.82 | 0.63 | 0.54 | 0.52 | 0.50 | 0.66 | 0.55 | 0.54 | 0.50 | 0.44  | 0.57    | 0.60 | 0.59 | 0.52 | 0.53 | 0.43 | 0.92 | 0.61 | 0.65 | 0.91 | 0.58 | 0.59 |
| ruRoBERTa-<br>ruatd-binary                   | Vicuna-13b               | 0.31 | 0.69 | 0.54 | 0.45 | 0.43 | 0.36 | 0.66 | 0.49 | 0.43 | 0.51 | 0.57  | 0.53    | 0.49 | 0.51 | 0.44 | 0.45 | 0.34 | 0.82 | 0.45 | 0.50 | 0.62 | 0.57 | 0.51 |
|                                              | v5-Eagle-7B-HF           | 0.35 | 0.72 | 0.53 | 0.50 | 0.44 | 0.29 | 0.66 | 0.51 | 0.51 | 0.59 | 0.50  | 0.56    | 0.50 | 0.51 | 0.51 | 0.46 | 0.33 | 0.78 | 0.45 | 0.50 | 0.66 | 0.45 | 0.52 |

Table 21: Per-LLM AUC ROC performance of pre-trained MGT detectors category. N/A refers to not enough samples per each class (at least 10) making AUC ROC value irrelevant.

<span id="page-21-1"></span>

|                                   |                                    |      |      |      |      |      |      |      |              |              | Test | Langu       | iage [A | UC R | OC]       |      |      |      |              |      |      |      |      |      |
|-----------------------------------|------------------------------------|------|------|------|------|------|------|------|--------------|--------------|------|-------------|---------|------|-----------|------|------|------|--------------|------|------|------|------|------|
| Detector                          | Generator                          | ar   | bg   | ca   | cs   | de   | el   | en   | es           | et           | ga   | gd          | hr      | hu   | nl        | pl   | pt   | ro   | ru           | sk   | sl   | uk   | zh   | all  |
|                                   | Aya-101                            | 0.92 | 0.98 | 0.97 | 0.98 | 0.97 | 0.96 | 0.96 | 0.96         | 0.98         | 0.92 | 0.91        | 0.98    | 0.99 | 0.97      | 0.97 | 0.96 | 0.96 | 0.93         | 0.97 | 0.92 | 0.90 | 0.93 | 0.96 |
|                                   | GPT-3.5-Turbo-0125                 | 0.99 | 1.00 | 1.00 | 0.99 | 0.99 | 0.99 | 1.00 | 0.99         | 0.99         | 0.97 | 0.96        | 0.99    | 1.00 | 0.99      | 0.99 | 0.99 | 0.99 | 0.99         | 0.99 | 0.97 | 0.98 | 0.99 | 0.99 |
| -                                 | Gemini                             | 0.90 | 0.96 | 0.88 | 0.95 | 0.92 | 0.92 | 0.96 | 0.96         | 0.99         | 0.85 | N/A         | 0.96    | 0.99 | 0.92      | 0.94 | 0.94 | 0.96 | 0.83         | 0.92 | 0.89 | 0.87 | 0.94 | 0.93 |
| - cia                             | Mistral-7B-Instruct-v0.2           | 0.99 | 1.00 | 0.99 | 1.00 | 0.99 | 0.99 | 1.00 | 0.99         | 0.99         | 0.99 | 0.99        | 0.99    | 1.00 | 0.98      | 0.99 | 0.99 | 1.00 | 0.99         | 1.00 | 0.98 | 0.98 | 0.98 | 0.99 |
| 101<br>102                        | OPT-IML-Max-30b                    | 0.98 | 0.99 | 0.96 | 0.97 | 0.94 | 0.96 | 0.92 | 0.95         | 0.95         | 0.84 | 0.77        | 0.95    | 0.98 | 0.95      | 0.97 | 0.95 | 0.96 | 0.98         | 0.96 | 0.91 | 0.95 | 0.99 | 0.95 |
| Aya-101-<br>MultiSocial           | Vicuna-13b                         | 1.00 | 1.00 | 0.99 | 1.00 | 0.99 | 0.99 | 1.00 | 0.99         | 0.99         | 0.98 | 0.95        | 0.99    | 1.00 | 0.99      | 0.99 | 0.99 | 1.00 | 0.99         | 0.99 | 0.98 | 0.97 | 0.99 | 0.99 |
| Ϋ́                                | v5-Eagle-7B-HF                     | 0.99 | 1.00 | 1.00 | 1.00 | 0.99 | 0.99 | 1.00 | 1.00         | 1.00         | 0.98 | 0.94        | 1.00    | 1.00 | 0.99      | 1.00 | 1.00 | 1.00 | 0.99         | 1.00 | 0.98 | 0.99 | 0.99 | 0.99 |
|                                   | Ava-101                            | 0.92 | 0.97 | 0.96 | 0.96 | 0.95 | 0.94 | 0.96 | 0.96         | 0.96         | 0.86 | 0.71        | 0.94    | 0.99 | 0.94      | 0.94 | 0.95 | 0.92 | 0.90         | 0.94 | 0.84 | 0.85 | 0.90 | 0.94 |
|                                   | GPT-3.5-Turbo-0125                 | 0.99 | 0.99 | 0.99 | 0.98 | 0.98 | 0.97 | 0.99 | 0.99         | 0.98         | 0.90 | 0.84        | 0.97    | 0.99 | 0.97      | 0.97 | 0.99 | 0.96 | 0.97         | 0.97 | 0.91 | 0.93 | 0.99 | 0.98 |
| BLOOMZ-3b-<br>MultiSocial         | Gemini                             | 0.91 | 0.93 | 0.88 | 0.96 | 0.91 | 0.93 | 0.97 | 0.97         | 0.99         | 0.91 | N/A         | 0.95    | 0.99 | 0.90      | 0.92 | 0.96 | 0.93 | 0.82         | 0.89 | 0.75 | 0.77 | 0.96 | 0.93 |
| BLOOMZ-:<br>MultiSocial           | Mistral-7B-Instruct-v0.2           | 0.96 | 0.99 | 0.98 | 0.98 | 0.97 | 0.97 | 1.00 | 0.98         | 0.99         | 0.95 | 0.81        | 0.98    | 1.00 | 0.93      | 0.96 | 0.99 | 0.97 | 0.96         | 0.98 | 0.91 | 0.94 | 0.98 | 0.97 |
| S O                               | OPT-IML-Max-30b                    | 0.95 | 0.98 | 0.93 | 0.94 | 0.89 | 0.93 | 0.93 | 0.94         | 0.94         | 0.84 | 0.76        | 0.90    | 0.97 | 0.89      | 0.92 | 0.93 | 0.90 | 0.96         | 0.94 | 0.86 | 0.92 | 0.97 | 0.93 |
| QH                                | Vicuna-13b                         | 0.99 | 0.99 | 0.98 | 0.98 | 0.98 | 0.98 | 1.00 | 0.99         | 0.98         | 0.93 | 0.88        | 0.97    | 0.99 | 0.97      | 0.98 | 0.99 | 0.96 | 0.97         | 0.98 | 0.90 | 0.91 | 0.99 | 0.98 |
| M                                 | v5-Eagle-7B-HF                     | 0.98 | 0.99 | 0.99 | 0.99 | 0.98 | 0.98 | 1.00 | 1.00         | 0.99         | 0.94 | 0.86        | 0.99    | 1.00 | 0.98      | 0.98 | 1.00 | 0.99 | 0.98         | 0.98 | 0.95 | 0.95 | 1.00 | 0.99 |
|                                   | Aya-101                            | 0.93 | 0.97 | 0.97 | 0.97 | 0.95 | 0.94 | 0.96 | 0.96         | 0.97         | 0.88 | 0.82        | 0.95    | 0.99 | 0.95      | 0.95 | 0.95 | 0.93 | 0.91         | 0.95 | 0.85 | 0.87 | 0.91 | 0.94 |
|                                   | GPT-3.5-Turbo-0125                 | 0.98 | 0.99 | 0.99 | 0.98 | 0.98 | 0.98 | 0.99 | 0.98         | 0.98         | 0.93 | 0.89        | 0.98    | 0.99 | 0.98      | 0.98 | 0.98 | 0.97 | 0.97         | 0.97 | 0.91 | 0.94 | 0.99 | 0.98 |
| 4                                 | Gemini                             | 0.86 | 0.96 | 0.89 | 0.95 | 0.92 | 0.93 | 0.99 | 0.94         | 0.98         | 0.92 | N/A         | 0.95    | 0.99 | 0.91      | 0.92 | 0.92 | 0.92 | 0.87         | 0.88 | 0.72 | 0.87 | 0.90 | 0.93 |
| Falcon-rw-1b-<br>MultiSocial      | Mistral-7B-Instruct-v0.2           | 0.80 | 0.90 | 0.98 | 0.95 | 0.92 | 0.95 | 1.00 | 0.94         | 0.98         | 0.92 | 0.93        | 0.95    | 0.99 | 0.91      | 0.92 | 0.92 | 0.92 | 0.94         | 0.88 | 0.72 | 0.92 | 0.90 | 0.95 |
| Soc Soc                           | OPT-IML-Max-30b                    | 0.96 | 0.98 | 0.94 | 0.95 | 0.90 | 0.95 | 0.93 | 0.93         | 0.94         | 0.84 | 0.72        | 0.92    | 0.98 | 0.90      | 0.93 | 0.92 | 0.92 | 0.95         | 0.95 | 0.88 | 0.92 | 0.97 | 0.93 |
| ilti:                             | Vicuna-13b                         | 0.90 | 0.98 | 0.94 | 0.99 | 0.90 | 0.95 | 1.00 | 0.93         | 0.94         | 0.95 | 0.93        | 0.92    | 0.98 | 0.90      | 0.93 | 0.92 | 0.92 | 0.95         | 0.95 | 0.89 | 0.88 | 0.97 | 0.93 |
| Mr                                | v5-Eagle-7B-HF                     | 0.98 | 0.98 | 0.98 | 0.99 | 0.98 | 0.98 | 1.00 | 0.98         | 0.99         | 0.95 | 0.93        | 0.99    | 1.00 | 0.97      | 0.98 | 0.98 | 0.99 | 0.90         | 0.98 | 0.94 | 0.88 | 0.98 | 0.98 |
|                                   | Aya-101                            | 0.98 | 0.99 | 0.98 | 0.99 | 0.98 | 0.96 | 0.97 | 0.99         | 0.99         | 0.90 | 0.92        | 0.99    | 0.99 | 0.98      | 0.98 | 0.99 | 0.95 | 0.97         | 0.98 | 0.94 | 0.93 | 0.95 | 0.96 |
|                                   | GPT-3.5-Turbo-0125                 | 0.94 | 1.00 | 1.00 | 1.00 | 0.97 | 0.96 | 1.00 | 0.97         | 0.98         | 0.90 | 0.81        | 0.98    | 1.00 | 0.97      | 0.97 | 0.96 | 0.95 | 0.94         | 0.97 | 0.91 | 0.92 | 1.00 | 0.96 |
|                                   | GP 1-5.5- Turbo-0125<br>Gemini     | 0.99 | 0.96 | 0.91 | 0.96 | 0.99 | 0.99 |      |              |              |      | 0.95<br>N/A | 0.99    | 0.99 |           | 0.99 | 0.99 | 0.99 |              | 0.99 | 0.96 | 0.98 | 0.96 | 0.99 |
| d8<br>Iai                         | Gemini<br>Mistral-7B-Instruct-v0.2 | 0.89 | 1.00 | 0.91 | 0.96 | 0.95 | 0.89 | 0.99 | 0.97<br>0.99 | 0.98<br>0.99 | 0.91 | N/A<br>0.99 | 0.98    | 1.00 | 0.94 0.98 | 0.96 | 0.96 | 0.96 | 0.87<br>0.99 | 0.94 | 0.91 | 0.88 | 0.96 | 0.95 |
| Llama-3-8b-<br>MultiSocial        | OPT-IML-Max-30b                    | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.94 | 0.99         | 0.99         | 0.99 | 0.99        | 0.99    | 0.99 | 0.98      | 0.99 | 0.99 | 0.99 | 0.99         | 0.99 | 0.97 | 0.98 | 0.99 | 0.99 |
| liti m                            | Vicuna-13b                         | 1.00 | 1.00 | 0.97 | 1.00 | 0.94 | 0.90 | 1.00 | 0.90         | 0.90         | 0.85 | 0.72        | 0.90    | 1.00 | 0.95      | 0.97 | 0.90 | 0.95 | 0.98         | 0.97 | 0.92 | 0.90 | 1.00 | 0.90 |
| Mu                                |                                    | 0.99 | 1.00 |      |      |      | 0.99 |      |              | 1.00         | 0.98 | 0.92        | 1.00    | 1.00 | 0.99      |      | 1.00 | 1.00 |              | 0.99 | 0.98 | 0.97 | 1.00 | 0.99 |
|                                   | v5-Eagle-7B-HF                     |      |      | 1.00 | 1.00 | 0.99 |      | 1.00 | 1.00         |              |      |             |         |      |           | 1.00 |      |      | 0.99         |      |      |      |      |      |
| ÷                                 | Aya-101                            | 0.94 | 0.98 | 0.97 | 0.98 | 0.97 | 0.95 | 0.97 | 0.97         | 0.98         | 0.90 | 0.89        | 0.98    | 0.99 | 0.97      | 0.97 | 0.96 | 0.95 | 0.94         | 0.96 | 0.90 | 0.91 | 0.94 | 0.96 |
| <u>ç</u>                          | GPT-3.5-Turbo-0125                 | 0.99 | 1.00 | 1.00 | 1.00 | 0.99 | 0.98 | 1.00 | 0.99         | 0.99         | 0.95 | 0.96        | 0.99    | 1.00 | 0.99      | 0.99 | 0.99 | 0.99 | 0.99         | 0.98 | 0.96 | 0.98 | 1.00 | 0.99 |
| ا أ                               | Gemini                             | 0.91 | 0.98 | 0.91 | 0.97 | 0.96 | 0.93 | 0.99 | 0.99         | 0.98         | 0.95 | N/A         | 0.99    | 0.99 | 0.95      | 0.96 | 0.97 | 0.96 | 0.90         | 0.93 | 0.90 | 0.92 | 0.95 | 0.96 |
| Mistral-7b-v0.1-<br>MultiSocial   | Mistral-7B-Instruct-v0.2           | 0.98 | 1.00 | 0.99 | 1.00 | 0.99 | 0.99 | 1.00 | 0.99         | 0.99         | 0.96 | 0.97        | 1.00    | 1.00 | 0.98      | 0.99 | 0.99 | 0.99 | 0.99         | 0.99 | 0.97 | 0.98 | 0.99 | 0.99 |
| ttra<br>ItiS                      | OPT-IML-Max-30b                    | 0.98 | 0.99 | 0.96 | 0.98 | 0.94 | 0.97 | 0.95 | 0.96         | 0.96         | 0.86 | 0.79        | 0.96    | 0.99 | 0.95      | 0.97 | 0.95 | 0.95 | 0.98         | 0.97 | 0.91 | 0.96 | 0.99 | 0.96 |
| Au Vis                            | Vicuna-13b                         | 0.99 | 1.00 | 0.99 | 1.00 | 0.99 | 0.99 | 1.00 | 0.99         | 0.99         | 0.95 | 0.95        | 0.99    | 1.00 | 0.99      | 0.99 | 0.99 | 0.99 | 0.99         | 0.99 | 0.97 | 0.97 | 0.99 | 0.99 |
| ~ ~                               | v5-Eagle-7B-HF                     | 0.98 | 1.00 | 1.00 | 1.00 | 0.99 | 0.99 | 1.00 | 1.00         | 1.00         | 0.96 | 0.97        | 1.00    | 1.00 | 0.99      | 1.00 | 1.00 | 0.99 | 0.99         | 0.99 | 0.99 | 0.99 | 1.00 | 0.99 |
| a la                              | Aya-101                            | 0.89 | 0.97 | 0.94 | 0.97 | 0.95 | 0.94 | 0.93 | 0.94         | 0.95         | 0.81 | 0.73        | 0.94    | 0.98 | 0.95      | 0.95 | 0.91 | 0.93 | 0.90         | 0.96 | 0.87 | 0.86 | 0.85 | 0.93 |
| XLM-RoBERTa-<br>large-MultiSocial | GPT-3.5-Turbo-0125                 | 0.98 | 0.99 | 0.98 | 0.99 | 0.98 | 0.97 | 0.99 | 0.98         | 0.97         | 0.92 | 0.88        | 0.98    | 0.99 | 0.98      | 0.98 | 0.98 | 0.98 | 0.97         | 0.98 | 0.95 | 0.96 | 0.98 | 0.98 |
| BE                                | Gemini                             | 0.89 | 0.96 | 0.78 | 0.96 | 0.93 | 0.94 | 0.98 | 0.96         | 0.99         | 0.84 | N/A         | 0.98    | 0.99 | 0.91      | 0.95 | 0.96 | 0.97 | 0.89         | 0.95 | 0.88 | 0.91 | 0.93 | 0.94 |
| Au Ro                             | Mistral-7B-Instruct-v0.2           | 0.98 | 0.99 | 0.96 | 0.99 | 0.97 | 0.97 | 0.99 | 0.96         | 0.97         | 0.96 | 0.91        | 0.98    | 0.99 | 0.93      | 0.98 | 0.98 | 0.98 | 0.97         | 0.97 | 0.94 | 0.96 | 0.95 | 0.97 |
|                                   | OPT-IML-Max-30b                    | 0.94 | 0.97 | 0.88 | 0.94 | 0.88 | 0.90 | 0.88 | 0.92         | 0.91         | 0.69 | 0.47        | 0.90    | 0.95 | 0.88      | 0.93 | 0.88 | 0.91 | 0.93         | 0.90 | 0.82 | 0.86 | 0.89 | 0.90 |
|                                   | Vicuna-13b                         | 0.99 | 0.99 | 0.96 | 0.99 | 0.98 | 0.97 | 0.99 | 0.98         | 0.97         | 0.91 | 0.80        | 0.98    | 0.99 | 0.97      | 0.99 | 0.98 | 0.98 | 0.98         | 0.97 | 0.94 | 0.94 | 0.96 | 0.98 |
| × 1                               | v5-Eagle-7B-HF                     | 0.98 | 0.99 | 0.98 | 1.00 | 0.99 | 0.98 | 0.99 | 0.99         | 0.99         | 0.95 | 0.83        | 0.99    | 1.00 | 0.98      | 0.99 | 0.99 | 0.99 | 0.98         | 0.99 | 0.97 | 0.98 | 0.98 | 0.99 |
|                                   | Aya-101                            | 0.88 | 0.97 | 0.92 | 0.97 | 0.95 | 0.93 | 0.94 | 0.94         | 0.97         | 0.86 | 0.74        | 0.96    | 0.99 | 0.95      | 0.95 | 0.93 | 0.93 | 0.89         | 0.95 | 0.89 | 0.87 | 0.88 | 0.93 |
| mDeBERTa-v3-<br>base-MultiSocial  | GPT-3.5-Turbo-0125                 | 0.97 | 0.99 | 0.98 | 0.99 | 0.98 | 0.97 | 0.99 | 0.98         | 0.98         | 0.91 | 0.82        | 0.98    | 0.99 | 0.98      | 0.98 | 0.97 | 0.97 | 0.96         | 0.98 | 0.95 | 0.96 | 0.99 | 0.98 |
| Ia-                               | Gemini                             | 0.83 | 0.95 | 0.81 | 0.92 | 0.88 | 0.87 | 0.94 | 0.93         | 0.97         | 0.78 | N/A         | 0.92    | 0.99 | 0.89      | 0.91 | 0.91 | 0.91 | 0.80         | 0.88 | 0.81 | 0.85 | 0.83 | 0.90 |
| EK                                | Mistral-7B-Instruct-v0.2           | 0.98 | 0.99 | 0.96 | 0.99 | 0.97 | 0.97 | 0.99 | 0.97         | 0.98         | 0.96 | 0.86        | 0.99    | 1.00 | 0.95      | 0.98 | 0.98 | 0.99 | 0.97         | 0.98 | 0.96 | 0.96 | 0.97 | 0.97 |
| ₩.                                | OPT-IML-Max-30b                    | 0.96 | 0.99 | 0.91 | 0.95 | 0.91 | 0.92 | 0.90 | 0.93         | 0.93         | 0.75 | 0.54        | 0.93    | 0.97 | 0.91      | 0.95 | 0.92 | 0.93 | 0.95         | 0.94 | 0.89 | 0.92 | 0.97 | 0.93 |
| ase                               | Vicuna-13b                         | 0.99 | 0.99 | 0.96 | 0.99 | 0.98 | 0.98 | 0.99 | 0.98         | 0.98         | 0.95 | 0.86        | 0.99    | 0.99 | 0.98      | 0.99 | 0.99 | 0.99 | 0.98         | 0.98 | 0.96 | 0.95 | 0.98 | 0.98 |
| p n                               | v5-Eagle-7B-HF                     | 0.97 | 0.99 | 0.98 | 0.99 | 0.99 | 0.98 | 0.99 | 0.99         | 0.99         | 0.95 | 0.87        | 0.99    | 1.00 | 0.99      | 0.99 | 0.99 | 0.99 | 0.98         | 0.99 | 0.98 | 0.98 | 0.98 | 0.99 |

Table 22: Per-LLM AUC ROC performance of fine-tuned MGT detectors category. N/A refers to not enough samples per each class (at least 10) making AUC ROC value irrelevant.

<span id="page-22-0"></span>

|                     |          |      |      |      |      |      |      |      |      |      | Test | Langu | age [A | UC R | OC]  |      |      |      |      |      |      |      |      |      |
|---------------------|----------|------|------|------|------|------|------|------|------|------|------|-------|--------|------|------|------|------|------|------|------|------|------|------|------|
| Detector            | Platform | ar   | bg   | ca   | cs   | de   | el   | en   | es   | et   | ga   | gd    | hr     | hu   | nl   | pl   | pt   | ro   | ru   | sk   | sl   | uk   | zh   | all  |
|                     | Discord  | N/A  | N/A  | 0.78 | 0.76 | 0.80 | N/A  | 0.86 | 0.81 | 0.75 | 0.67 | 0.70  | 0.83   | 0.82 | 0.80 | 0.84 | 0.81 | 0.77 | N/A  | 0.72 | N/A  | N/A  | N/A  | 0.79 |
| ar                  | Gab      | 0.61 | 0.62 | 0.47 | 0.58 | 0.69 | 0.71 | 0.78 | 0.72 | 0.70 | 0.72 | N/A   | 0.72   | 0.68 | 0.71 | 0.67 | 0.72 | 0.67 | 0.69 | 0.62 | 0.70 | 0.63 | 0.75 | 0.68 |
| cu                  | Telegram | 0.71 | 0.69 | 0.61 | 0.77 | 0.74 | 0.83 | 0.81 | 0.77 | 0.75 | 0.72 | N/A   | 0.79   | 0.82 | 0.72 | 0.78 | 0.76 | 0.79 | 0.65 | 0.72 | 0.77 | 0.63 | 0.73 | 0.73 |
| Binoculars          | Twitter  | 0.67 | 0.69 | 0.64 | 0.76 | 0.82 | 0.92 | 0.81 | 0.67 | 0.84 | N/A  | N/A   | 0.85   | 0.79 | 0.72 | 0.82 | 0.80 | 0.77 | 0.85 | 0.87 | N/A  | N/A  | 0.80 | 0.74 |
|                     | WhatsApp | 0.82 | N/A  | 0.73 | 0.66 | 0.79 | N/A  | 0.75 | 0.81 | 0.52 | N/A  | N/A   | N/A    | N/A  | 0.74 | N/A  | 0.70 | 0.85 | 0.66 | N/A  | N/A  | N/A  | N/A  | 0.73 |
| DetectLLM-<br>LRR   | Discord  | N/A  | N/A  | 0.95 | 0.98 | 0.91 | N/A  | 0.92 | 0.93 | 0.94 | 0.83 | 0.75  | 0.94   | 0.98 | 0.94 | 0.98 | 0.95 | 0.96 | N/A  | 0.90 | N/A  | N/A  | N/A  | 0.94 |
| Ξ                   | Gab      | 0.74 | 0.80 | 0.58 | 0.80 | 0.74 | 0.79 | 0.78 | 0.75 | 0.71 | 0.75 | N/A   | 0.77   | 0.81 | 0.72 | 0.82 | 0.75 | 0.77 | 0.72 | 0.71 | 0.74 | 0.77 | 0.78 | 0.69 |
| ⊊ ct                | Telegram | 0.76 | 0.86 | 0.63 | 0.94 | 0.69 | 0.94 | 0.81 | 0.79 | 0.92 | 0.77 | N/A   | 0.94   | 0.97 | 0.74 | 0.88 | 0.86 | 0.92 | 0.79 | 0.89 | 0.96 | 0.74 | 0.76 | 0.75 |
| RI etc              | Twitter  | 0.81 | 0.87 | 0.75 | 0.91 | 0.91 | 0.96 | 0.85 | 0.78 | 0.87 | N/A  | N/A   | 0.94   | 0.94 | 0.85 | 0.95 | 0.91 | 0.90 | 0.93 | 0.95 | N/A  | N/A  | 0.92 | 0.75 |
|                     | WhatsApp | 0.87 | N/A  | 0.87 | 0.96 | 0.80 | N/A  | 0.66 | 0.89 | 0.91 | N/A  | N/A   | N/A    | N/A  | 0.86 | N/A  | 0.80 | 0.95 | 0.69 | N/A  | N/A  | N/A  | N/A  | 0.70 |
| Fast-Detect-<br>GPT | Discord  | N/A  | N/A  | 0.64 | 0.81 | 0.81 | N/A  | 0.86 | 0.82 | 0.70 | 0.76 | 0.76  | 0.81   | 0.79 | 0.81 | 0.83 | 0.84 | 0.80 | N/A  | 0.65 | N/A  | N/A  | N/A  | 0.79 |
| ete                 | Gab      | 0.70 | 0.68 | 0.57 | 0.69 | 0.72 | 0.60 | 0.76 | 0.71 | 0.62 | 0.64 | N/A   | 0.72   | 0.65 | 0.74 | 0.73 | 0.72 | 0.71 | 0.69 | 0.64 | 0.69 | 0.72 | 0.78 | 0.70 |
| 9                   | Telegram | 0.76 | 0.65 | 0.62 | 0.83 | 0.73 | 0.69 | 0.81 | 0.74 | 0.70 | 0.70 | N/A   | 0.83   | 0.81 | 0.72 | 0.78 | 0.76 | 0.80 | 0.69 | 0.72 | 0.83 | 0.69 | 0.72 | 0.74 |
| ast<br>PT           | Twitter  | 0.71 | 0.65 | 0.62 | 0.82 | 0.82 | 0.78 | 0.81 | 0.65 | 0.67 | N/A  | N/A   | 0.83   | 0.78 | 0.72 | 0.77 | 0.79 | 0.78 | 0.87 | 0.77 | N/A  | N/A  | 0.85 | 0.75 |
| <b>H</b> O          | WhatsApp | 0.85 | N/A  | 0.69 | 0.75 | 0.79 | N/A  | 0.77 | 0.80 | 0.70 | N/A  | N/A   | N/A    | N/A  | 0.72 | N/A  | 0.77 | 0.80 | 0.64 | N/A  | N/A  | N/A  | N/A  | 0.77 |
|                     | Discord  | N/A  | N/A  | 0.95 | 0.98 | 0.92 | N/A  | 0.93 | 0.95 | 0.95 | 0.86 | 0.81  | 0.94   | 0.99 | 0.94 | 0.98 | 0.96 | 0.97 | N/A  | 0.91 | N/A  | N/A  | N/A  | 0.94 |
| on                  | Gab      | 0.75 | 0.80 | 0.58 | 0.82 | 0.76 | 0.82 | 0.76 | 0.77 | 0.75 | 0.78 | N/A   | 0.78   | 0.82 | 0.73 | 0.83 | 0.75 | 0.78 | 0.73 | 0.70 | 0.76 | 0.81 | 0.81 | 0.70 |
| iati                | Telegram | 0.79 | 0.85 | 0.62 | 0.94 | 0.72 | 0.93 | 0.81 | 0.79 | 0.92 | 0.80 | N/A   | 0.95   | 0.98 | 0.76 | 0.89 | 0.86 | 0.93 | 0.79 | 0.90 | 0.97 | 0.74 | 0.77 | 0.74 |
| LLM-<br>Deviation   | Twitter  | 0.83 | 0.88 | 0.74 | 0.92 | 0.92 | 0.97 | 0.85 | 0.75 | 0.88 | N/A  | N/A   | 0.94   | 0.95 | 0.82 | 0.96 | 0.92 | 0.91 | 0.93 | 0.96 | N/A  | N/A  | 0.92 | 0.74 |
| ПП                  | WhatsApp | 0.90 |      | 0.88 | 0.97 | 0.82 | N/A  | 0.64 | 0.89 | 0.91 | N/A  | N/A   | N/A    | N/A  | 0.87 | N/A  | 0.82 | 0.96 |      | N/A  | N/A  | N/A  | N/A  | 0.70 |
|                     | Discord  | N/A  | N/A  | 0.94 | 0.97 | 0.91 | N/A  | 0.91 | 0.93 | 0.93 | 0.86 | 0.79  | 0.93   | 0.98 | 0.93 | 0.97 | 0.95 | 0.96 | N/A  | 0.89 | N/A  | N/A  | N/A  | 0.93 |
|                     | Gab      | 0.74 | 0.79 | 0.59 | 0.81 | 0.74 | 0.82 | 0.73 | 0.76 | 0.76 | 0.77 | N/A   | 0.77   | 0.80 | 0.72 | 0.81 | 0.74 | 0.77 | 0.72 | 0.67 | 0.74 | 0.79 | 0.81 | 0.69 |
|                     | Telegram | 0.78 | 0.84 | 0.62 | 0.93 | 0.70 | 0.93 | 0.79 | 0.77 | 0.91 |      | N/A   | 0.94   | 0.97 | 0.75 | 0.88 | 0.85 | 0.92 | 0.79 | 0.89 | 0.95 | 0.74 | 0.76 | 0.73 |
| 10                  | Twitter  | 0.80 | 0.87 | 0.71 | 0.91 | 0.90 | 0.97 | 0.83 | 0.73 | 0.86 | N/A  | N/A   | 0.94   | 0.93 | 0.78 | 0.95 | 0.91 | 0.90 | 0.93 | 0.94 | N/A  | N/A  | 0.90 | 0.74 |
| Ś                   | WhatsApp | 0.90 | N/A  | 0.86 | 0.97 | 0.83 | N/A  | 0.62 | 0.87 | 0.90 | N/A  | N/A   | N/A    | N/A  | 0.86 | N/A  | 0.81 | 0.96 | 0.70 | N/A  | N/A  | N/A  | N/A  | 0.70 |

Table 23: Per-platform AUC ROC performance of statistical MGT detectors category. N/A refers to not enough samples per each class (at least 10) making AUC ROC value irrelevant.

|                                             |          |      |      |      |      |      |      |      |      |      | Test | Langu | age [A | UC R | OC]  |      |      |      |      |      |      |      |      |      |
|---------------------------------------------|----------|------|------|------|------|------|------|------|------|------|------|-------|--------|------|------|------|------|------|------|------|------|------|------|------|
| Detector                                    | Platform | ar   | bg   | ca   | cs   | de   | el   | en   | es   | et   | ga   | gd    | hr     | hu   | nl   | pl   | pt   | ro   | ru   | sk   | sl   | uk   | zh   | all  |
| 3                                           | Discord  | N/A  | N/A  | 0.96 | 0.89 | 0.87 | N/A  | 0.87 | 0.90 | 0.87 | 0.80 | 0.66  | 0.84   | 0.86 | 0.91 | 0.91 | 0.89 | 0.78 | N/A  | 0.71 | N/A  | N/A  | N/A  | 0.87 |
| Σų - μ                                      | Gab      | 0.69 | 0.71 | 0.77 | 0.69 | 0.67 | 0.62 | 0.77 | 0.69 | 0.73 | 0.75 | N/A   | 0.61   | 0.75 | 0.69 | 0.70 | 0.69 | 0.53 | 0.59 | 0.70 | 0.54 | 0.58 | 0.64 | 0.66 |
| BLOOMZ-<br>3b-mixed-<br>Detector            | Telegram | 0.81 | 0.79 | 0.73 | 0.81 | 0.76 | 0.85 | 0.83 | 0.80 | 0.84 | 0.78 | N/A   | 0.80   | 0.88 | 0.74 | 0.77 | 0.80 | 0.75 | 0.75 | 0.76 | 0.84 | 0.71 | 0.71 | 0.78 |
| ete FLC                                     | Twitter  | 0.82 | 0.68 | 0.70 | 0.71 | 0.78 | 0.68 | 0.81 | 0.68 | 0.77 | N/A  | N/A   | 0.81   | 0.71 | 0.70 | 0.70 | 0.80 | 0.59 | 0.73 | 0.90 | N/A  | N/A  | 0.68 | 0.72 |
| a ど U                                       | WhatsApp | 0.82 | N/A  | 0.88 | 0.46 | 0.78 | N/A  | 0.76 | 0.87 | 0.81 | N/A  | N/A   | N/A    | N/A  | 0.67 | N/A  | 0.81 | 0.78 | 0.80 | N/A  | N/A  | N/A  | N/A  | 0.79 |
|                                             | Discord  | N/A  | N/A  | 0.88 | 0.72 | 0.86 | N/A  | 0.95 | 0.88 | 0.85 | 0.73 | 0.62  | 0.66   | 0.82 | 0.71 | 0.70 | 0.75 | 0.83 | N/A  | 0.68 | N/A  | N/A  | N/A  | 0.77 |
| ChatGPT-<br>Detector-<br>RoBERTa<br>Chinese | Gab      | 0.57 | 0.81 | 0.65 | 0.55 | 0.74 | 0.68 | 0.88 | 0.74 | 0.74 | 0.72 | N/A   | 0.60   | 0.66 | 0.72 | 0.58 | 0.75 | 0.68 | 0.65 | 0.75 | 0.61 | 0.74 | 0.78 | 0.67 |
| ER Cto                                      | Telegram | 0.65 | 0.75 | 0.73 | 0.66 | 0.82 | 0.78 | 0.90 | 0.80 | 0.83 | 0.75 | N/A   | 0.61   | 0.83 | 0.63 | 0.62 | 0.75 | 0.80 | 0.78 | 0.60 | 0.74 | 0.74 | 0.81 | 0.70 |
| ChatGPT<br>Detector-<br>RoBERTa<br>Chinese  | Twitter  | 0.86 | 0.90 | 0.73 | 0.67 | 0.77 | 0.78 | 0.90 | 0.79 | 0.72 | N/A  | N/A   | 0.61   | 0.68 | 0.76 | 0.58 | 0.71 | 0.70 | 0.88 | 0.77 | N/A  | N/A  | 0.83 | 0.74 |
| 0 A A O                                     | WhatsApp | 0.82 | N/A  | 0.77 | 0.36 | 0.91 | N/A  | 0.89 | 0.85 | 0.83 | N/A  | N/A   | N/A    | N/A  | 0.58 | N/A  | 0.71 | 0.83 | 0.81 | N/A  | N/A  | N/A  | N/A  | 0.79 |
| E                                           | Discord  | N/A  | N/A  | 0.25 | 0.41 | 0.47 | N/A  | 0.68 | 0.44 | 0.40 | 0.50 | 0.46  | 0.50   | 0.40 | 0.42 | 0.44 | 0.36 | 0.33 | N/A  | 0.36 | N/A  | N/A  | N/A  | 0.44 |
| Longformer<br>Detector                      | Gab      | 0.56 | 0.39 | 0.32 | 0.44 | 0.42 | 0.49 | 0.71 | 0.45 | 0.48 | 0.56 | N/A   | 0.49   | 0.45 | 0.41 | 0.39 | 0.48 | 0.50 | 0.54 | 0.34 | 0.46 | 0.48 | 0.49 | 0.48 |
| Longfor                                     | Telegram | 0.37 | 0.78 | 0.37 | 0.55 | 0.36 | 0.56 | 0.67 | 0.43 | 0.55 | 0.51 | N/A   | 0.55   | 0.61 | 0.47 | 0.54 | 0.44 | 0.41 | 0.58 | 0.49 | 0.45 | 0.62 | 0.46 | 0.51 |
| ete                                         | Twitter  | 0.24 | 0.16 | 0.30 | 0.38 | 0.49 | 0.67 | 0.65 | 0.31 | 0.43 | N/A  | N/A   | 0.38   | 0.44 | 0.33 | 0.47 | 0.38 | 0.40 | 0.31 | 0.16 | N/A  | N/A  | 0.49 | 0.38 |
| DL                                          | WhatsApp | 0.22 | N/A  | 0.41 | 0.63 | 0.30 | N/A  | 0.52 | 0.53 | 0.42 | N/A  | N/A   | N/A    | N/A  | 0.44 | N/A  | 0.42 | 0.38 | 0.31 | N/A  | N/A  | N/A  | N/A  | 0.44 |
|                                             | Discord  | N/A  | N/A  | 0.25 | 0.05 | 0.12 | N/A  | 0.44 | 0.10 | 0.18 | 0.32 | 0.52  | 0.12   | 0.09 | 0.09 | 0.07 | 0.09 | 0.09 | N/A  | 0.22 | N/A  | N/A  | N/A  | 0.16 |
| E                                           | Gab      | 0.58 | 0.56 | 0.30 | 0.25 | 0.38 | 0.75 | 0.51 | 0.33 | 0.29 | 0.30 | N/A   | 0.38   | 0.20 | 0.35 | 0.31 | 0.33 | 0.37 | 0.50 | 0.45 | 0.37 | 0.43 | 0.56 | 0.40 |
| RoBERTa<br>large-<br>OpenAI-<br>Detector    | Telegram | 0.72 | 0.21 | 0.50 | 0.11 | 0.42 | 0.74 | 0.50 | 0.32 | 0.17 | 0.27 | N/A   | 0.10   | 0.09 | 0.27 | 0.17 | 0.20 | 0.10 | 0.36 | 0.22 | 0.18 | 0.35 | 0.62 | 0.33 |
| RoBE<br>large-<br>Open/<br>Detect           | Twitter  | 0.80 | 0.60 | 0.44 | 0.17 | 0.19 | 0.75 | 0.55 | 0.42 | 0.21 | N/A  | N/A   | 0.20   | 0.16 | 0.31 | 0.17 | 0.15 | 0.28 | 0.66 | 0.12 | N/A  | N/A  | 0.59 | 0.42 |
| <b>x</b> = 0 0                              | WhatsApp | 0.82 | N/A  | 0.26 | 0.20 | 0.32 | N/A  | 0.55 | 0.17 | 0.21 | N/A  | N/A   | N/A    | N/A  | 0.18 | N/A  | 0.27 | 0.07 | 0.71 | N/A  | N/A  | N/A  | N/A  | 0.40 |
| la-                                         | Discord  | N/A  | N/A  | 0.51 | 0.41 | 0.25 | N/A  | 0.53 | 0.42 | 0.35 | 0.41 | 0.36  | 0.45   | 0.43 | 0.43 | 0.40 | 0.35 | 0.25 | N/A  | 0.32 | N/A  | N/A  | N/A  | 0.40 |
| SR.                                         | Gab      | 0.38 | 0.59 | 0.62 | 0.46 | 0.46 | 0.40 | 0.61 | 0.47 | 0.40 | 0.53 | N/A   | 0.48   | 0.46 | 0.55 | 0.47 | 0.44 | 0.42 | 0.71 | 0.50 | 0.45 | 0.47 | 0.45 | 0.49 |
| ry -b                                       | Telegram | 0.39 | 0.63 | 0.56 | 0.47 | 0.50 | 0.32 | 0.55 | 0.44 | 0.46 | 0.54 | N/A   | 0.50   | 0.49 | 0.45 | 0.40 | 0.43 | 0.32 | 0.74 | 0.45 | 0.51 | 0.60 | 0.45 | 0.48 |
| ruRoBERTa-<br>ruatd-<br>binary              | Twitter  | 0.39 | 0.67 | 0.55 | 0.31 | 0.38 | 0.23 | 0.58 | 0.49 | 0.45 | N/A  | N/A   | 0.37   | 0.39 | 0.50 | 0.41 | 0.42 | 0.33 | 0.69 | 0.23 | N/A  | N/A  | 0.25 | 0.49 |
| E E 36                                      | WhatsApp | 0.41 | N/A  | 0.58 | 0.57 | 0.37 | N/A  | 0.54 | 0.49 | 0.43 | N/A  | N/A   | N/A    | N/A  | 0.43 | N/A  | 0.48 | 0.34 | 0.70 | N/A  | N/A  | N/A  | N/A  | 0.50 |

Table 24: Per-platform AUC ROC performance of pre-trained MGT detectors category. N/A refers to not enough samples per each class (at least 10) making AUC ROC value irrelevant.

<span id="page-23-0"></span>

|                                           |          |      |      |      |      |      |      |      |      |      | Test | Lang | age [A | UC R | OC]  |      |      |      |      |      |      |      |      |      |
|-------------------------------------------|----------|------|------|------|------|------|------|------|------|------|------|------|--------|------|------|------|------|------|------|------|------|------|------|------|
| Detector                                  | Platform | ar   | bg   | ca   | cs   | de   | el   | en   | es   | et   | ga   | gd   | hr     | hu   | nl   | pl   | pt   | ro   | ru   | sk   | sl   | uk   | zh   | all  |
| al                                        | Discord  | N/A  | N/A  | 0.99 | 1.00 | 0.97 | N/A  | 0.99 | 0.99 | 1.00 | 0.96 | 0.90 | 0.99   | 1.00 | 0.99 | 0.99 | 0.99 | 0.99 | N/A  | 0.99 | N/A  | N/A  | N/A  | 0.99 |
|                                           | Gab      | 0.91 | 0.96 | 0.97 | 0.96 | 0.96 | 0.95 | 0.96 | 0.96 | 0.94 | 0.94 | N/A  | 0.94   | 0.98 | 0.97 | 0.97 | 0.96 | 0.95 | 0.92 | 0.96 | 0.92 | 0.89 | 0.94 | 0.94 |
| 19<br>19                                  | Telegram | 0.97 | 0.99 | 0.97 | 0.98 | 0.98 | 0.98 | 0.98 | 0.97 | 0.99 | 0.94 | N/A  | 0.99   | 1.00 | 0.96 | 0.98 | 0.97 | 0.99 | 0.97 | 0.98 | 1.00 | 0.96 | 0.99 | 0.98 |
| Aya-101-<br>MultiSocial                   | Twitter  | 0.99 | 0.99 | 0.98 | 0.97 | 0.97 | 0.99 | 0.97 | 0.99 | 0.98 | N/A  | N/A  | 1.00   | 0.98 | 0.97 | 0.99 | 0.98 | 0.98 | 0.98 | 1.00 | N/A  | N/A  | 0.97 | 0.98 |
| < 4                                       | WhatsApp | 0.99 | N/A  | 0.97 | 0.98 | 0.95 | N/A  | 0.98 | 0.98 | 1.00 | N/A  | N/A  | N/A    | N/A  | 0.97 | N/A  | 0.97 | 0.99 | 0.95 | N/A  | N/A  | N/A  | N/A  | 0.97 |
| 5 E                                       | Discord  | N/A  | N/A  | 0.99 | 1.00 | 0.97 | N/A  | 0.99 | 0.99 | 0.99 | 0.97 | 0.84 | 0.98   | 1.00 | 0.98 | 0.99 | 0.99 | 0.99 | N/A  | 0.97 | N/A  | N/A  | N/A  | 0.99 |
| BLOOMZ-<br>3b-<br>MultiSocial             | Gab      | 0.87 | 0.96 | 0.96 | 0.93 | 0.92 | 0.93 | 0.96 | 0.95 | 0.94 | 0.88 | N/A  | 0.90   | 0.97 | 0.92 | 0.94 | 0.95 | 0.89 | 0.88 | 0.95 | 0.81 | 0.81 | 0.93 | 0.91 |
| Q SI                                      | Telegram | 0.95 | 0.98 | 0.94 | 0.97 | 0.96 | 0.98 | 0.98 | 0.97 | 0.98 | 0.86 | N/A  | 0.97   | 1.00 | 0.92 | 0.94 | 0.97 | 0.97 | 0.94 | 0.95 | 0.98 | 0.90 | 0.98 | 0.96 |
| BL(<br>3b-                                | Twitter  | 0.99 | 0.98 | 0.98 | 0.94 | 0.96 | 0.99 | 0.98 | 0.99 | 0.92 | N/A  | N/A  | 0.99   | 0.97 | 0.94 | 0.97 | 0.97 | 0.96 | 0.99 | 0.98 | N/A  | N/A  | 0.99 | 0.97 |
| H m M                                     | WhatsApp | 0.99 | N/A  | 0.97 | 0.98 | 0.97 | N/A  | 0.98 | 0.98 | 0.99 | N/A  | N/A  | N/A    | N/A  | 0.95 | N/A  | 0.98 | 0.99 | 0.97 | N/A  | N/A  | N/A  | N/A  | 0.98 |
| le                                        | Discord  | N/A  | N/A  | 0.99 | 1.00 | 0.97 | N/A  | 0.99 | 0.98 | 0.99 | 0.96 | 0.87 | 0.98   | 1.00 | 0.99 | 0.99 | 0.99 | 0.99 | N/A  | 0.97 | N/A  | N/A  | N/A  | 0.99 |
| Falcon-<br>rw-1b-<br>MultiSocial          | Gab      | 0.88 | 0.94 | 0.97 | 0.94 | 0.93 | 0.94 | 0.97 | 0.93 | 0.94 | 0.92 | N/A  | 0.91   | 0.97 | 0.93 | 0.95 | 0.92 | 0.89 | 0.88 | 0.93 | 0.80 | 0.87 | 0.92 | 0.92 |
| Falcon-<br>rw-1b-<br>MultiSc              | Telegram | 0.94 | 0.98 | 0.93 | 0.97 | 0.96 | 0.98 | 0.98 | 0.96 | 0.99 | 0.89 | N/A  | 0.98   | 1.00 | 0.94 | 0.95 | 0.95 | 0.98 | 0.94 | 0.96 | 0.98 | 0.91 | 0.97 | 0.96 |
| Falcon<br>rw-1b-<br>MultiS                | Twitter  | 0.98 | 0.98 | 0.98 | 0.95 | 0.96 | 0.99 | 0.98 | 0.98 | 0.95 | N/A  | N/A  | 0.97   | 0.96 | 0.95 | 0.96 | 0.97 | 0.96 | 0.99 | 0.99 | N/A  | N/A  | 0.99 | 0.97 |
| H L M                                     | WhatsApp | 0.99 | N/A  | 0.95 | 0.97 | 0.96 | N/A  | 0.99 | 0.97 | 0.99 | N/A  | N/A  | N/A    | N/A  | 0.95 | N/A  | 0.96 | 0.99 | 0.96 | N/A  | N/A  | N/A  | N/A  | 0.97 |
| ੜ ਦ                                       | Discord  | N/A  | N/A  | 0.99 | 1.00 | 0.97 | N/A  | 0.99 | 0.99 | 1.00 | 0.96 | 0.88 | 0.99   | 1.00 | 0.99 | 1.00 | 0.99 | 0.99 | N/A  | 0.98 | N/A  | N/A  | N/A  | 0.99 |
| -3-5<br>0ci                               | Gab      | 0.90 | 0.95 | 0.98 | 0.97 | 0.97 | 0.95 | 0.98 | 0.97 | 0.95 | 0.94 | N/A  | 0.96   | 0.99 | 0.97 | 0.97 | 0.96 | 0.95 | 0.93 | 0.97 | 0.92 | 0.88 | 0.96 | 0.95 |
| Llama-3-8b-<br>MultiSocial                | Telegram | 0.97 | 0.99 | 0.97 | 0.99 | 0.98 | 0.98 | 0.98 | 0.98 | 0.99 | 0.94 | N/A  | 0.99   | 1.00 | 0.97 | 0.98 | 0.97 | 0.98 | 0.97 | 0.98 | 1.00 | 0.96 | 0.99 | 0.98 |
| la la                                     | Twitter  | 0.99 | 0.99 | 0.99 | 0.97 | 0.98 | 0.98 | 0.99 | 0.99 | 0.97 | N/A  | N/A  | 0.99   | 0.98 | 0.98 | 0.99 | 0.98 | 0.98 | 0.98 | 1.00 | N/A  | N/A  | 0.98 | 0.98 |
|                                           | WhatsApp | 0.99 | N/A  | 0.98 | 0.98 | 0.96 | N/A  | 0.99 | 0.98 | 1.00 | N/A  | N/A  | N/A    | N/A  | 0.98 | N/A  | 0.98 | 0.99 | 0.98 | N/A  | N/A  | N/A  | N/A  | 0.98 |
| al I                                      | Discord  | N/A  | N/A  | 0.99 | 1.00 | 0.97 | N/A  | 0.99 | 0.99 | 1.00 | 0.96 | 0.91 | 0.99   | 1.00 | 0.99 | 1.00 | 0.99 | 0.99 | N/A  | 0.98 | N/A  | N/A  | N/A  | 0.99 |
| Mistral-<br>7b-v0.1-<br>MultiSocial       | Gab      | 0.90 | 0.97 | 0.97 | 0.98 | 0.97 | 0.96 | 0.98 | 0.97 | 0.94 | 0.92 | N/A  | 0.96   | 0.99 | 0.97 | 0.98 | 0.96 | 0.94 | 0.94 | 0.95 | 0.91 | 0.89 | 0.96 | 0.95 |
| Mistral-<br>7b-v0.1-<br>MultiSoo          | Telegram | 0.97 | 0.99 | 0.97 | 0.99 | 0.98 | 0.98 | 0.98 | 0.98 | 0.99 | 0.91 | N/A  | 0.99   | 1.00 | 0.97 | 0.98 | 0.97 | 0.99 | 0.97 | 0.97 | 1.00 | 0.96 | 0.99 | 0.98 |
| dul<br>dul                                | Twitter  | 0.99 | 0.99 | 0.98 | 0.98 | 0.98 | 0.99 | 0.99 | 0.99 | 0.96 | N/A  | N/A  | 0.99   | 0.98 | 0.97 | 0.98 | 0.98 | 0.98 | 0.99 | 0.99 | N/A  | N/A  | 0.98 | 0.98 |
| 464                                       | WhatsApp | 0.99 | N/A  | 0.97 | 0.98 | 0.96 | N/A  | 0.99 | 0.98 | 0.99 | N/A  | N/A  | N/A    | N/A  | 0.97 | N/A  | 0.98 | 0.99 | 0.98 | N/A  | N/A  | N/A  | N/A  | 0.98 |
| ⊥ le                                      | Discord  | N/A  | N/A  | 0.96 | 0.99 | 0.96 | N/A  | 0.98 | 0.97 | 0.99 | 0.89 | 0.72 | 0.98   | 1.00 | 0.98 | 0.99 | 0.97 | 0.98 | N/A  | 0.99 | N/A  | N/A  | N/A  | 0.97 |
| oci KT                                    | Gab      | 0.88 | 0.94 | 0.95 | 0.93 | 0.94 | 0.94 | 0.95 | 0.95 | 0.90 | 0.87 | N/A  | 0.94   | 0.96 | 0.95 | 0.95 | 0.93 | 0.94 | 0.90 | 0.95 | 0.88 | 0.84 | 0.92 | 0.93 |
| HE SEI                                    | Telegram | 0.94 | 0.98 | 0.93 | 0.98 | 0.97 | 0.97 | 0.96 | 0.96 | 0.98 | 0.87 | N/A  | 0.98   | 0.99 | 0.94 | 0.97 | 0.96 | 0.97 | 0.96 | 0.96 | 0.98 | 0.93 | 0.94 | 0.96 |
| XLM-<br>RoBERTa-<br>large-<br>MultiSocial | Twitter  | 0.98 | 0.99 | 0.94 | 0.96 | 0.96 | 0.97 | 0.97 | 0.97 | 0.97 | N/A  | N/A  | 0.95   | 0.97 | 0.94 | 0.98 | 0.96 | 0.97 | 0.98 | 0.98 | N/A  | N/A  | 0.96 | 0.96 |
|                                           | WhatsApp | 0.98 | N/A  | 0.90 | 0.96 | 0.97 | N/A  | 0.97 | 0.97 | 0.97 | N/A  | N/A  | N/A    | N/A  | 0.93 | N/A  | 0.95 | 0.97 | 0.95 | N/A  | N/A  | N/A  | N/A  | 0.96 |
| al Ta-                                    | Discord  | N/A  | N/A  | 0.98 | 1.00 | 0.97 | N/A  | 0.98 | 0.98 | 0.99 | 0.91 | 0.75 | 0.98   | 1.00 | 0.98 | 0.99 | 0.99 | 0.99 | N/A  | 0.97 | N/A  | N/A  | N/A  | 0.98 |
| Si F K                                    | Gab      | 0.85 | 0.96 | 0.92 | 0.94 | 0.93 | 0.90 | 0.95 | 0.93 | 0.92 | 0.88 | N/A  | 0.92   | 0.97 | 0.94 | 0.94 | 0.92 | 0.92 | 0.89 | 0.92 | 0.88 | 0.79 | 0.91 | 0.91 |
| eBi<br>ItiS                               | Telegram | 0.94 | 0.99 | 0.93 | 0.98 | 0.95 | 0.97 | 0.97 | 0.95 | 0.98 | 0.90 | N/A  | 0.98   | 1.00 | 0.93 | 0.96 | 0.96 | 0.97 | 0.96 | 0.96 | 0.99 | 0.94 | 0.96 | 0.96 |
| mDeBERTa-<br>v3-base-<br>MultiSocial      | Twitter  | 0.98 | 0.98 | 0.96 | 0.93 | 0.95 | 0.97 | 0.96 | 0.97 | 0.97 | N/A  | N/A  | 0.96   | 0.96 | 0.95 | 0.97 | 0.95 | 0.96 | 0.96 | 0.99 | N/A  | N/A  | 0.95 | 0.96 |
| = > <                                     | WhatsApp | 0.97 | N/A  | 0.94 | 0.96 | 0.96 | N/A  | 0.96 | 0.97 | 1.00 | N/A  | N/A  | N/A    | N/A  | 0.96 | N/A  | 0.95 | 0.99 | 0.95 | N/A  | N/A  | N/A  | N/A  | 0.96 |

<span id="page-23-1"></span>Table 25: Per-platform AUC ROC performance of fine-tuned MGT detectors category. N/A refers to not enough samples per each class (at least 10) making AUC ROC value irrelevant.

|                     |                          |         |      | Platform [ | AUC RO  | C]       |      |
|---------------------|--------------------------|---------|------|------------|---------|----------|------|
| Detector            | Generator                | Discord | Gab  | Telegram   | Twitter | WhatsApp | all  |
|                     | Mistral-7B-Instruct-v0.2 | 0.77    | 0.62 | 0.68       | 0.68    | 0.67     | 0.68 |
|                     | Aya-101                  | 0.76    | 0.65 | 0.68       | 0.70    | 0.72     | 0.69 |
| ars                 | Gemini                   | 0.88    | 0.79 | 0.84       | 0.83    | 0.84     | 0.83 |
| G                   | GPT-3.5-Turbo-0125       | 0.75    | 0.64 | 0.71       | 0.68    | 0.69     | 0.68 |
| Binoculars          | OPT-IML-Max-30b          | 0.71    | 0.60 | 0.64       | 0.66    | 0.65     | 0.64 |
| B                   | v5-Eagle-7B-HF           | 0.85    | 0.76 | 0.80       | 0.81    | 0.79     | 0.7  |
|                     | Vicuna-13b               | 0.82    | 0.73 | 0.76       | 0.80    | 0.77     | 0.7  |
|                     | Mistral-7B-Instruct-v0.2 | 0.94    | 0.63 | 0.71       | 0.71    | 0.68     | 0.7  |
| 4                   | Aya-101                  | 0.91    | 0.64 | 0.69       | 0.70    | 0.66     | 0.7  |
| Ę.                  | Gemini                   | 0.96    | 0.80 | 0.84       | 0.82    | 0.78     | 0.8  |
| DetectLLM-<br>LRR   | GPT-3.5-Turbo-0125       | 0.93    | 0.65 | 0.72       | 0.71    | 0.67     | 0.7  |
| Detec<br>LRR        | OPT-IML-Max-30b          | 0.87    | 0.60 | 0.68       | 0.67    | 0.63     | 0.6  |
| <b>D</b>            | v5-Eagle-7B-HF           | 0.98    | 0.78 | 0.81       | 0.83    | 0.77     | 0.8  |
|                     | Vicuna-13b               | 0.96    | 0.73 | 0.77       | 0.80    | 0.73     | 0.7  |
|                     | Mistral-7B-Instruct-v0.2 | 0.65    | 0.51 | 0.58       | 0.59    | 0.62     | 0.5  |
| ÷                   | Aya-101                  | 0.80    | 0.71 | 0.74       | 0.75    | 0.77     | 0.7  |
| stec                | Gemini                   | 0.89    | 0.84 | 0.88       | 0.86    | 0.88     | 0.8  |
| Fast-Detect-<br>GPT | GPT-3.5-Turbo-0125       | 0.75    | 0.66 | 0.73       | 0.68    | 0.74     | 0.7  |
| Fast<br>GPT         | OPT-IML-Max-30b          | 0.75    | 0.61 | 0.65       | 0.66    | 0.71     | 0.6  |
| e o                 | v5-Eagle-7B-HF           | 0.88    | 0.81 | 0.84       | 0.86    | 0.86     | 0.8  |
|                     | Vicuna-13b               | 0.81    | 0.74 | 0.75       | 0.82    | 0.82     | 0.7  |
|                     | Mistral-7B-Instruct-v0.2 | 0.95    | 0.63 | 0.69       | 0.70    | 0.67     | 0.7  |
|                     | Aya-101                  | 0.92    | 0.65 | 0.70       | 0.70    | 0.66     | 0.7  |
| E                   | Gemini                   | 0.97    | 0.80 | 0.82       | 0.82    | 0.78     | 0.8  |
| -1-                 | GPT-3.5-Turbo-0125       | 0.93    | 0.65 | 0.71       | 0.69    | 0.67     | 0.7  |
| LLM-<br>Deviation   | OPT-IML-Max-30b          | 0.89    | 0.62 | 0.69       | 0.67    | 0.64     | 0.6  |
| DL                  | v5-Eagle-7B-HF           | 0.98    | 0.78 | 0.80       | 0.83    | 0.77     | 0.8  |
|                     | Vicuna-13b               | 0.96    | 0.73 | 0.76       | 0.80    | 0.73     | 0.7  |
|                     | Mistral-7B-Instruct-v0.2 | 0.93    | 0.63 | 0.69       | 0.69    | 0.67     | 0.6  |
|                     | Aya-101                  | 0.91    | 0.65 | 0.70       | 0.70    | 0.66     | 0.7  |
|                     | Gemini                   | 0.96    | 0.78 | 0.81       | 0.81    | 0.77     | 0.8  |
|                     | GPT-3.5-Turbo-0125       | 0.91    | 0.64 | 0.70       | 0.68    | 0.66     | 0.7  |
| io.                 | OPT-IML-Max-30b          | 0.87    | 0.62 | 0.69       | 0.67    | 0.64     | 0.6  |
| SS                  | v5-Eagle-7B-HF           | 0.98    | 0.78 | 0.80       | 0.83    | 0.77     | 0.8  |
|                     | Vicuna-13b               | 0.96    | 0.73 | 0.75       | 0.80    | 0.73     | 0.7  |

Table 26: Per-platform per-LLM AUC ROC performance of statistical MGT detectors category. N/A refers to not enough samples per each class (at least 10) making AUC ROC value irrelevant.

|                                             |                          |         |      | Platform [ | AUC RO  | C]       |      |
|---------------------------------------------|--------------------------|---------|------|------------|---------|----------|------|
| Detector                                    | Generator                | Discord | Gab  | Telegram   | Twitter | WhatsApp | all  |
|                                             | Mistral-7B-Instruct-v0.2 | 0.87    | 0.62 | 0.76       | 0.70    | 0.78     | 0.74 |
|                                             | Aya-101                  | 0.92    | 0.75 | 0.85       | 0.81    | 0.85     | 0.83 |
| BLOOMZ-<br>3b-mixed-<br>Detector            | Gemini                   | 0.76    | 0.47 | 0.63       | 0.52    | 0.58     | 0.59 |
| Q i g                                       | GPT-3.5-Turbo-0125       | 0.91    | 0.70 | 0.80       | 0.75    | 0.86     | 0.80 |
| BLOOMZ<br>3b-mixed-<br>Detector             | OPT-IML-Max-30b          | 0.81    | 0.67 | 0.76       | 0.69    | 0.79     | 0.7  |
| 8 6 0                                       | v5-Eagle-7B-HF           | 0.92    | 0.70 | 0.83       | 0.78    | 0.86     | 0.8  |
|                                             | Vicuna-13b               | 0.89    | 0.68 | 0.81       | 0.78    | 0.83     | 0.7  |
|                                             | Mistral-7B-Instruct-v0.2 | 0.85    | 0.70 | 0.75       | 0.74    | 0.85     | 0.7  |
|                                             | Aya-101                  | 0.70    | 0.64 | 0.65       | 0.71    | 0.73     | 0.6  |
| 5 4 E                                       | Gemini                   | 0.88    | 0.73 | 0.78       | 0.81    | 0.86     | 0.8  |
| ChatGPT-<br>Detector-<br>RoBERTa<br>Chinese | GPT-3.5-Turbo-0125       | 0.72    | 0.59 | 0.66       | 0.65    | 0.73     | 0.6  |
| hin de te                                   | OPT-IML-Max-30b          | 0.65    | 0.63 | 0.64       | 0.70    | 0.72     | 0.6  |
| O A A O                                     | v5-Eagle-7B-HF           | 0.80    | 0.71 | 0.74       | 0.80    | 0.82     | 0.7  |
|                                             | Vicuna-13b               | 0.76    | 0.68 | 0.71       | 0.77    | 0.81     | 0.7  |
|                                             | Mistral-7B-Instruct-v0.2 | 0.51    | 0.55 | 0.59       | 0.44    | 0.47     | 0.5  |
| 5                                           | Aya-101                  | 0.32    | 0.38 | 0.40       | 0.28    | 0.37     | 0.3  |
| Ĕ.                                          | Gemini                   | 0.50    | 0.54 | 0.58       | 0.47    | 0.52     | 0.5  |
| E B                                         | GPT-3.5-Turbo-0125       | 0.37    | 0.50 | 0.49       | 0.44    | 0.44     | 0.4  |
| Longformer<br>Detector                      | OPT-IML-Max-30b          | 0.36    | 0.38 | 0.40       | 0.27    | 0.34     | 0.3  |
| 70                                          | v5-Eagle-7B-HF           | 0.53    | 0.55 | 0.60       | 0.41    | 0.52     | 0.5  |
|                                             | Vicuna-13b               | 0.43    | 0.48 | 0.53       | 0.38    | 0.44     | 0.4  |
|                                             | Mistral-7B-Instruct-v0.2 | 0.11    | 0.38 | 0.30       | 0.38    | 0.33     | 0.3  |
|                                             | Aya-101                  | 0.25    | 0.48 | 0.40       | 0.49    | 0.52     | 0.4  |
| Ê L                                         | Gemini                   | 0.07    | 0.25 | 0.22       | 0.29    | 0.21     | 0.2  |
| EE Y EE                                     | GPT-3.5-Turbo-0125       | 0.17    | 0.40 | 0.33       | 0.42    | 0.38     | 0.3  |
| RoBERTa-<br>large-<br>OpenAI-<br>Detector   | OPT-IML-Max-30b          | 0.29    | 0.56 | 0.47       | 0.57    | 0.57     | 0.4  |
| a a o o                                     | v5-Eagle-7B-HF           | 0.11    | 0.36 | 0.30       | 0.38    | 0.40     | 0.3  |
|                                             | Vicuna-13b               | 0.13    | 0.39 | 0.33       | 0.40    | 0.41     | 0.3  |
|                                             | Mistral-7B-Instruct-v0.2 | 0.47    | 0.52 | 0.54       | 0.53    | 0.53     | 0.5  |
| -e                                          | Aya-101                  | 0.42    | 0.57 | 0.52       | 0.53    | 0.57     | 0.5  |
| ruRoBERTa-<br>ruatd-<br>binary              | Gemini                   | 0.16    | 0.17 | 0.16       | 0.20    | 0.19     | 0.1  |
| 월 <del>-</del> 한                            | GPT-3.5-Turbo-0125       | 0.40    | 0.52 | 0.49       | 0.53    | 0.56     | 0.5  |
| ruRoBl<br>ruatd-<br>binary                  | OPT-IML-Max-30b          | 0.45    | 0.63 | 0.60       | 0.62    | 0.64     | 0.5  |
| 2 2 3                                       | v5-Eagle-7B-HF           | 0.46    | 0.50 | 0.53       | 0.49    | 0.53     | 0.5  |
|                                             | Vicuna-13b               | 0.41    | 0.51 | 0.53       | 0.51    | 0.53     | 0.5  |

<span id="page-24-0"></span>Table 27: Per-platform per-LLM AUC ROC performance of pre-trained MGT detectors category. N/A refers to not enough samples per each class (at least 10) making AUC ROC value irrelevant.

|                                          |                          |         |      | Platform [ | AUC RO  | C]       |      |
|------------------------------------------|--------------------------|---------|------|------------|---------|----------|------|
| Detector                                 | Generator                | Discord | Gab  | Telegram   | Twitter | WhatsApp | all  |
|                                          | Mistral-7B-Instruct-v0.2 | 1.00    | 0.98 | 0.99       | 0.99    | 1.00     | 0.99 |
| -                                        | Aya-101                  | 0.99    | 0.91 | 0.97       | 0.96    | 0.95     | 0.96 |
| ġ                                        | Gemini                   | 0.98    | 0.86 | 0.95       | 0.93    | 0.93     | 0.93 |
| 10<br>13                                 | GPT-3.5-Turbo-0125       | 1.00    | 0.98 | 0.99       | 1.00    | 1.00     | 0.99 |
| Aya-101-<br>MultiSocial                  | OPT-IML-Max-30b          | 0.97    | 0.92 | 0.97       | 0.97    | 0.96     | 0.95 |
| A N                                      | v5-Eagle-7B-HF           | 1.00    | 0.98 | 1.00       | 1.00    | 0.99     | 0.99 |
|                                          | Vicuna-13b               | 1.00    | 0.98 | 0.99       | 1.00    | 1.00     | 0.99 |
|                                          | Mistral-7B-Instruct-v0.2 | 1.00    | 0.94 | 0.97       | 0.98    | 0.99     | 0.97 |
|                                          | Aya-101                  | 0.98    | 0.88 | 0.94       | 0.95    | 0.96     | 0.94 |
| ci: IZ                                   | Gemini                   | 0.99    | 0.85 | 0.94       | 0.93    | 0.96     | 0.93 |
| is o                                     | GPT-3.5-Turbo-0125       | 0.99    | 0.95 | 0.98       | 0.99    | 0.99     | 0.98 |
| BLOOMZ-<br>3b-<br>MultiSocial            | OPT-IML-Max-30b          | 0.96    | 0.87 | 0.94       | 0.95    | 0.95     | 0.93 |
| a te z                                   | v5-Eagle-7B-HF           | 1.00    | 0.96 | 0.99       | 0.99    | 0.99     | 0.99 |
|                                          | Vicuna-13b               | 0.99    | 0.95 | 0.98       | 0.99    | 0.99     | 0.98 |
|                                          | Mistral-7B-Instruct-v0.2 | 1.00    | 0.93 | 0.97       | 0.98    | 0.98     | 0.97 |
| -                                        | Aya-101                  | 0.98    | 0.89 | 0.95       | 0.96    | 0.96     | 0.94 |
| Falcon-<br>rw-1b-<br>MultiSocial         | Gemini                   | 0.98    | 0.84 | 0.94       | 0.94    | 0.94     | 0.93 |
| ë 7 S                                    | GPT-3.5-Turbo-0125       | 0.99    | 0.96 | 0.98       | 0.99    | 0.99     | 0.98 |
| Falcon-<br>rw-1b-<br>MultiSe             | OPT-IML-Max-30b          | 0.96    | 0.88 | 0.94       | 0.95    | 0.95     | 0.93 |
| 2 L Z                                    | v5-Eagle-7B-HF           | 1.00    | 0.96 | 0.99       | 0.99    | 0.99     | 0.98 |
|                                          | Vicuna-13b               | 0.99    | 0.95 | 0.97       | 0.99    | 0.99     | 0.98 |
|                                          | Mistral-7B-Instruct-v0.2 | 1.00    | 0.98 | 0.99       | 0.99    | 1.00     | 0.99 |
| _ خ                                      | Aya-101                  | 0.99    | 0.92 | 0.97       | 0.97    | 0.97     | 0.96 |
| cia 28                                   | Gemini                   | 0.99    | 0.89 | 0.95       | 0.94    | 0.95     | 0.95 |
| Llama-3-8b-<br>MultiSocial               | GPT-3.5-Turbo-0125       | 1.00    | 0.98 | 0.99       | 1.00    | 1.00     | 0.99 |
| ult au                                   | OPT-IML-Max-30b          | 0.97    | 0.93 | 0.97       | 0.97    | 0.97     | 0.96 |
| $\Box \Sigma$                            | v5-Eagle-7B-HF           | 1.00    | 0.99 | 1.00       | 1.00    | 1.00     | 0.99 |
|                                          | Vicuna-13b               | 1.00    | 0.98 | 0.99       | 1.00    | 1.00     | 0.99 |
|                                          | Mistral-7B-Instruct-v0.2 | 1.00    | 0.97 | 0.99       | 0.99    | 1.00     | 0.99 |
| _                                        | Aya-101                  | 0.99    | 0.92 | 0.97       | 0.96    | 0.97     | 0.96 |
| Mistral-<br>7b-v0.1-<br>MultiSocial      | Gemini                   | 0.99    | 0.91 | 0.96       | 0.95    | 0.97     | 0.96 |
| Mistral-<br>7b-v0.1-<br>MultiSo          | GPT-3.5-Turbo-0125       | 1.00    | 0.98 | 0.99       | 1.00    | 1.00     | 0.99 |
| it v                                     | OPT-IML-Max-30b          | 0.97    | 0.92 | 0.97       | 0.97    | 0.97     | 0.96 |
| Z F Z                                    | v5-Eagle-7B-HF           | 1.00    | 0.98 | 1.00       | 1.00    | 1.00     | 0.99 |
|                                          | Vicuna-13b               | 1.00    | 0.98 | 0.99       | 0.99    | 1.00     | 0.99 |
|                                          | Mistral-7B-Instruct-v0.2 | 0.99    | 0.95 | 0.97       | 0.97    | 0.99     | 0.97 |
|                                          | Aya-101                  | 0.96    | 0.89 | 0.93       | 0.95    | 0.92     | 0.93 |
| KLM-<br>RoBERTa-<br>arge-<br>MultiSocial | Gemini                   | 0.97    | 0.89 | 0.96       | 0.94    | 0.96     | 0.94 |
| - H ' S                                  | GPT-3.5-Turbo-0125       | 0.98    | 0.96 | 0.98       | 0.99    | 0.99     | 0.98 |
| XLM-<br>RoBE<br>large-<br>MultiS         | OPT-IML-Max-30b          | 0.91    | 0.87 | 0.91       | 0.93    | 0.91     | 0.90 |
| XXEX                                     | v5-Eagle-7B-HF           | 1.00    | 0.97 | 0.99       | 0.99    | 0.99     | 0.99 |
|                                          | Vicuna-13b               | 0.99    | 0.96 | 0.97       | 0.99    | 0.99     | 0.98 |
|                                          | Mistral-7B-Instruct-v0.2 | 0.99    | 0.94 | 0.98       | 0.98    | 0.99     | 0.97 |
| .÷                                       | Aya-101                  | 0.98    | 0.88 | 0.94       | 0.94    | 0.93     | 0.93 |
| Cial                                     | Gemini                   | 0.97    | 0.80 | 0.92       | 0.88    | 0.90     | 0.90 |
| BE So                                    | GPT-3.5-Turbo-0125       | 0.99    | 0.95 | 0.98       | 0.98    | 0.99     | 0.98 |
| mDeBERTa-<br>v3-base-<br>MultiSocial     | OPT-IML-Max-30b          | 0.95    | 0.89 | 0.94       | 0.94    | 0.94     | 0.93 |
| Ц С Ц                                    | v5-Eagle-7B-HF           | 1.00    | 0.97 | 0.99       | 0.99    | 0.99     | 0.99 |
|                                          | Vicuna-13b               | 0.99    | 0.96 | 0.98       | 0.99    | 0.99     | 0.98 |
|                                          | ricula-150               | 0.99    | 0.90 | 0.96       | 0.99    | 0.99     | 0.90 |

Table 28: Per-platform per-LLM AUC ROC performance of fine-tuned MGT detectors category. N/A refers to not enough samples per each class (at least 10) making AUC ROC value irrelevant.

<span id="page-25-0"></span>

|                           | Train    | Test Language [AUC ROC] |      |      |      |      |      |      |      |      |     |     |      |      |      |      |      |      |      |     |     |      |      |      |
|---------------------------|----------|-------------------------|------|------|------|------|------|------|------|------|-----|-----|------|------|------|------|------|------|------|-----|-----|------|------|------|
| Detector                  | Language | ar                      | bg   | ca   | cs   | de   | el   | en   | es   | et   | ga  | gd  | hr   | hu   | nl   | pl   | pt   | ro   | ru   | sk  | sl  | uk   | zh   | all  |
| Aya-101                   | en       | 0.89                    | 0.97 | 0.84 | 0.97 | 0.92 | 0.95 | 0.97 | 0.93 | 0.96 | N/A | N/A | 0.96 | 0.99 | 0.87 | 0.95 | 0.95 | 0.97 | 0.93 | N/A | N/A | 0.90 | 0.91 | 0.93 |
|                           | es       | 0.91                    | 0.97 | 0.90 | 0.96 | 0.95 | 0.93 | 0.95 | 0.96 | 0.96 | N/A | N/A | 0.96 | 0.99 | 0.90 | 0.95 | 0.95 | 0.97 | 0.95 | N/A | N/A | 0.90 | 0.91 | 0.94 |
|                           | ru       | 0.92                    | 0.98 | 0.89 | 0.95 | 0.92 | 0.94 | 0.95 | 0.93 | 0.96 | N/A | N/A | 0.96 | 0.98 | 0.89 | 0.95 | 0.93 | 0.97 | 0.97 | N/A | N/A | 0.94 | 0.91 | 0.94 |
|                           | en-es-ru | 0.93                    | 0.98 | 0.90 | 0.96 | 0.94 | 0.94 | 0.96 | 0.95 | 0.96 | N/A | N/A | 0.96 | 0.99 | 0.89 | 0.95 | 0.95 | 0.97 | 0.96 | N/A | N/A | 0.91 | 0.92 | 0.94 |
| BLOOMZ-<br>3b             | en       | 0.75                    | 0.91 | 0.73 | 0.89 | 0.87 | 0.90 | 0.95 | 0.74 | 0.88 | N/A | N/A | 0.90 | 0.95 | 0.84 | 0.87 | 0.90 | 0.92 | 0.87 | N/A | N/A | 0.82 | 0.61 | 0.82 |
|                           | es       | 0.78                    | 0.85 | 0.84 | 0.84 | 0.85 | 0.82 | 0.90 | 0.93 | 0.83 | N/A | N/A | 0.82 | 0.88 | 0.80 | 0.80 | 0.90 | 0.88 | 0.85 | N/A | N/A | 0.79 | 0.61 | 0.81 |
|                           | ru       | 0.69                    | 0.86 | 0.57 | 0.80 | 0.80 | 0.76 | 0.81 | 0.56 | 0.80 | N/A | N/A | 0.78 | 0.83 | 0.73 | 0.75 | 0.74 | 0.74 | 0.90 | N/A | N/A | 0.84 | 0.64 | 0.72 |
|                           | en-es-ru | 0.87                    | 0.90 | 0.83 | 0.87 | 0.89 | 0.92 | 0.95 | 0.92 | 0.86 | N/A | N/A | 0.85 | 0.92 | 0.79 | 0.85 | 0.92 | 0.90 | 0.90 | N/A | N/A | 0.86 | 0.88 | 0.86 |
| w.                        | en       | 0.74                    | 0.74 | 0.78 | 0.85 | 0.86 | 0.87 | 0.95 | 0.87 | 0.88 | N/A | N/A | 0.91 | 0.95 | 0.79 | 0.85 | 0.89 | 0.92 | 0.81 | N/A | N/A | 0.74 | 0.80 | 0.83 |
| Falcon-rw<br>1b           | es       | 0.78                    | 0.73 | 0.79 | 0.85 | 0.89 | 0.91 | 0.86 | 0.93 | 0.86 | N/A | N/A | 0.90 | 0.92 | 0.82 | 0.87 | 0.92 | 0.93 | 0.85 | N/A | N/A | 0.75 | 0.80 | 0.83 |
| IIC .                     | ru       | 0.68                    | 0.85 | 0.71 | 0.76 | 0.78 | 0.86 | 0.82 | 0.83 | 0.82 | N/A | N/A | 0.84 | 0.80 | 0.71 | 0.82 | 0.80 | 0.88 | 0.90 | N/A | N/A | 0.84 | 0.75 | 0.78 |
| Fa<br>1b                  | en-es-ru | 0.82                    | 0.87 | 0.83 | 0.91 | 0.90 | 0.93 | 0.96 | 0.92 | 0.91 | N/A |     | 0.93 | 0.97 | 0.84 | 0.89 | 0.92 | 0.94 | 0.89 |     | N/A | 0.84 | 0.87 | 0.89 |
| *                         | en       | 0.85                    | 0.96 | 0.76 | 0.89 | 0.89 | 0.90 | 0.97 | 0.91 | 0.93 | N/A | N/A | 0.96 | 0.98 | 0.84 | 0.92 | 0.94 | 0.93 | 0.91 |     | N/A | 0.85 | 0.73 | 0.87 |
| -e                        | es       | 0.80                    | 0.94 | 0.85 | 0.79 | 0.90 | 0.88 | 0.91 | 0.95 | 0.86 |     | N/A | 0.90 | 0.95 | 0.84 | 0.92 | 0.92 | 0.92 | 0.90 |     | N/A | 0.86 | 0.64 | 0.83 |
| Llama-3-<br>8b            | ru       | 0.76                    | 0.95 | 0.67 | 0.79 | 0.84 | 0.88 | 0.86 | 0.81 | 0.76 |     |     | 0.83 | 0.89 | 0.80 | 0.90 | 0.83 | 0.82 | 0.95 |     | N/A | 0.90 | 0.59 | 0.78 |
| \$                        | en-es-ru | 0.92                    | 0.98 | 0.89 | 0.93 | 0.95 | 0.95 | 0.97 | 0.96 | 0.95 | N/A |     | 0.97 | 0.99 | 0.89 | 0.95 | 0.95 | 0.95 | 0.96 |     | N/A | 0.93 | 0.93 | 0.93 |
|                           | en       | 0.77                    | 0.83 | 0.81 | 0.86 | 0.89 | 0.85 | 0.96 | 0.86 | 0.89 | N/A | N/A | 0.92 | 0.92 | 0.82 | 0.81 | 0.88 | 0.92 | 0.80 | N/A | N/A | 0.75 | 0.46 | 0.82 |
| la 1.0                    | es       | 0.77                    | 0.82 | 0.86 | 0.85 | 0.89 | 0.75 | 0.83 | 0.93 | 0.89 | N/A |     | 0.91 | 0.94 | 0.80 | 0.78 | 0.91 | 0.91 | 0.82 |     | N/A | 0.76 | 0.51 | 0.82 |
| Mistral-<br>7b-v0.1       | ru       | 0.78                    | 0.94 | 0.76 | 0.89 | 0.82 | 0.85 | 0.82 | 0.84 | 0.89 | N/A |     | 0.92 | 0.90 | 0.74 | 0.80 | 0.87 | 0.88 | 0.95 |     | N/A | 0.90 | 0.44 | 0.82 |
|                           | en-es-ru | 0.90                    | 0.94 | 0.87 | 0.93 | 0.91 | 0.92 | 0.95 | 0.93 | 0.93 | N/A | N/A | 0.95 | 0.97 | 0.85 | 0.86 | 0.93 | 0.94 | 0.96 |     | N/A | 0.91 | 0.68 | 0.90 |
| XLM-<br>RoBERTa-<br>large | en       | 0.82                    | 0.92 | 0.77 | 0.94 | 0.88 | 0.92 | 0.96 | 0.91 | 0.95 | N/A | N/A | 0.96 | 0.99 | 0.87 | 0.91 | 0.94 | 0.96 | 0.88 | N/A | N/A | 0.81 | 0.83 | 0.90 |
|                           | es       | 0.87                    | 0.95 | 0.86 | 0.95 | 0.91 | 0.94 | 0.92 | 0.94 | 0.95 | N/A | N/A | 0.97 | 0.99 | 0.86 | 0.91 | 0.94 | 0.97 | 0.90 | N/A | N/A | 0.83 | 0.85 | 0.91 |
|                           | ru       | 0.92                    | 0.98 | 0.87 | 0.96 | 0.89 | 0.95 | 0.92 | 0.91 | 0.96 | N/A |     | 0.97 | 0.98 | 0.86 | 0.94 | 0.93 | 0.96 | 0.96 |     | N/A | 0.92 | 0.91 | 0.93 |
|                           | en-es-ru | 0.91                    | 0.97 | 0.88 | 0.97 | 0.92 | 0.96 | 0.96 | 0.93 | 0.97 | N/A |     | 0.98 | 0.99 | 0.90 | 0.94 | 0.95 | 0.97 | 0.95 |     | N/A | 0.90 | 0.90 | 0.94 |
| ase                       | en       | 0.83                    | 0.96 | 0.76 | 0.97 | 0.84 | 0.93 | 0.96 | 0.88 | 0.94 | N/A | N/A | 0.96 | 0.99 | 0.84 | 0.92 | 0.94 | 0.96 | 0.90 |     | N/A | 0.84 | 0.74 | 0.90 |
| 39 BE                     | es       | 0.86                    | 0.95 | 0.82 | 0.96 | 0.87 | 0.87 | 0.94 | 0.92 | 0.94 |     | N/A | 0.95 | 0.98 | 0.85 | 0.93 | 0.94 | 0.95 | 0.91 | N/A | N/A | 0.87 | 0.82 | 0.90 |
| mDeBER-<br>Ta-v3-base     | ru       | 0.89                    | 0.96 | 0.82 | 0.96 | 0.85 | 0.92 | 0.94 | 0.88 | 0.95 |     | N/A | 0.94 | 0.99 | 0.84 | 0.95 | 0.93 | 0.94 | 0.95 |     | N/A | 0.91 | 0.87 | 0.91 |
| E (2                      | en-es-ru | 0.88                    | 0.96 | 0.82 | 0.96 | 0.86 | 0.90 | 0.95 | 0.90 | 0.95 | N/A | N/A | 0.95 | 0.99 | 0.86 | 0.93 | 0.94 | 0.95 | 0.93 | N/A | N/A | 0.89 | 0.85 | 0.91 |

<span id="page-25-1"></span>Table 29: Cross-lingual AUC ROC performance of the selected MGT detectors fine-tuned monolingually (*en*, *es* and *ru*) and multilingually (*en-es-ru*), evaluated based on Telegram data (for training as well as for testing). N/A refers to not enough samples (at least 2000) in MultiSocial Telegram data.

|                           | Train    | Test Platform [AUC ROC] |      |          |         |          |      |  |  |  |  |  |  |
|---------------------------|----------|-------------------------|------|----------|---------|----------|------|--|--|--|--|--|--|
| Detector                  | Platform | Discord                 | Gab  | Telegram | Twitter | WhatsApp | all  |  |  |  |  |  |  |
|                           | Discord  | 0.99                    | 0.82 | 0.89     | 0.81    | 0.92     | 0.89 |  |  |  |  |  |  |
| Aya-101                   | Gab      | 0.97                    | 0.96 | 0.95     | 0.96    | 0.94     | 0.96 |  |  |  |  |  |  |
|                           | Telegram | 0.98                    | 0.92 | 0.97     | 0.95    | 0.96     | 0.95 |  |  |  |  |  |  |
|                           | Twitter  | 0.98                    | 0.92 | 0.94     | 0.98    | 0.94     | 0.95 |  |  |  |  |  |  |
|                           | WhatsApp | 0.97                    | 0.88 | 0.94     | 0.93    | 0.98     | 0.94 |  |  |  |  |  |  |
|                           | all      | 0.98                    | 0.95 | 0.97     | 0.97    | 0.97     | 0.97 |  |  |  |  |  |  |
|                           | Discord  | 0.98                    | 0.85 | 0.87     | 0.84    | 0.88     | 0.88 |  |  |  |  |  |  |
| ż                         | Gab      | 0.93                    | 0.92 | 0.89     | 0.91    | 0.89     | 0.91 |  |  |  |  |  |  |
| No.                       | Telegram | 0.97                    | 0.90 | 0.94     | 0.92    | 0.93     | 0.93 |  |  |  |  |  |  |
| ŏ                         | Twitter  | 0.95                    | 0.89 | 0.88     | 0.98    | 0.87     | 0.91 |  |  |  |  |  |  |
| BLOOMZ-<br>3b             | WhatsApp | 0.96                    | 0.90 | 0.91     | 0.92    | 0.97     | 0.93 |  |  |  |  |  |  |
|                           | all      | 0.98                    | 0.94 | 0.95     | 0.97    | 0.96     | 0.96 |  |  |  |  |  |  |
|                           | Discord  | 0.98                    | 0.80 | 0.87     | 0.82    | 0.85     | 0.86 |  |  |  |  |  |  |
| -M                        | Gab      | 0.95                    | 0.93 | 0.92     | 0.95    | 0.91     | 0.93 |  |  |  |  |  |  |
|                           | Telegram | 0.98                    | 0.90 | 0.95     | 0.95    | 0.93     | 0.94 |  |  |  |  |  |  |
| Falcon-rw-<br>Ib          | Twitter  | 0.97                    | 0.91 | 0.93     | 0.98    | 0.92     | 0.94 |  |  |  |  |  |  |
|                           | WhatsApp | 0.97                    | 0.91 | 0.93     | 0.95    | 0.98     | 0.94 |  |  |  |  |  |  |
|                           | all      | 0.97                    | 0.91 | 0.94     | 0.96    | 0.94     | 0.94 |  |  |  |  |  |  |
|                           | Discord  | 0.99                    | 0.83 | 0.82     | 0.72    | 0.86     | 0.83 |  |  |  |  |  |  |
| Llama-3-8b                | Gab      | 0.95                    | 0.96 | 0.93     | 0.97    | 0.90     | 0.94 |  |  |  |  |  |  |
| a-3                       | Telegram | 0.98                    | 0.94 | 0.97     | 0.97    | 0.96     | 0.97 |  |  |  |  |  |  |
| Ï                         | Twitter  | 0.96                    | 0.90 | 0.92     | 0.98    | 0.91     | 0.93 |  |  |  |  |  |  |
| Γľ                        | WhatsApp | 0.97                    | 0.89 | 0.93     | 0.93    | 0.98     | 0.94 |  |  |  |  |  |  |
|                           | all      | 0.98                    | 0.94 | 0.95     | 0.97    | 0.95     | 0.96 |  |  |  |  |  |  |
|                           | Discord  | 0.98                    | 0.82 | 0.88     | 0.86    | 0.88     | 0.88 |  |  |  |  |  |  |
| φ.                        | Gab      | 0.95                    | 0.95 | 0.93     | 0.96    | 0.92     | 0.94 |  |  |  |  |  |  |
| al                        | Telegram | 0.98                    | 0.94 | 0.97     | 0.97    | 0.95     | 0.96 |  |  |  |  |  |  |
| Mistral-7b-<br>v0.1       | Twitter  | 0.96                    | 0.90 | 0.90     | 0.98    | 0.91     | 0.93 |  |  |  |  |  |  |
| Mis<br>v0.1               | WhatsApp | 0.96                    | 0.90 | 0.92     | 0.94    | 0.97     | 0.93 |  |  |  |  |  |  |
|                           | all      | 0.96                    | 0.92 | 0.93     | 0.96    | 0.94     | 0.94 |  |  |  |  |  |  |
| XLM-<br>RoBERTa-<br>large | Discord  | 0.99                    | 0.87 | 0.90     | 0.83    | 0.94     | 0.90 |  |  |  |  |  |  |
|                           | Gab      | 0.97                    | 0.93 | 0.92     | 0.90    | 0.90     | 0.92 |  |  |  |  |  |  |
|                           | Telegram | 0.98                    | 0.92 | 0.96     | 0.93    | 0.95     | 0.95 |  |  |  |  |  |  |
|                           | Twitter  | 0.98                    | 0.93 | 0.94     | 0.97    | 0.94     | 0.95 |  |  |  |  |  |  |
|                           | WhatsApp | 0.97                    | 0.88 | 0.92     | 0.88    | 0.96     | 0.92 |  |  |  |  |  |  |
|                           | all      | 0.98                    | 0.94 | 0.96     | 0.96    | 0.96     | 0.96 |  |  |  |  |  |  |
| mDeBERTa-<br>v3-base      | Discord  | 0.99                    | 0.86 | 0.89     | 0.84    | 0.93     | 0.89 |  |  |  |  |  |  |
|                           | Gab      | 0.97                    | 0.93 | 0.93     | 0.92    | 0.94     | 0.94 |  |  |  |  |  |  |
|                           | Telegram | 0.98                    | 0.90 | 0.95     | 0.93    | 0.95     | 0.94 |  |  |  |  |  |  |
|                           | Twitter  | 0.97                    | 0.90 | 0.92     | 0.97    | 0.94     | 0.94 |  |  |  |  |  |  |
|                           | WhatsApp | 0.98                    | 0.91 | 0.93     | 0.90    | 0.97     | 0.93 |  |  |  |  |  |  |
|                           | all      | 0.98                    | 0.91 | 0.94     | 0.95    | 0.95     | 0.95 |  |  |  |  |  |  |

Table 30: Cross-platform evaluation of the selected fine-tuned MGT detectors.