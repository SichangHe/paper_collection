Title page: **Deepfake fraud detection: safeguarding trust in generative** AI
Author name: Dr Felipe Romero-Moreno Affiliation: University of Hertfordshire, Schools of Law and Education (UK) Adress: 6 Webber Close, SG17 5TA, Shefford (Bedfordshire), UK Phone number: 07776186110 Email address: f.romero-moreno@herts.ac.uk

# Deepfake Fraud Detection: Safeguarding Trust In Generative Ai

## Abstract

![1_image_0.png](1_image_0.png)

The rapid advancement of generative artificial intelligence has enabled the creation of increasingly convincing deepfakes, posing an unprecedented threat to trust and security in the increasingly digital world. This paper delves into the intricate challenges of deepfake fraud detection, scrutinising the limitations of existing legal frameworks in effectively addressing this evolving threat. Guided by UN Resolution A/78/L.49 and grounded in the principles of Article 114 of the Treaty on the Functioning of the European Union, it critically analyses the intersection of deepfake fraud detection tools with human rights and evolving laws, including the EU AI Act, General Data Protection Regulation, and Digital Services Act. The research explores the efficacy of various detection methods, encompassing artifact-based, physiological signal analysis, deep learning, and blockchain approaches, while also examining the role of the Coalition for Content Provenance and Authenticity (C2PA) in bolstering media authenticity. Crucially, this paper proposes a comprehensive strategy for combating deepfake fraud, integrating legal, ethical, and technical considerations to develop robust and responsible detection methods with the potential for global application. This multifaceted approach is vital to safeguarding trust in generative AI while upholding fundamental rights and data protection principles in the face of this rapidly evolving technology.

Keywords: Generative AI; Deepfake Detection; Trust & Security

## 1. Combating Fraud: The Urgent Need For Deepfake Detection 1.1. Deepfakes Weaponised: Ai-Powered Deception And The Erosion Of Trust

In 2024, a deepfake AI-generated video call of a CEO authorising a fraudulent transfer of $25.6 million shocked the financial world, exposing the alarming potential of deepfake technology for malicious purposes.1 This incident served as a stark reminder of the growing threat of deepfake fraud, a sophisticated form of deception that leverages artificial intelligence to create synthetic media convincingly mimicking reality.

Preprint While deepfakes can be used for entertainment, education or satire,2 their increasing weaponization for fraud poses a significant challenge. Fraudsters exploit this technology to bypass security measures, manipulate individuals, and steal identities, leading to 2024).

1 CNN World Asia, Finance worker pays out $25 million after video call with deepfake 'chief financial officer'. https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html, 2024 (accessed 19 November 2024). 2 PA, Deepfakes: a human challenge, how can tooling be used to assist humans in deepfake detection? https://www.weprotect.org/wp-content/uploads/Deepfakes_A-Human-Challenge_PA-Report_v3.pdf, 2024 (accessed 19 November 
substantial financial losses and erosion of trust.3 This exploitation is facilitated by readily available AI tools and AI-powered bots that automate communication, enabling fraudsters to fabricate identities, impersonate others, or mimic trusted contacts to deceive victims at an unprecedented scale.4 The Federal Trade Commission highlights the severity of this issue, ranking imposter scams, which often utilise deepfakes, as the most reported fraud, with losses reaching $2.7 billion in 2023 alone.5

## 1.2. Navigating The Complexities Of Deepfake Fraud Detection

Given the growing threat and sophistication of deepfake fraud, it is crucial to examine the challenges associated with its detection. Deepfake detection is a complex and evolving field. The adaptability of deepfake technology constantly evolves, making telltale 'artefacts' like inconsistencies in lighting, unnatural blinking patterns, or subtle distortions less obvious. This hinders both human and automated detection of fraudulent media.6 Moreover, deepfake detection tools, despite their high accuracy, are often inaccessible to the public, creating a 
'deepfake divide'7 where only select entities can effectively combat this growing threat. Robustness is also a significant hurdle. The prevalence of 'stripped' files, where metadata is removed, further complicates analysis. These alterations impact file quality and can lead to inaccurate results, especially given the limitations of training datasets.8 Deepfake detection systems also face interoperability issues. Company-specific classifiers, like those from ElevenLabs, may only identify content generated by their own AI tools, missing deepfakes created elsewhere. 9 While effective in analysing photos and videos for deepfakes created by specific AI models,10 tools like Microsoft's Video Authenticator may have limitations in detecting those generated through other means. Furthermore, 

Preprint 3 Microsoft, Protecting the public from abusive AI-generated content. https://cdn-dynmedia1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Protecting-Public-Abusive-AI-Generated- Content.pdf, 2024 (accessed 19 November 2024). 4 Microsoft, Microsoft Digital Defense Report 2024, the foundations and new frontiers of cybersecurity. https://cdn-dynmedia1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoftbrand/documents/Microsoft%20Digital%20Defense%20Report%202024%20%281%29.pdf, 2024 (accessed 19 November 2024). 5 Federal Trade Commission, The big view: all sentinel reports, tableau public. https://public.tableau.com/app/profile/federal.trade.commission/viz/TheBigViewAllSentinelReports/TopReports, 2024 (accessed 19 November 2024). 6 Tolosana, R., Vera-Rodriguez, R., Fierrez, J., Morales, A., & Ortega-Garcia, J., Deepfakes and beyond: a survey of face manipulation and fake detection, Information Fusion. 64 (2020) 131â€“148. https://www.sciencedirect.com/science/article/pii/S1566253520303110?via%3Dihub 7 Bitton, D. B., Hoffmann, C. P., & Godulla, A., Deepfakes in the context of AI inequalities: analysing disparities in knowledge and attitudes, Information, Communication & Society. (2024) 1-21 https://www.tandfonline.com/doi/full/10.1080/1369118X.2024.2420037 8 Reuters Institute & University of Oxford, Spotting the deepfakes in this year of elections: how AI detection tools work and where they fail. https://reutersinstitute.politics.ox.ac.uk/news/spotting-deepfakes-year-elections-how-ai-detection-tools-work-and-wherethey-fail, 2024 (accessed 19 November 2024). 9 Ibid. 10 US Department of Homeland Security, S&T Digital forgeries report technology landscape threat assessment January 24, 2023. https://www.dhs.gov/sites/default/files/2023-06/23_0630_st_digital_forgeries_report_signed.pdf, 2023 (accessed 19 November 2024).
these classifiers are vulnerable to manipulation, hindering detection. 11 The lack of standardised watermarks necessitates coordinated efforts for effective deepfake identification.12 These challenges hinder efforts to combat deepfake fraud and protect individuals from its harmful consequences.

## 1.3. Confronting The Deepfake Challenge: Limitations Of Existing Frameworks

These technical difficulties highlight the urgent need for robust legal frameworks to guide the ethical development and deployment of deepfake detection tools. However, existing legal frameworks face serious limitations in addressing this evolving threat. Deepfake fraud poses a significant challenge to existing legal frameworks, including those addressing AI technologies. 

Current case law primarily focuses on privacy, intellectual property,13 and unauthorized likeness,14 while some cases touch upon harmful applications,15 and free speech,16 but not deepfake fraud directly. This leaves a critical gap in legal protection against this evolving threat.

In the US, the DEEPFAKES ACCOUNTABILITY ACT 202317 mandates labelling and disclosure for online deepfakes and establishes a task force to develop detection technology. However, the lack of a comprehensive federal privacy law, comparable to the EU's General Data Protection Regulation (GDPR)18, creates a fragmented approach that may hinder effective regulation and human rights protection. For example, without a unified standard for data protection, it becomes difficult to regulate the collection and use of biometric data often used in deepfake detection.

19 While the recent US AI 
Governance Framework seeks to align AI with fundamental rights,20 its effectiveness in addressing deepfake fraud remains to be seen.

Preprint 11 Reuters Institute & University of Oxford, Spotting the deepfakes in this year of elections: how AI detection tools work and where they fail. https://reutersinstitute.politics.ox.ac.uk/news/spotting-deepfakes-year-elections-how-ai-detection-tools-work-and-wherethey-fail, 2024 (accessed 19 November 2024). 12 Brookings, Detecting AI fingerprints: a guide to watermarking and beyond. https://www.brookings.edu/articles/detecting-aifingerprints-a-guide-to-watermarking-and-beyond/, 2024 (accessed 19 November 2024). 13 *PM et al v OpenAI LP*., 3:23-cv-03199 (US District Court, N.D. Cal. 2023). 14 *Young v Neocortext, Inc*., 2:23-cv-02496 (US District Court, C.D. Cal. 2023). 15 *People of the State of California (David Chiu) v Sol Ecom, Inc., et al*., CGC-24-617237 (US Superior Court, C.S.F. Cal. 2024). 16 *Kohls v Bonta*., 2:24-cv-02527 (US District Court, E.D. Cal. 2024). 17 DEEPFAKES Accountability Act of 2023, Pub. L. No. 118-181, 137 Stat. 2127 (2023). 18 Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation). Official Journal of the European Union, L119, 1â€“88. 19 For instance, deepfake detection tools, like Sentinel, utilise AI to analyse various aspects of media, such as visuals, audio, and even biometric data, to identify fakes. These tools examine details like facial expressions, blinking patterns, and audio manipulations, while also leveraging biological signals and facial recognition to accurately spot manipulated media. See Expert Insights, The top 8 deepfake detection solutions. https://expertinsights.com/insights/the-top-deepfake-detection-solutions/ 2024 (accessed 19 November 2024). 20 The White House Washington, Framework to advance AI governance and risk management in national security. https://ai.gov/wpcontent/uploads/2024/10/NSM-Framework-to-Advance-AI-Governance-and-Risk-Management-in-National-Security.pdf, 2024 (accessed 19 November 2024).
In contrast to the fragmented approach in the US, the EU's proactive stance to AI regulation includes the AI Act, 21 which categorises AI systems by risk, and the Digital Services Act, 22 which addresses online harms and platform accountability. However, the specific application of these regulations to deepfake fraud detection tools requires further analysis.

In the UK, the Online Safety Act 2023,23 combats online harms, including deepfake fraud, by requiring services to assess and mitigate risks from illegal content like fraudulent deepfakes. The Financial Conduct Authority and Ofcom collaborate to reduce online fraud, including deepfakes, working with the Information Commissioner's Office on data sharing for scam prevention. 24 This fragmented legal landscape creates a regulatory gap, raising concerns about the ethical development and deployment of deepfake detection tools.

## 1.4. Bridging The Gap: A Comprehensive Strategy Guided By Un Resolution A/78/L.49

Despite growing academic interest in AI and deepfakes (Farid et al.;25 Gregory;26 Ajder and Glick;27 Citron and Chesney;28 Anonymous29) a comprehensive exploration of the legal and ethical implications of deepfake fraud detection tools remains lacking. This research addresses this gap by critically examining the intersection of these tools, human rights, and evolving laws, specifically focusing on how UN Resolution A/78/L.4930 can provide a framework for addressing these challenges within the context of EU legislation, including the AI Act, GDPR, and Digital Services Act. 

21 Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act). Official Journal of the European Union, L 187/1, 14.7.2024. 22 Regulation (EU) 2022/2065 of the European Parliament and of the Council of 19 October 2022 on a single market for digital services and amending Directive 2000/31/EC (Digital Services Act). Official Journal of the European Union, L 277/1, 27.10.2022. 23 Online Safety Act 2023. (c. 50). London: The Stationery Office. 24 Digital Regulation Cooperation Forum, Tackling online fraud and scams: Ofcom and FCA collaboration. https://www.drcf.org.uk/publications/blogs/tackling-online-fraud-and-scams-ofcom-and-fca-collaboration/, 2024 (accessed 19 November 2024). 25 Agarwal, S., Farid, H., Gu, Y., He, M., Nagano, K., & Li, H., Protecting world leaders against deep fakes, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. (2019) 38-45. https://farid.berkeley.edu/downloads/publications/cvpr19/cvpr19a.pdf 26 Gregory, S., Fortify the truth: how to defend human rights in an age of deepfakes and generative AI, Journal of Human Rights Practice. 15(3) (2023) 702-714. https://academic.oup.com/jhrp/article/15/3/702/7261649 27 Ajder, H., & Glick, J., Just joking: deepfakes, satire, and the politics of synthetic media [Report]. WITNESS; Co-Creation Studio at MIT Open Documentary Lab. https://cocreationstudio.mit.edu/wp-content/uploads/2021/12/JustJoking.pdf, 2021 (accessed 19 November 2024). 28 Citron, D. K., & Chesney, R., Deep fakes: A looming challenge for privacy, democracy, and national security, California Law Review. 107:1753 (2019) 1753â€“1820. https://scholarship.law.bu.edu/faculty_scholarship/640/ 29 Anonymous., Generative AI and deepfakes: a human rights approach to tackling harmful content, International Review of Law, Computers & Technology. 38(3) (2024) 297-326. https://doi.org/10.1080/13600869.2024.2324540 30 United Nations General Assembly. (2024). Seizing the opportunities of safe, secure and trustworthy artificial intelligence systems for sustainable development. A/RES/78/L.49. New York: United Nations.

Preprint 
This analysis considers the broader EU legal landscape, including the potential role of Article 114 of the Treaty on the Functioning of the European Union (TFEU)31 in harmonizing standards for deepfake detection tools. This harmonization could foster innovation and accessibility while upholding ethical considerations and consumer protection.

UN Resolution A/78/L.49 advocates for robust, accessible, adaptable, and interoperable tools, emphasizing transparency, accountability, and international cooperation. It promotes responsible innovation by safeguarding intellectual property rights and protecting personal data. The resolution also stresses the importance of robust safeguards, along with risk and impact assessments when developing and deploying deepfake detection tools. 32 However, navigating existing legal frameworks and translating these principles into concrete safeguards is complex. Without robust safeguards, these tools risk becoming instruments of discrimination, surveillance, and censorship, infringing upon fundamental human rights.

This paper proposes a comprehensive strategy to combat deepfake fraud, guided by UN Resolution A/78/L.49. It highlights the need for a multifaceted approach integrating legal, ethical, technical, and societal considerations to develop effective and ethical deepfake detection strategies that protect individuals and society while upholding fundamental rights.

2. Deepfakes and Fraud: Case studies 2.1. Cross-Platform Impersonation Attacks: Beware of AI-Powered Identity 

## Theft

Cybercriminals are increasingly using Cross-Platform Impersonation Attacks (XPIA) to commit identity theft. Echoing the concerns raised in *OpenAI v Clarkson*,
33 they exploit readily available information on social media platforms like LinkedIn, Facebook, and X to convincingly impersonate high-profile individuals, such as CEOs, mimicking communication styles and gathering personal details to create fake profiles on messaging platforms like WhatsApp and Telegram.34 Preprint To amplify the deception, criminals leverage deepfake technology. They clone voices from public videos to make phone calls requesting urgent money transfers. Deceived by the convincing impersonation, employees authorise these transfers, leading to 

31 Article 114 of Consolidated version of the Treaty on the Functioning of the European Union, 2016 O.J. (C 202) 1. 32 United Nations General Assembly. (2024). Seizing the opportunities of safe, secure and trustworthy artificial intelligence systems for sustainable development. A/RES/78/L.49. New York: United Nations. 33 *PM et al v OpenAI LP*., 3:23-cv-03199 (US District Court, N.D. Cal. 2023) [221]. 34 US Homeland Security, Increasing threats of deepfake identities. https://www.dhs.gov/sites/default/files/publications/increasing_threats_of_deepfake_identities_0.pdf, 2021 (accessed 19 November 2024).
substantial financial losses. This combination of XPIA and deepfakes highlights the alarming potential of AI-powered fraud, as criminals exploit human trust and bypass security measures.35

![6_image_0.png](6_image_0.png)

XPIA and deepfakes are actively impacting businesses and individuals, with numerous examples highlighting the severe consequences. In 2019, an attacker used AI- generated voice cloning to impersonate a German CEO, defrauding a British energy company of â‚¬220,000.36 In 2022, Binance's Chief Communications Officer was targeted, with criminals creating a deepfake hologram of him to carry out attacks against crypto projects.37 More recently, in January 2024, an employee from the UK engineering firm Arup fell victim to a deepfake video call in Hong Kong, resulting in a loss of HK$200 million (approximately Â£20 million).38 These cases emphasise the urgent need for individuals and organisations to bolster defences against these evolving threats. Implementing robust authentication measures, employee training, and advanced detection technologies are crucial for mitigating the risks posed by XPIA and deepfakes.

## 2.2. Celebrity Scams Go Viral

![6_image_1.png](6_image_1.png)

Beyond targeted attacks on businesses, deepfakes are also being used to deceive the public at large. Deepfake scams are rapidly becoming a major threat, leveraging the trust people place in celebrities and influencers. Fraudsters use deepfakes to generate fake endorsements, promote fraudulent investment schemes, and manipulate individuals for financial gain. The scale of this threat is significant, with millions of dollars lost to deepfake scams.

Deepfake scams are surging, eroding trust in online content. Scammers exploit AI to create alarmingly realistic videos of trusted figures like Martin Lewis and Elon Musk, promoting fake investments or giveaways to trick people into sending money or sharing personal information. In 2023, a convincing deepfake of Lewis promoting a fake investment app circulated on social media, fooling even savvy individuals.39 This threat is widespread. In October 2024 alone, Australians lost $43.4 million to 'celeb-bait' 

Preprint 35 Built In, Is Your CEO's Social Media Presence Putting Your Company at Risk? https://builtin.com/articles/is-ceo-social-mediapresence-a-cybersecurity-risk, 2024 (accessed 19 November 2024). 36 Forbes, A Voice Deepfake Was Used To Scam A CEO Out Of $243,000. https://www.forbes.com/sites/jessedamiani/2019/09/03/a-voice-deepfake-was-used-to-scam-a-ceo-out-of-243000/, 2019 (accessed 19 November 2024). 37 Bitdefender, Crypto Projects Scammed with Deepfake AI Video of Binance Executive. https://www.bitdefender.com/engb/blog/hotforsecurity/crypto-projects-scammed-with-deepfake-ai-video-of-binance-executive, 2022 (accessed 19 November 2024). 38 CNN World Asia, Finance worker pays out $25 million after video call with deepfake 'chief financial officer'. https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html, 2024 (accessed 19 November 2024). 39 MoneySavingExpert, WARNING: Beware frightening new 'deepfake' Martin Lewis video scam promoting a fake 'Elon Musk investment' - it's not real. https://www.moneysavingexpert.com/news/2023/07/beware-terrifying-new--deepfake--martin-lewis-videoscam-promoti/, 2023 (accessed 19 November 2024).
scams on Facebook, with thousands of deepfake-laden pages deleted.40 A deepfake of Musk promoting cryptocurrency even aired during a SpaceX event.41 Audio deepfakes are also being used, as seen with tech reviewer Marques Brownlee's voice cloned in ads for 'smart' business cards.42 These cases underscore the urgent need for increased awareness, improved detection methods, and collaborative efforts to combat the growing threat of deepfake scams. 

Platforms are actively working to detect and remove deepfakes,43 but individuals must also be wary of content and offers that seem too good to be true. Collaboration between individuals, platforms, and authorities is crucial to combat this growing threat.

## 2.3. Deepfakes And Romance Scams

Imagine falling in love with someone you've never actually met... That's the terrifying reality of deepfake romance scams. These scams, often incorporating elements of XPIA, involve criminals gathering information from various sources to create convincing fake identities across different online platforms. They then use deepfake technology to generate highly realistic video calls, fostering trust and intimacy before manipulating victims into parting with their money.44 The Yahoo Boys, a notorious group of con artists, are leading this trend, using faceswapping apps and software to conduct real-time video calls with unsuspecting victims. These scammers prey on victims' emotions and vulnerabilities, causing both financial loss and long-lasting psychological harm.45 The impact of these scams is far-reaching, with victims suffering both financial and emotional distress. The scale of this threat is significant. In 2023, the FBI reported over 
$650 million in losses to romance scams,46 while the FTC reported an even higher figure of $1.14 billion, with a median loss of $2,000 per person - the highest for any 

Preprint 40 The Guardian, More than 9,000 scam Facebook pages deleted after Australians lose $43.4m to celebrity deepfakes. https://www.theguardian.com/technology/2024/oct/02/more-than-9000-scam-facebook-pages-deleted-after-australians-lose-millionsto-celebrity-deepfakes, 2024 (accessed 19 November 2024). 41 LinkedIn, Generative AI crypto *scam* live now on YouTube. https://www.linkedin.com/posts/giorgiopatrini_generative-ai-cryptoscam-live-now-on-youtube-activity-7251255365848154112-2Phm/?utm_source=share&utm_medium=member_android, 2024 (accessed 19 November 2024). 42 The Verge, What happens when a business steals your voice with AI? https://www.theverge.com/2024/10/14/24270097/whathappens-when-a-business-steals-your-voice-with-ai, 2024 (accessed 19 November 2024). 43 404 Media, YouTube Deletes 1,000 Videos of Celebrity AI Scam Ads. https://www.404media.co/youtube-deletes-1-000-videos-ofcelebrity-ai-scam-ads/, 2024 (accessed 19 November 2024). 44 McAfee, How Romance Scammers are Using Deepfakes to Swindle Victims. https://www.mcafee.com/ai/news/how-romancescammers-are-using-deepfakes-to-swindle-victims/, 2024 (accessed 19 November 2024). 45 Wired, The Real-Time Deepfake Romance Scams Have Arrived. https://www.wired.com/story/yahoo-boys-real-time-deepfakescams/, 2024 (accessed 19 November 2024). 46 FBI, Internet Crime Report 2023. https://www.ic3.gov/media/PDF/AnnualReport/2023_IC3Report.pdf, 2023 (accessed 19 November 2024).
imposter scam.47 The UNODC has also raised concerns about the increasing use of deepfakes by organised crime syndicates in Asia.48 Recent cases highlight the growing threat. In August 2024, authorities in Australia warned of AI and deepfake technology being used in romance scams, with Western Australian victims losing $2.9 million.49 In October 2024, Hong Kong police reported a deepfake romance scam that raked in $46 million from victims across Asia.50

## 3. Deepfake Deception: Exposing Fraud With Advanced Detection Methods 3.1. Inside Deepfake Detection: Exploring The Core Techniques

Deepfake detection methods typically employ one or a combination of four main categories: artefact-based methods that scrutinise images and videos for inconsistencies, physiological signals analysis methods that analyse biological cues like heart rate to identify discrepancies, deep learning-based methods that leverage AI to identify manipulation patterns, and blockchain-based methods that verify authenticity using secure records. Often, the most effective solutions combine multiple approaches for enhanced accuracy and robustness.51

## 3.1.2. Artefact-Based Methods

![8_image_0.png](8_image_0.png)

Deepfakes frequently leave behind subtle inconsistencies, and detecting these 'artefacts' is crucial. Artefacts can be minute inconsistencies in facial features or patterns in the image data itself. Several approaches exist to expose these flaws. Pixellevel analysis scrutinises individual pixels for irregularities,52 while frequency domain analysis examines the image in a different spectrum to reveal unnatural distortions.53 However, while analysing individual pixels or frequencies can reveal some flaws, these methods may struggle with more sophisticated forgeries.54 Codec-based analysis 

Preprint 47 FTC, 'Love Stinks' - when a scammer is involved. https://www.ftc.gov/business-guidance/blog/2024/02/love-stinks-whenscammer-involved, 2024 (accessed 19 November 2024). 48 UN, Billion-dollar cyberfraud industry expands in Southeast Asia as criminals adopt new technologies. https://www.unodc.org/roseap/en/2024/10/cyberfraud-industry-expands-southeast-asia/story.html, 2024 (accessed 19 November 2024). 49 ABC, Authorities warn AI, deepfake technology in romance scams costing WA victims thousands. https://www.abc.net.au/news/2024-08-28/deepfake-ai-used-in-wa-romance-scams/104279902, 2024 (accessed 19 November 2024). 50 The Record, Hong Kong police bust fraud ring that used face-swapping tech for romance scams. https://therecord.media/hongkong-police-bust-romance-scammers-face-swapping-deepfakes, 2024 (accessed 19 November 2024). 51 Expert Insights, The top 8 deepfake detection solutions. https://expertinsights.com/insights/the-top-deepfake-detection-solutions/ 2024 (accessed 19 November 2024). 52 Rafique, R., Gantassi, R., Amin, R., Frnda, J., Mustapha, A., & Alshehri, A. H., Deep fake detection and classification using errorlevel analysis and deep learning, Scientific Reports. 13:7422 (2023) 1-13. https://doi.org/10.1038/s41598-023-34629-3 53 Wang, B., Wu, X., Tang, Y., Ma, Y., Shan, Z., & Wei, F., Frequency domain filtered residual network for deepfake detection, Mathematics. 11(4) (2023) 1-13. https://doi.org/10.3390/math11040816 54 Kaur, A., Noori Hoshyar, A., Saikrishna, V. *et al.,* Deepfake video detection: challenges and opportunities, Springer. 57(159) (2024) 1-47. https://doi.org/10.1007/s10462-024-10810-6
exploits compression artefacts, as deepfakes often undergo multiple compression rounds.55 Researchers are pushing the boundaries with more advanced techniques. BiG-Arts, for example, employs a novel multi-task learning approach to detect artefacts from both AI model generation and video post-processing, improving accuracy in various scenarios.56 Similarly, LAA-Net utilises a specialised attention mechanism to focus on vulnerable areas and efficiently share information, improving accuracy and overcoming limitations of existing methods that do not generalise well to unseen manipulations.57 Building on this research, companies like VisionLabs are at the forefront of real-time deepfake detection. Their solutions, combining facial recognition and artefact analysis, are crucial for combating the spread of fraud in real-time applications like video conferencing and live streaming.58

## 3.1.3. Physiological Signal Analysis

Deepfakes convincingly replace someone's likeness in a video and are increasingly used for fraudulent purposes. However, these manipulated videos often fail to replicate subtle physiological signals associated with real human emotions. These include heart rate, detectable through minute colour changes in the skin,59 unique eye blinking patterns,60 breathing patterns that subtly influence head and chest movements,61 and pupil dilation in response to stimuli.62 To detect these discrepancies, researchers extract physiological signals from videos using techniques like photoplethysmography for heart rate and eye-tracking algorithms for blinking and pupil dilation. Machine learning models are then trained on datasets of real and deepfake videos to identify these subtle physiological inconsistencies.63

Preprint 55 Xie, Y., Lu, Y., Fu, R., Wen, Z., Wang, Z., Tao, J., ... & Sun, Y., The Codecfake dataset and countermeasures for the universally detection of deepfake audio, arXiv preprint arXiv:2405.04880. (2024) 1-13. https://arxiv.org/abs/2405.04880 56 Chen, H., Li, Y., Lin, D., Li, B., & Wu, J., Watching the big artifacts: exposing deepfake videos via bi-granularity artifacts, Pattern Recognition. 135 (2023) 1-11. https://doi.org/10.1016/j.patcog.2022.109179 57 Nguyen, D., Mejri, N., Singh, I. P., Kuleshova, P., Astrid, M., Kacem, A., Ghorbel, E., & Aouada, D., LAA-Net: Localized artifact attention network for quality-agnostic and generalizable deepfake detection, arXiv preprint arXiv:2401.13856 (2024) 1-13. https://arxiv.org/abs/2401.13856 58 VisionLabs, VisionLabs Knowledge Hub. https://docs.visionlabs.ai/ founded in 2012 (accessed 19 November 2024). 59 https://arxiv.org/abs/2010.00400 60 Jung, T., Kim, S., & Kim, K., DeepVision: Deepfakes detection using human eye blinking pattern, IEEE Access. 8 (2020) 8314483154. https://doi.org/10.1109/ACCESS.2020.2988660 61 Doan, T.-P., Nguyen-Vu, L., Jung, S., & Hong, K., BTS-E: audio deepfake detection using breathing-talking-silence encoder, ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2023) 1â€“5. https://ieeexplore.ieee.org/document/10095927 62 Patil, K., Kale, S., & Dhokey, J., Deepfake detection using biological features: a survey, arXiv preprint arXiv:2301.05819 (2023) 118. https://arxiv.org/pdf/2301.05819 63 Almars, A. M., Deepfakes detection techniques using deep learning: A survey, Journal of Computer and Communications. 9(5) (2021) 20â€“35. https://www.scirp.org/journal/paperinformation?paperid=109149
Some researchers are even combining physiological signals with facial analysis, creating 'physiological maps' to expose discrepancies between expressions and underlying physiological cues.64 Intel's FakeCatcher platform, for instance, analyses 'blood flow' patterns in video pixels using photoplethysmography to achieve real-time deepfake detection. This technology boasts a 96% accuracy rate, making it a valuable tool for combating the spread of deceptive videos in live streams and online interactions.65

## 3.1.4. Deep Learning-Based Methods

![10_image_0.png](10_image_0.png)

Deep learning methods, inspired by the human brain, are powerful tools for detecting deepfake fraud. Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) are deep learning models that analyse images and videos, respectively, to identify inconsistencies and anomalies indicative of deepfakes. Generative Adversarial Networks (GANs) can create convincing fakes by learning patterns from real data, but this same ability allows them to detect subtle inconsistencies that arise from the generative process.66 However, while deep learning shows promise in deepfake detection, research identifies key challenges including data limitations, computational costs, and evolving manipulation techniques.67 This emphasises the need for robust models adapting to the evolving deepfake techniques and high-quality datasets for effective real-time detection. Researchers are exploring optimal network architectures and ways to integrate detection into social media platforms to combat the spread of increasingly sophisticated deepfakes.68 Addressing these challenges, companies like Sensity AI are developing platforms that use CNNs and RNNs-based deep learning methods to detect deepfakes in real-time with an impressive 98% accuracy. These platforms protect against identity verification fraud by detecting AI-powered alterations in camera feeds. For example, they can automatically flag suspicious accounts or transactions, preventing fraudsters from exploiting stolen identities.69

Preprint 64 Gong, L. Y., & Li, X. J., A contemporary survey on deepfake detection: Datasets, algorithms, and challenges, Electronics. 13(3) (2024) 1-22. https://doi.org/10.3390/electronics13030585 65 Intel, Intel introduces real-time deepfake detector. https://www.intel.com/content/www/us/en/newsroom/news/intel-introduces-realtime-deepfake-detector.html, 2022 (accessed 19 November 2024). 66 Nguyen, T. T., Nguyen, Q. V. H., Nguyen, D. T., Nguyen, D. T., Huynh-The, T., Nahavandi, S., Nguyen, T. T., Pham, Q.-V., & Nguyen, C. M, Deep learning for deepfakes creation and detection: a survey, Computer Vision and Image Understanding. 23 (2022) 1-14. https://doi.org/10.1016/j.cviu.2022.103525 67 Kaur, A., Hoshyar, A. N., Saikrishna, V., Firmin, S., & Xia, F., Deepfake video detection: Challenges and opportunities. Artificial Intelligence Review, 57(1) (2024) 1-47. https://link.springer.com/article/10.1007/s10462-024-10810-6 68 Almars, A. M., Deepfakes detection techniques using deep learning: A survey, Journal of Computer and Communications. 9(5) (2021) 20â€“35. https://www.scirp.org/journal/paperinformation?paperid=109149 69 Sensity AI, All-in-one deepfake detection. https://sensity.ai/, founded in 2018 (accessed 19 November 2024).

## 3.1.5. Blockchain-Based Methods

![11_image_0.png](11_image_0.png)

Blockchain technology offers a powerful solution to combat the growing threat of deepfake fraud and ensure media authenticity. By recording a media file's history on a permanent and unchangeable record using secure digital fingerprints, blockchain prevents tampering and enables verifiable timestamps for accurate source identification. Because multiple parties verify the information, it is harder for any single person to manipulate it, enhancing trust and reducing bias in authentication. Smart contracts further automate this process, ensuring transparent ownership and facilitating the detection of unauthorised alterations.70 Researchers are leveraging this technology to develop advanced deepfake detection methods. Combining AI-powered image analysis with blockchain's security, these methods demonstrate significant improvements in accuracy. For example, one study shows a 6.6% increase in accuracy and a 5.1% improvement in AUC, a metric indicating how well the model distinguishes real videos from deepfakes.71 This research has led to practical tools like WeVerify's Deepfake Detector, which analyses images and videos and assigns probability scores to identify potential deepfakes.72 Its blockchain-based public database of known fakes allows for collaborative verification. Accessible through apps and plugins, WeVerify empowers individuals to detect and expose deepfakes, mitigating their impact.73

## 3.1.6. Emerging Trends In Deepfake Detection

A powerful approach to fighting deepfake fraud is combining multiple detection methods, such as analysing audio, video, and text, in a multi-modal analysis to improve accuracy and robustness.74 Reality Defender, for example, uses this approach for realtime detection, even developing a tool to spot AI-generated impersonators on Zoom.75 Explainable AI (XAI) also builds trust in deepfake detection by allowing AI models to explain their decisions.76 For instance, DuckDuckGoose offers tools like AI Voice 

Preprint 70 Struck, Deepfakes and blockchain. https://struckcapital.com/deepfakes-and-blockchain/, 2024 (accessed 19 November 2024).

![11_image_1.png](11_image_1.png) 71 Heidari, A., Navimipour, N.J., Dag, H. et al., A Novel Blockchain-Based Deepfake Detection Method Using Federated and Deep Learning Models, Springer. (16) (2004) 1073â€“1091. https://doi.org/10.1007/s12559-024-10255-7 72 WeVerify, Deepfake Detector. https://weverify.eu/tools/deepfake-detector/, founded in 2020 (accessed 19 November 2024). 73 Expert Insights, The top 8 deepfake detection solutions. https://expertinsights.com/insights/the-top-deepfake-detection-solutions/, 2024 (accessed 19 November 2024). 74 Salvi, D., Liu, H., Mandelli, S., Bestagini, P., Zhou, W., Zhang, W., & Tubaro, S, A robust approach to multimodal deepfake detection, Journal of Imaging. 9(6) ( 2023) 1-18.https://doi.org/10.3390/jimaging9060122 75 Reality Defender. https://www.realitydefender.com/, 2024 (accessed 19 November 2024). 76 Tsigos, K., Apostolidis, E., Baxevanakis, S., Papadopoulos, S., & Mezaris, V., Towards quantitative evaluation of explainable AI methods for deepfake detection, Proceedings of the 3rd ACM International Workshop on Multimedia AI against Disinformation, arXiv:2404.18649. (2024) 1-9. https://doi.org/10.48550/arXiv.2404.18649 
Detector (which encrypts voice data) and DeepDetector, which provides detailed explanations of how they identify deepfakes in audio, images, and videos.77 Proactive detection aims to identify deepfakes before they spread,78 and Google's SynthID is a prime example. It embeds an invisible watermark directly into images and videos at creation. This allows Google to flag deepfakes, even after common image manipulations like cropping, resizing, and adding filters, before they are widely circulated.79 To counter 'adversarial perturbations' (subtle manipulations that evade detection), researchers also employ adversarial training.80 Adversarial Feature Similarity Learning is one example that optimises similarity between samples to distinguish real from fake, while also maximising similarity between perturbed and unperturbed examples and using regularisation to separate real and fake samples.81 The field of deepfake fraud detection is constantly evolving as new techniques for creating and detecting deepfakes emerge. Continued research and development are crucial to stay ahead of this rapidly advancing technology. These emerging trends highlight the dynamic nature of deepfake detection and the need for ongoing innovation.

## 3.2. C2Pa: Ensuring Content Authenticity 3.2.1. C2Pa'S Role In Deepfake Fraud Detection

The Coalition for Content Provenance and Authenticity (C2PA) is establishing a new standard for trust and transparency online. It allows verifiable information about content creation and subsequent actions to be recorded, providing valuable provenance data. C2PA focuses on verifying the authenticity and integrity of this information, not on making subjective judgments about its quality.82 This open standard allows creators to embed verifiable information about the origin and history of their content, creating a 'digital nutrition label' for media. This provenance data helps users understand how content was created and modified, including whether AI tools like deepfake generators were used. To illustrate this, imagine a photo with a 

Preprint 77 Expert Insights, The top 8 deepfake detection solutions. https://expertinsights.com/insights/the-top-deepfake-detection-solutions/, 2024 (accessed 19 November 2024). 78 Nadimpalli, A. V., & Rattani, A., Proactive deepfake detection using GAN-based visible watermarking, ACM Transactions on Multimedia Computing, Communications and Applications. 20(11) (2024) 1-27. https://doi.org/10.1145/3625547 79 Google DeepMind, SynthID identifying AI-generated content with SynthID. https://deepmind.google/technologies/synthid/, 2024 (accessed 19 November 2024). 80 P. Neekhara, B. Dolhansky, J. Bitton and C. C. Ferrer, Adversarial threats to deepfake detection: a practical perspective, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), Nashville, TN, USA. (2021) 923-932. https://ieeexplore.ieee.org/document/9522903 81 Khan, S., Chen, J.-C., Liao, W.-H., & Chen, C.-S., Adversarially robust deepfake detection via adversarial feature similarity learning, arXiv preprint arXiv:2403.08806. (2024) 1-15. https://doi.org/10.48550/arXiv.2403.08806 82 C2PA, Guiding Principles. https://c2pa.org/principles/, 2024 (accessed 19 November 2024).
C2PA 'Content Credentials' icon. Clicking it reveals details about the image's creation: the camera used, the date and time, any edits made, and even if AI was involved.83 This verifiable history makes it harder for deepfakes to spread undetected. By promoting accountability and empowering users with information, C2PA helps build a more trustworthy online environment.

C2PA is driven by a cross-industry community, including leading organisations and the Content Authenticity Initiative, who are developing open-source tools to implement this standard. The initiative prioritises privacy, ensuring that any personal information remains optional.84 The technology relies on robust cryptographic techniques to ensure the integrity of provenance data, making tampering easily detectable. This commitment to open standards, transparency, and accountability makes C2PA a crucial framework for combating misinformation and fostering trust in digital media.85

## 3.2.2. C2Pa: Principles For Trust And Transparency

C2PA aims to build trust in digital content by establishing a standardised method for certifying its source and history, benefiting creators, publishers, and consumers. The core principles guiding C2PA are as follows. Firstly, privacy is key, allowing for the removal of sensitive information and not requiring the identity of those making assertions about the content. Tools using C2PA must disclose what information they capture and obtain user informed consent. Secondly, accessibility is prioritised, with specifications designed for implementation across diverse devices, including those prevalent in less developed regions. Moreover, tools should cater to users with varying levels of digital literacy and disabilities.

Additionally, interoperability ensures that content encoded with one C2PA-compliant tool can be read by another, fostering a robust ecosystem for provenance information. C2PA is also designed for seamless integration into existing workflows, minimising disruption, and with performance optimised for various platforms and content types. Furthermore, simplicity and cost-effectiveness are emphasised to encourage widespread adoption.

Preprint Crucially, C2PA is built for the future. The framework can accommodate new technologies, content formats, and methods for verifying authenticity, ensuring its continued relevance. Importantly, C2PA specifications undergo rigorous security reviews to mitigate potential misuse, including preventing the spread of deepfake fraud, 

83 C2PA, Content Credentials. https://c2pa.org/post/contentcredentials/, 2024 (accessed 19 November 2024). 84 CR, Content Credentials. https://contentcredentials.org/, 2024 (accessed 19 November 2024). 85 Ibid.
protecting intellectual property, and reducing the risk of online abuse.86 By promoting transparency and accountability, C2PA can effectively combat deepfakes and restore confidence in digital media authenticity. However, it is arguable that its success will depend on widespread adoption across the digital ecosystem.

## 3.2.3. C2Pa: Towards A Transparent Online Future

The C2PA principles align with global efforts to combat AI-generated misinformation. They echo Resolution UN A/78/L.49, which advocates for 'effective', 'accessible', 'adaptable', and 'internationally interoperable' tools, like watermarking and labelling, to identify AI-generated content and empower individuals to understand its origin and authenticity.87 These principles are also consistent with legislation, such as the EU AI Act, which mandates that AI-generated content be clearly marked and detectable.88 This fosters transparency and empowers users to assess the authenticity of digital media. Additionally, C2PA principles respect legal precedents like the CJEU *SABAM* cases, ensuring a balance between intellectual property rights, freedom of expression, and data protection, promoting responsible AI innovation.89 In practice, industry leaders like Google are already taking significant steps to implement these principles. Google is leading the fight against deepfakes and harmful AI-generated content through a multi-pronged approach. Central to this is integrating the C2PA content provenance standard across its products. Users can access provenance information (Search's 'About this image'), see labels for C2PA-verified content (YouTube's 'captured with a camera'), and benefit from C2PA-informed policy enforcement (Ads).90 Google is also developing SynthID, which embeds invisible watermarks in AI-generated content, fostering transparency and easier identification of synthetic media.91 Additionally, Google utilises IPTC metadata to label images in its Photos app, marking those edited with generative AI and identifying composites created from multiple photos. These labels are displayed with file information. 92 Google empowers users to make informed decisions and promotes a safer online environment.

Preprint 86 C2PA, Guiding Principles. https://c2pa.org/principles/, 2024 (accessed 19 November 2024). 87 United Nations General Assembly. (2024). Seizing the opportunities of safe, secure and trustworthy artificial intelligence systems for sustainable development. A/RES/78/L.49. New York: United Nations. 88 See Article 50(1)-(7) and Recitals 133-137 of Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act), Official Journal of the European Union, L 187/1, 14.7.2024. 89 C-360/10 *Belgische Vereniging van Auteurs, Componisten en Uitgevers CVBA (SABAM) v Netlog NV* [2012] ECLI:EU:C:2012:85 [48]-[51]; C-70/10 *Scarlet Extended SA v SociÃ©tÃ©Â´ belge des auteurs, compositeurs et Ã©diteurs SCRL (SABAM)* [2012] ECLI:EU:C:2011:771[51]-[53]. 90 Google, How we're increasing transparency for gen AI content with the C2PA. https://blog.google/technology/ai/google-gen-aicontent-transparency-c2pa/, 2024 (accessed 19 November 2024). 91 Google DeepMind, SynthID identifying AI-generated content with SynthID. https://deepmind.google/technologies/synthid/, 2024 (accessed 19 November 2024). 92 Google, More transparency for AI edits in Google Photos. https://blog.google/products/photos/ai-editing-transparency/, 2024 (accessed 19 November 2024).
# 3.2.4. C2Pa: Watermarking'S Limitations Against Deepfakes

While C2PA holds great potential and is grounded in ethical principles, it faces challenges. One key problem is that watermarking, while valuable against deepfake fraud, has limitations. Attackers can manipulate or remove watermarks, even falsely labelling human-made content as AI-generated. This 'liar's dividend' tactic undermines trust in authenticity verification methods, further complicating the issue.93 Evolving deepfake technology necessitates continuous improvement in watermarking and detection tools to ensure accountability and robustness.

Deepfakes pose a growing challenge, and watermarking tools like Steg.AI leverage deep learning to combat them.94 However, Steg.AI's lack of transparency around its proprietary technology limits independent evaluation and improvement, hindering research and wider adoption. In contrast, SynthID Text, DeepMind's open-source tool, allows researchers to examine and improve its code to embed invisible watermarks in AI-generated text, fostering collaboration and accelerating development.95 This contrasts with Steg.AI's closed approach, which hinders research and highlights the crucial role of open research in watermarking detection to address this problem.

Another obstacle is the prevalence of company-specific classifiers, trained on limited datasets. These classifiers can only reliably detect watermarks generated by specific AI models, leading to false negatives, where watermarks from other AI models go undetected. Furthermore, rapid AI evolution necessitates constant updates to classifiers, which can be challenging.96 To effectively combat deepfakes, a multi-faceted approach is needed. This includes developing generalised watermarking detection methods, collaborative data sharing, and standardised evaluation frameworks. Open research and ongoing innovation in watermarking detection techniques are crucial to counter the escalating threat of deepfake fraud.

Preprint 

![15_image_0.png](15_image_0.png)

93 Brookings, Detecting AI fingerprints: a guide to watermarking and beyond. https://www.brookings.edu/articles/detecting-aifingerprints-a-guide-to-watermarking-and-beyond/, 2024 (accessed 19 November 2024). 94 Steg.AI, Forensic watermarking for digital media. https://steg.ai/, founded in 2019 (accessed 19 November 2024). 95 Google AI for developers, SynthID: tools for watermarking and detecting LLM-generated text. https://ai.google.dev/responsible/docs/safeguards/synthid, 2024 (accessed 19 November 2024). 96 Reuters Institute & University of Oxford, Spotting the deepfakes in this year of elections: how AI detection tools work and where they fail. https://reutersinstitute.politics.ox.ac.uk/news/spotting-deepfakes-year-elections-how-ai-detection-tools-work-and-wherethey-fail, 2024 (accessed 19 November 2024).

4. A Framework for Responsible Deepfake Fraud Detection: Grounding Research in the UN Resolution and EU Law 4.1 The UN Resolution: A Global Call for Responsible Deepfake Fraud Detection This section establishes a comprehensive framework for responsible AI governance in deepfake fraud detection, grounded in the UN Resolution on Safe, Secure, and Trustworthy AI (A/78/L.49) and aligned with key EU instruments - the AI Act, GDPR, and Digital Services Act. This framework aims to ensure the ethical and legally compliant development and deployment of deepfake detection technologies, recognising the potential of Article 114 TFEU to harmonize standards across the EU, mitigating fragmentation and fostering a cohesive approach that could serve as a global model.

Deepfakes, particularly those used for fraud, pose a significant threat by enabling deception and manipulation. UN Resolution A/78/L.49 acknowledges this risk and underscores the global commitment to ethical and trustworthy AI. While non-binding, it significantly influences international norms.97 To address this threat, the resolution advocates for developing robust, accessible, adaptable, and internationally interoperable tools, such as watermarking and labelling, empowering individuals and organizations to verify the authenticity of digital media. It also promotes internationally interoperable frameworks and standards for AI training and testing, ensuring fairness, accuracy, and the protection of intellectual property rights, crucial for developing innovative and reliable deepfake fraud detection tools.

Furthermore, the resolution prioritizes the protection of personal data and privacy, crucial for deepfake detection tools that often handle sensitive biometric data. This necessitates transparency and adherence to data usage laws. The resolution also calls for transparent, predictable, understandable, and reliable AI systems, fostering trust by ensuring individuals and organizations understand how these tools function. This necessitates clear explanations of AI-driven decisions and human oversight to ensure accountability and address potential biases. Finally, prioritizing safety, the resolution emphasizes adequate safeguards and risk assessments throughout the AI life cycle to protect human rights and mitigate potential harms, which is critical for deepfake detection tools with significant societal implications.98 Preprint The UN Resolution's principles strongly align with the EU's approach to AI governance, as reflected in the AI Act, GDPR, and DSA. These legal frameworks complement and 

97 UN, How decisions are made at the UN. https://www.un.org/en/model-united-nations/how-decisions-are-made-un, (accessed 19 November 2024). 98 United Nations General Assembly. (2024). Seizing the opportunities of safe, secure and trustworthy artificial intelligence systems for sustainable development. A/RES/78/L.49. New York: United Nations.
reinforce the ethical considerations of the resolution. The AI Act takes a risk-based approach to regulating AI systems, recognizing the potential impact of deepfake detection tools on financial security and privacy.99 Recital 136 emphasizes their crucial role in mitigating disinformation to effectively implement the DSA. Article 35 of the GDPR mandates a Data Protection Impact Assessment (DPIA) for deepfake detection tools that utilize biometric data, requiring a systematic analysis of potential risks to privacy and data protection.

## 4.2. Article 114 Tfeu: Harmonization And A Global Model For Responsible Ai

Article 114 TFEU provides a crucial legal basis for harmonizing standards for deepfake detection tools across the EU. While primarily aimed at preventing trade barriers, its scope, as articulated in Recital 3 of the AI Act, extends to technologies with the potential to disrupt the market, even without pre-existing national laws. 100 This recital highlights the risk of market fragmentation and the need for uniform rules to ensure consistent protection and prevent obstacles to the free circulation of AI systems. Recitals 132 and 133 further emphasise the specific risks that deepfakes pose to the EU market by manipulating consumer perception and enabling fraud, impersonation, and deception. Harmonizing standards under Article 114 ensures consistent data protection, accuracy, and transparency, fostering a cohesive approach to development and deployment while safeguarding individuals and businesses. This unified framework promotes innovation, accessibility, and a single market for deepfake detection technologies, contributing to responsible AI practices worldwide. This research leverages this framework to create robust, ethically sound, and legally compliant deepfake detection tools.

## 5. The Eu'S Multi-Layered Approach To Deepfake Fraud Detection Regulation 5.1. Deepfake Detection Tools: The Unseen Risk In The Eu Ai Act

The EU AI Act takes a risk-based approach to regulating AI systems, classifying them according to their potential for harm. This classification ranges from unacceptable risk (systems that are strictly prohibited, such as social scoring systems and manipulative AI)101 to minimal risk (systems that are currently unregulated, such as AI-enabled video 

Preprint 99 See Recitals 132 and 133 of Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act), Official Journal of the European Union, L 187/1, 14.7.2024. 100 Engel, A. Licence to regulate: Article 114 TFEU as choice of legal basis in the Digital Single Market in: A. Engel, X. Groussot, & G. T. Petursson (Eds.), New directions in digitalisation: European Union and its neighbours in a globalized world, Springer, Cham, 2025, pp. 13-28. 101 See Article 5 and Chapter II of Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act), Official Journal of the European Union, L 187/1, 14.7.2024.
games).102 High-risk systems, including those used for deepfake electoral misinformation, healthcare, or law enforcement, face strict requirements such as ongoing monitoring, conformity assessments, and registration in an EU database.103 Additionally, the Act mandates transparency obligations for limited-risk systems like deepfakes, ensuring users are aware they are interacting with AI.104 Article 50 of the AI Act acknowledges the unique threat of deepfakes, implementing tailored labelling obligations for providers of AI-generated content and disclosure obligations for deployers. This is reflected in Recitals 132 and 133, which specifically address the risks posed by deepfakes, such as fraud, impersonation, and deception. These Recitals emphasise the need for transparency in AI systems that interact with individuals or generate content, recognizing deepfakes as a distinct category of AI systems with 'specific' risks.

While the Act mandates transparency for deepfake creation, it unfortunately overlooks specific requirements for detection tools, risking their misuse for fraudulent activities (e.g., fabricating evidence) and underutilisation due to inaccuracy. This uncertainty could stifle innovation and hinder the fight against deepfake fraud.

To illustrate the urgent need for regulation, consider the alarming rise of deepfakes in fraudulent schemes. In a recent case, fraudsters used deepfakes to impersonate a multinational firm's CFO in a video call, tricking a finance worker into transferring $25.6 million.105 This highlights the growing sophistication of deepfakes, which can convincingly mimic individuals across various platforms, facilitating XPIA (Cross-
Platform Impersonation Attacks) and identity theft.106 Deepfake investment scams exploit public trust by creating deepfake endorsements from celebrities or experts, deceiving individuals into investing in fraudulent schemes.107 Beyond financial loss, deepfakes can inflict severe emotional distress, as seen in cases 

![18_image_0.png](18_image_0.png) of deepfake romance scams where victims are manipulated into believing they have a 

Preprint 102 European Commission, European Artificial Intelligence Act comes into force. https://ec.europa.eu/commission/presscorner/detail/en/ip_24_4123, 2024 (accessed 19 November 2024). 103 See Article 6(4), Article 49(2), and Article 71 of Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act), Official Journal of the European Union, L 187/1, 14.7.2024. 104 *Ibid* see Article 50(1)-(7) and Recitals 133-137. 105 CNN World Asia, Finance worker pays out $25 million after video call with deepfake 'chief financial officer'. https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html, 2024 (accessed 19 November 2024). 106 Microsoft, Microsoft Digital Defense Report 2024, the foundations and new frontiers of cybersecurity. https://cdn-dynmedia1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoftbrand/documents/Microsoft%20Digital%20Defense%20Report%202024%20%281%29.pdf, 2024 (accessed 19 November 2024). 107 PA, Deepfakes: a human challenge, how can tooling be used to assist humans in deepfake detection? https://www.weprotect.org/wp-content/uploads/Deepfakes_A-Human-Challenge_PA-Report_v3.pdf, 2024 (accessed 19 November 2024).
genuine connection with a fabricated persona.108 These examples demonstrate how deepfakes bypass critical faculties, leading to impaired decision-making and detrimental choices, causing harm that extends beyond financial loss to include emotional distress, reputational damage, and erosion of trust.

Looking ahead, Recital 29 of the AI Act recognises the disturbing potential of deepfakes being integrated into emerging technologies like virtual reality. Imagine being manipulated within a VR environment by a deepfake of a deceased parent, created with a tool like Deep Nostalgia AI109. This amplified potential for deception and emotional exploitation underscores the urgent need for proactive regulation to mitigate these evolving threats.

While Article 5(1)(a) of the Act prohibits AI systems designed to employ subliminal or manipulative techniques to materially distort a person's behaviour, impairing their informed decision-making and causing significant harm, its focus remains on the creation of deepfakes. It does not explicitly address the tools designed to detect and combat this manipulation.

Some may argue that regulating deepfake detection tools could stifle innovation, increase development costs,110 and be seen as infringing on free speech by restricting deepfake creation, as shown in *Kohls v Bonta*.

111 However, the responsible development and use of these tools are crucial to safeguarding everyone from the harms of deepfakes. While legitimate uses of deepfakes, such as satire and artistic expression, should be protected, as recognised in Recital 134 of the AI Act, their potential for malicious use necessitates a balanced approach.

5.2. How the EU AI Act Promotes Transparency and Responsible Use of Deepfake Detection Tools Transparency Obligations for Deepfake Detection Tools The EU AI Act promotes transparency around AI tools, including those used for deepfake detection. Article 50(1) requires that users be informed when interacting with an AI system, such as when uploading a video to a detection tool for deepfake analysis like Deepware Scanner.112 This empowers users to make 

Preprint 108 US Homeland Security, Increasing threats of deepfake identities. https://www.dhs.gov/sites/default/files/publications/increasing_threats_of_deepfake_identities_0.pdf, 2021 (accessed 19 November 2024). 109 Deep Nostalgia AI, Revive your memories with Deep Nostalgia AI. https://deep-nostalgia-ai.com/, 2021 (accessed 19 November 2024). 110 Feeney, M., Deepfake laws risk creating more problems than they solve, Regulatory Transparency Project of the Federal Society. (2021) 1-12. https://rtp.fedsoc.org/wp-content/uploads/Paper-Deepfake-Laws-Risk-Creating-More-Problems-Than-They- Solve.pdf 111 *Kohls v Bonta*., 2:24-cv-02527 (US District Court, E.D. Cal. 2024). 112 Expert Insights, The top 8 deepfake detection solutions. https://expertinsights.com/insights/the-top-deepfake-detection-solutions/ 2024 (accessed 19 November 2024).
informed decisions about the video they are uploading and understand how the data is being processed.

![20_image_0.png](20_image_0.png)

Furthermore, under Article 50(3) of the AI Act, deployers using emotion recognition or biometric systems within their deepfake detection tools must be upfront with users about how these systems work. They must also clearly explain what data they collect and ensure compliance with data protection laws like the GDPR and the Law Enforcement Directive. This covers deepfake detection tools that analyse facial expressions or voices. For example, if a bank uses Sentinel's deepfake detection tool113 to assess loan applications, Sentinel must ensure the bank understands how the system works (Article 16). The bank then needs to inform customers about the use of this technology, potentially through a clear notice on their website or during the application process. This transparency is crucial to protect individual rights and ensure individuals understand how AI is being used to make decisions that affect them.

## Labelling Requirements For Ai-Generated Content

Article 50(2) focuses on AI systems that generate synthetic media, which can be relevant to certain deepfake detection tools. This provision requires these systems to clearly label their outputs as artificially generated or manipulated. For example, this applies to companies like Synthesia,114 which provides an AI platform for generating synthetic videos including avatars. Any video created using Synthesia's platform should be clearly identifiable as artificially generated. However, this obligation does not apply if the AI system is used for standard video editing that does not significantly alter the input data, or for law enforcement purposes. This nuanced approach strikes a balance between promoting transparency and enabling the development of valuable AI tools for a wide range of purposes, from enhancing existing content to creating entirely new forms of media.

Preprint While deepfake detection tools primarily analyse existing content, the marking obligation under Article 50(2) could also apply if the tool generates synthetic content during its analysis. For instance, Reality Defender's deepfake detection system has a tool that can 'reconstruct' faces in a video, generating a synthetic version.115 In such cases, the tool would need to clearly label any synthetically generated content.

113 Ibid.

![20_image_1.png](20_image_1.png)

explainability, 2024 (accessed 19 November 2024).

114 Synthesia, Turn text to video, in minutes. https://www.synthesia.io/, founded in 2017 (accessed 19 November 2024). 115 Reality Defender, Visual deepfake detection explainability. https://www.realitydefender.com/blog/visual-deepfake-detection-

## Disclosure Requirements For Deepfake Deployers

Article 50(4) adds another layer of transparency by requiring that deployers of AI systems that generate deepfakes disclose that the content has been artificially generated or manipulated. This applies to any deepfake, whether it is an image, audio, or video. This disclosure helps prevent the misuse of deepfakes for malicious purposes like fraudulent activities. While this provision primarily targets deepfake creators, requiring them to disclose manipulated content, the emphasis on transparency could indirectly encourage the use of detection tools for content verification, similar to how C2PA promotes provenance and authentication.

## Exceptions And Contextual Considerations

The AI Act provides exceptions for law enforcement using these tools for criminal investigations, provided it does not infringe on third-party rights (Article 50(1)-(4)). These exceptions, along with the nuanced approach to labelling synthetic content, demonstrate the Act's commitment to considering the context and purpose of AI systems.

In sum, while the EU AI Act does not explicitly address deepfake detection tools in detail, its emphasis on transparency lays the groundwork for their responsible development and use. By encouraging disclosure and facilitating the creation of codes of practice under Article 50(7), the EU Act fosters a climate where detection tools can play a crucial role in combating deepfake fraud and protecting individuals and society from its harms. However, ongoing dialogue and multistakeholder collaboration are essential to ensure these tools strike a fair balance between innovation, ethical considerations, and safeguarding fundamental rights.

## Limitations Of The Ai Act And Interplay With Other Eu Legislation

Preprint While the AI Act represents a significant step towards regulating AI, it is crucial to acknowledge its limitations. The Act may not be a complete solution to the challenges posed by deepfakes, and ongoing monitoring and adaptation will be necessary as technology evolves. It is also important to consider the AI Act in the context of other relevant EU legislation, such as the GDPR, which governs data protection, and the DSA, which addresses online content moderation. This holistic perspective will help ensure a comprehensive and effective regulatory framework for deepfake detection tools.

![21_image_0.png](21_image_0.png)

# 5.2. Deepfake Detection Under The General Data Protection Regulation

## 5.2.1 Biometric Data And Deepfake Fraud Detection

![22_image_0.png](22_image_0.png)

Under Article 14(4) of the GDPR, biometric data is defined as unique physical, physiological, or behavioural characteristics that are processed to identify an individual, such as facial images and fingerprints. This type of data must fulfil three conditions: firstly, it must relate to physical, physiological, or behavioural traits, such as facial features, voice patterns, or even typing styles. Secondly, it must undergo processing using specific technologies, like analysing an audio recording to detect unique vocal qualities. Lastly, it must be capable of uniquely identifying an individual, effectively distinguishing one person from another. This definition encompasses both biometric identification (e.g., using facial recognition to identify an unknown person) and biometric verification (e.g., using voice authentication to confirm that a person is who they claim to be).116 Consequently, deepfakes are subject to data protection laws because they involve manipulating and generating biometric data.

Furthermore, many advanced deepfake detection techniques themselves rely on analysing biometric data.117 For instance, artefact-based methods detect subtle inconsistencies in facial expressions or voice patterns, while deep learning trains AI to recognize patterns of manipulation in biometric data.118 Similarly, multimodal analysis combines information from multiple biometric sources, such as face and voice, to identify discrepancies.119 Adversarial training, on the other hand, improves detection by pitting deepfake generation AI against detection AI.120 Finally, Explainable AI is sometimes employed to highlight inconsistencies in biometric data that indicate a deepfake. These techniques often establish a baseline of 'normal' facial and vocal behaviour. This baseline helps identify the subtle anomalies that often reveal a deepfake.121

Preprint 116 ICO, Biometric data guidance: biometric recognition. https://ico.org.uk/media/for-organisations/uk-gdpr-guidance-andresources/lawful-basis/biometric-data-guidance-biometric-recognition-1-0.pdf, 2024 (accessed November 19, 2024). 117 For example, Sentinel's AI-powered deepfake detection analyses visual, audio, and biometric cues to identify forgeries. Deep learning models examine facial expressions, blinking patterns, and audio manipulations, while also leveraging biological signals and facial recognition data for precise detection of manipulated media. See Expert Insights, The top 8 deepfake detection solutions. https://expertinsights.com/insights/the-top-deepfake-detection-solutions/, 2024 (accessed November 19, 2024). Similarly, Oz Liveness and HyperVerge appear to rely most directly on biometric data analysis for deepfake detection, although the specific data and methods used may vary. See Vlink, 10 top deepfake detectors tools for 2024 & beyond. https://www.vlinkinfo.com/blog/top-aideepfake-detector-tools/, 2024 (accessed November 19, 2024). 118 Tolosana, R., Vera-Rodriguez, R., Fierrez, J., Morales, A., & Ortega-Garcia, J., Deepfakes and beyond: a survey of face manipulation and fake detection, Information Fusion. 64 (2020) 131â€“148. https://www.sciencedirect.com/science/article/pii/S1566253520303110?via%3Dihub 119 Agarwal, S., Farid, H., Gu, Y., He, M., Nagano, K., & Li, H., Protecting world leaders against deep fakes, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. (2019) 38-45. https://farid.berkeley.edu/downloads/publications/cvpr19/cvpr19a.pdf 120 Yang, C., Ding, L., Chen, Y., & Li, H., Defending against GAN-based deepfake attacks via transformation-aware adversarial faces, International Joint Conference on Neural Networks (IJCNN). (2020) 1â€“14 https://arxiv.org/abs/2006.07421 121 Silva, S. H., Bethany, M., Votto, A. M., Scarff, I. H., Beebe, N., & Najafirad, P., Deepfake forensics analysis: an explainable hierarchical ensemble of weakly supervised models, Forensic Science International: Synergy 4 (2022) 100217 1-14. https://www.sciencedirect.com/science/article/pii/S2589871X2200002X?via%3Dihub

## 5.2.3. Lawful Processing Of Biometric Data In Deepfake Detection

![23_image_0.png](23_image_0.png)

To comply with the GDPR (specifically Articles 6 and 9), providers of deepfake detection tools using biometric data must establish lawful reasons for processing. Explicit consent is generally the most appropriate lawful basis, particularly when dealing with sensitive personal data.122 This requires obtaining informed, freely given, specific, and unambiguous consent from individuals whose likeness is used in the content. However, providers may also rely on legitimate interest (Article 6(1)(f)) or substantial public interest (Article 9), provided they can demonstrate that processing is necessary and proportionate, and that obtaining explicit consent would be unduly difficult or impossible. This requires satisfying the 'strict necessity' requirement, balancing interests, and considering less intrusive means, as established in CJEU case law.123

## 5.2.4. Data Protection By Design And Default

Given the potential for deepfakes to infringe on privacy by manipulating identities, creating fake evidence, and deceiving individuals for financial gain, data protection must be at the core of deepfake fraud detection systems, embedding privacy by design and default and adhering to the principles of purpose limitation, data minimization, and storage limitation.124 Before employing biometric data, it is crucial to demonstrate its necessity and proportionality. A Data Protection Impact Assessment (DPIA) is essential to identify and mitigate risks to rights and freedoms, especially when using sensitive biometric data. This involves analysing the nature, scope, context, and purpose of the processing, and evaluating the likelihood and severity of potential harms. Article 35 GDPR requires controllers of deepfake detection tools to document their choice and the rationale for their tools in a DPIA, as the use of face and voice biometric data is 'likely to result in high risk'.125

## Strict Necessity Of Deepfake Detection Tools Under The Gdpr

The purpose of using deepfake fraud detection tools that process biometric data is to enhance security and prevent fraud by accurately identifying and flagging potentially 

Preprint 122 For further guidance on obtaining informed, freely given, specific, and unambiguous consent, see CJEU case law, such as C- 673/17 Bundesverband der Verbraucherzentralen und VerbraucherverbÃ¤nde - Verbraucherzentrale Bundesverband e.V. v Planet49 GmbH [2019] EU:C:2019:246 [72]; C-61/19 Orange Romania SA v Autoritatea NaÅ£ionalÄƒ de Supraveghere a PrelucrÄƒrii Datelor cu Caracter Personal (ANSPDCP) [2020] ECLI:EU:C:2020:901 [36].

123 See Joined Cases Câ€‘17/22 and C-18/22 HTB Neunte Immobilien Portfolio geschlossene Investment UG & Co. KG and Ã–korenta Neue Energien Ã–kostabil IV geschlossene Investment GmbH & Co. KG v MÃ¼ller Rechtsanwaltsgesellschaft mbH and Others [2024] EU:C:2024:738 [51], [53], [59], [62], [63], [73], [74], [76], [78]; Câ€‘621/22 *Koninklijke Nederlandse Lawn Tennisbond v Autoriteit* Persoonsgegevens [2024] ECLI:EU:C:2024:857 [42], [44], [51], [54], [55], [57], [58]. 124 ICO, Biometric data guidance: biometric recognition. https://ico.org.uk/media/for-organisations/uk-gdpr-guidance-andresources/lawful-basis/biometric-data-guidance-biometric-recognition-1-0.pdf, 2024 (accessed November 19, 2024). 125 Ibid.
manipulated or synthetic media, particularly those involving human faces and voices. It is crucial to demonstrate that the processing carried out by the tool is directly necessary to achieve this purpose.

To prove the necessity of deepfake detection tools, consider their wide-ranging benefits. For example, these tools combat misinformation and protect against online scams, such as romance scams or investment scams that use deepfakes to impersonate trusted figures. In the legal field, they can verify the authenticity of evidence. Deepfake detection is critical in the financial sector, where these tools help prevent identity theft, unauthorised access, and ensure decisions are based on reliable information by verifying the legitimacy of documents and media.126 To ensure deepfake detection tools respect individual privacy, the CJEU in *KNLTB*127 and *HTB Neunte Immobilien Portfolio*128 emphasises 'strict necessity,' prioritising the least restrictive data processing approach. This principle, outlined in Article 6(1)(f) 
GDPR, aims to protect user rights while achieving legitimate commercial interests,129 such as preventing fraud. This is crucial for deepfake detection tools, as they often handle sensitive biometric data.

Privacy-enhancing technologies (PETs) offer solutions to the challenge of protecting user data while still enabling effective deepfake detection.130 For example, federated learning trains models on decentralised data, avoiding the need to centralise sensitive information.131 Similarly, differential privacy adds noise, protecting identities while allowing accurate deepfake detection.132 Furthermore, homomorphic encryption allows computation on encrypted data133 like voice data, as used in DuckDuckGoose's AI Voice Detector.134 Other PETs, like secure multi-party computation, enable collaborative detection without data sharing.135 One such model, SecDFDNet, uses secure protocols 

126 EDPS, Deepfake detection. https://www.edps.europa.eu/data-protection/technology-monitoring/techsonar/deepfakedetection_en, 2024 (accessed November 19, 2024).

127 Câ€‘621/22 *Koninklijke Nederlandse Lawn Tennisbond v Autoriteit Persoonsgegevens* [2024] ECLI:EU:C:2024:857 [42], [51], [57], 
[58]. 128 Joined Cases Câ€‘17/22 and C-18/22 HTB Neunte Immobilien Portfolio geschlossene Investment UG & Co. KG and Ã–korenta Neue Energien Ã–kostabil IV geschlossene Investment GmbH & Co. KG v MÃ¼ller Rechtsanwaltsgesellschaft mbH and Others [2024] EU:C:2024:738 [51], [59], [73], [74], [76], [78]. 129 C-621/22 *Koninklijke Nederlandse Lawn Tennisbond v Autoriteit Persoonsgegevens* [2024] ECLI:EU:C:2024:857 [36], [39], [40], [41], [47]-[50], [57], [58]; Joined Cases Câ€‘17/22 and C-18/22 HTB Neunte Immobilien Portfolio geschlossene Investment UG & Co. KG and Ã–korenta Neue Energien Ã–kostabil IV geschlossene Investment GmbH & Co. KG v MÃ¼ller Rechtsanwaltsgesellschaft mbH and Others [2024] EU:C:2024:738 [48], [56], [57], [58], [65], [76], [78]. 130 ICO, Biometric data guidance: biometric recognition. https://ico.org.uk/media/for-organisations/uk-gdpr-guidance-andresources/lawful-basis/biometric-data-guidance-biometric-recognition-1-0.pdf, 2024 (accessed November 19, 2024). 131 Kairouz, P., McMahan, H. B., et al., Advances and open problems in federated learning, Foundations and Trends in Machine Learning. 4(1) (2021) 1-121. https://arxiv.org/abs/1912.04977 132 Dwork, C., Roth, A., The algorithmic foundations of differential privacy, Foundations and Trends in Theoretical Computer Science. 9 (3-4) (2014) 211-407. http://dx.doi.org/10.1561/0400000042 133 Acar, A., Aksu, H., Uluagac, A. S., & Conti, M., A survey on homomorphic encryption schemes: Theory and implementation, ACM Computing Surveys (CSUR). 51(4) (2018) 1-35. https://dl.acm.org/doi/10.1145/3214303 134 Expert Insights, The top 8 deepfake detection solutions. https://expertinsights.com/insights/the-top-deepfake-detectionsolutions/, 2024 (accessed November 19, 2024). 135 Lindell, Y., & Pinkas, B., Secure multiparty computation for privacy-preserving data mining, Journal of Privacy and Confidentiality. 1(1) (2009) 59-98. https://journalprivacyconfidentiality.org/index.php/jpc/article/view/566 Preprint 
and secret sharing to detect deepfakes without directly accessing or reconstructing raw facial data. This design aligns with GDPR principles by limiting data collection and exposure, and avoiding centralised data storage, thereby reducing risks.136

## Data Security In Deepfake Fraud Detection

![25_image_0.png](25_image_0.png)

In addition to these privacy-enhancing technologies, deepfake fraud detection providers must prioritize data security. This includes encrypting biometric data during transmission and storage to prevent unauthorized access, pursuant to Article 32 of the GDPR.137 Strict access controls should be implemented to limit who can access and process this sensitive data, in line with the CJEU case *SCHUFA*138 and Article 32's requirement for 'appropriate technical and organizational measures' to ensure a level of security appropriate to the risk. Additionally, providers need a comprehensive data breach response plan ready to mitigate the impact of any potential security incidents, fulfilling the GDPR's Article 33 breach notification requirements.139 Regular security audits and updates are also crucial to identify and address system vulnerabilities, reflecting the GDPR's emphasis on ongoing risk assessment and security improvement. 140

## Proportionality Of Deepfake Detection Tools Under The Gdpr

Ensuring proportionality in the use of deepfake detection tools is crucial, particularly when deployed for fraud detection. Providers must carefully consider the potential harms their technology could inflict, balancing these against the intended benefits. This includes acknowledging the potential for the tool to produce biassed results, lack transparency, or generate inaccurate outcomes. They must then evaluate the impact these risks could have on individuals, such as identity theft, investment and romance fraud. Finally, providers need to carefully weigh these potential harms against the intended benefits of the deepfake detection tool.141

Preprint 136 Chen, B., Liu, X., Xia, Z., & Zhao, G., Privacy-preserving deepfake face image detection, Digital Signal Processing. 143 (2023) 110. https://www.sciencedirect.com/science/article/pii/S1051200423003287?via%3Dihub. The necessity of using privacy-preserving biometric systems is underscored in cases like *Glukhin v Russia* App no 11519/20 (ECtHR, 4 July 2023), which stressed the need for safeguards when using facial recognition technology, even for legitimate security purposes, and Court of Justice of the European Union C-118/22 *NG v Direktor na Glavna direktsia 'Natsionalna politsia' pri MVR - Sofia* [2024] ECLI:EU:C:2024:97, which addressed the balance between data protection and law enforcement access to biometric data. Ultimately, a privacy-preserving approach to deepfake fraud detection is essential to ensure both security and fundamental rights protection. 137 Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation), Official Journal of the European Union, L119, 1â€“88. 138 C-634/21 *SCHUFA Holding and Others (Scoring)* [2023] ECLI:EU:C:2023:957 [59], [66]. 139 EDPB, Guidelines 9/2022 on personal data breach notification under GDPR. https://www.edpb.europa.eu/our-work-tools/our-documents/guidelines/guidelines-92022-personal-data-breach-notification-under_en, 2022 (accessed November 19, 2024). 140 ICO, A guide to data security. https://ico.org.uk/media/for-organisations/uk-gdpr-guidance-and-resources/security/a-guide-todata-security-0-0.pdf, 2023 (accessed November 19, 2024). 141 ICO, Biometric data guidance: biometric recognition. https://ico.org.uk/media/for-organisations/uk-gdpr-guidance-andresources/lawful-basis/biometric-data-guidance-biometric-recognition-1-0.pdf, 2024 (accessed November 19, 2024).
# Challenges In Deepfake Detection: Bias, Transparency, And Accuracy

![26_image_0.png](26_image_0.png)

One major concern regarding deepfake detection tools is bias, potentially leading to discriminatory outcomes. This can disproportionately flag individuals with certain features as potential fraudsters, indicating racial and gender bias, and further discrimination.142 To protect users from discriminatory AI decisions, controllers must follow CJEU *SCHUFA* and GDPR provisions (Article 22 and Recital 71) by implementing appropriate safeguards including using suitable procedures, minimising errors, correcting inaccuracies, securing personal data, and allowing users to obtain human intervention, express their views, and contest decisions.143 Furthermore, the opacity of many deepfake detection tools raises alarm about trust and fairness. Clear explanations of detection results are needed to ensure trust before taking action.144 However, achieving this transparency is challenging because many providers restrict access to their technology, making it difficult to independently verify the accuracy of their detection systems or assess for potential biases. Deepfake detection providers should be obligated to provide detailed explanations of the algorithms used, the factors influencing detection scores, and the steps taken to mitigate bias in line with the CJEU AG in *SCHUFA*.

145 Additionally, deepfake detection is often limited to binary classifying content as simply 'real' or 'fake,' overlooking the nuances of social media alterations, obscuring signs of manipulation, and potentially leading to undetected deepfake fraud. This, coupled with modifications like compression and resizing, further exacerbates the issue.146 In the context of fraud detection, this could mean that sophisticated deepfakes used for malicious purposes go undetected, conflicting with the CJEU AG *Poland v Council and* Parliament, 147 allowing criminals to exploit this technology for financial gain.

To ensure fairness, accountability, and trust in deepfake detection technology, stricter regulations and greater transparency are urgently needed. This necessitates incorporating Explainable AI (XAI) methods, like Local Interpretable Model-Agnostic 

Preprint 142 EDPS, Deepfake detection. https://www.edps.europa.eu/data-protection/technology-monitoring/techsonar/deepfake-

![26_image_1.png](26_image_1.png) detection_en, 2024 (accessed November 19, 2024). 143 C-634/21 *SCHUFA Holding and Others (Scoring)* [2023] ECLI:EU:C:2023:957 [66]. 144 EDPS, Deepfake detection. https://www.edps.europa.eu/data-protection/technology-monitoring/techsonar/deepfakedetection_en, 2024 (accessed November 19, 2024). 145 AG opinion in C-634/21 *SCHUFA Holding and Others (Scoring)* [2023] ECLI:EU:C:2023:220 [58]. 146 EDPS, Deepfake detection. https://www.edps.europa.eu/data-protection/technology-monitoring/techsonar/deepfakedetection_en, 2024 (accessed November 19, 2024). 147 AG opinion in C-401/19 *Poland v Parliament and Council* [2021] ECLI:EU:C:2021:613 [214].
Explanations (LIME)148 and SHapley Additive exPlanations (SHAP), which provide insights into the AI's decision-making process.149 By making these processes more understandable, XAI enhances transparency and builds trust in deepfake detection tools, such as those developed by DuckDuckGoose,150 and Reality Defender.151

## 5.2.5 The Right To Erasure And Rectification In The Context Of Deepfakes

While the GDPR aims to protect personal data, enforcing data subject rights like the right to be forgotten (Article 17) or the right to rectification (Article 16) presents unique challenges in the context of deepfakes used for fraud.

Deepfakes mix real personal details, like someone's face and voice, into fake content, making it tough to separate and remove that information.152 This is further complicated by their viral spread and replication across platforms and jurisdictions, which makes removal efforts challenging.153 Addressing these challenges requires a multi-faceted approach involving technical solutions, proactive measures, and legal frameworks, which are explored in further detail in the following sections.

## 5.2.6. Role Of The Data Protection Officer (Dpo)

Under Article 37 of the GDPR, organizations developing or deploying deepfake detection tools should strongly consider designating a DPO. This is particularly important due to the sensitive nature of biometric data often processed by these tools. The DPO plays a crucial role in advising on data protection obligations, monitoring compliance, and acting as a point of contact for data subjects and supervisory authorities.154

## 5.2.7. Impact Of The Ai Act And Dsa

While the GDPR provides a strong foundation for data protection in deepfake detection, the EU AI Act adopts a risk-based approach to regulating AI systems, including those used for this purpose. The DSA, which addresses online content moderation, may also 

Preprint 148 Ribeiro, M. T., Singh, S., & Guestrin, C., 'Why should I trust you?': explaining the predictions of any classifier, in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. (2016) 1-10. https://dl.acm.org/doi/10.1145/2939672.2939778 149 Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., & Pedreschi, D., A survey of methods for explaining black box models, ACM computing surveys (CSUR). (2018) 1-45. https://dl.acm.org/doi/10.1145/3236009 150 DuckDuckGoose AI, https://www.duckduckgoose.ai/, founded in 2020 (accessed 19 November 2024). 151 Reality Defender, Visual deepfake detection explainability. https://www.realitydefender.com/blog/visual-deepfake-detectionexplainability, 2024 (accessed 19 November 2024). 152 Novelli, C., Casolari, F., Hacker, P., Spedicato, G., Floridi, L., Generative AI in EU law: liability, privacy, intellectual property, and cybersecurity, Computer Law & Security Review. 52 (2024) 1-9. https://doi.org/10.1016/j.clsr.2024.106066 153 Brown, H., Lee, K., Mireshghallah, F., Shokri, R., TramÃ¨r, F., What does it mean for a language model to preserve privacy? FAccT 22: Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. (2022) 1-21. https://doi.org/10.1145/3531146.3534642 154 See Article 39 of Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation). Official Journal of the European Union, L119, 1â€“88.
introduce additional requirements for developers, potentially impacting data processing activities. Careful consideration of the interplay between the GDPR, the AI Act, and the DSA is crucial for comprehensive data protection.

In conclusion, deepfake detection tools must be developed and deployed responsibly, with careful consideration of data protection principles and the challenges specific to this technology. Striking a balance between technological innovation and safeguarding individual rights is crucial in the fight against deepfake fraud.

## 5.3. The Eu Digital Services Act And Deepfake Fraud Detection Tools

The EU Digital Services Act (DSA) does not explicitly mention deepfakes. However, its framework for combating illegal and harmful content online provides tools to address the growing threat of deepfake fraud.

## 5.3.1. Mechanisms For Reporting Deepfakes And Other Illegal Content

Article 6 of the DSA mandates that platforms create accessible mechanisms for users to report illegal content, which could include deepfakes used for malicious purposes like fraud. Article 6(6) obligates platforms to act 'diligently' and 'objectively' on such reports, highlighting the need for robust content moderation systems capable of evaluating and responding to sophisticated deepfake fraud attempts. By requiring transparency about decision-making processes, including the use of automated tools, the DSA promotes accountability and builds trust in addressing the evolving challenges of deepfakes and online fraud.

## 5.3.2. Transparency And Accountability For Deepfake Mitigation

The DSA goes beyond reactive measures like reporting mechanisms. It requires platforms to proactively assess and mitigate the risks posed by deepfakes, including their potential use for fraud. Article 24 of the DSA requires platforms to be transparent about their content moderation efforts, including how they handle deepfakes. This transparency allows for scrutiny of deepfake detection and removal processes, holding platforms accountable for their actions.

Preprint The DSA also mandates comprehensive risk assessments (Article 34) that consider the potential harms of deepfakes, such as their negative effects on fundamental rights, and the risks associated with algorithmic systems and user manipulation. Furthermore, Article 35 empowers platforms to implement mitigation measures against deepfakes, including prominent markings to clearly identify them as synthetic media and tools for users to easily report suspected deepfakes.

## 5.3.3. Addressing The Risks Of Manipulative Ai-Generated Content

Recital 84 of the DSA broadly addresses the risks of manipulative content generated through AI misuse, such as employing bots and deceptive tactics to spread deepfakes for disinformation campaigns. This includes the potential for deepfakes to be used to manipulate individuals for financial gain.

![29_image_0.png](29_image_0.png)

Similarly, to prevent discriminatory advertising practices, Recital 69 highlights the potential for targeted advertising to be misused for manipulative purposes with deepfakes, particularly when exploiting users' vulnerabilities. This could involve using deepfakes to create discriminatory ads or to target vulnerable individuals with manipulative content. This underscores the need for platforms to be vigilant against the manipulative use of targeted advertising and ensure that deepfake detection technologies are developed and used responsibly to avoid bias and discrimination. Therefore, platforms must prioritise ethical considerations and user protection when developing and deploying AI-powered tools for deepfake detection.

## 5.3.4. Balancing User Safety And Privacy: Challenges For Platforms

Meta faces the ongoing challenge of balancing user safety with privacy considerations. They achieve this through automated ad review using machine learning to detect policy violations. Facial recognition technology helps detect celeb-bait scams by comparing faces in suspect ads with verified profiles on Facebook and Instagram, with the facial data being immediately deleted after this comparison and not used for any other purpose. For account security, Meta is also testing encrypted video selfies which utilise facial recognition. These videos are never shared and are deleted immediately after comparison, offering a faster and more secure identity verification method.155 However, recent CJEU rulings reveal conflicts between Meta's practices and GDPR requirements. Specifically, the CJEU clarified in *Schrems v Meta* that the GDPR's 'data minimization' principle (Article 5(1)(c)) clashes with Meta's broad data collection practices, including the collection of sensitive data for targeted ads.156 The court also found that public statements do not automatically justify using related data for targeted advertising.157 This means that, without further justification and in line with Article 9(2)(e) 
GDPR, Meta cannot use related data, potentially gathered from third-party apps and websites, to aggregate and analyse sensitive information (like sexual orientation or racial and ethnic information) for targeted advertising.158 This compels Meta to re-
Preprint 

155 Meta, Testing new ways to combat scams and help restore access to compromised accounts. https://about.fb.com/news/2024/10/testing-combat-scams-restore-compromised-accounts/, 2024 (accessed 19 November 2024). 156 C-446/21 *Maximilian Schrems v Meta Platforms Ireland Limited, anciennement Facebook Ireland Limited* [2024] ECLI:EU:C:2024:834 [49], [52], [59], [65], [84]. 157 *Ibid* [83]. 158 *Ibid* [84].
evaluate its data collection practices, particularly those related to deepfake fraud detection, and underscores the need for careful consideration when designing deepfake detection systems. Relying on sensitive data, even indirectly, may require robust data protection safeguards.

Additionally, the *Meta v Bundeskartellamt* ruling emphasises that even seemingly harmless data used in deepfakes can fall under stricter GDPR protections, requiring explicit user consent under Article 9(2)(a).159 This is because the CJEU recognizes that such data, when aggregated and analysed, can reveal sensitive information indirectly, even if it does not explicitly contain details like political opinions or religious beliefs.160 This ruling could potentially restrict Meta's use of deepfake detection tools that rely on analysing facial features or other personal characteristics, which might indirectly reveal sensitive information.

To comply with these rulings, Meta must revise its policies and potentially limit the use of such tools, ensuring they adhere to stricter data usage guidelines and only operate with explicit consent. This may involve developing new techniques that rely on anonymized or aggregated data, or that focus on detecting the technical artefacts of deepfakes rather than analysing personal characteristics.

In sum, the EU AI Act, GDPR, and DSA collectively form a robust legal framework to address the challenges of deepfakes. The AI Act governs deepfake detection tool development, the GDPR safeguards sensitive data, and the DSA ensures platform accountability. Together, they mitigate fraud and promote a safer online environment.

Building on this, the next section leverages the UN Resolution on Safe, Secure, and Trustworthy AI (A/78/L.49) and EU legislation to establish a comprehensive framework for developing ethically sound, legally compliant deepfake fraud detection tools.

7. Deepfake Fraud and the UN's Call for Safe AI: Detection Strategies and Safeguards 

## 7.1. Towards A Global Framework For Identifying Ai-Generated Content

Preprint The United Nations, through Resolution A/78/L.49, advocates for robust, accessible, adaptable, and internationally interoperable tools to identify such content, empowering individuals to understand its origin. This includes tools like watermarking and labelling.161 As mandated by the EU AI Act under Recital 133, these tools and other techniques such as metadata identification and cryptographic methods require 

159 C-252/21 *Meta Platforms Inc and Others v Bundeskartellamt* [2023] ECLI:EU:C:2023:537 [77], [78], [83], [141], [154], [155]. 160 *Ibid* [66], [72], [73]. 161 United Nations General Assembly. (2024). Seizing the opportunities of safe, secure and trustworthy artificial intelligence systems for sustainable development. A/RES/78/L.49. New York: United Nations.
continuous refinement to be sufficiently reliable, interoperable, and effective in combating misinformation and fraud.

The rapid proliferation of AI-generated content, including deceptive deepfakes and manipulative synthetic media, necessitates a coordinated global response. The Coalition for Content Provenance and Authenticity (C2PA) offers a valuable framework for embedding provenance information within media. It utilises metadata, fingerprinting, and watermarking to verify the authenticity of images and videos, crucial for combating deepfakes.162 However, even C2PA metadata can be altered,163 highlighting the need for continuous improvement and diligent monitoring of deepfake detection tools. Moreover, the malicious use of AI-generated content poses broader societal threats, including misinformation, political manipulation, and erosion of trust Case law, such as the CJEU in *UPC Telekabel*, underscores the importance of adaptable and proportionate measures in online content authentication.164 Robust detection requires multiple layers of protection, including cryptographic techniques, tamper detection, and blockchain-based provenance tracking.

Combating the fraudulent use of deepfakes requires clear and accessible information about the origin and authenticity of digital content. This empowers users to verify the legitimacy of images and videos, a crucial step in detecting deepfakes. The C2PA champions clear communication and adherence to Web Content Accessibility Guidelines, ensuring that information is readily understandable for all users, including those with disabilities or limited internet access.165 Adaptability is crucial in combating the evolving threat of AI-generated misuse, given the rapid advancement of AI technology. The use of AI by criminals to generate synthetic identities, including fake selfies and documents, is rendering traditional fraud detection methods obsolete.166 Consequently, a dynamic ecosystem is needed, encompassing decentralised platforms for information sharing, open-source intelligence, and AI-driven detection models that learn and adapt continuously.167 Supporting ongoing research 

Preprint 162 Collomosse, J., & Parsons, A., To authenticity, and beyond! Building safe and fair generative AI upon the three pillars of provenance, IEEE Computer Graphics and Applications. 44(3) (2024) 82â€“90. https://dl.acm.org/doi/10.1109/MCG.2024.3380168 163 Hackerfactor, C2PA from the attacker's perspective. https://www.hackerfactor.com/blog/index.php?/archives/1031-C2PA-fromthe-Attackers-Perspective.html, 2024 (accessed 19 November 2024). 164 C-314/12 *UPC Telekabel Wien GmbH v Constantin FilmVerleih GmbH and Wega Filmproduktionsgesellschaft GmbH* [2013] EU:C:2014:192 [62]-[64]. 165 C2PA, Guiding principles. https://c2pa.org/principles/, 2024 (accessed 19 November 2024). 166 Biometric Update, Deepfake raises concerns head of 2024 US elections https://www.biometricupdate.com/202411/deepfakeraises-concerns-ahead-of-2024-us-elections, 2024 (accessed 19 November 2024). 167 Sharma, K., Qian, F., Jiang, H., Ruchansky, N., Zhang, M., & Liu, Y., Combating fake news: a survey on identification and mitigation techniques, ACM Transactions on Intelligent Systems and Technology (TIST). 10(3) (2019) 1-42. https://doi.org/10.1145/3305260; Zannettou, S., Sirivianos, M., Blackburn, J., & Kourtellis, N., The web of false information: rumours, fake news, hoaxes, clickbait, and various other shenanigans, Journal of Data and Information Quality (JDIQ). 11(3) (2019) 1-37. https://doi.org/10.1145/3309699; Tolosana, R., Vera-Rodriguez, R., Fierrez, J., Morales, A., & Ortega-Garcia, J., Deepfakes and beyond: a survey of face manipulation and fake detection, Information Fusion. 64 (2020) 131â€“148. https://www.sciencedirect.com/science/article/pii/S1566253520303110?via%3Dihub
and collaboration between researchers, developers, and policymakers is fundamental for long-term adaptability.

International interoperability is essential for a coordinated global response. This means ensuring that deepfake detection tools developed in one country can be used effectively in others, facilitating international collaboration. The UN's International Telecommunication Union (ITU) is working towards industry-wide standards and protocols for content provenance and authenticity to ensure these tools work seamlessly across platforms and jurisdictions.168 However, this goal faces challenges like varying legal frameworks, geopolitical tensions, and the need for trust between nations. Harmonising legal frameworks, fostering collaboration through diplomatic efforts, and establishing clear protocols for data sharing are crucial to overcome these obstacles and hold perpetrators accountable.

The threat of malicious AI-generated content is imminent and escalating. Achieving the goals outlined in this framework requires continuous research, collaboration, and a commitment to outpacing those who seek to exploit these technologies.

## 7.2. International Frameworks And Standards For Ai Training And Testing

UN Resolution A/78/L.49 promotes internationally interoperable frameworks and standards for training and testing AI systems to ensure fairness, mitigate bias, and curb harmful content.169 This aligns with Recital 66 of the EU AI Act, which stresses the importance of high-quality data for training and testing AI systems to mitigate potential biases, prevent discrimination, and ensure the development of accurate and reliable AI tools. Furthermore, the Act's Recital 121 suggests that these international AI frameworks may fall under the scope of the AI Act, which emphasises 'state-of-the-art' solutions.

Establishing global standards for AI-powered deepfake detection tools is crucial, but reaching consensus across diverse legal systems, cultural values, and technological capabilities requires careful negotiation and compromise. Differing legal definitions of fraud and varying cultural norms around freedom of expression could create hurdles.170 Nevertheless, the urgency of the issue necessitates international cooperation.

Preprint To address these challenges, the UN's ITU convened a multistakeholder collaboration in 2024 under the World Standards Cooperation. This initiative promotes transparency and security in deepfake detection while respecting cultural diversity. For example, the 

168 ITU, Detecting deepfakes and generative AI: report on standards for AI watermarking and multimedia authenticity workshop, the need for standards collaboration on AI and multimedia authenticity 2024 Report. https://acrobat.adobe.com/id/urn:aaid:sc:EU:764a0bb2-52cc-4617-b8c3-690cf6f2d022, 2024 (accessed 19 November 2024). 169 United Nations General Assembly. (2024). Seizing the opportunities of safe, secure and trustworthy artificial intelligence systems for sustainable development. A/RES/78/L.49. New York: United Nations. 170 Ofcom, Use of AI in online content moderation. https://www.ofcom.org.uk/online-safety/safety-technology/online-contentmoderation/, 2023 (accessed 19 November 2024).
ITU aims to establish standards for embedding watermarks and using blockchain to track the origin and manipulation of media. Furthermore, the ITU is working to standardise algorithms and models, including datasets and detection techniques, while promoting open-source tools and data sharing. It also emphasises the standardisation of hardware acceleration for real-time detection and multimodal data processing. These standards are crucial and require support from robust policy frameworks. The ITU initiative aims to establish adaptable standards and promote ongoing research to keep pace with the evolving nature of deepfakes.171 While international standardisation offers significant benefits, it is important to consider the potential downsides of standardisation, such as stifling innovation or creating barriers to entry for smaller players.172 The CJEU ruling in Public.Resource.Org and Right to Know v Commission, which focused on making harmonised standards freely available, highlights the importance of open access to foster innovation and prevent such barriers.173 Mitigating these risks requires a commitment to inclusivity and ongoing evaluation of the standardisation process.

UN Report A/73/348 on AI and human rights stresses the crucial need for independent auditing and certification to ensure the accuracy and reliability of AI systems, including deepfake detection tools. This report also highlights the importance of establishing clear mechanisms for redress and accountability for those harmed by the misuse of AI.174 For instance, if a deepfake video manipulates a stock price,175 standardised detection techniques could quickly identify and flag it. Subsequently, blockchain technology could be used to trace the video's origin and identify the perpetrators.176 International cooperation would then enable law enforcement agencies to hold those responsible accountable.177 Beyond these specific measures, a comprehensive approach is crucial to combat the evolving threat of deepfakes. International collaboration on robust and adaptable standards, coupled with ethical considerations and public awareness, will empower the global community to mitigate the harms of AI-generated content.

Preprint 171 ITU, Detecting deepfakes and generative AI: report on standards for AI watermarking and multimedia authenticity workshop, the need for standards collaboration on AI and multimedia authenticity 2024 Report. https://acrobat.adobe.com/id/urn:aaid:sc:EU:764a0bb2-52cc-4617-b8c3-690cf6f2d022, 2024 (accessed 19 November 2024). 172 Blind, K., The economics of standards: theory, evidence, policy, Edward Elgar Publishing, Cheltenham, 2004. 173 Câ€‘588/21 P *Public.Resource.Org, Inc. and Right to Know CLG v European Commission* [2024] ECLI:EU:C:2024:201 [89]. 174 United Nations General Assembly. (2018). Promotion and protection of the right to freedom of opinion and expression: Note by the Secretary-General. (A/73/348). 175 Business Standard, NSE raises the raid flag on deepfakes: here is how you can fall prey to such scams. https://www.businessstandard.com/finance/personal-finance/nse-raises-the-raid-flag-on-deepfakes-here-is-how-you-can-fall-prey-to-such-scams124041000562_1.html, 2024 (accessed 19 November 2024). 176 Hasan, H. R., & Salah, K., Combating deepfake videos using blockchain and smart contracts, IEEE Access. 7 (2019) 4159641606. https://ieeexplore.ieee.org/document/8668407 177 Interpol, Beyond Illusions: unmasking the threat of synthetic media for law enforcement. https://www.interpol.int/content/download/21179/file/BEYOND%20ILLUSIONS_Report_2024.pdf, 2024 (accessed 19 November 2024).
7.3. Intellectual Property and Deepfake Detection: Balancing Copyright with Fraud Prevention The UN Resolution A/78/L.49 underscores the need to protect intellectual property rights, particularly copyright.178 This is crucial for deepfake fraud detection tools, which combat fraud but often utilise copyrighted training data.

FaceForensics++, a dataset for deepfake detection, sources videos from YouTube, potentially raising copyright concerns due to unauthorised use, even with criteria like 
'trackable frontal faces.'179 Similarly, WildDeepfake sources deepfakes 'completely from the internet,'180 prompting anxieties regarding potential copyright infringement due to a lack of explicit permissions or licences.

In contrast, the Deepfake Detection Challenge Dataset used paid actors with explicit consent, focusing on synthetic deepfakes, and making the dataset publicly available for research and educational purposes, minimising copyright infringement risks.181 The CJEU's *SABAM* cases highlight the potential for technical measures to inadvertently impact legitimate user activities, such as fair use, parody, or quotation, which are protected by copyright exceptions.182 The ITU advocates for standardising AI training data usage to protect copyright holders. This includes enhancing the practicality of opt-out mechanisms, enabling efficient control over copyrighted works across AI platforms, and developing clear guidelines for data licensing and attribution. These efforts aim to simplify copyright management and ensure compliance with international standards.183 Additionally, transparency is paramount in AI development. The EU AI Act mandates disclosure of training data (Article 53(1)(d) and Recital 107), exemplified by Synthesia requiring explicit consent for voice and facial cloning for AI avatars.184 Global 

Preprint 178 United Nations General Assembly. (2024). Seizing the opportunities of safe, secure and trustworthy artificial intelligence systems for sustainable development. A/RES/78/L.49. New York: United Nations. 179 RÃ¶ssler, A., Cozzolino, D., Verdoliva, L., Riess, C., Thies, J., & Niessner, M., FaceForensics++: learning to detect manipulated facial images, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), Seoul, Korea (South). (2009) 111. https://ieeexplore.ieee.org/document/9010912 180 Zi, B., Chang, M., Chen, J., Ma, X., & Jiang, Y.-G., WildDeepfake: a challenging real-world dataset for deepfake detection, arXiv preprint arXiv:2101.01456. (2024) 1-9. https://arxiv.org/abs/2101.01456 181 Dolhansky, B., Bitton, J., Pflaum, B., Lu, J., Howes, R., Wang, M., & Canton Ferrer, C., The DeepFake detection challenge (DFDC) dataset, arXiv preprint arXiv:2006.07397. (2020) 1-13. https://arxiv.org/abs/2006.07397 182 C-360/10 *Belgische Vereniging van Auteurs, Componisten en Uitgevers CVBA (SABAM) v Netlog NV* [2012] ECLI:EU:C:2012:85 [50]; C-70/10 *Scarlet Extended SA v SociÃ©tÃ©Â´ belge des auteurs, compositeurs et Ã©diteurs SCRL (SABAM)* [2012] ECLI:EU:C:2011:771 [52]. 183 ITU, Detecting deepfakes and generative AI: report on standards for AI watermarking and multimedia authenticity workshop, the need for standards collaboration on AI and multimedia authenticity 2024 Report. https://acrobat.adobe.com/id/urn:aaid:sc:EU:764a0bb2-52cc-4617-b8c3-690cf6f2d022, 2024 (accessed 19 November 2024). 184 MIT Technology Review, An AI startup made a hyper realistic deepfake of me that's so good it is scary. https://www.technologyreview.com/2024/04/25/1091772/new-generative-ai-avatar-deepfake-synthesia/, 2024 (accessed 19 November 2024).
standardisation of this disclosure process would enhance accountability and streamline compliance.185 To address these copyright challenges, Adobe proposes a promising solution: Content Credentials (CR). This technology embeds metadata into digital content, enabling the identification of specific training data. This is particularly relevant for deepfake detection datasets, ensuring creators are properly credited and compensated when their work contributes to deepfake detection AI through the use of CR. Imagine a deepfake fraud detection tool that not only spots fakes, but also verifies the ethical sourcing of its training images by tracing their origins. Combining CR with technologies like Non- Fungible Tokens (NFTs) fosters ethical and transparent use of copyrighted material in AI by recording ownership on a secure and immutable ledger.186 While CR offers a promising solution, addressing copyright in AI requires a nuanced, multifaceted approach that balances protection with innovation. Determining fair use in AI training is complex as highlighted in *Getty Images (US) v Stability AI*,
187 and licensing agreements can significantly impact AI development.188 Requiring licences for every image in massive datasets could become prohibitively expensive, potentially hindering innovation.189 Conversely, unchecked use of copyrighted material undermines creators' rights and discourages them from sharing their work,190 limiting the data available for developing effective deepfake detection tools.

Clear legal frameworks and responsible data usage are crucial for navigating copyright issues in AI. Policymakers must provide guidance on copyright, while developers should prioritise ethical practices like seeking consent, using public datasets with clear licences, and exploring synthetic data generation. Adopting solutions like CR and ethical AI practices fosters the development of deepfake detection tools that respect copyright and combat fraud effectively.

Preprint 185 ITU, Detecting deepfakes and generative AI: report on standards for AI watermarking and multimedia authenticity workshop, the 

![35_image_0.png](35_image_0.png) need for standards collaboration on AI and multimedia authenticity 2024 Report. https://acrobat.adobe.com/id/urn:aaid:sc:EU:764a0bb2-52cc-4617-b8c3-690cf6f2d022, 2024 (accessed 19 November 2024). 186 Collomosse, J., & Parsons, A., To authenticity, and beyond! Building safe and fair generative AI upon the three pillars of provenance, IEEE Computer Graphics and Applications. 44(3) (2024) 82â€“90. https://dl.acm.org/doi/10.1109/MCG.2024.3380168 187 *Getty Images (US), Inc. v. Stability AI, Inc.*, 1:23-cv-00135 (US District Court. Del. 2023); see also Getty Images (US) Inc & Ors v Stability AI Ltd [2023] EWHC 3090 (Ch). 188 World Economic Forum, Governance in the age of generative ai: a 360Âº approach for resilient policy and regulation White Paper October 2024. https://www3.weforum.org/docs/WEF_Governance_in_the_Age_of_Generative_AI_2024.pdf, 2024 (accessed 19 November 2024). 189 Lemley, Mark A. & Casey, Bryan., Fair learning, Texas Law Review. 99(4) (2021) 743-785. https://www.law.berkeley.edu/wpcontent/uploads/2024/04/Fair-learning.pdf 190 WIPO, Generative AI navigating intellectual property. https://www.wipo.int/export/sites/www/aboutip/en/frontier_technologies/pdf/generative-ai-factsheet.pdf, 2024 (accessed 19 November 2024).
7.4. Data Protection in Deepfake Fraud Detection: Upholding Privacy throughout the AI Lifecycle The UN Resolution A/78/L.49 underscores the importance of protecting personal data and privacy throughout the AI lifecycle, including deepfake detection. This necessitates transparency and accountability in data usage, aligning with international standards like the EU AI Act.191 Even during testing and evaluation, robust data protection measures must be implemented, especially when handling sensitive personal data.

Transparency is essential for building trust in deepfake detection. Effective reporting should include clear metrics (accuracy, F1-score, etc.), detailed methodologies, and insightful analysis, ideally presented with visualisations and contextualised to the broader impact of deepfakes. Such transparency enables benchmarking and drives the development of more robust and ethical detection tools.

However, a concerning trend has emerged: many deepfake detection tools remain inaccessible to the public. For instance, Intel's FakeCatcher, with its impressive 96% 
accuracy,192 and Sensity AI, boasting a 98% accuracy rate,193 are primarily accessible to businesses and organisations. Restricting access to these tools creates a dangerous power imbalance. This leaves individuals vulnerable to deepfakes, exacerbates inequalities, and hinders public understanding, limiting the ability to critically evaluate such content.

In contrast, companies like DuckDuckGoose and Facia prioritise transparency, openly sharing their methods, metrics, and results. DuckDuckGoose, offers a 96% accuracy, 0.1% false rejection rate, 100% explainable results, and 1-second analysis time.194 Facia also provides a 90% accuracy rate achieved through rigorous testing on 3,430 unique AI-generated images and videos, focusing on micro-level inconsistencies.195 Recital 27 of the EU AI Act echoes this sentiment, emphasising the need for AI systems to be developed and used in a way that allows for traceability, explainability, and human awareness of interaction.

The AG in the *SCHUFA* case, upheld by the CJEU, emphasises the importance of transparency and explainability in automated decision-making systems, including those using AI. While acknowledging the need to protect trade secrets (Recital 63 GDPR),196 disclosing the complex AI algorithms themselves is unnecessary (Article 12(1), Recital 

Preprint 191 United Nations General Assembly. (2024). Seizing the opportunities of safe, secure and trustworthy artificial intelligence systems for sustainable development. A/RES/78/L.49. New York: United Nations. 192 Intel, Intel introduces real-time deepfake detector. https://www.intel.com/content/www/us/en/newsroom/news/intel-introducesreal-time-deepfake-detector.html, 2022 (accessed 19 November 2024). 193 Sensity AI, All-in-one deepfake detection. https://sensity.ai/, founded in 2018 (accessed 19 November 2024). 194 DuckDuckGoose AI. https://www.duckduckgoose.ai/, founded in 2020 (accessed 19 November 2024). 195 Facia, Market leading accuracy in Facia's new deepfake detection algorithm. https://facia.ai/wp-content/uploads/2024/09/Market- Leading-Accuracy-in-Facias-New-Deepfake-Detection-Algorithm.pdf, 2024 (accessed 19 November 2024). 196 AG opinion in C-634/21 *SCHUFA Holding and Others (Scoring)* [2023] ECLI:EU:C:2023:220 [AG 54].
58 GDPR).197 Instead, individuals have the right to meaningful information about the logic involved in AI decisions. This includes understanding the factors considered, their weight, and how they influence the outcome, allowing them to potentially challenge the decision (Article 22(1) GDPR) and ensuring fair and transparent processing in line with GDPR principles.198 The EDPS echoes that deepfake detection tools must be accurate, transparent, and explainable, going beyond simple 'real' or 'fake' classifications to provide nuanced explanations of the underlying logic. This includes details about the factors, analysis methods, and key indicators used to identify deepfakes.199 This transparency fosters accountability, trust, and ethical use. Developers should adhere to data protection by design and default, ensuring compliance with data minimization, purpose limitation, and storage limitation principles.

While intended to protect individuals, deepfake detection technologies themselves can pose privacy risks if they collect and analyse excessive or sensitive biometric data.200 To mitigate this, developers should prioritise data protection principles and implement strong data security measures.201 This includes techniques like anonymization and pseudonymization to protect the identity of individuals whose data is being processed and ensuring compliance with all relevant data protection regulations.

## 7.5. Trustworthy Ai For Deepfake Fraud Detection

UN Resolution A/78/L.49 stresses the need for transparent, predictable, understandable, and reliable AI systems with clear explanations of decisions and human oversight to ensure accountability and minimise bias.202 The CJEU, in *UPC Telekabel Wien*, acknowledges that companies need flexibility in choosing anti-fraud measures proportionate to their capabilities and resources.203 In the context of deepfake detection, this flexibility must be balanced with the need for accurate and fair tools that do not unduly infringe on individual rights. However, this flexibility should not compromise accuracy and fairness, as emphasised by the AG's opinion in *Poland v Council and Parliament*, which cautioned against excessive 'false positives'.204 To effectively combat deepfake fraud, AI systems must prioritise accuracy 

Preprint 197 *Ibid* [AG 57]. 198 *Ibid* [AG 58]. 199 EDPS, Deepfake detection. https://www.edps.europa.eu/data-protection/technology-monitoring/techsonar/deepfakedetection_en, 2024 (accessed November 19, 2024). 200 ICO, Biometric data guidance: biometric recognition. https://ico.org.uk/media/for-organisations/uk-gdpr-guidance-andresources/lawful-basis/biometric-data-guidance-biometric-recognition-1-0.pdf, 2024 (accessed November 19, 2024). 201 ICO, A guide to data security. https://ico.org.uk/media/for-organisations/uk-gdpr-guidance-and-resources/security/a-guide-todata-security-0-0.pdf, 2023 (accessed November 19, 2024). 202 United Nations General Assembly. (2024). Seizing the opportunities of safe, secure and trustworthy artificial intelligence systems for sustainable development. A/RES/78/L.49. New York: United Nations. 203 C-314/12 *UPC Telekabel Wien GmbH v Constantin FilmVerleih GmbH and Wega Filmproduktionsgesellschaft GmbH* [2013] EU:C:2014:192 [52]. 204 AG opinion in C-401/19 *Poland v Parliament and Council* [2021] ECLI:EU:C:2021:613 [214].
and fairness, as inaccuracies can undermine detection efforts, erode trust, and infringe on individual rights.

Developing trustworthy AI for deepfake detection faces challenges. Firstly, the lack of diverse and unbiased training data can lead to biassed models that misidentify deepfakes based on factors like ethnicity, gender, or age, resulting in wrongful accusations (false positives) or missed detection (false negatives).205 This echoes Recital 67 of the EU AI Act, which highlights the potential for bias in AI systems. To prevent discrimination, datasets must be comprehensive, accurate, and representative, incorporating privacy-preserving techniques like differential privacy or federated learning.

Secondly, deepfake detection models struggle to generalise to new or unseen deepfakes. Overfitting to training data can lead to misclassification of genuine videos as deepfakes, while underfitting can result in missing actual deepfakes. Both overfitting and underfitting hinder accurate detection. For instance, a deepfake detection model trained primarily on Caucasian faces may struggle to accurately identify deepfakes of individuals from other ethnicities.206 Promisingly, new research explores modifying existing algorithms, such as incorporating adversarial training or ensemble methods, to improve fairness and accuracy across diverse datasets.207 The CJEU, in *SCHUFA*, emphasised mitigating risks and discrimination in automated decision-making systems. Echoing GDPR Recital 71, the court highlighted the need for safeguards and fair, transparent processing.208 This includes using appropriate procedures, minimising errors, and securing personal data. Critically, individuals subject to automated decisions have the right to human intervention, to express their views, and to challenge the decision (Article 22(2)(b) GDPR).209 For example, in the context of deepfake detection, this could include the right to contest a flagged video or request a manual review of the AI's decision.

To prevent deepfake detection tools from perpetuating biases or being misused for discriminatory purposes, careful consideration must be given to the entire system, including the training data, algorithmic design, and potential impact on individuals. This includes minimising errors, providing clear explanations of AI-driven decisions, and offering avenues for recourse and redress. Human oversight is crucial to ensure 

Preprint 205 Ju, Y., Hu, S., Jia, S., Chen, G. H., & Lyu, S., Improving fairness in deepfake detection, In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). (2024). 4655-4665. https://www.ftc.gov/system/files/ftc_gov/pdf/20-Ju- Improving-Fairness-in-Deepfake-Detection.pdf 206 Xu, Y., TerhÃ¶rst, P., Raja, K., & Pedersen, M., Analyzing fairness in deepfake detection with massively annotated databases. arXiv preprint arXiv:2208.05845. (2024). 1-30. https://arxiv.org/abs/2208.05845 207 Hooda, A., Mangaokar, N., Feng, R., Fawaz, K., Jha, S., & Prakash, A.,Towards adversarially robust deepfake detection: an ensemble approach, ICLR. (2023). 1-20. https://openreview.net/forum?id=4bH8SxYNcI 208 C-634/21 *SCHUFA Holding and Others (Scoring)* [2023] ECLI:EU:C:2023:957 [59], [66]. 209 *Ibid* [65], [66].

![39_image_0.png](39_image_0.png)

accountability, especially in high-stakes situations where fundamental rights are at stake. Robust data protection measures must also be prioritised throughout the AI lifecycle.

Responsible deepfake detection requires a multifaceted approach that prioritises accuracy, fairness, and transparency. Addressing bias, generalisation, and data protection, while incorporating human oversight and adhering to legal frameworks like the GDPR, fosters AI systems that effectively combat deepfake fraud while upholding fundamental rights.

7.6 Responsible AI for Deepfake Fraud Detection: Safeguards and Impact Assessments The United Nations Resolution A/78/L.49, recognizing the profound impact of AI, emphasises the need for robust safeguards and impact assessments for AI systems like deepfake detection tools.210 These tools, crucial for combating fraud, often rely on vast amounts of data, raising concerns about privacy, bias, and potential misuse.

To address these concerns and navigate the complex landscape of deepfakes, the World Economic Forum advocates for strategic foresight. This proactive approach utilises methods like scenario planning and horizon scanning to analyse future trends and their impact. By engaging diverse stakeholders, governments and organisations can anticipate needs, adapt regulations, and address potential risks proactively. This agility is critical, as the rapid advancement of AI necessitates flexible and adaptable regulations. Policymakers must be responsive to new developments, considering diverse perspectives and exploring innovative approaches like regulatory sandboxes for controlled testing of new technologies.211 To ensure the ethical use of AI systems and protect fundamental rights, comprehensive impact assessments are crucial. The UN Report A/73/348 advocates for radical transparency and external scrutiny of AI systems, with assessments conducted before and during their deployment to identify and mitigate risks. Similarly, Article 27 of the EU AI Act provides a framework for assessing the impact of high-risk AI systems on rights like data protection, privacy, and non-discrimination. Furthermore, when deepfake detection tools utilise biometric data, a Data Protection Impact Assessment under Article 35 of the GDPR is necessary to identify and mitigate risks to individual freedoms.

Preprint November 2024).

210 United Nations General Assembly. (2024). Seizing the opportunities of safe, secure and trustworthy artificial intelligence systems 

![39_image_1.png](39_image_1.png) for sustainable development. A/RES/78/L.49. New York: United Nations. 211 World Economic Forum, Governance in the age of generative ai: a 360Âº approach for resilient policy and regulation White Paper October 2024. https://www3.weforum.org/docs/WEF_Governance_in_the_Age_of_Generative_AI_2024.pdf, 2024 (accessed 19 
DuckDuckGoose and Reality Defender are leading the charge in responsible AI. DuckDuckGoose's DeepDetector boasts 95% accuracy in detecting faces and provides detailed explanations through Activation Maps and 'Explainable AI' dashboards.212 Their AI Voice Detector also prioritises privacy by encrypting voice data.213 Similarly, Reality Defender214 offers explainable anti-fraud tools for voice and document verification, empowering organisations to scan calls in real-time, verify media authenticity, and enhance user verification processes.215 Beyond legal frameworks, mitigating bias in deepfake detection algorithms is crucial. This requires a multifaceted approach, including the use of diverse datasets that accurately reflect different demographics, regular audits of algorithms to identify and rectify discriminatory outcomes, and the involvement of experts from various fields, including social science and ethics, in the evaluation process. Clear guidance is needed on the ethical collection and use of sensitive data like ethnicity or gender, and models should be assessed for potential biases on a case-by-case basis, with transparent evaluations.216 The good news is that research is actively addressing these challenges. New methods are being developed to combat bias in deepfake detection algorithms, ensuring fairer and more equitable outcomes for everyone, regardless of race or gender, through a focus on balanced accuracy across demographics.217 In conclusion, robust safeguards, agile regulations, and comprehensive impact assessments are essential for the responsible development and deployment of deepfake detection tools. These measures help ensure that these powerful technologies are used ethically and effectively in combating deepfakes while protecting individual rights and freedoms.

8. Discussion of findings 

## 8.1. Human Rights, Ai, And The Fight Against Deepfakes

This research critically examines the intersection of deepfake fraud detection tools, human rights, and evolving laws, UN Resolution A/78/L.49 and EU legislation such as 

Preprint 212 DuckDuckGoose AI. https://www.duckduckgoose.ai/, founded in 2020 (accessed 19 November 2024). 213 Expert Insights, The top 8 deepfake detection solutions. https://expertinsights.com/insights/the-top-deepfake-detection-solutions/ 2024 (accessed 19 November 2024). 214 Reality Defender, Visual deepfake detection explainability. https://www.realitydefender.com/blog/visual-deepfake-detectionexplainability, 2024 (accessed 19 November 2024). 215 Expert Insights, The top 8 deepfake detection solutions. https://expertinsights.com/insights/the-top-deepfake-detection-solutions/ 2024 (accessed 19 November 2024). 216 FRA, Test algorithms for bias to avoid discrimination. https://fra.europa.eu/en/news/2022/test-algorithms-bias-avoiddiscrimination, 2022 (accessed 19 November 2024). 217 Ju, Y., Hu, S., Jia, S., Chen, G. H., & Lyu, S., Improving fairness in deepfake detection, In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). (2024). 4655-4665. https://www.ftc.gov/system/files/ftc_gov/pdf/20-Ju- Improving-Fairness-in-Deepfake-Detection.pdf
the AI Act, GDPR, and DSA. It underscores the need for a multifaceted approach to guide responsible development, integrating legal, ethical, technical, and societal considerations. This includes developing robust, accessible tools, such as those enhancing provenance and authenticity, to keep pace with deepfake technology. Bridging the 'deepfake divide'218 requires global access to detection tools and promoting international standards for AI training with diverse datasets to maximize accuracy and avoid bias. Protecting individuals requires a commitment to intellectual property, data protection, robust safeguards, and thorough risk assessments to mitigate deepfake harms, which can infringe upon human rights.

This analysis also highlights how Article 114 TFEU can harmonize standards across the EU, fostering innovation and accessibility while upholding ethical considerations. This harmonized approach could serve as a model for global adoption, promoting responsible AI practices worldwide. Furthermore, these findings extend significantly beyond deepfake fraud detection to encompass other harmful AI-generated content, including non-consensual deepfakes and electoral misinformation, which are being addressed through evolving legislation like the US DEFIANCE Act 2024, the DEEPFAKES Accountability Act 2023, and the UK Online Safety Act 2023. 

This research, therefore, stresses the urgent need for comprehensive legal and ethical frameworks governing AI-generated content.

## 8.2. Beyond Accuracy: Deepfake Detection And Society

Responsible deepfake detection requires addressing societal, economic, and environmental concerns alongside technical advancements, promoting fairness, affordability, and sustainability in AI development, including systems trained on synthetic data.

The economic cost of deepfake detection presents a significant challenge due to the high computational demands of current methods.219 However, emerging research offers potential solutions by focusing on computational efficiency and developing models that perform well in real-time with lower costs.220 Ensuring fairness is crucial, as existing datasets often lack diversity, leading to biassed algorithms.221 Ongoing research aims to mitigate this by developing efficient models 

Preprint 218 Bitton, D. B., Hoffmann, C. P., & Godulla, A., Deepfakes in the context of AI inequalities: analysing disparities in knowledge and attitudes, Information, Communication & Society. (2024) 1-21 https://www.tandfonline.com/doi/full/10.1080/1369118X.2024.2420037 219 Kaur, A., Hoshyar, A. N., Saikrishna, V., Firmin, S., & Xia, F., Deepfake video detection: challenges and opportunities, Artificial Intelligence Review, 57(159) (2024) 1-47. https://link.springer.com/article/10.1007/s10462-024-10810-6 220 Bansal, N., Aljrees, T., Yadav, D. P., Singh, K. U., Kumar, A., Verma, G. K., & Singh, T., Real-time advanced computational intelligence for deep fake video detection, Applied Sciences, 13(5) 3095 (2023) 1-23. https://doi.org/10.3390/app13053095 221 Trinh, L., & Liu, Y., An examination of fairness of AI models for deepfake detection, In Proceedings of the 30th International Joint Conference on Artificial Intelligence (IJCAI-21), (2021) 1-9. https://arxiv.org/abs/2105.00558; Xu, Y., TerhÃ¶rst, P., Raja, K., & Pedersen, M., Analyzing fairness in deepfake detection with massively annotated databases, arXiv preprint arXiv:2208.05845 (2024) 1-30. https://arxiv.org/abs/2208.05845
capable of utilising larger, more diverse datasets.222 Biassed deepfake detection systems risk worsening existing inequalities and can be used to unfairly target specific individuals or groups. For example, a system trained primarily on images of individuals with lighter skin tones might be less accurate at detecting deepfakes featuring those with darker skin tones, highlighting the need for diverse training data.223 Furthermore, the environmental impact of deepfake detection is a growing concern. Recent studies address this by optimising core mathematical operations. Algorithms like L-Mul drastically reduce energy consumption, promoting sustainable AI practices, and greener technologies.224 While tools like DeepMind's SynthID Text focus on watermarking AI-generated text,225 its underlying principles could inspire new approaches to deepfake detection in other media, further reducing their environmental impact.

This research advances deepfake detection while considering its societal impact. It promotes efficiency and sustainability, fostering a responsible and accessible approach. Future research should prioritise these considerations to ensure widespread benefits.

## 8.3. Synthetic Data For Deepfake Detection: Balancing Benefits And Risks

Building on data-related matters, both the EU AI Act (Article 10(5)(a)) and the UN acknowledge the potential of synthetic data for training deepfake detection AI systems. Synthetic data offers solutions to data scarcity, privacy concerns, and bias. However, it also presents challenges related to data quality, security, and ethics.226 Used alone or with real-world data, synthetic data can lead to more effective and inclusive deepfake detection systems. By enabling diverse and representative datasets, it can contribute to fairer systems and potentially reduce the environmental impact of data collection.

Despite these benefits, it is crucial to address the potential risks associated with synthetic data. Caution is advised, as it carries risks like cybersecurity threats, amplifying biases, and causing model inaccuracies.227 For example, the World 

Preprint 222 Ju, Y., Hu, S., Jia, S., Chen, G. H., & Lyu, S., Improving fairness in deepfake detection, In Proceedings of the IEEE/CVF Winter 

![42_image_0.png](42_image_0.png) Conference on Applications of Computer Vision (WACV). (2024). 4655-4665. https://www.ftc.gov/system/files/ftc_gov/pdf/20-Ju- Improving-Fairness-in-Deepfake-Detection.pdf 223 EDPS, Deepfake detection. https://www.edps.europa.eu/data-protection/technology-monitoring/techsonar/deepfakedetection_en, 2024 (accessed November 19, 2024). 224 Luo, H., & Sun, W., Addition is all you need for energy-efficient Language Models, arXiv preprint arXiv:2410.00907. (2024) 1-13. https://arxiv.org/abs/2410.00907 225 Google, SynthID: tools for watermarking and detecting LLM-generated Text. https://g.co/kgs/BM1FSWw, 2024 (accessed November 19, 2024). 226 UN University, Policy guideline recommendations on the use of synthetic data to train AI models. https://acrobat.adobe.com/id/urn:aaid:sc:EU:7c09413f-af5e-4fc4-abd0-ddfa60b3cf4d, 2024 (accessed 19 November 2024). 227 Ibid.
Economic Forum warns of 'model collapse,' where model performance degrades as it learns from its own increasingly inaccurate outputs.228 To ensure responsible use, the UN recommends prioritising diverse data sources and varied generative models, along with transparent disclosure of data origins and quality metrics. Robust cybersecurity protocols are crucial, and prioritising non-synthetic data when possible is recommended. Furthermore, policymakers should integrate synthetic data into global AI governance efforts, with specific regulations and guidelines addressing its unique challenges. Global quality standards, security measures, and research networks are needed to ensure responsible use, along with clear ethical guidelines, including transparency requirements.229 Addressing the challenges of synthetic data unlocks its potential to combat deepfakes and foster a safer digital environment.

## 8.4. Combating Deepfakes: A Multifaceted Approach

Deepfakes raise complex ethical concerns, as they can be misused for fraudulent purposes such as creating fake identities for endorsements or scams. However, they also offer valuable legitimate applications, including simulating real-world scams for training and awareness, and generating synthetic data to improve fraud detection models without compromising privacy.230 This duality creates a significant challenge for regulators, who must curb harmful uses while fostering innovation and beneficial applications. Striking this balance is crucial for the responsible and ethical development of deepfake technology.

Beyond these ethical considerations, a significant societal challenge lies in the lack of public awareness and education. Despite the growing threat of deepfakes, many remain unaware of their potential for deception. Ofcom research indicates that less than 10% of adults feel confident in their ability to identify deepfakes.231 These sophisticated forgeries can easily deceive even discerning individuals, especially in the current climate of widespread misinformation.232 Combating deepfakes requires a comprehensive strategy, combining technological advancements with robust legislation, industry self-regulation, transparency initiatives, 

Preprint 228 World Economic Forum, Governance in the age of generative ai: a 360Âº approach for resilient policy and regulation White Paper October 2024. https://www3.weforum.org/docs/WEF_Governance_in_the_Age_of_Generative_AI_2024.pdf, 2024 (accessed 19 November 2024). 229 UN University, Policy guideline recommendations on the use of synthetic data to train AI models. https://acrobat.adobe.com/id/urn:aaid:sc:EU:7c09413f-af5e-4fc4-abd0-ddfa60b3cf4d, 2024 (accessed 19 November 2024). 230 Sensity AI, All-in-one deepfake detection. https://sensity.ai/, founded in 2018 (accessed 19 November 2024). 231 Ofcom, A deep dive into deepfakes that demean, defraud and disinform. https://www.ofcom.org.uk/online-safety/illegal-andharmful-content/deepfakes-demean-defraud-disinform/, 2024 (accessed 19 November 2024). 232 Groh, M., Sankaranarayanan, A., Singh, N., Kim, D.Y., Lippman, A., & Picard, R., Human detection of political speech deepfakes across transcripts, audio, and video, Nature Communications. 15 (7629) (2024) 1-16. https://doi.org/10.1038/s41467-024-51998-z
technical standards, public education, and incentives for responsible AI use. With experts predicting 90% of content to be AI-generated by 2025,233 the findings of this research become even more crucial, underscoring the critical importance of addressing the ethical, societal, and technical challenges posed by deepfakes. The rapid advancement of deepfake technology demands urgent action to harness its potential while mitigating its harms. This necessitates a multifaceted approach, engaging policymakers, researchers, educators, and the tech industry to develop effective solutions.

## 9. Protecting The Digital Future: A Call To Action On Deepfake Detection

The urgent threat of deepfake fraud demands immediate action. Businesses are losing nearly $450,000 on average to this crime in 2024,234 and the potential for deepfakes to undermine democratic values is a grave concern. While existing legal frameworks like the EU AI Act, GDPR, and DSA offer a foundation, navigating them effectively to ensure fundamental rights protection and responsible AI development, in line with UN Resolution A/78/L.49, proves challenging. Without robust measures for transparency, accountability, and oversight, deepfake detection tools risk becoming instruments of discrimination, surveillance, and censorship.

To address these challenges, this paper proposes a series of safeguards to ensure responsible and ethical deepfake detection. It examines the complex intersection of deepfake fraud detection, human rights, and evolving legislation, analysing how UN Resolution A/78/L.49 interacts with EU law, including the potential role of Article 114 TFEU in harmonizing standards. It proposes safeguards to ensure responsible development and deployment of deepfake detection tools while upholding fundamental rights.

Firstly, combat the threat of AI-generated content by prioritising robust and accessible identification tools, adaptable models that continuously learn, and international interoperability. Continuous research and collaboration are crucial to foster a safer digital environment.

Preprint Secondly, ensure responsible AI for deepfake detection through internationally interoperability frameworks, robust standards, global collaboration, and open access. Prioritise standardised algorithms and independent auditing to promote accountability and ethical development.

on-idv-regula-survey/, 2024 (accessed 19 November 2024).

233 ITU, Detecting deepfakes and generative AI: report on standards for AI watermarking and multimedia authenticity workshop, the need for standards collaboration on AI and multimedia authenticity 2024 Report. https://acrobat.adobe.com/id/urn:aaid:sc:EU:764a0bb2-52cc-4617-b8c3-690cf6f2d022, 2024 (accessed 19 November 2024). 234 Regula, The impact of deepfake fraud: risks, solutions and global trends. https://regulaforensics.com/blog/impact-of-deepfakes-
Thirdly, safeguard copyright by prioritising standardised AI training data usage with clear opt-out mechanisms and mandatory disclosure of training data content. Utilise technologies like Content Credentials to foster ethical AI and effective deepfake detection.

![45_image_0.png](45_image_0.png)

Furthermore, uphold data protection in deepfake detection by prioritising transparency and model explainability throughout the AI lifecycle. Ensure accessibility to detection tools while balancing trade secrets with individuals' right to meaningful information about the AI's decision-making process.

Additionally, ensure trustworthy AI for deepfake detection by prioritising accuracy, fairness, and diverse datasets. Incorporate human review in high-stakes decisions to prevent discrimination (avoiding false positives/negatives, overfitting/underfitting) and protect individual rights.

Lastly, prioritise adaptable regulations and strategic foresight to address evolving deepfake technology. Conduct comprehensive Fundamental Rights Impact Assessments, Data Protection Impact Assessments, and Bias Risks Assessments, throughout the AI lifecycle to ensure ethical deployment and safeguard human rights.

Harnessing the potential of deepfake detection tools through these safeguards requires international collaboration and the development of standardised algorithms to ensure interoperability and prevent fragmentation. Policymakers must lead by enacting adaptable regulations and conducting thorough impact assessments. Researchers must develop robust and ethical deepfake detection tools, while educators equip future generations with the critical thinking skills needed to navigate the complexities of deepfakes. Technology companies must prioritise responsible AI development and deployment, ensuring transparency and protecting user rights. Collaboration should foster joint research, shared deepfake databases, and international standards for AI ethics and governance. Prioritising these safeguards will ensure ethical and responsible development of deepfake detection, preventing misuse while fostering a future where AI contributes to a more secure and trustworthy digital world.

Declaration of generative AI and AI-assisted technologies in the writing process.

Preprint Statement: During the preparation of this work the author(s) used Google's Gemini in order to improve readability and language of the work. After using this tool, the author(s) reviewed and edited the content as needed and take(s) full responsibility for the content of the published article.

Preprint 
This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=5031627

![46_image_0.png](46_image_0.png)

